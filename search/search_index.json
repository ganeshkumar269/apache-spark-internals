{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Spark 3.1.2 \u00b6 Welcome to The Internals of Apache Spark online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Spark \ud83d\udd25 Last update: 2021-08-13","title":"Home"},{"location":"#the-internals-of-apache-spark-312","text":"Welcome to The Internals of Apache Spark online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Spark \ud83d\udd25 Last update: 2021-08-13","title":"The Internals of Apache Spark 3.1.2"},{"location":"AsyncEventQueue/","text":"AsyncEventQueue \u00b6 AsyncEventQueue is...FIXME","title":"AsyncEventQueue"},{"location":"AsyncEventQueue/#asynceventqueue","text":"AsyncEventQueue is...FIXME","title":"AsyncEventQueue"},{"location":"Broadcast/","text":"= Broadcast Variable From http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables[the official documentation about Broadcast Variables]: Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. And later in the document: Explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align=\"center\"] To use a broadcast value in a Spark transformation you have to create it first using SparkContext.md#broadcast[SparkContext.broadcast] and then use value method to access the shared value. Learn it in < > section. The Broadcast feature in Spark uses SparkContext to create broadcast values and core:BroadcastManager.md[] and core:ContextCleaner.md[ContextCleaner] to manage their lifecycle. .SparkContext to broadcast using BroadcastManager and ContextCleaner image::sparkcontext-broadcastmanager-contextcleaner.png[align=\"center\"] TIP: Not only can Spark developers use broadcast variables for efficient data distribution, but Spark itself uses them quite often. A very notable use case is when scheduler:DAGScheduler.md#submitMissingTasks[Spark distributes tasks to executors for their execution]. That does change my perspective on the role of broadcast variables in Spark. The idea is to transfer values used in transformations from a driver to executors in a most effective way so they are copied once and used many times by tasks (rather than being copied every time a task is launched). == [[developer-contract]] Broadcast Spark Developer-Facing Contract The developer-facing Broadcast contract allows Spark developers to use it in their applications. .Broadcast API [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Method Name | Description | id | The unique identifier | < > | The value | < > | Asynchronously deletes cached copies of this broadcast on the executors. | < > | Destroys all data and metadata related to this broadcast variable. | toString | The string representation |=== == [[lifecycle]] Lifecycle of Broadcast Variable You can create a broadcast variable of type T using SparkContext.md#broadcast[SparkContext.broadcast] method. scala> val b = sc.broadcast(1) b: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0) [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.storage.BlockManager logger to debug broadcast method. Read storage:BlockManager.md[BlockManager] to find out how to enable the logging level. \u00b6 With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Put block broadcast_0 locally took 430 ms DEBUG BlockManager: Putting block broadcast_0 without replication took 431 ms DEBUG BlockManager: Told master about block broadcast_0_piece0 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 4 ms DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 4 ms After creating an instance of a broadcast variable, you can then reference the value using < > method. [source, scala] \u00b6 scala> b.value res0: Int = 1 NOTE: value method is the only way to access the value of a broadcast variable. With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Getting local block broadcast_0 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas) When you are done with a broadcast variable, you should < > it to release memory. [source, scala] \u00b6 scala> b.destroy \u00b6 With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Removing broadcast 0 DEBUG BlockManager: Removing block broadcast_0_piece0 DEBUG BlockManager: Told master about block broadcast_0_piece0 DEBUG BlockManager: Removing block broadcast_0 Before < > a broadcast variable, you may want to < > it. [source, scala] \u00b6 scala> b.unpersist \u00b6 == [[value]] Getting the Value of Broadcast Variable -- value Method [source, scala] \u00b6 value: T \u00b6 value returns the value of a broadcast variable. You can only access the value until it is < > after which you will see the following SparkException exception in the logs: org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:69) ... 48 elided Internally, value makes sure that the broadcast variable is valid , i.e. < > was not called, and, if so, calls the abstract getValue method. [NOTE] \u00b6 getValue is abstracted and broadcast variable implementations are supposed to provide a concrete behaviour. Refer to core:TorrentBroadcast.md#getValue[TorrentBroadcast]. \u00b6 == [[unpersist]] Unpersisting Broadcast Variable -- unpersist Methods [source, scala] \u00b6 unpersist(): Unit unpersist(blocking: Boolean): Unit == [[destroy]] Destroying Broadcast Variable -- destroy Method [source, scala] \u00b6 destroy(): Unit \u00b6 destroy removes a broadcast variable. NOTE: Once a broadcast variable has been destroyed, it cannot be used again. If you try to destroy a broadcast variable more than once, you will see the following SparkException exception in the logs: scala> b.destroy org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:107) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:98) ... 48 elided Internally, destroy executes the internal < > (with blocking enabled). == [[destroy-internal]] Removing Persisted Data of Broadcast Variable -- destroy Internal Method [source, scala] \u00b6 destroy(blocking: Boolean): Unit \u00b6 destroy destroys all data and metadata of a broadcast variable. NOTE: destroy is a private[spark] method. Internally, destroy marks a broadcast variable destroyed, i.e. the internal _isValid flag is disabled. You should see the following INFO message in the logs: INFO TorrentBroadcast: Destroying Broadcast([id]) (from [destroySite]) In the end, doDestroy method is executed (that broadcast implementations are supposed to provide). NOTE: doDestroy is part of the < > for broadcast implementations so they can provide their own custom behaviour. == [[introductory-example]] Introductory Example Let's start with an introductory example to check out how to use broadcast variables and build your initial understanding. You're going to use a static mapping of interesting projects with their websites, i.e. Map[String, String] that the tasks, i.e. closures (anonymous functions) in transformations, use. scala> val pws = Map(\"Apache Spark\" -> \"http://spark.apache.org/\", \"Scala\" -> \"http://www.scala-lang.org/\") pws: scala.collection.immutable.Map[String,String] = Map(Apache Spark -> http://spark.apache.org/, Scala -> http://www.scala-lang.org/) scala> val websites = sc.parallelize(Seq(\"Apache Spark\", \"Scala\")).map(pws).collect ... websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/) It works, but is very ineffective as the pws map is sent over the wire to executors while it could have been there already. If there were more tasks that need the pws map, you could improve their performance by minimizing the number of bytes that are going to be sent over the network for task execution. Enter broadcast variables. val pwsB = sc.broadcast(pws) val websites = sc.parallelize(Seq(\"Apache Spark\", \"Scala\")).map(pwsB.value).collect // websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/) Semantically, the two computations - with and without the broadcast value - are exactly the same, but the broadcast-based one wins performance-wise when there are more executors spawned to execute many tasks that use pws map. == [[introduction]] Introduction Broadcast is part of Spark that is responsible for broadcasting information across nodes in a cluster. You use broadcast variable to implement map-side join , i.e. a join using a map . For this, lookup tables are distributed across nodes in a cluster using broadcast and then looked up inside map (to do the join implicitly). When you broadcast a value, it is copied to executors only once (while it is copied multiple times for tasks otherwise). It means that broadcast can help to get your Spark application faster if you have a large value to use in tasks or there are more tasks than executors. It appears that a Spark idiom emerges that uses broadcast with collectAsMap to create a Map for broadcast. When an RDD is map over to a smaller dataset (column-wise not record-wise), collectAsMap , and broadcast , using the very big RDD to map its elements to the broadcast RDDs is computationally faster. [source, scala] \u00b6 val acMap = sc.broadcast(myRDD.map { case (a,b,c,b) => (a, c) }.collectAsMap) val otherMap = sc.broadcast(myOtherRDD.collectAsMap) myBigRDD.map { case (a, b, c, d) => (acMap.value.get(a).get, otherMap.value.get\u00a9.get) }.collect Use large broadcasted HashMaps over RDDs whenever possible and leave RDDs with a key to lookup necessary data as demonstrated above. Spark comes with a BitTorrent implementation. It is not enabled by default. == [[contract]] Broadcast Contract The Broadcast contract is made up of the following methods that custom Broadcast implementations are supposed to provide: getValue doUnpersist doDestroy NOTE: core:TorrentBroadcast.md[TorrentBroadcast] is the only implementation of the Broadcast contract. NOTE: < Broadcast Spark Developer-Facing Contract>> is the developer-facing Broadcast contract that allows Spark developers to use it in their applications. == [[i-want-more]] Further Reading or Watching http://dmtolpeko.com/2015/02/20/map-side-join-in-spark/[Map-Side Join in Spark]","title":"Broadcast"},{"location":"Broadcast/#tip","text":"Enable DEBUG logging level for org.apache.spark.storage.BlockManager logger to debug broadcast method.","title":"[TIP]"},{"location":"Broadcast/#read-storageblockmanagermdblockmanager-to-find-out-how-to-enable-the-logging-level","text":"With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Put block broadcast_0 locally took 430 ms DEBUG BlockManager: Putting block broadcast_0 without replication took 431 ms DEBUG BlockManager: Told master about block broadcast_0_piece0 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 4 ms DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 4 ms After creating an instance of a broadcast variable, you can then reference the value using < > method.","title":"Read storage:BlockManager.md[BlockManager] to find out how to enable the logging level."},{"location":"Broadcast/#source-scala","text":"scala> b.value res0: Int = 1 NOTE: value method is the only way to access the value of a broadcast variable. With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Getting local block broadcast_0 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas) When you are done with a broadcast variable, you should < > it to release memory.","title":"[source, scala]"},{"location":"Broadcast/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Broadcast/#scala-bdestroy","text":"With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Removing broadcast 0 DEBUG BlockManager: Removing block broadcast_0_piece0 DEBUG BlockManager: Told master about block broadcast_0_piece0 DEBUG BlockManager: Removing block broadcast_0 Before < > a broadcast variable, you may want to < > it.","title":"scala&gt; b.destroy"},{"location":"Broadcast/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Broadcast/#scala-bunpersist","text":"== [[value]] Getting the Value of Broadcast Variable -- value Method","title":"scala&gt; b.unpersist"},{"location":"Broadcast/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Broadcast/#value-t","text":"value returns the value of a broadcast variable. You can only access the value until it is < > after which you will see the following SparkException exception in the logs: org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:69) ... 48 elided Internally, value makes sure that the broadcast variable is valid , i.e. < > was not called, and, if so, calls the abstract getValue method.","title":"value: T"},{"location":"Broadcast/#note","text":"getValue is abstracted and broadcast variable implementations are supposed to provide a concrete behaviour.","title":"[NOTE]"},{"location":"Broadcast/#refer-to-coretorrentbroadcastmdgetvaluetorrentbroadcast","text":"== [[unpersist]] Unpersisting Broadcast Variable -- unpersist Methods","title":"Refer to core:TorrentBroadcast.md#getValue[TorrentBroadcast]."},{"location":"Broadcast/#source-scala_4","text":"unpersist(): Unit unpersist(blocking: Boolean): Unit == [[destroy]] Destroying Broadcast Variable -- destroy Method","title":"[source, scala]"},{"location":"Broadcast/#source-scala_5","text":"","title":"[source, scala]"},{"location":"Broadcast/#destroy-unit","text":"destroy removes a broadcast variable. NOTE: Once a broadcast variable has been destroyed, it cannot be used again. If you try to destroy a broadcast variable more than once, you will see the following SparkException exception in the logs: scala> b.destroy org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:107) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:98) ... 48 elided Internally, destroy executes the internal < > (with blocking enabled). == [[destroy-internal]] Removing Persisted Data of Broadcast Variable -- destroy Internal Method","title":"destroy(): Unit"},{"location":"Broadcast/#source-scala_6","text":"","title":"[source, scala]"},{"location":"Broadcast/#destroyblocking-boolean-unit","text":"destroy destroys all data and metadata of a broadcast variable. NOTE: destroy is a private[spark] method. Internally, destroy marks a broadcast variable destroyed, i.e. the internal _isValid flag is disabled. You should see the following INFO message in the logs: INFO TorrentBroadcast: Destroying Broadcast([id]) (from [destroySite]) In the end, doDestroy method is executed (that broadcast implementations are supposed to provide). NOTE: doDestroy is part of the < > for broadcast implementations so they can provide their own custom behaviour. == [[introductory-example]] Introductory Example Let's start with an introductory example to check out how to use broadcast variables and build your initial understanding. You're going to use a static mapping of interesting projects with their websites, i.e. Map[String, String] that the tasks, i.e. closures (anonymous functions) in transformations, use. scala> val pws = Map(\"Apache Spark\" -> \"http://spark.apache.org/\", \"Scala\" -> \"http://www.scala-lang.org/\") pws: scala.collection.immutable.Map[String,String] = Map(Apache Spark -> http://spark.apache.org/, Scala -> http://www.scala-lang.org/) scala> val websites = sc.parallelize(Seq(\"Apache Spark\", \"Scala\")).map(pws).collect ... websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/) It works, but is very ineffective as the pws map is sent over the wire to executors while it could have been there already. If there were more tasks that need the pws map, you could improve their performance by minimizing the number of bytes that are going to be sent over the network for task execution. Enter broadcast variables. val pwsB = sc.broadcast(pws) val websites = sc.parallelize(Seq(\"Apache Spark\", \"Scala\")).map(pwsB.value).collect // websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/) Semantically, the two computations - with and without the broadcast value - are exactly the same, but the broadcast-based one wins performance-wise when there are more executors spawned to execute many tasks that use pws map. == [[introduction]] Introduction Broadcast is part of Spark that is responsible for broadcasting information across nodes in a cluster. You use broadcast variable to implement map-side join , i.e. a join using a map . For this, lookup tables are distributed across nodes in a cluster using broadcast and then looked up inside map (to do the join implicitly). When you broadcast a value, it is copied to executors only once (while it is copied multiple times for tasks otherwise). It means that broadcast can help to get your Spark application faster if you have a large value to use in tasks or there are more tasks than executors. It appears that a Spark idiom emerges that uses broadcast with collectAsMap to create a Map for broadcast. When an RDD is map over to a smaller dataset (column-wise not record-wise), collectAsMap , and broadcast , using the very big RDD to map its elements to the broadcast RDDs is computationally faster.","title":"destroy(blocking: Boolean): Unit"},{"location":"Broadcast/#source-scala_7","text":"val acMap = sc.broadcast(myRDD.map { case (a,b,c,b) => (a, c) }.collectAsMap) val otherMap = sc.broadcast(myOtherRDD.collectAsMap) myBigRDD.map { case (a, b, c, d) => (acMap.value.get(a).get, otherMap.value.get\u00a9.get) }.collect Use large broadcasted HashMaps over RDDs whenever possible and leave RDDs with a key to lookup necessary data as demonstrated above. Spark comes with a BitTorrent implementation. It is not enabled by default. == [[contract]] Broadcast Contract The Broadcast contract is made up of the following methods that custom Broadcast implementations are supposed to provide: getValue doUnpersist doDestroy NOTE: core:TorrentBroadcast.md[TorrentBroadcast] is the only implementation of the Broadcast contract. NOTE: < Broadcast Spark Developer-Facing Contract>> is the developer-facing Broadcast contract that allows Spark developers to use it in their applications. == [[i-want-more]] Further Reading or Watching http://dmtolpeko.com/2015/02/20/map-side-join-in-spark/[Map-Side Join in Spark]","title":"[source, scala]"},{"location":"CompressionCodec/","text":"CompressionCodec \u00b6 CompressionCodec is an abstraction of < >. A concrete CompressionCodec is supposed to < >. The default compression codec is configured using configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property. == [[implementations]][[shortCompressionCodecNames]] Available CompressionCodecs [cols=\"30,10m,60\",options=\"header\",width=\"100%\"] |=== | CompressionCodec | Alias | Description | LZ4CompressionCodec | lz4 a| [[LZ4CompressionCodec]] https://github.com/lz4/lz4-java[LZ4 compression] The default compression codec based on configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property Uses configuration-properties.md#spark.io.compression.lz4.blockSize[spark.io.compression.lz4.blockSize] configuration property for the block size | LZFCompressionCodec | lzf | [[LZFCompressionCodec]] https://github.com/ning/compress[LZF compression] | SnappyCompressionCodec | snappy a| [[SnappyCompressionCodec]] https://google.github.io/snappy/[Snappy compression] Uses configuration-properties.md#spark.io.compression.snappy.blockSize[spark.io.compression.snappy.blockSize] configuration property for the block size | ZStdCompressionCodec | zstd a| [[ZStdCompressionCodec]] https://facebook.github.io/zstd/[ZStandard compression] configuration-properties.md#spark.io.compression.zstd.bufferSize[spark.io.compression.zstd.bufferSize] for the buffer size configuration-properties.md#spark.io.compression.zstd.level[spark.io.compression.zstd.level] for the compression level |=== == [[compressedOutputStream]] Compressing Output Stream [source,scala] \u00b6 compressedOutputStream( s: OutputStream): OutputStream compressedOutputStream is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#blockifyObject[blockifyObject] ReliableCheckpointRDD is requested to writePartitionToCheckpointFile EventLoggingListener is requested to spark-history-server:EventLoggingListener.md#start[start] GenericAvroSerializer is requested to compress a schema SerializerManager is requested to serializer:SerializerManager.md#wrapForCompression[wrap an output stream of a block for compression] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpillsWithFileStream[mergeSpillsWithFileStream] == [[compressedInputStream]] Compressing Input Stream [source,scala] \u00b6 compressedInputStream( s: InputStream): InputStream compressedInputStream is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#unBlockifyObject[unBlockifyObject] ReliableCheckpointRDD is requested to readCheckpointFile EventLoggingListener is requested to spark-history-server:EventLoggingListener.md#openEventLog[openEventLog] GenericAvroSerializer is requested to decompress a schema SerializerManager is requested to serializer:SerializerManager.md#wrapForCompression[wrap an input stream of a block for compression] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpillsWithFileStream[mergeSpillsWithFileStream] == [[createCodec]] Creating CompressionCodec [source, scala] \u00b6 createCodec( conf: SparkConf): CompressionCodec createCodec( conf: SparkConf, codecName: String): CompressionCodec createCodec creates an instance of the compression codec by the given name (using a constructor that accepts a SparkConf.md[SparkConf]). createCodec uses < > utility to find the codec name unless specified explicitly. createCodec finds the class name in the < > internal lookup table or assumes that the codec name is already a fully-qualified class name. createCodec throws an IllegalArgumentException exception if a compression codec could not be found: [source,plaintext] \u00b6 Codec [codecName] is not available. Consider setting spark.io.compression.codec=snappy \u00b6 createCodec is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#setConf[setConf] ReliableCheckpointRDD is requested to writePartitionToCheckpointFile and readCheckpointFile spark-history-server:EventLoggingListener.md[EventLoggingListener] is created and requested to spark-history-server:EventLoggingListener.md#openEventLog[openEventLog] GenericAvroSerializer is created SerializerManager is created UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spills] Finding Compression Codec Name \u00b6 getCodecName ( conf : SparkConf ): String getCodecName takes the name of a compression codec based on configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property (using the SparkConf.md[SparkConf]) if available or defaults to lz4 . getCodecName is used when: SparkContext is created CompressionCodec utility is used to creating a CompressionCodec == [[supportsConcatenationOfSerializedStreams]] supportsConcatenationOfSerializedStreams Method [source, scala] \u00b6 supportsConcatenationOfSerializedStreams( codec: CompressionCodec): Boolean supportsConcatenationOfSerializedStreams returns true when the given CompressionCodec is one of the < >. supportsConcatenationOfSerializedStreams is used when UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spills].","title":"CompressionCodec"},{"location":"CompressionCodec/#compressioncodec","text":"CompressionCodec is an abstraction of < >. A concrete CompressionCodec is supposed to < >. The default compression codec is configured using configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property. == [[implementations]][[shortCompressionCodecNames]] Available CompressionCodecs [cols=\"30,10m,60\",options=\"header\",width=\"100%\"] |=== | CompressionCodec | Alias | Description | LZ4CompressionCodec | lz4 a| [[LZ4CompressionCodec]] https://github.com/lz4/lz4-java[LZ4 compression] The default compression codec based on configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property Uses configuration-properties.md#spark.io.compression.lz4.blockSize[spark.io.compression.lz4.blockSize] configuration property for the block size | LZFCompressionCodec | lzf | [[LZFCompressionCodec]] https://github.com/ning/compress[LZF compression] | SnappyCompressionCodec | snappy a| [[SnappyCompressionCodec]] https://google.github.io/snappy/[Snappy compression] Uses configuration-properties.md#spark.io.compression.snappy.blockSize[spark.io.compression.snappy.blockSize] configuration property for the block size | ZStdCompressionCodec | zstd a| [[ZStdCompressionCodec]] https://facebook.github.io/zstd/[ZStandard compression] configuration-properties.md#spark.io.compression.zstd.bufferSize[spark.io.compression.zstd.bufferSize] for the buffer size configuration-properties.md#spark.io.compression.zstd.level[spark.io.compression.zstd.level] for the compression level |=== == [[compressedOutputStream]] Compressing Output Stream","title":"CompressionCodec"},{"location":"CompressionCodec/#sourcescala","text":"compressedOutputStream( s: OutputStream): OutputStream compressedOutputStream is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#blockifyObject[blockifyObject] ReliableCheckpointRDD is requested to writePartitionToCheckpointFile EventLoggingListener is requested to spark-history-server:EventLoggingListener.md#start[start] GenericAvroSerializer is requested to compress a schema SerializerManager is requested to serializer:SerializerManager.md#wrapForCompression[wrap an output stream of a block for compression] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpillsWithFileStream[mergeSpillsWithFileStream] == [[compressedInputStream]] Compressing Input Stream","title":"[source,scala]"},{"location":"CompressionCodec/#sourcescala_1","text":"compressedInputStream( s: InputStream): InputStream compressedInputStream is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#unBlockifyObject[unBlockifyObject] ReliableCheckpointRDD is requested to readCheckpointFile EventLoggingListener is requested to spark-history-server:EventLoggingListener.md#openEventLog[openEventLog] GenericAvroSerializer is requested to decompress a schema SerializerManager is requested to serializer:SerializerManager.md#wrapForCompression[wrap an input stream of a block for compression] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpillsWithFileStream[mergeSpillsWithFileStream] == [[createCodec]] Creating CompressionCodec","title":"[source,scala]"},{"location":"CompressionCodec/#source-scala","text":"createCodec( conf: SparkConf): CompressionCodec createCodec( conf: SparkConf, codecName: String): CompressionCodec createCodec creates an instance of the compression codec by the given name (using a constructor that accepts a SparkConf.md[SparkConf]). createCodec uses < > utility to find the codec name unless specified explicitly. createCodec finds the class name in the < > internal lookup table or assumes that the codec name is already a fully-qualified class name. createCodec throws an IllegalArgumentException exception if a compression codec could not be found:","title":"[source, scala]"},{"location":"CompressionCodec/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"CompressionCodec/#codec-codecname-is-not-available-consider-setting-sparkiocompressioncodecsnappy","text":"createCodec is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#setConf[setConf] ReliableCheckpointRDD is requested to writePartitionToCheckpointFile and readCheckpointFile spark-history-server:EventLoggingListener.md[EventLoggingListener] is created and requested to spark-history-server:EventLoggingListener.md#openEventLog[openEventLog] GenericAvroSerializer is created SerializerManager is created UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spills]","title":"Codec [codecName] is not available. Consider setting spark.io.compression.codec=snappy"},{"location":"CompressionCodec/#finding-compression-codec-name","text":"getCodecName ( conf : SparkConf ): String getCodecName takes the name of a compression codec based on configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property (using the SparkConf.md[SparkConf]) if available or defaults to lz4 . getCodecName is used when: SparkContext is created CompressionCodec utility is used to creating a CompressionCodec == [[supportsConcatenationOfSerializedStreams]] supportsConcatenationOfSerializedStreams Method","title":" Finding Compression Codec Name"},{"location":"CompressionCodec/#source-scala_1","text":"supportsConcatenationOfSerializedStreams( codec: CompressionCodec): Boolean supportsConcatenationOfSerializedStreams returns true when the given CompressionCodec is one of the < >. supportsConcatenationOfSerializedStreams is used when UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spills].","title":"[source, scala]"},{"location":"ConsoleProgressBar/","text":"ConsoleProgressBar \u00b6 ConsoleProgressBar shows the progress of active stages to standard error, i.e. stderr . It uses SparkStatusTracker to poll the status of stages periodically and print out active stages with more than one task. It keeps overwriting itself to hold in one line for at most 3 first concurrent stages at a time. [Stage 0:====> (316 + 4) / 1000][Stage 1:> (0 + 0) / 1000][Stage 2:> (0 + 0) / 1000]]] The progress includes the stage id, the number of completed, active, and total tasks. TIP: ConsoleProgressBar may be useful when you ssh to workers and want to see the progress of active stages. < ConsoleProgressBar is created>> when SparkContext is created with spark.ui.showConsoleProgress enabled and the logging level of SparkContext.md[org.apache.spark.SparkContext] logger as WARN or higher (i.e. less messages are printed out and so there is a \"space\" for ConsoleProgressBar ). [source, scala] \u00b6 import org.apache.log4j._ Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN) To print the progress nicely ConsoleProgressBar uses COLUMNS environment variable to know the width of the terminal. It assumes 80 columns. The progress bar prints out the status after a stage has ran at least 500 milliseconds every spark-webui-properties.md#spark.ui.consoleProgress.update.interval[spark.ui.consoleProgress.update.interval] milliseconds. NOTE: The initial delay of 500 milliseconds before ConsoleProgressBar show the progress is not configurable. See the progress bar in Spark shell with the following: [source] \u00b6 $ ./bin/spark-shell --conf spark.ui.showConsoleProgress=true # <1> scala> sc.setLogLevel(\"OFF\") // <2> import org.apache.log4j._ scala> Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN) // <3> scala> sc.parallelize(1 to 4, 4).map { n => Thread.sleep(500 + 200 * n); n }.count // <4> [Stage 2:> (0 + 4) / 4] [Stage 2:==============> (1 + 3) / 4] [Stage 2:=============================> (2 + 2) / 4] [Stage 2:============================================> (3 + 1) / 4] <1> Make sure spark.ui.showConsoleProgress is true . It is by default. <2> Disable ( OFF ) the root logger (that includes Spark's logger) <3> Make sure org.apache.spark.SparkContext logger is at least WARN . <4> Run a job with 4 tasks with 500ms initial sleep and 200ms sleep chunks to see the progress bar. TIP: https://youtu.be/uEmcGo8rwek[Watch the short video] that show ConsoleProgressBar in action. You may want to use the following example to see the progress bar in full glory - all 3 concurrent stages in console (borrowed from https://github.com/apache/spark/pull/3029#issuecomment-63244719[a comment to [SPARK-4017] show progress bar in console #3029]): > ./bin/spark-shell scala> val a = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _) scala> val b = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _) scala> a.union(b).count() === [[creating-instance]] Creating ConsoleProgressBar Instance ConsoleProgressBar requires a SparkContext.md[SparkContext]. When being created, ConsoleProgressBar reads spark-webui-properties.md#spark.ui.consoleProgress.update.interval[spark.ui.consoleProgress.update.interval] configuration property to set up the update interval and COLUMNS environment variable for the terminal width (or assumes 80 columns). ConsoleProgressBar starts the internal timer refresh progress that does < > and shows progress. NOTE: ConsoleProgressBar is created when SparkContext is created, spark.ui.showConsoleProgress configuration property is enabled, and the logging level of SparkContext.md[org.apache.spark.SparkContext] logger is WARN or higher (i.e. less messages are printed out and so there is a \"space\" for ConsoleProgressBar ). NOTE: Once created, ConsoleProgressBar is available internally as _progressBar . === [[finishAll]] finishAll Method CAUTION: FIXME === [[stop]] stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop cancels (stops) the internal timer. NOTE: stop is executed when SparkContext.md#stop[ SparkContext stops]. === [[refresh]] refresh Internal Method [source, scala] \u00b6 refresh(): Unit \u00b6 refresh ...FIXME NOTE: refresh is used when...FIXME","title":"ConsoleProgressBar"},{"location":"ConsoleProgressBar/#consoleprogressbar","text":"ConsoleProgressBar shows the progress of active stages to standard error, i.e. stderr . It uses SparkStatusTracker to poll the status of stages periodically and print out active stages with more than one task. It keeps overwriting itself to hold in one line for at most 3 first concurrent stages at a time. [Stage 0:====> (316 + 4) / 1000][Stage 1:> (0 + 0) / 1000][Stage 2:> (0 + 0) / 1000]]] The progress includes the stage id, the number of completed, active, and total tasks. TIP: ConsoleProgressBar may be useful when you ssh to workers and want to see the progress of active stages. < ConsoleProgressBar is created>> when SparkContext is created with spark.ui.showConsoleProgress enabled and the logging level of SparkContext.md[org.apache.spark.SparkContext] logger as WARN or higher (i.e. less messages are printed out and so there is a \"space\" for ConsoleProgressBar ).","title":"ConsoleProgressBar"},{"location":"ConsoleProgressBar/#source-scala","text":"import org.apache.log4j._ Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN) To print the progress nicely ConsoleProgressBar uses COLUMNS environment variable to know the width of the terminal. It assumes 80 columns. The progress bar prints out the status after a stage has ran at least 500 milliseconds every spark-webui-properties.md#spark.ui.consoleProgress.update.interval[spark.ui.consoleProgress.update.interval] milliseconds. NOTE: The initial delay of 500 milliseconds before ConsoleProgressBar show the progress is not configurable. See the progress bar in Spark shell with the following:","title":"[source, scala]"},{"location":"ConsoleProgressBar/#source","text":"$ ./bin/spark-shell --conf spark.ui.showConsoleProgress=true # <1> scala> sc.setLogLevel(\"OFF\") // <2> import org.apache.log4j._ scala> Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN) // <3> scala> sc.parallelize(1 to 4, 4).map { n => Thread.sleep(500 + 200 * n); n }.count // <4> [Stage 2:> (0 + 4) / 4] [Stage 2:==============> (1 + 3) / 4] [Stage 2:=============================> (2 + 2) / 4] [Stage 2:============================================> (3 + 1) / 4] <1> Make sure spark.ui.showConsoleProgress is true . It is by default. <2> Disable ( OFF ) the root logger (that includes Spark's logger) <3> Make sure org.apache.spark.SparkContext logger is at least WARN . <4> Run a job with 4 tasks with 500ms initial sleep and 200ms sleep chunks to see the progress bar. TIP: https://youtu.be/uEmcGo8rwek[Watch the short video] that show ConsoleProgressBar in action. You may want to use the following example to see the progress bar in full glory - all 3 concurrent stages in console (borrowed from https://github.com/apache/spark/pull/3029#issuecomment-63244719[a comment to [SPARK-4017] show progress bar in console #3029]): > ./bin/spark-shell scala> val a = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _) scala> val b = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _) scala> a.union(b).count() === [[creating-instance]] Creating ConsoleProgressBar Instance ConsoleProgressBar requires a SparkContext.md[SparkContext]. When being created, ConsoleProgressBar reads spark-webui-properties.md#spark.ui.consoleProgress.update.interval[spark.ui.consoleProgress.update.interval] configuration property to set up the update interval and COLUMNS environment variable for the terminal width (or assumes 80 columns). ConsoleProgressBar starts the internal timer refresh progress that does < > and shows progress. NOTE: ConsoleProgressBar is created when SparkContext is created, spark.ui.showConsoleProgress configuration property is enabled, and the logging level of SparkContext.md[org.apache.spark.SparkContext] logger is WARN or higher (i.e. less messages are printed out and so there is a \"space\" for ConsoleProgressBar ). NOTE: Once created, ConsoleProgressBar is available internally as _progressBar . === [[finishAll]] finishAll Method CAUTION: FIXME === [[stop]] stop Method","title":"[source]"},{"location":"ConsoleProgressBar/#source-scala_1","text":"","title":"[source, scala]"},{"location":"ConsoleProgressBar/#stop-unit","text":"stop cancels (stops) the internal timer. NOTE: stop is executed when SparkContext.md#stop[ SparkContext stops]. === [[refresh]] refresh Internal Method","title":"stop(): Unit"},{"location":"ConsoleProgressBar/#source-scala_2","text":"","title":"[source, scala]"},{"location":"ConsoleProgressBar/#refresh-unit","text":"refresh ...FIXME NOTE: refresh is used when...FIXME","title":"refresh(): Unit"},{"location":"FileCommitProtocol/","text":"FileCommitProtocol \u00b6 FileCommitProtocol is an abstraction of file committers that can setup, commit or abort a Spark job or task (while writing out a pair RDD and partitions). FileCommitProtocol is used for RDD.saveAsNewAPIHadoopDataset and RDD.saveAsHadoopDataset transformations (that use SparkHadoopWriter utility to write a key-value RDD out ). FileCommitProtocol is created using FileCommitProtocol.instantiate utility. Contract \u00b6 Aborting Job \u00b6 abortJob ( jobContext : JobContext ): Unit Aborts a job Used when: SparkHadoopWriter utility is used to write a key-value RDD (and writing fails) (Spark SQL) FileFormatWriter utility is used to write a result of a structured query (and writing fails) (Spark SQL) FileBatchWrite is requested to abort Aborting Task \u00b6 abortTask ( taskContext : TaskAttemptContext ): Unit Abort a task Used when: SparkHadoopWriter utility is used to write an RDD partition (Spark SQL) FileFormatDataWriter is requested to abort Committing Job \u00b6 commitJob ( jobContext : JobContext , taskCommits : Seq [ TaskCommitMessage ]): Unit Commits a job after the writes succeed Used when: SparkHadoopWriter utility is used to write a key-value RDD (Spark SQL) FileFormatWriter utility is used to write a result of a structured query (Spark SQL) FileBatchWrite is requested to commit Committing Task \u00b6 commitTask ( taskContext : TaskAttemptContext ): TaskCommitMessage Used when: SparkHadoopWriter utility is used to write an RDD partition (Spark SQL) FileFormatDataWriter is requested to commit Deleting Path with Job \u00b6 deleteWithJob ( fs : FileSystem , path : Path , recursive : Boolean ): Boolean deleteWithJob requests the given Hadoop FileSystem to delete a path directory. Used when InsertIntoHadoopFsRelationCommand logical command (Spark SQL) is executed newTaskTempFile \u00b6 newTaskTempFile ( taskContext : TaskAttemptContext , dir : Option [ String ], ext : String ): String Used when: (Spark SQL) SingleDirectoryDataWriter and DynamicPartitionDataWriter are requested to write (and in turn newOutputWriter ) newTaskTempFileAbsPath \u00b6 newTaskTempFileAbsPath ( taskContext : TaskAttemptContext , absoluteDir : String , ext : String ): String Used when: (Spark SQL) DynamicPartitionDataWriter is requested to write On Task Committed \u00b6 onTaskCommit ( taskCommit : TaskCommitMessage ): Unit Used when: (Spark SQL) FileFormatWriter is requested to write Setting Up Job \u00b6 setupJob ( jobContext : JobContext ): Unit Used when: SparkHadoopWriter utility is used to write an RDD partition (while writing out a key-value RDD ) (Spark SQL) FileFormatWriter utility is used to write a result of a structured query (Spark SQL) FileWriteBuilder is requested to buildForBatch Setting Up Task \u00b6 setupTask ( taskContext : TaskAttemptContext ): Unit Sets up the task with the Hadoop TaskAttemptContext Used when: SparkHadoopWriter is requested to write an RDD partition (while writing out a key-value RDD ) (Spark SQL) FileFormatWriter utility is used to write out a RDD partition (while writing out a result of a structured query) (Spark SQL) FileWriterFactory is requested to createWriter Implementations \u00b6 HadoopMapReduceCommitProtocol ManifestFileCommitProtocol (qv. Spark Structured Streaming ) Instantiating FileCommitProtocol Committer \u00b6 instantiate ( className : String , jobId : String , outputPath : String , dynamicPartitionOverwrite : Boolean = false ): FileCommitProtocol instantiate prints out the following DEBUG message to the logs: Creating committer [className]; job [jobId]; output=[outputPath]; dynamic=[dynamicPartitionOverwrite] instantiate tries to find a constructor method that takes three arguments (two of type String and one Boolean ) for the given jobId , outputPath and dynamicPartitionOverwrite flag. If found, instantiate prints out the following DEBUG message to the logs: Using (String, String, Boolean) constructor In case of NoSuchMethodException , instantiate prints out the following DEBUG message to the logs: Falling back to (String, String) constructor instantiate tries to find a constructor method that takes two arguments (two of type String ) for the given jobId and outputPath . With two String arguments, instantiate requires that the given dynamicPartitionOverwrite flag is disabled ( false ) or throws an IllegalArgumentException : requirement failed: Dynamic Partition Overwrite is enabled but the committer [className] does not have the appropriate constructor instantiate is used when: HadoopMapRedWriteConfigUtil and HadoopMapReduceWriteConfigUtil are requested to create a HadoopMapReduceCommitProtocol committer (Spark SQL) InsertIntoHadoopFsRelationCommand , InsertIntoHiveDirCommand , and InsertIntoHiveTable logical commands are executed ( Spark Structured Streaming ) FileStreamSink is requested to write out a micro-batch data Logging \u00b6 Enable ALL logging level for org.apache.spark.internal.io.FileCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.internal.io.FileCommitProtocol=ALL Refer to Logging .","title":"FileCommitProtocol"},{"location":"FileCommitProtocol/#filecommitprotocol","text":"FileCommitProtocol is an abstraction of file committers that can setup, commit or abort a Spark job or task (while writing out a pair RDD and partitions). FileCommitProtocol is used for RDD.saveAsNewAPIHadoopDataset and RDD.saveAsHadoopDataset transformations (that use SparkHadoopWriter utility to write a key-value RDD out ). FileCommitProtocol is created using FileCommitProtocol.instantiate utility.","title":"FileCommitProtocol"},{"location":"FileCommitProtocol/#contract","text":"","title":"Contract"},{"location":"FileCommitProtocol/#aborting-job","text":"abortJob ( jobContext : JobContext ): Unit Aborts a job Used when: SparkHadoopWriter utility is used to write a key-value RDD (and writing fails) (Spark SQL) FileFormatWriter utility is used to write a result of a structured query (and writing fails) (Spark SQL) FileBatchWrite is requested to abort","title":" Aborting Job"},{"location":"FileCommitProtocol/#aborting-task","text":"abortTask ( taskContext : TaskAttemptContext ): Unit Abort a task Used when: SparkHadoopWriter utility is used to write an RDD partition (Spark SQL) FileFormatDataWriter is requested to abort","title":" Aborting Task"},{"location":"FileCommitProtocol/#committing-job","text":"commitJob ( jobContext : JobContext , taskCommits : Seq [ TaskCommitMessage ]): Unit Commits a job after the writes succeed Used when: SparkHadoopWriter utility is used to write a key-value RDD (Spark SQL) FileFormatWriter utility is used to write a result of a structured query (Spark SQL) FileBatchWrite is requested to commit","title":" Committing Job"},{"location":"FileCommitProtocol/#committing-task","text":"commitTask ( taskContext : TaskAttemptContext ): TaskCommitMessage Used when: SparkHadoopWriter utility is used to write an RDD partition (Spark SQL) FileFormatDataWriter is requested to commit","title":" Committing Task"},{"location":"FileCommitProtocol/#deleting-path-with-job","text":"deleteWithJob ( fs : FileSystem , path : Path , recursive : Boolean ): Boolean deleteWithJob requests the given Hadoop FileSystem to delete a path directory. Used when InsertIntoHadoopFsRelationCommand logical command (Spark SQL) is executed","title":" Deleting Path with Job"},{"location":"FileCommitProtocol/#newtasktempfile","text":"newTaskTempFile ( taskContext : TaskAttemptContext , dir : Option [ String ], ext : String ): String Used when: (Spark SQL) SingleDirectoryDataWriter and DynamicPartitionDataWriter are requested to write (and in turn newOutputWriter )","title":" newTaskTempFile"},{"location":"FileCommitProtocol/#newtasktempfileabspath","text":"newTaskTempFileAbsPath ( taskContext : TaskAttemptContext , absoluteDir : String , ext : String ): String Used when: (Spark SQL) DynamicPartitionDataWriter is requested to write","title":" newTaskTempFileAbsPath"},{"location":"FileCommitProtocol/#on-task-committed","text":"onTaskCommit ( taskCommit : TaskCommitMessage ): Unit Used when: (Spark SQL) FileFormatWriter is requested to write","title":" On Task Committed"},{"location":"FileCommitProtocol/#setting-up-job","text":"setupJob ( jobContext : JobContext ): Unit Used when: SparkHadoopWriter utility is used to write an RDD partition (while writing out a key-value RDD ) (Spark SQL) FileFormatWriter utility is used to write a result of a structured query (Spark SQL) FileWriteBuilder is requested to buildForBatch","title":" Setting Up Job"},{"location":"FileCommitProtocol/#setting-up-task","text":"setupTask ( taskContext : TaskAttemptContext ): Unit Sets up the task with the Hadoop TaskAttemptContext Used when: SparkHadoopWriter is requested to write an RDD partition (while writing out a key-value RDD ) (Spark SQL) FileFormatWriter utility is used to write out a RDD partition (while writing out a result of a structured query) (Spark SQL) FileWriterFactory is requested to createWriter","title":" Setting Up Task"},{"location":"FileCommitProtocol/#implementations","text":"HadoopMapReduceCommitProtocol ManifestFileCommitProtocol (qv. Spark Structured Streaming )","title":"Implementations"},{"location":"FileCommitProtocol/#instantiating-filecommitprotocol-committer","text":"instantiate ( className : String , jobId : String , outputPath : String , dynamicPartitionOverwrite : Boolean = false ): FileCommitProtocol instantiate prints out the following DEBUG message to the logs: Creating committer [className]; job [jobId]; output=[outputPath]; dynamic=[dynamicPartitionOverwrite] instantiate tries to find a constructor method that takes three arguments (two of type String and one Boolean ) for the given jobId , outputPath and dynamicPartitionOverwrite flag. If found, instantiate prints out the following DEBUG message to the logs: Using (String, String, Boolean) constructor In case of NoSuchMethodException , instantiate prints out the following DEBUG message to the logs: Falling back to (String, String) constructor instantiate tries to find a constructor method that takes two arguments (two of type String ) for the given jobId and outputPath . With two String arguments, instantiate requires that the given dynamicPartitionOverwrite flag is disabled ( false ) or throws an IllegalArgumentException : requirement failed: Dynamic Partition Overwrite is enabled but the committer [className] does not have the appropriate constructor instantiate is used when: HadoopMapRedWriteConfigUtil and HadoopMapReduceWriteConfigUtil are requested to create a HadoopMapReduceCommitProtocol committer (Spark SQL) InsertIntoHadoopFsRelationCommand , InsertIntoHiveDirCommand , and InsertIntoHiveTable logical commands are executed ( Spark Structured Streaming ) FileStreamSink is requested to write out a micro-batch data","title":" Instantiating FileCommitProtocol Committer"},{"location":"FileCommitProtocol/#logging","text":"Enable ALL logging level for org.apache.spark.internal.io.FileCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.internal.io.FileCommitProtocol=ALL Refer to Logging .","title":"Logging"},{"location":"HadoopMapRedCommitProtocol/","text":"HadoopMapRedCommitProtocol \u00b6 HadoopMapRedCommitProtocol is...FIXME","title":"HadoopMapRedCommitProtocol"},{"location":"HadoopMapRedCommitProtocol/#hadoopmapredcommitprotocol","text":"HadoopMapRedCommitProtocol is...FIXME","title":"HadoopMapRedCommitProtocol"},{"location":"HadoopMapRedWriteConfigUtil/","text":"HadoopMapRedWriteConfigUtil \u00b6 HadoopMapRedWriteConfigUtil is...FIXME == [[createCommitter]] createCommitter Method [source, scala] \u00b6 createCommitter( jobId: Int): HadoopMapReduceCommitProtocol NOTE: createCommitter is part of the < > contract to...FIXME. createCommitter ...FIXME","title":"HadoopMapRedWriteConfigUtil"},{"location":"HadoopMapRedWriteConfigUtil/#hadoopmapredwriteconfigutil","text":"HadoopMapRedWriteConfigUtil is...FIXME == [[createCommitter]] createCommitter Method","title":"HadoopMapRedWriteConfigUtil"},{"location":"HadoopMapRedWriteConfigUtil/#source-scala","text":"createCommitter( jobId: Int): HadoopMapReduceCommitProtocol NOTE: createCommitter is part of the < > contract to...FIXME. createCommitter ...FIXME","title":"[source, scala]"},{"location":"HadoopMapReduceCommitProtocol/","text":"HadoopMapReduceCommitProtocol \u00b6 HadoopMapReduceCommitProtocol is...FIXME","title":"HadoopMapReduceCommitProtocol"},{"location":"HadoopMapReduceCommitProtocol/#hadoopmapreducecommitprotocol","text":"HadoopMapReduceCommitProtocol is...FIXME","title":"HadoopMapReduceCommitProtocol"},{"location":"HadoopMapReduceWriteConfigUtil/","text":"HadoopMapReduceWriteConfigUtil \u00b6 HadoopMapReduceWriteConfigUtil is...FIXME == [[createCommitter]] createCommitter Method [source, scala] \u00b6 createCommitter( jobId: Int): HadoopMapReduceCommitProtocol NOTE: createCommitter is part of the < > contract to...FIXME. createCommitter ...FIXME","title":"HadoopMapReduceWriteConfigUtil"},{"location":"HadoopMapReduceWriteConfigUtil/#hadoopmapreducewriteconfigutil","text":"HadoopMapReduceWriteConfigUtil is...FIXME == [[createCommitter]] createCommitter Method","title":"HadoopMapReduceWriteConfigUtil"},{"location":"HadoopMapReduceWriteConfigUtil/#source-scala","text":"createCommitter( jobId: Int): HadoopMapReduceCommitProtocol NOTE: createCommitter is part of the < > contract to...FIXME. createCommitter ...FIXME","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/","text":"HadoopWriteConfigUtil \u00b6 HadoopWriteConfigUtil[K, V] is an abstraction of writer configurers for SparkHadoopWriter to write a key-value RDD (for RDD.saveAsNewAPIHadoopDataset and RDD.saveAsHadoopDataset operators). Contract \u00b6 assertConf \u00b6 assertConf ( jobContext : JobContext , conf : SparkConf ): Unit closeWriter \u00b6 closeWriter ( taskContext : TaskAttemptContext ): Unit createCommitter \u00b6 createCommitter ( jobId : Int ): HadoopMapReduceCommitProtocol Creates a HadoopMapReduceCommitProtocol committer Used when: SparkHadoopWriter is requested to write data out createJobContext \u00b6 createJobContext ( jobTrackerId : String , jobId : Int ): JobContext createTaskAttemptContext \u00b6 createTaskAttemptContext ( jobTrackerId : String , jobId : Int , splitId : Int , taskAttemptId : Int ): TaskAttemptContext Creates a Hadoop TaskAttemptContext initOutputFormat \u00b6 initOutputFormat ( jobContext : JobContext ): Unit initWriter \u00b6 initWriter ( taskContext : TaskAttemptContext , splitId : Int ): Unit write \u00b6 write ( pair : ( K , V )): Unit Writes out the key-value pair Used when: SparkHadoopWriter is requested to executeTask Implementations \u00b6 HadoopMapReduceWriteConfigUtil HadoopMapRedWriteConfigUtil","title":"HadoopWriteConfigUtil"},{"location":"HadoopWriteConfigUtil/#hadoopwriteconfigutil","text":"HadoopWriteConfigUtil[K, V] is an abstraction of writer configurers for SparkHadoopWriter to write a key-value RDD (for RDD.saveAsNewAPIHadoopDataset and RDD.saveAsHadoopDataset operators).","title":"HadoopWriteConfigUtil"},{"location":"HadoopWriteConfigUtil/#contract","text":"","title":"Contract"},{"location":"HadoopWriteConfigUtil/#assertconf","text":"assertConf ( jobContext : JobContext , conf : SparkConf ): Unit","title":" assertConf"},{"location":"HadoopWriteConfigUtil/#closewriter","text":"closeWriter ( taskContext : TaskAttemptContext ): Unit","title":" closeWriter"},{"location":"HadoopWriteConfigUtil/#createcommitter","text":"createCommitter ( jobId : Int ): HadoopMapReduceCommitProtocol Creates a HadoopMapReduceCommitProtocol committer Used when: SparkHadoopWriter is requested to write data out","title":" createCommitter"},{"location":"HadoopWriteConfigUtil/#createjobcontext","text":"createJobContext ( jobTrackerId : String , jobId : Int ): JobContext","title":" createJobContext"},{"location":"HadoopWriteConfigUtil/#createtaskattemptcontext","text":"createTaskAttemptContext ( jobTrackerId : String , jobId : Int , splitId : Int , taskAttemptId : Int ): TaskAttemptContext Creates a Hadoop TaskAttemptContext","title":" createTaskAttemptContext"},{"location":"HadoopWriteConfigUtil/#initoutputformat","text":"initOutputFormat ( jobContext : JobContext ): Unit","title":" initOutputFormat"},{"location":"HadoopWriteConfigUtil/#initwriter","text":"initWriter ( taskContext : TaskAttemptContext , splitId : Int ): Unit","title":" initWriter"},{"location":"HadoopWriteConfigUtil/#write","text":"write ( pair : ( K , V )): Unit Writes out the key-value pair Used when: SparkHadoopWriter is requested to executeTask","title":" write"},{"location":"HadoopWriteConfigUtil/#implementations","text":"HadoopMapReduceWriteConfigUtil HadoopMapRedWriteConfigUtil","title":"Implementations"},{"location":"HeartbeatReceiver/","text":"HeartbeatReceiver RPC Endpoint \u00b6 HeartbeatReceiver is a ThreadSafeRpcEndpoint that is registered on the driver as HeartbeatReceiver . HeartbeatReceiver receives Heartbeat messages from executors for accumulator updates (with task metrics and a Spark application's accumulators) and pass them along to TaskScheduler . HeartbeatReceiver is registered immediately after a Spark application is started (i.e. when SparkContext is created). HeartbeatReceiver is a SparkListener to get notified about new executors or executors that are no longer available . Creating Instance \u00b6 HeartbeatReceiver takes the following to be created: SparkContext Clock (default: SystemClock ) HeartbeatReceiver is created when SparkContext is created TaskScheduler \u00b6 HeartbeatReceiver manages a reference to TaskScheduler . RPC Messages \u00b6 ExecutorRemoved \u00b6 Attributes: Executor ID Posted when HeartbeatReceiver is notified that an executor is no longer available When received, HeartbeatReceiver removes the executor (from executorLastSeen internal registry). ExecutorRegistered \u00b6 Attributes: Executor ID Posted when HeartbeatReceiver is notified that a new executor has been registered When received, HeartbeatReceiver registers the executor and the current time (in executorLastSeen internal registry). ExpireDeadHosts \u00b6 No attributes When received, HeartbeatReceiver prints out the following TRACE message to the logs: Checking for hosts with no recent heartbeats in HeartbeatReceiver. Each executor (in executorLastSeen internal registry) is checked whether the time it was last seen is not past spark.network.timeout . For any such executor, HeartbeatReceiver prints out the following WARN message to the logs: Removing executor [executorId] with no recent heartbeats: [time] ms exceeds timeout [timeout] ms HeartbeatReceiver TaskScheduler.executorLost (with SlaveLost(\"Executor heartbeat timed out after [timeout] ms\" ). SparkContext.killAndReplaceExecutor is asynchronously called for the executor (i.e. on killExecutorThread ). The executor is removed from the executorLastSeen internal registry. Heartbeat \u00b6 Attributes: Executor ID AccumulatorV2 updates (by task ID) BlockManagerId ExecutorMetrics peaks (by stage and stage attempt IDs) Posted when Executor informs that it is alive and reports task metrics . When received, HeartbeatReceiver finds the executorId executor (in executorLastSeen internal registry). When the executor is found, HeartbeatReceiver updates the time the heartbeat was received (in executorLastSeen internal registry). HeartbeatReceiver uses the Clock to know the current time. HeartbeatReceiver then submits an asynchronous task to notify TaskScheduler that the heartbeat was received from the executor (using TaskScheduler internal reference). HeartbeatReceiver posts a HeartbeatResponse back to the executor (with the response from TaskScheduler whether the executor has been registered already or not so it may eventually need to re-register). If however the executor was not found (in executorLastSeen internal registry), i.e. the executor was not registered before, you should see the following DEBUG message in the logs and the response is to notify the executor to re-register. Received heartbeat from unknown executor [executorId] In a very rare case, when TaskScheduler is not yet assigned to HeartbeatReceiver , you should see the following WARN message in the logs and the response is to notify the executor to re-register. Dropping [heartbeat] because TaskScheduler is not ready yet TaskSchedulerIsSet \u00b6 No attributes Posted when SparkContext informs that TaskScheduler is available . When received, HeartbeatReceiver sets the internal reference to TaskScheduler . onExecutorAdded \u00b6 onExecutorAdded ( executorAdded : SparkListenerExecutorAdded ): Unit onExecutorAdded sends an ExecutorRegistered message to itself . onExecutorAdded is part of the SparkListener abstraction. addExecutor \u00b6 addExecutor ( executorId : String ): Option [ Future [ Boolean ]] addExecutor ...FIXME onExecutorRemoved \u00b6 onExecutorRemoved ( executorRemoved : SparkListenerExecutorRemoved ): Unit onExecutorRemoved removes the executor . onExecutorRemoved is part of the SparkListener abstraction. removeExecutor \u00b6 removeExecutor ( executorId : String ): Option [ Future [ Boolean ]] removeExecutor ...FIXME Starting HeartbeatReceiver \u00b6 onStart (): Unit onStart sends a blocking ExpireDeadHosts every spark.network.timeoutInterval on eventLoopThread . onStart is part of the RpcEndpoint abstraction. Stopping HeartbeatReceiver \u00b6 onStop (): Unit onStop shuts down the eventLoopThread and killExecutorThread thread pools. onStop is part of the RpcEndpoint abstraction. Handling Two-Way Messages \u00b6 receiveAndReply ( context : RpcCallContext ): PartialFunction [ Any , Unit ] receiveAndReply ...FIXME receiveAndReply is part of the RpcEndpoint abstraction. Thread Pools \u00b6 kill-executor-thread \u00b6 killExecutorThread is a daemon ScheduledThreadPoolExecutor with a single thread. The name of the thread pool is kill-executor-thread . heartbeat-receiver-event-loop-thread \u00b6 eventLoopThread is a daemon ScheduledThreadPoolExecutor with a single thread. The name of the thread pool is heartbeat-receiver-event-loop-thread . Expiring Dead Hosts \u00b6 expireDeadHosts (): Unit expireDeadHosts ...FIXME expireDeadHosts is used when HeartbeatReceiver is requested to receives an ExpireDeadHosts message . Logging \u00b6 Enable ALL logging level for org.apache.spark.HeartbeatReceiver logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.HeartbeatReceiver=ALL Refer to Logging .","title":"HeartbeatReceiver"},{"location":"HeartbeatReceiver/#heartbeatreceiver-rpc-endpoint","text":"HeartbeatReceiver is a ThreadSafeRpcEndpoint that is registered on the driver as HeartbeatReceiver . HeartbeatReceiver receives Heartbeat messages from executors for accumulator updates (with task metrics and a Spark application's accumulators) and pass them along to TaskScheduler . HeartbeatReceiver is registered immediately after a Spark application is started (i.e. when SparkContext is created). HeartbeatReceiver is a SparkListener to get notified about new executors or executors that are no longer available .","title":"HeartbeatReceiver RPC Endpoint"},{"location":"HeartbeatReceiver/#creating-instance","text":"HeartbeatReceiver takes the following to be created: SparkContext Clock (default: SystemClock ) HeartbeatReceiver is created when SparkContext is created","title":"Creating Instance"},{"location":"HeartbeatReceiver/#taskscheduler","text":"HeartbeatReceiver manages a reference to TaskScheduler .","title":" TaskScheduler"},{"location":"HeartbeatReceiver/#rpc-messages","text":"","title":"RPC Messages"},{"location":"HeartbeatReceiver/#executorremoved","text":"Attributes: Executor ID Posted when HeartbeatReceiver is notified that an executor is no longer available When received, HeartbeatReceiver removes the executor (from executorLastSeen internal registry).","title":" ExecutorRemoved"},{"location":"HeartbeatReceiver/#executorregistered","text":"Attributes: Executor ID Posted when HeartbeatReceiver is notified that a new executor has been registered When received, HeartbeatReceiver registers the executor and the current time (in executorLastSeen internal registry).","title":" ExecutorRegistered"},{"location":"HeartbeatReceiver/#expiredeadhosts","text":"No attributes When received, HeartbeatReceiver prints out the following TRACE message to the logs: Checking for hosts with no recent heartbeats in HeartbeatReceiver. Each executor (in executorLastSeen internal registry) is checked whether the time it was last seen is not past spark.network.timeout . For any such executor, HeartbeatReceiver prints out the following WARN message to the logs: Removing executor [executorId] with no recent heartbeats: [time] ms exceeds timeout [timeout] ms HeartbeatReceiver TaskScheduler.executorLost (with SlaveLost(\"Executor heartbeat timed out after [timeout] ms\" ). SparkContext.killAndReplaceExecutor is asynchronously called for the executor (i.e. on killExecutorThread ). The executor is removed from the executorLastSeen internal registry.","title":" ExpireDeadHosts"},{"location":"HeartbeatReceiver/#heartbeat","text":"Attributes: Executor ID AccumulatorV2 updates (by task ID) BlockManagerId ExecutorMetrics peaks (by stage and stage attempt IDs) Posted when Executor informs that it is alive and reports task metrics . When received, HeartbeatReceiver finds the executorId executor (in executorLastSeen internal registry). When the executor is found, HeartbeatReceiver updates the time the heartbeat was received (in executorLastSeen internal registry). HeartbeatReceiver uses the Clock to know the current time. HeartbeatReceiver then submits an asynchronous task to notify TaskScheduler that the heartbeat was received from the executor (using TaskScheduler internal reference). HeartbeatReceiver posts a HeartbeatResponse back to the executor (with the response from TaskScheduler whether the executor has been registered already or not so it may eventually need to re-register). If however the executor was not found (in executorLastSeen internal registry), i.e. the executor was not registered before, you should see the following DEBUG message in the logs and the response is to notify the executor to re-register. Received heartbeat from unknown executor [executorId] In a very rare case, when TaskScheduler is not yet assigned to HeartbeatReceiver , you should see the following WARN message in the logs and the response is to notify the executor to re-register. Dropping [heartbeat] because TaskScheduler is not ready yet","title":" Heartbeat"},{"location":"HeartbeatReceiver/#taskschedulerisset","text":"No attributes Posted when SparkContext informs that TaskScheduler is available . When received, HeartbeatReceiver sets the internal reference to TaskScheduler .","title":" TaskSchedulerIsSet"},{"location":"HeartbeatReceiver/#onexecutoradded","text":"onExecutorAdded ( executorAdded : SparkListenerExecutorAdded ): Unit onExecutorAdded sends an ExecutorRegistered message to itself . onExecutorAdded is part of the SparkListener abstraction.","title":" onExecutorAdded"},{"location":"HeartbeatReceiver/#addexecutor","text":"addExecutor ( executorId : String ): Option [ Future [ Boolean ]] addExecutor ...FIXME","title":" addExecutor"},{"location":"HeartbeatReceiver/#onexecutorremoved","text":"onExecutorRemoved ( executorRemoved : SparkListenerExecutorRemoved ): Unit onExecutorRemoved removes the executor . onExecutorRemoved is part of the SparkListener abstraction.","title":" onExecutorRemoved"},{"location":"HeartbeatReceiver/#removeexecutor","text":"removeExecutor ( executorId : String ): Option [ Future [ Boolean ]] removeExecutor ...FIXME","title":" removeExecutor"},{"location":"HeartbeatReceiver/#starting-heartbeatreceiver","text":"onStart (): Unit onStart sends a blocking ExpireDeadHosts every spark.network.timeoutInterval on eventLoopThread . onStart is part of the RpcEndpoint abstraction.","title":" Starting HeartbeatReceiver"},{"location":"HeartbeatReceiver/#stopping-heartbeatreceiver","text":"onStop (): Unit onStop shuts down the eventLoopThread and killExecutorThread thread pools. onStop is part of the RpcEndpoint abstraction.","title":" Stopping HeartbeatReceiver"},{"location":"HeartbeatReceiver/#handling-two-way-messages","text":"receiveAndReply ( context : RpcCallContext ): PartialFunction [ Any , Unit ] receiveAndReply ...FIXME receiveAndReply is part of the RpcEndpoint abstraction.","title":" Handling Two-Way Messages"},{"location":"HeartbeatReceiver/#thread-pools","text":"","title":"Thread Pools"},{"location":"HeartbeatReceiver/#kill-executor-thread","text":"killExecutorThread is a daemon ScheduledThreadPoolExecutor with a single thread. The name of the thread pool is kill-executor-thread .","title":" kill-executor-thread"},{"location":"HeartbeatReceiver/#heartbeat-receiver-event-loop-thread","text":"eventLoopThread is a daemon ScheduledThreadPoolExecutor with a single thread. The name of the thread pool is heartbeat-receiver-event-loop-thread .","title":" heartbeat-receiver-event-loop-thread"},{"location":"HeartbeatReceiver/#expiring-dead-hosts","text":"expireDeadHosts (): Unit expireDeadHosts ...FIXME expireDeadHosts is used when HeartbeatReceiver is requested to receives an ExpireDeadHosts message .","title":" Expiring Dead Hosts"},{"location":"HeartbeatReceiver/#logging","text":"Enable ALL logging level for org.apache.spark.HeartbeatReceiver logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.HeartbeatReceiver=ALL Refer to Logging .","title":"Logging"},{"location":"InterruptibleIterator/","text":"== [[InterruptibleIterator]] InterruptibleIterator -- Iterator With Support For Task Cancellation InterruptibleIterator is a custom Scala https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator[Iterator ] that supports task cancellation, i.e. < >. Quoting the official Scala https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator[Iterator ] documentation: Iterators are data structures that allow to iterate over a sequence of elements. They have a hasNext method for checking if there is a next element available, and a next method which returns the next element and discards it from the iterator. InterruptibleIterator is < > when: RDD is requested to rdd:RDD.md#getOrCompute[get or compute a RDD partition] CoGroupedRDD , rdd:HadoopRDD.md#compute[HadoopRDD], rdd:NewHadoopRDD.md#compute[NewHadoopRDD], rdd:ParallelCollectionRDD.md#compute[ParallelCollectionRDD] are requested to compute a partition BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined key-value records for a reduce task] PairRDDFunctions is requested to rdd:PairRDDFunctions.md#combineByKeyWithClassTag[combineByKeyWithClassTag] Spark SQL's DataSourceRDD and JDBCRDD are requested to compute a partition Spark SQL's RangeExec physical operator is requested to doExecute PySpark's BasePythonRunner is requested to compute [[creating-instance]] InterruptibleIterator takes the following when created: [[context]] TaskContext [[delegate]] Scala Iterator[T] NOTE: InterruptibleIterator is a Developer API which is a lower-level, unstable API intended for Spark developers that may change or be removed in minor versions of Apache Spark. === [[hasNext]] hasNext Method [source, scala] \u00b6 hasNext: Boolean \u00b6 NOTE: hasNext is part of ++ https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator@hasNext:Boolean++[Iterator Contract] to test whether this iterator can provide another element. hasNext requests the < > to kill the task if interrupted (that simply throws a TaskKilledException that in turn breaks the task execution). In the end, hasNext requests the < > to hasNext . === [[next]] next Method [source, scala] \u00b6 next(): T \u00b6 NOTE: next is part of ++ https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator@next():A++[Iterator Contract] to produce the next element of this iterator. next simply requests the < > to next .","title":"InterruptibleIterator"},{"location":"InterruptibleIterator/#source-scala","text":"","title":"[source, scala]"},{"location":"InterruptibleIterator/#hasnext-boolean","text":"NOTE: hasNext is part of ++ https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator@hasNext:Boolean++[Iterator Contract] to test whether this iterator can provide another element. hasNext requests the < > to kill the task if interrupted (that simply throws a TaskKilledException that in turn breaks the task execution). In the end, hasNext requests the < > to hasNext . === [[next]] next Method","title":"hasNext: Boolean"},{"location":"InterruptibleIterator/#source-scala_1","text":"","title":"[source, scala]"},{"location":"InterruptibleIterator/#next-t","text":"NOTE: next is part of ++ https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator@next():A++[Iterator Contract] to produce the next element of this iterator. next simply requests the < > to next .","title":"next(): T"},{"location":"ListenerBus/","text":"ListenerBus \u00b6 ListenerBus is an abstraction of event buses that can notify listeners about scheduling events . Contract \u00b6 Notifying Listener about Event \u00b6 doPostEvent ( listener : L , event : E ): Unit Used when ListenerBus is requested to postToAll Implementations \u00b6 ExecutionListenerBus ExternalCatalogWithListener SparkListenerBus StreamingListenerBus StreamingQueryListenerBus Posting Event To All Listeners \u00b6 postToAll ( event : E ): Unit postToAll ...FIXME postToAll is used when: AsyncEventQueue is requested to dispatch an event ReplayListenerBus is requested to replay events Registering Listener \u00b6 addListener ( listener : L ): Unit addListener ...FIXME addListener is used when: LiveListenerBus is requested to addToQueue EventLogFileCompactor is requested to initializeBuilders FsHistoryProvider is requested to doMergeApplicationListing and rebuildAppStore","title":"ListenerBus"},{"location":"ListenerBus/#listenerbus","text":"ListenerBus is an abstraction of event buses that can notify listeners about scheduling events .","title":"ListenerBus"},{"location":"ListenerBus/#contract","text":"","title":"Contract"},{"location":"ListenerBus/#notifying-listener-about-event","text":"doPostEvent ( listener : L , event : E ): Unit Used when ListenerBus is requested to postToAll","title":" Notifying Listener about Event"},{"location":"ListenerBus/#implementations","text":"ExecutionListenerBus ExternalCatalogWithListener SparkListenerBus StreamingListenerBus StreamingQueryListenerBus","title":"Implementations"},{"location":"ListenerBus/#posting-event-to-all-listeners","text":"postToAll ( event : E ): Unit postToAll ...FIXME postToAll is used when: AsyncEventQueue is requested to dispatch an event ReplayListenerBus is requested to replay events","title":" Posting Event To All Listeners"},{"location":"ListenerBus/#registering-listener","text":"addListener ( listener : L ): Unit addListener ...FIXME addListener is used when: LiveListenerBus is requested to addToQueue EventLogFileCompactor is requested to initializeBuilders FsHistoryProvider is requested to doMergeApplicationListing and rebuildAppStore","title":" Registering Listener"},{"location":"OutputCommitCoordinator/","text":"OutputCommitCoordinator \u00b6 From the scaladoc (it's a private[spark] class so no way to find it outside the code ): Authority that decides whether tasks can commit output to HDFS. Uses a \"first committer wins\" policy. OutputCommitCoordinator is instantiated in both the drivers and executors. On executors, it is configured with a reference to the driver's OutputCommitCoordinatorEndpoint, so requests to commit output will be forwarded to the driver's OutputCommitCoordinator. This class was introduced in SPARK-4879 ; see that JIRA issue (and the associated pull requests) for an extensive design discussion. Creating Instance \u00b6 OutputCommitCoordinator takes the following to be created: SparkConf isDriver flag OutputCommitCoordinator is created when: SparkEnv utility is used to create a SparkEnv on the driver OutputCommitCoordinator RPC Endpoint \u00b6 coordinatorRef : Option [ RpcEndpointRef ] OutputCommitCoordinator is registered as OutputCommitCoordinator (with OutputCommitCoordinatorEndpoint RPC Endpoint) in the RPC Environment on the driver (when SparkEnv utility is used to create \"base\" SparkEnv ). Executors have an RpcEndpointRef to the endpoint on the driver. coordinatorRef is used to post an AskPermissionToCommitOutput (by executors) to the OutputCommitCoordinator (when canCommit ). coordinatorRef is used to stop the OutputCommitCoordinator on the driver (when stop ). canCommit \u00b6 canCommit ( stage : Int , stageAttempt : Int , partition : Int , attemptNumber : Int ): Boolean canCommit creates a AskPermissionToCommitOutput message and sends it (asynchronously) to the OutputCommitCoordinator RPC Endpoint . canCommit is used when: SparkHadoopMapRedUtil is requested to commitTask (with spark.hadoop.outputCommitCoordination.enabled configuration property enabled) DataWritingSparkTask ( Spark SQL ) utility is used to run handleAskPermissionToCommit \u00b6 handleAskPermissionToCommit ( stage : Int , stageAttempt : Int , partition : Int , attemptNumber : Int ): Boolean handleAskPermissionToCommit ...FIXME handleAskPermissionToCommit is used when: OutputCommitCoordinatorEndpoint is requested to handle a AskPermissionToCommitOutput message (that happens after it was sent out in canCommit ) Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.OutputCommitCoordinator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.OutputCommitCoordinator=ALL Refer to Logging .","title":"OutputCommitCoordinator"},{"location":"OutputCommitCoordinator/#outputcommitcoordinator","text":"From the scaladoc (it's a private[spark] class so no way to find it outside the code ): Authority that decides whether tasks can commit output to HDFS. Uses a \"first committer wins\" policy. OutputCommitCoordinator is instantiated in both the drivers and executors. On executors, it is configured with a reference to the driver's OutputCommitCoordinatorEndpoint, so requests to commit output will be forwarded to the driver's OutputCommitCoordinator. This class was introduced in SPARK-4879 ; see that JIRA issue (and the associated pull requests) for an extensive design discussion.","title":"OutputCommitCoordinator"},{"location":"OutputCommitCoordinator/#creating-instance","text":"OutputCommitCoordinator takes the following to be created: SparkConf isDriver flag OutputCommitCoordinator is created when: SparkEnv utility is used to create a SparkEnv on the driver","title":"Creating Instance"},{"location":"OutputCommitCoordinator/#outputcommitcoordinator-rpc-endpoint","text":"coordinatorRef : Option [ RpcEndpointRef ] OutputCommitCoordinator is registered as OutputCommitCoordinator (with OutputCommitCoordinatorEndpoint RPC Endpoint) in the RPC Environment on the driver (when SparkEnv utility is used to create \"base\" SparkEnv ). Executors have an RpcEndpointRef to the endpoint on the driver. coordinatorRef is used to post an AskPermissionToCommitOutput (by executors) to the OutputCommitCoordinator (when canCommit ). coordinatorRef is used to stop the OutputCommitCoordinator on the driver (when stop ).","title":" OutputCommitCoordinator RPC Endpoint"},{"location":"OutputCommitCoordinator/#cancommit","text":"canCommit ( stage : Int , stageAttempt : Int , partition : Int , attemptNumber : Int ): Boolean canCommit creates a AskPermissionToCommitOutput message and sends it (asynchronously) to the OutputCommitCoordinator RPC Endpoint . canCommit is used when: SparkHadoopMapRedUtil is requested to commitTask (with spark.hadoop.outputCommitCoordination.enabled configuration property enabled) DataWritingSparkTask ( Spark SQL ) utility is used to run","title":" canCommit"},{"location":"OutputCommitCoordinator/#handleaskpermissiontocommit","text":"handleAskPermissionToCommit ( stage : Int , stageAttempt : Int , partition : Int , attemptNumber : Int ): Boolean handleAskPermissionToCommit ...FIXME handleAskPermissionToCommit is used when: OutputCommitCoordinatorEndpoint is requested to handle a AskPermissionToCommitOutput message (that happens after it was sent out in canCommit )","title":" handleAskPermissionToCommit"},{"location":"OutputCommitCoordinator/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.OutputCommitCoordinator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.OutputCommitCoordinator=ALL Refer to Logging .","title":"Logging"},{"location":"RDDBarrier/","text":"== [[RDDBarrier]] RDDBarrier RDDBarrier is used to mark the current stage as a < > in < >. RDDBarrier is < > exclusively as the result of < > transformation (which is new in Spark 2.4.0 ). [source, scala] \u00b6 barrier(): RDDBarrier[T] \u00b6 [[creating-instance]] [[rdd]] RDDBarrier takes a single rdd:RDD.md[RDD] to be created and gives the single < > transformation (on the RDD ) that simply changes the regular rdd:spark-rdd-transformations.md#mapPartitions[RDD.mapPartitions] transformation to create a rdd:MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. [[mapPartitions]] [source, scala] mapPartitions S: ClassTag : RDD[S] [[example]] val rdd = sc.parallelize(0 to 3, 1) scala> :type rdd.barrier org.apache.spark.rdd.RDDBarrier[Int] val barrierRdd = rdd .barrier .mapPartitions(identity) scala> :type barrierRdd org.apache.spark.rdd.RDD[Int] scala> println(barrierRdd.toDebugString) (1) MapPartitionsRDD[5] at mapPartitions at <console>:26 [] | ParallelCollectionRDD[3] at parallelize at <console>:25 [] // MapPartitionsRDD is private[spark] // so is RDD.isBarrier // Use org.apache.spark package then // :paste -raw the following code in spark-shell / Scala REPL // BEGIN package org.apache.spark object IsBarrier { import org.apache.spark.rdd.RDD implicit class BypassPrivateSpark[T](rdd: RDD[T]) { def myIsBarrier = rdd.isBarrier } } // END import org.apache.spark.IsBarrier._ assert(barrierRdd.myIsBarrier)","title":"RDDBarrier"},{"location":"RDDBarrier/#source-scala","text":"","title":"[source, scala]"},{"location":"RDDBarrier/#barrier-rddbarriert","text":"[[creating-instance]] [[rdd]] RDDBarrier takes a single rdd:RDD.md[RDD] to be created and gives the single < > transformation (on the RDD ) that simply changes the regular rdd:spark-rdd-transformations.md#mapPartitions[RDD.mapPartitions] transformation to create a rdd:MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. [[mapPartitions]] [source, scala] mapPartitions S: ClassTag : RDD[S] [[example]] val rdd = sc.parallelize(0 to 3, 1) scala> :type rdd.barrier org.apache.spark.rdd.RDDBarrier[Int] val barrierRdd = rdd .barrier .mapPartitions(identity) scala> :type barrierRdd org.apache.spark.rdd.RDD[Int] scala> println(barrierRdd.toDebugString) (1) MapPartitionsRDD[5] at mapPartitions at <console>:26 [] | ParallelCollectionRDD[3] at parallelize at <console>:25 [] // MapPartitionsRDD is private[spark] // so is RDD.isBarrier // Use org.apache.spark package then // :paste -raw the following code in spark-shell / Scala REPL // BEGIN package org.apache.spark object IsBarrier { import org.apache.spark.rdd.RDD implicit class BypassPrivateSpark[T](rdd: RDD[T]) { def myIsBarrier = rdd.isBarrier } } // END import org.apache.spark.IsBarrier._ assert(barrierRdd.myIsBarrier)","title":"barrier(): RDDBarrier[T]"},{"location":"SparkConf/","text":"SparkConf \u00b6 Every user program starts with creating an instance of SparkConf that holds the spark-deployment-environments.md#master-urls[master URL] to connect to ( spark.master ), the name for your Spark application (that is later displayed in webui:index.md[web UI] and becomes spark.app.name ) and other Spark properties required for proper runs. The instance of SparkConf can be used to create SparkContext.md[SparkContext]. [TIP] \u00b6 Start tools:spark-shell.md[Spark shell] with --conf spark.logConf=true to log the effective Spark configuration as INFO when SparkContext is started. $ ./bin/spark-shell --conf spark.logConf=true ... 15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT 15/10/19 17:13:49 INFO SparkContext: Spark configuration: spark.app.name=Spark shell spark.home=/Users/jacek/dev/oss/spark spark.jars= spark.logConf=true spark.master=local[*] spark.repl.class.uri=http://10.5.10.20:64055 spark.submit.deployMode=client ... Use sc.getConf.toDebugString to have a richer output once SparkContext has finished initializing. \u00b6 You can query for the values of Spark properties in Spark shell as follows: scala> sc.getConf.getOption(\"spark.local.dir\") res0: Option[String] = None scala> sc.getConf.getOption(\"spark.app.name\") res1: Option[String] = Some(Spark shell) scala> sc.getConf.get(\"spark.master\") res2: String = local[*] == [[setIfMissing]] setIfMissing Method CAUTION: FIXME == [[isExecutorStartupConf]] isExecutorStartupConf Method CAUTION: FIXME == [[set]] set Method CAUTION: FIXME == Setting up Spark Properties There are the following places where a Spark application looks for Spark properties (in the order of importance from the least important to the most important): conf/spark-defaults.conf - the configuration file with the default Spark properties. Read spark-properties.md#spark-defaults-conf[spark-defaults.conf]. --conf or -c - the command-line option used by tools:spark-submit.md[spark-submit] (and other shell scripts that use spark-submit or spark-class under the covers, e.g. spark-shell ) SparkConf == [[default-configuration]] Default Configuration The default Spark configuration is created when you execute the following code: [source, scala] \u00b6 import org.apache.spark.SparkConf val conf = new SparkConf It simply loads spark.* system properties. You can use conf.toDebugString or conf.getAll to have the spark.* system properties loaded printed out. [source, scala] \u00b6 scala> conf.getAll res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,\"\"), (spark.master,local[*]), (spark.submit.deployMode,client)) scala> conf.toDebugString res1: String = spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client scala> println(conf.toDebugString) spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client == [[getAppId]] Unique Identifier of Spark Application -- getAppId Method [source, scala] \u00b6 getAppId: String \u00b6 getAppId returns the value of configuration-properties.md#spark.app.id[spark.app.id] configuration property or throws a NoSuchElementException if not set. getAppId is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#init[init] (and creates a storage:NettyBlockRpcServer.md#creating-instance[NettyBlockRpcServer] as well as storage:NettyBlockTransferService.md#appId[saves the identifier for later use]). Executor executor:Executor.md#creating-instance[is created] (in non-local mode and storage:BlockManager.md#initialize[requests BlockManager to initialize]). == [[getAvroSchema]] getAvroSchema Method [source, scala] \u00b6 getAvroSchema: Map[Long, String] \u00b6 getAvroSchema takes all avro.schema -prefixed configuration properties from < > and...FIXME getAvroSchema is used when KryoSerializer is created (and initializes avroSchemas).","title":"SparkConf"},{"location":"SparkConf/#sparkconf","text":"Every user program starts with creating an instance of SparkConf that holds the spark-deployment-environments.md#master-urls[master URL] to connect to ( spark.master ), the name for your Spark application (that is later displayed in webui:index.md[web UI] and becomes spark.app.name ) and other Spark properties required for proper runs. The instance of SparkConf can be used to create SparkContext.md[SparkContext].","title":"SparkConf"},{"location":"SparkConf/#tip","text":"Start tools:spark-shell.md[Spark shell] with --conf spark.logConf=true to log the effective Spark configuration as INFO when SparkContext is started. $ ./bin/spark-shell --conf spark.logConf=true ... 15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT 15/10/19 17:13:49 INFO SparkContext: Spark configuration: spark.app.name=Spark shell spark.home=/Users/jacek/dev/oss/spark spark.jars= spark.logConf=true spark.master=local[*] spark.repl.class.uri=http://10.5.10.20:64055 spark.submit.deployMode=client ...","title":"[TIP]"},{"location":"SparkConf/#use-scgetconftodebugstring-to-have-a-richer-output-once-sparkcontext-has-finished-initializing","text":"You can query for the values of Spark properties in Spark shell as follows: scala> sc.getConf.getOption(\"spark.local.dir\") res0: Option[String] = None scala> sc.getConf.getOption(\"spark.app.name\") res1: Option[String] = Some(Spark shell) scala> sc.getConf.get(\"spark.master\") res2: String = local[*] == [[setIfMissing]] setIfMissing Method CAUTION: FIXME == [[isExecutorStartupConf]] isExecutorStartupConf Method CAUTION: FIXME == [[set]] set Method CAUTION: FIXME == Setting up Spark Properties There are the following places where a Spark application looks for Spark properties (in the order of importance from the least important to the most important): conf/spark-defaults.conf - the configuration file with the default Spark properties. Read spark-properties.md#spark-defaults-conf[spark-defaults.conf]. --conf or -c - the command-line option used by tools:spark-submit.md[spark-submit] (and other shell scripts that use spark-submit or spark-class under the covers, e.g. spark-shell ) SparkConf == [[default-configuration]] Default Configuration The default Spark configuration is created when you execute the following code:","title":"Use sc.getConf.toDebugString to have a richer output once SparkContext has finished initializing."},{"location":"SparkConf/#source-scala","text":"import org.apache.spark.SparkConf val conf = new SparkConf It simply loads spark.* system properties. You can use conf.toDebugString or conf.getAll to have the spark.* system properties loaded printed out.","title":"[source, scala]"},{"location":"SparkConf/#source-scala_1","text":"scala> conf.getAll res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,\"\"), (spark.master,local[*]), (spark.submit.deployMode,client)) scala> conf.toDebugString res1: String = spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client scala> println(conf.toDebugString) spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client == [[getAppId]] Unique Identifier of Spark Application -- getAppId Method","title":"[source, scala]"},{"location":"SparkConf/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkConf/#getappid-string","text":"getAppId returns the value of configuration-properties.md#spark.app.id[spark.app.id] configuration property or throws a NoSuchElementException if not set. getAppId is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#init[init] (and creates a storage:NettyBlockRpcServer.md#creating-instance[NettyBlockRpcServer] as well as storage:NettyBlockTransferService.md#appId[saves the identifier for later use]). Executor executor:Executor.md#creating-instance[is created] (in non-local mode and storage:BlockManager.md#initialize[requests BlockManager to initialize]). == [[getAvroSchema]] getAvroSchema Method","title":"getAppId: String"},{"location":"SparkConf/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkConf/#getavroschema-maplong-string","text":"getAvroSchema takes all avro.schema -prefixed configuration properties from < > and...FIXME getAvroSchema is used when KryoSerializer is created (and initializes avroSchemas).","title":"getAvroSchema: Map[Long, String]"},{"location":"SparkContext-creating-instance-internals/","text":"Inside Creating SparkContext \u00b6 This document describes the internals of what happens when a new SparkContext is created . import org.apache.spark.{SparkConf, SparkContext} // 1. Create Spark configuration val conf = new SparkConf() .setAppName(\"SparkMe Application\") .setMaster(\"local[*]\") // 2. Create Spark context val sc = new SparkContext(conf) creationSite \u00b6 creationSite : CallSite SparkContext determines call site . assertOnDriver \u00b6 SparkContext ...FIXME markPartiallyConstructed \u00b6 SparkContext ...FIXME startTime \u00b6 startTime : Long SparkContext records the current time (in ms). stopped \u00b6 stopped : AtomicBoolean SparkContext initializes stopped flag to false . Printing Out Spark Version \u00b6 SparkContext prints out the following INFO message to the logs: Running Spark version [SPARK_VERSION] sparkUser \u00b6 sparkUser : String SparkContext determines Spark user . SparkConf \u00b6 _conf : SparkConf SparkContext clones the SparkConf and requests it to validateSettings . Enforcing Mandatory Configuration Properties \u00b6 SparkContext asserts that spark.master and spark.app.name are defined (in the SparkConf ). A master URL must be set in your configuration An application name must be set in your configuration DriverLogger \u00b6 _driverLogger : Option [ DriverLogger ] SparkContext creates a DriverLogger . ResourceInformation \u00b6 _resources : Map [ String , ResourceInformation ] SparkContext uses spark.driver.resourcesFile configuration property to discovery driver resources and prints out the following INFO message to the logs: ============================================================== Resources for [componentName]: [resources] ============================================================== Submitted Application \u00b6 SparkContext prints out the following INFO message to the logs (with the value of spark.app.name configuration property): Submitted application: [appName] Spark on YARN and spark.yarn.app.id \u00b6 For Spark on YARN in cluster deploy mode], SparkContext checks whether spark.yarn.app.id configuration property is defined. SparkException is thrown if it does not exist. Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit. Displaying Spark Configuration \u00b6 With spark.logConf configuration property enabled, SparkContext prints out the following INFO message to the logs: Spark configuration: [conf.toDebugString] Note SparkConf.toDebugString is used very early in the initialization process and other settings configured afterwards are not included. Use SparkContext.getConf.toDebugString once SparkContext is initialized. Setting Configuration Properties \u00b6 spark.driver.host to the current value of the property (to override the default) spark.driver.port to 0 unless defined already spark.executor.id to driver User-Defined Jar Files \u00b6 _jars : Seq [ String ] SparkContext sets the _jars to spark.jars configuration property. User-Defined Files \u00b6 _files : Seq [ String ] SparkContext sets the _files to spark.files configuration property. spark.eventLog.dir \u00b6 _eventLogDir : Option [ URI ] If spark-history-server:EventLoggingListener.md[event logging] is enabled, i.e. EventLoggingListener.md#spark_eventLog_enabled[spark.eventLog.enabled] flag is true , the internal field _eventLogDir is set to the value of EventLoggingListener.md#spark_eventLog_dir[spark.eventLog.dir] setting or the default value /tmp/spark-events . spark.eventLog.compress \u00b6 _eventLogCodec : Option [ String ] Also, if spark-history-server:EventLoggingListener.md#spark_eventLog_compress[spark.eventLog.compress] is enabled (it is not by default), the short name of the io:CompressionCodec.md[CompressionCodec] is assigned to _eventLogCodec . The config key is core:BroadcastManager.md#spark_io_compression_codec[spark.io.compression.codec] (default: lz4 ). Creating LiveListenerBus \u00b6 _listenerBus : LiveListenerBus SparkContext creates a LiveListenerBus . Creating AppStatusStore (and AppStatusSource) \u00b6 _statusStore : AppStatusStore SparkContext creates an in-memory store (with an optional AppStatusSource if enabled ) and requests the LiveListenerBus to register the AppStatusListener with the status queue . The AppStatusStore is available using the statusStore property of the SparkContext . Creating SparkEnv \u00b6 _env : SparkEnv SparkContext creates a SparkEnv and requests SparkEnv to use the instance as the default SparkEnv . spark.repl.class.uri \u00b6 With spark.repl.class.outputDir configuration property defined, SparkContext sets spark.repl.class.uri configuration property to be...FIXME Creating SparkStatusTracker \u00b6 _statusTracker : SparkStatusTracker SparkContext creates a SparkStatusTracker (with itself and the AppStatusStore ). Creating ConsoleProgressBar \u00b6 _progressBar : Option [ ConsoleProgressBar ] SparkContext creates a ConsoleProgressBar only when spark.ui.showConsoleProgress configuration property is enabled. Creating SparkUI \u00b6 _ui : Option [ SparkUI ] SparkContext creates a SparkUI only when spark.ui.enabled configuration property is enabled. SparkContext requests the SparkUI to bind . Hadoop Configuration \u00b6 _hadoopConfiguration : Configuration SparkContext creates a new Hadoop Configuration . Adding User-Defined Jar Files \u00b6 If there are jars given through the SparkContext constructor, they are added using addJar . Adding User-Defined Files \u00b6 SparkContext adds the files in spark.files configuration property. _executorMemory \u00b6 _executorMemory : Int SparkContext determines the amount of memory to allocate to each executor. It is the value of executor:Executor.md#spark.executor.memory[spark.executor.memory] setting, or SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable (or currently-deprecated SPARK_MEM ), or defaults to 1024 . _executorMemory is later available as sc.executorMemory and used for LOCAL_CLUSTER_REGEX, spark-standalone.md#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set executorEnvs(\"SPARK_EXECUTOR_MEMORY\") , MesosSchedulerBackend, CoarseMesosSchedulerBackend. SPARK_PREPEND_CLASSES Environment Variable \u00b6 The value of SPARK_PREPEND_CLASSES environment variable is included in executorEnvs . For Mesos SchedulerBackend Only \u00b6 The Mesos scheduler backend's configuration is included in executorEnvs , i.e. SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY], _conf.getExecutorEnv , and SPARK_USER . ShuffleDriverComponents \u00b6 _shuffleDriverComponents : ShuffleDriverComponents SparkContext ...FIXME Registering HeartbeatReceiver \u00b6 SparkContext registers HeartbeatReceiver RPC endpoint . PluginContainer \u00b6 _plugins : Option [ PluginContainer ] SparkContext creates a PluginContainer (with itself and the _resources ). Creating SchedulerBackend and TaskScheduler \u00b6 SparkContext object is requested to SparkContext.md#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] (for the given master URL) and the result becomes the internal _schedulerBackend and _taskScheduler . scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created] (as _dagScheduler ). Sending Blocking TaskSchedulerIsSet \u00b6 SparkContext sends a blocking TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint (to inform that the TaskScheduler is now available). Heartbeater \u00b6 _heartbeater : Heartbeater SparkContext creates a Heartbeater and starts it. Starting TaskScheduler \u00b6 SparkContext requests the TaskScheduler to start . Setting Spark Application's and Execution Attempt's IDs \u00b6 SparkContext sets the internal fields -- _applicationId and _applicationAttemptId -- (using applicationId and applicationAttemptId methods from the scheduler:TaskScheduler.md#contract[TaskScheduler Contract]). NOTE: SparkContext requests TaskScheduler for the scheduler:TaskScheduler.md#applicationId[unique identifier of a Spark application] (that is currently only implemented by scheduler:TaskSchedulerImpl.md#applicationId[TaskSchedulerImpl] that uses SchedulerBackend to scheduler:SchedulerBackend.md#applicationId[request the identifier]). NOTE: The unique identifier of a Spark application is used to initialize spark-webui-SparkUI.md#setAppId[SparkUI] and storage:BlockManager.md#initialize[BlockManager]. NOTE: _applicationAttemptId is used when SparkContext is requested for the SparkContext.md#applicationAttemptId[unique identifier of execution attempt of a Spark application] and when EventLoggingListener spark-history-server:EventLoggingListener.md#creating-instance[is created]. Setting spark.app.id Spark Property in SparkConf \u00b6 SparkContext sets SparkConf.md#spark.app.id[spark.app.id] property to be the <<_applicationId, unique identifier of a Spark application>> and, if enabled, spark-webui-SparkUI.md#setAppId[passes it on to SparkUI ]. spark.ui.proxyBase \u00b6 Initializing SparkUI \u00b6 SparkContext requests the SparkUI (if defined) to setAppId with the _applicationId . Initializing BlockManager \u00b6 The storage:BlockManager.md#initialize[BlockManager (for the driver) is initialized] (with _applicationId ). Starting MetricsSystem \u00b6 SparkContext requests the MetricsSystem to start . NOTE: SparkContext starts MetricsSystem after < > as MetricsSystem uses it to build unique identifiers fo metrics sources . Attaching JSON Servlet Handler \u00b6 SparkContext requests the MetricsSystem for a JSON servlet handler and requests the <<_ui, SparkUI>> to spark-webui-WebUI.md#attachHandler[attach it]. Starting EventLoggingListener (with Event Log Enabled) \u00b6 _eventLogger : Option [ EventLoggingListener ] With spark.eventLog.enabled configuration property enabled, SparkContext creates an EventLoggingListener and requests it to start . SparkContext requests the LiveListenerBus to add the EventLoggingListener to eventLog event queue. With spark.eventLog.enabled disabled, _eventLogger is None (undefined). ContextCleaner \u00b6 _cleaner : Option [ ContextCleaner ] With spark.cleaner.referenceTracking configuration property enabled, SparkContext creates a ContextCleaner (with itself and the _shuffleDriverComponents ). SparkContext requests the ContextCleaner to start ExecutorAllocationManager \u00b6 _executorAllocationManager : Option [ ExecutorAllocationManager ] SparkContext initializes _executorAllocationManager internal registry. SparkContext creates an ExecutorAllocationManager when: Dynamic Allocation of Executors is enabled (based on spark.dynamicAllocation.enabled configuration property and the master URL) SchedulerBackend is an ExecutorAllocationClient The ExecutorAllocationManager is requested to start . Registering User-Defined SparkListeners \u00b6 SparkContext registers user-defined listeners and starts SparkListenerEvent event delivery to the listeners . postEnvironmentUpdate \u00b6 postEnvironmentUpdate is called that posts SparkListener.md#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] message on scheduler:LiveListenerBus.md[] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. postApplicationStart \u00b6 SparkListener.md#SparkListenerApplicationStart[SparkListenerApplicationStart] message is posted to scheduler:LiveListenerBus.md[] (using the internal postApplicationStart method). postStartHook \u00b6 TaskScheduler scheduler:TaskScheduler.md#postStartHook[is notified that SparkContext is almost fully initialized]. NOTE: scheduler:TaskScheduler.md#postStartHook[TaskScheduler.postStartHook] does nothing by default, but custom implementations offer more advanced features, i.e. TaskSchedulerImpl scheduler:TaskSchedulerImpl.md#postStartHook[blocks the current thread until SchedulerBackend is ready]. There is also YarnClusterScheduler for Spark on YARN in cluster deploy mode. Registering Metrics Sources \u00b6 SparkContext requests MetricsSystem to register metrics sources for the following services: DAGScheduler BlockManager ExecutorAllocationManager Adding Shutdown Hook \u00b6 SparkContext adds a shutdown hook (using ShutdownHookManager.addShutdownHook() ). SparkContext prints out the following DEBUG message to the logs: Adding shutdown hook CAUTION: FIXME ShutdownHookManager.addShutdownHook() Any non-fatal Exception leads to termination of the Spark context instance. CAUTION: FIXME What does NonFatal represent in Scala? CAUTION: FIXME Finish me Initializing nextShuffleId and nextRddId Internal Counters \u00b6 nextShuffleId and nextRddId start with 0 . CAUTION: FIXME Where are nextShuffleId and nextRddId used? A new instance of Spark context is created and ready for operation. Loading External Cluster Manager for URL (getClusterManager method) \u00b6 getClusterManager ( url : String ): Option [ ExternalClusterManager ] getClusterManager loads scheduler:ExternalClusterManager.md[] that scheduler:ExternalClusterManager.md#canCreate[can handle the input url ]. If there are two or more external cluster managers that could handle url , a SparkException is thrown: Multiple Cluster Managers ([serviceLoaders]) registered for the url [url]. NOTE: getClusterManager uses Java's ++ https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load ] method. NOTE: getClusterManager is used to find a cluster manager for a master URL when SparkContext.md#createTaskScheduler[creating a SchedulerBackend and a TaskScheduler for the driver]. setupAndStartListenerBus \u00b6 setupAndStartListenerBus (): Unit setupAndStartListenerBus is an internal method that reads configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property from the current SparkConf.md[SparkConf] to create and register SparkListenerInterface listeners. It expects that the class name represents a SparkListenerInterface listener with one of the following constructors (in this order): a single-argument constructor that accepts SparkConf.md[SparkConf] a zero-argument constructor setupAndStartListenerBus scheduler:LiveListenerBus.md#ListenerBus-addListener[registers every listener class]. You should see the following INFO message in the logs: INFO Registered listener [className] It scheduler:LiveListenerBus.md#start[starts LiveListenerBus] and records it in the internal _listenerBusStarted . When no single- SparkConf or zero-argument constructor could be found for a class name in configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property, a SparkException is thrown with the message: [className] did not have a zero-argument constructor or a single-argument constructor that accepts SparkConf. Note: if the class is defined inside of another Scala class, then its constructors may accept an implicit parameter that references the enclosing class; in this case, you must define the listener as a top-level class in order to prevent this extra parameter from breaking Spark's ability to find a valid constructor. Any exception while registering a SparkListenerInterface listener stops the SparkContext and a SparkException is thrown and the source exception's message. Exception when registering SparkListener Tip Set INFO logging level for org.apache.spark.SparkContext logger to see the extra listeners being registered. Registered listener pl.japila.spark.CustomSparkListener","title":"Inside Creating SparkContext"},{"location":"SparkContext-creating-instance-internals/#inside-creating-sparkcontext","text":"This document describes the internals of what happens when a new SparkContext is created . import org.apache.spark.{SparkConf, SparkContext} // 1. Create Spark configuration val conf = new SparkConf() .setAppName(\"SparkMe Application\") .setMaster(\"local[*]\") // 2. Create Spark context val sc = new SparkContext(conf)","title":"Inside Creating SparkContext"},{"location":"SparkContext-creating-instance-internals/#creationsite","text":"creationSite : CallSite SparkContext determines call site .","title":" creationSite"},{"location":"SparkContext-creating-instance-internals/#assertondriver","text":"SparkContext ...FIXME","title":" assertOnDriver"},{"location":"SparkContext-creating-instance-internals/#markpartiallyconstructed","text":"SparkContext ...FIXME","title":" markPartiallyConstructed"},{"location":"SparkContext-creating-instance-internals/#starttime","text":"startTime : Long SparkContext records the current time (in ms).","title":" startTime"},{"location":"SparkContext-creating-instance-internals/#stopped","text":"stopped : AtomicBoolean SparkContext initializes stopped flag to false .","title":" stopped"},{"location":"SparkContext-creating-instance-internals/#printing-out-spark-version","text":"SparkContext prints out the following INFO message to the logs: Running Spark version [SPARK_VERSION]","title":" Printing Out Spark Version"},{"location":"SparkContext-creating-instance-internals/#sparkuser","text":"sparkUser : String SparkContext determines Spark user .","title":" sparkUser"},{"location":"SparkContext-creating-instance-internals/#sparkconf","text":"_conf : SparkConf SparkContext clones the SparkConf and requests it to validateSettings .","title":" SparkConf"},{"location":"SparkContext-creating-instance-internals/#enforcing-mandatory-configuration-properties","text":"SparkContext asserts that spark.master and spark.app.name are defined (in the SparkConf ). A master URL must be set in your configuration An application name must be set in your configuration","title":" Enforcing Mandatory Configuration Properties"},{"location":"SparkContext-creating-instance-internals/#driverlogger","text":"_driverLogger : Option [ DriverLogger ] SparkContext creates a DriverLogger .","title":" DriverLogger"},{"location":"SparkContext-creating-instance-internals/#resourceinformation","text":"_resources : Map [ String , ResourceInformation ] SparkContext uses spark.driver.resourcesFile configuration property to discovery driver resources and prints out the following INFO message to the logs: ============================================================== Resources for [componentName]: [resources] ==============================================================","title":" ResourceInformation"},{"location":"SparkContext-creating-instance-internals/#submitted-application","text":"SparkContext prints out the following INFO message to the logs (with the value of spark.app.name configuration property): Submitted application: [appName]","title":"Submitted Application"},{"location":"SparkContext-creating-instance-internals/#spark-on-yarn-and-sparkyarnappid","text":"For Spark on YARN in cluster deploy mode], SparkContext checks whether spark.yarn.app.id configuration property is defined. SparkException is thrown if it does not exist. Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.","title":"Spark on YARN and spark.yarn.app.id"},{"location":"SparkContext-creating-instance-internals/#displaying-spark-configuration","text":"With spark.logConf configuration property enabled, SparkContext prints out the following INFO message to the logs: Spark configuration: [conf.toDebugString] Note SparkConf.toDebugString is used very early in the initialization process and other settings configured afterwards are not included. Use SparkContext.getConf.toDebugString once SparkContext is initialized.","title":"Displaying Spark Configuration"},{"location":"SparkContext-creating-instance-internals/#setting-configuration-properties","text":"spark.driver.host to the current value of the property (to override the default) spark.driver.port to 0 unless defined already spark.executor.id to driver","title":"Setting Configuration Properties"},{"location":"SparkContext-creating-instance-internals/#user-defined-jar-files","text":"_jars : Seq [ String ] SparkContext sets the _jars to spark.jars configuration property.","title":" User-Defined Jar Files"},{"location":"SparkContext-creating-instance-internals/#user-defined-files","text":"_files : Seq [ String ] SparkContext sets the _files to spark.files configuration property.","title":" User-Defined Files"},{"location":"SparkContext-creating-instance-internals/#sparkeventlogdir","text":"_eventLogDir : Option [ URI ] If spark-history-server:EventLoggingListener.md[event logging] is enabled, i.e. EventLoggingListener.md#spark_eventLog_enabled[spark.eventLog.enabled] flag is true , the internal field _eventLogDir is set to the value of EventLoggingListener.md#spark_eventLog_dir[spark.eventLog.dir] setting or the default value /tmp/spark-events .","title":" spark.eventLog.dir"},{"location":"SparkContext-creating-instance-internals/#sparkeventlogcompress","text":"_eventLogCodec : Option [ String ] Also, if spark-history-server:EventLoggingListener.md#spark_eventLog_compress[spark.eventLog.compress] is enabled (it is not by default), the short name of the io:CompressionCodec.md[CompressionCodec] is assigned to _eventLogCodec . The config key is core:BroadcastManager.md#spark_io_compression_codec[spark.io.compression.codec] (default: lz4 ).","title":" spark.eventLog.compress"},{"location":"SparkContext-creating-instance-internals/#creating-livelistenerbus","text":"_listenerBus : LiveListenerBus SparkContext creates a LiveListenerBus .","title":" Creating LiveListenerBus"},{"location":"SparkContext-creating-instance-internals/#creating-appstatusstore-and-appstatussource","text":"_statusStore : AppStatusStore SparkContext creates an in-memory store (with an optional AppStatusSource if enabled ) and requests the LiveListenerBus to register the AppStatusListener with the status queue . The AppStatusStore is available using the statusStore property of the SparkContext .","title":" Creating AppStatusStore (and AppStatusSource)"},{"location":"SparkContext-creating-instance-internals/#creating-sparkenv","text":"_env : SparkEnv SparkContext creates a SparkEnv and requests SparkEnv to use the instance as the default SparkEnv .","title":" Creating SparkEnv"},{"location":"SparkContext-creating-instance-internals/#sparkreplclassuri","text":"With spark.repl.class.outputDir configuration property defined, SparkContext sets spark.repl.class.uri configuration property to be...FIXME","title":" spark.repl.class.uri"},{"location":"SparkContext-creating-instance-internals/#creating-sparkstatustracker","text":"_statusTracker : SparkStatusTracker SparkContext creates a SparkStatusTracker (with itself and the AppStatusStore ).","title":" Creating SparkStatusTracker"},{"location":"SparkContext-creating-instance-internals/#creating-consoleprogressbar","text":"_progressBar : Option [ ConsoleProgressBar ] SparkContext creates a ConsoleProgressBar only when spark.ui.showConsoleProgress configuration property is enabled.","title":" Creating ConsoleProgressBar"},{"location":"SparkContext-creating-instance-internals/#creating-sparkui","text":"_ui : Option [ SparkUI ] SparkContext creates a SparkUI only when spark.ui.enabled configuration property is enabled. SparkContext requests the SparkUI to bind .","title":" Creating SparkUI"},{"location":"SparkContext-creating-instance-internals/#hadoop-configuration","text":"_hadoopConfiguration : Configuration SparkContext creates a new Hadoop Configuration .","title":" Hadoop Configuration"},{"location":"SparkContext-creating-instance-internals/#adding-user-defined-jar-files","text":"If there are jars given through the SparkContext constructor, they are added using addJar .","title":" Adding User-Defined Jar Files"},{"location":"SparkContext-creating-instance-internals/#adding-user-defined-files","text":"SparkContext adds the files in spark.files configuration property.","title":" Adding User-Defined Files"},{"location":"SparkContext-creating-instance-internals/#_executormemory","text":"_executorMemory : Int SparkContext determines the amount of memory to allocate to each executor. It is the value of executor:Executor.md#spark.executor.memory[spark.executor.memory] setting, or SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable (or currently-deprecated SPARK_MEM ), or defaults to 1024 . _executorMemory is later available as sc.executorMemory and used for LOCAL_CLUSTER_REGEX, spark-standalone.md#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set executorEnvs(\"SPARK_EXECUTOR_MEMORY\") , MesosSchedulerBackend, CoarseMesosSchedulerBackend.","title":" _executorMemory"},{"location":"SparkContext-creating-instance-internals/#spark_prepend_classes-environment-variable","text":"The value of SPARK_PREPEND_CLASSES environment variable is included in executorEnvs .","title":" SPARK_PREPEND_CLASSES Environment Variable"},{"location":"SparkContext-creating-instance-internals/#for-mesos-schedulerbackend-only","text":"The Mesos scheduler backend's configuration is included in executorEnvs , i.e. SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY], _conf.getExecutorEnv , and SPARK_USER .","title":"For Mesos SchedulerBackend Only"},{"location":"SparkContext-creating-instance-internals/#shuffledrivercomponents","text":"_shuffleDriverComponents : ShuffleDriverComponents SparkContext ...FIXME","title":" ShuffleDriverComponents"},{"location":"SparkContext-creating-instance-internals/#registering-heartbeatreceiver","text":"SparkContext registers HeartbeatReceiver RPC endpoint .","title":" Registering HeartbeatReceiver"},{"location":"SparkContext-creating-instance-internals/#plugincontainer","text":"_plugins : Option [ PluginContainer ] SparkContext creates a PluginContainer (with itself and the _resources ).","title":" PluginContainer"},{"location":"SparkContext-creating-instance-internals/#creating-schedulerbackend-and-taskscheduler","text":"SparkContext object is requested to SparkContext.md#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] (for the given master URL) and the result becomes the internal _schedulerBackend and _taskScheduler . scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created] (as _dagScheduler ).","title":"Creating SchedulerBackend and TaskScheduler"},{"location":"SparkContext-creating-instance-internals/#sending-blocking-taskschedulerisset","text":"SparkContext sends a blocking TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint (to inform that the TaskScheduler is now available).","title":" Sending Blocking TaskSchedulerIsSet"},{"location":"SparkContext-creating-instance-internals/#heartbeater","text":"_heartbeater : Heartbeater SparkContext creates a Heartbeater and starts it.","title":" Heartbeater"},{"location":"SparkContext-creating-instance-internals/#starting-taskscheduler","text":"SparkContext requests the TaskScheduler to start .","title":" Starting TaskScheduler"},{"location":"SparkContext-creating-instance-internals/#setting-spark-applications-and-execution-attempts-ids","text":"SparkContext sets the internal fields -- _applicationId and _applicationAttemptId -- (using applicationId and applicationAttemptId methods from the scheduler:TaskScheduler.md#contract[TaskScheduler Contract]). NOTE: SparkContext requests TaskScheduler for the scheduler:TaskScheduler.md#applicationId[unique identifier of a Spark application] (that is currently only implemented by scheduler:TaskSchedulerImpl.md#applicationId[TaskSchedulerImpl] that uses SchedulerBackend to scheduler:SchedulerBackend.md#applicationId[request the identifier]). NOTE: The unique identifier of a Spark application is used to initialize spark-webui-SparkUI.md#setAppId[SparkUI] and storage:BlockManager.md#initialize[BlockManager]. NOTE: _applicationAttemptId is used when SparkContext is requested for the SparkContext.md#applicationAttemptId[unique identifier of execution attempt of a Spark application] and when EventLoggingListener spark-history-server:EventLoggingListener.md#creating-instance[is created].","title":" Setting Spark Application's and Execution Attempt's IDs"},{"location":"SparkContext-creating-instance-internals/#setting-sparkappid-spark-property-in-sparkconf","text":"SparkContext sets SparkConf.md#spark.app.id[spark.app.id] property to be the <<_applicationId, unique identifier of a Spark application>> and, if enabled, spark-webui-SparkUI.md#setAppId[passes it on to SparkUI ].","title":" Setting spark.app.id Spark Property in SparkConf"},{"location":"SparkContext-creating-instance-internals/#sparkuiproxybase","text":"","title":" spark.ui.proxyBase"},{"location":"SparkContext-creating-instance-internals/#initializing-sparkui","text":"SparkContext requests the SparkUI (if defined) to setAppId with the _applicationId .","title":"Initializing SparkUI"},{"location":"SparkContext-creating-instance-internals/#initializing-blockmanager","text":"The storage:BlockManager.md#initialize[BlockManager (for the driver) is initialized] (with _applicationId ).","title":"Initializing BlockManager"},{"location":"SparkContext-creating-instance-internals/#starting-metricssystem","text":"SparkContext requests the MetricsSystem to start . NOTE: SparkContext starts MetricsSystem after < > as MetricsSystem uses it to build unique identifiers fo metrics sources .","title":"Starting MetricsSystem"},{"location":"SparkContext-creating-instance-internals/#attaching-json-servlet-handler","text":"SparkContext requests the MetricsSystem for a JSON servlet handler and requests the <<_ui, SparkUI>> to spark-webui-WebUI.md#attachHandler[attach it].","title":"Attaching JSON Servlet Handler"},{"location":"SparkContext-creating-instance-internals/#starting-eventlogginglistener-with-event-log-enabled","text":"_eventLogger : Option [ EventLoggingListener ] With spark.eventLog.enabled configuration property enabled, SparkContext creates an EventLoggingListener and requests it to start . SparkContext requests the LiveListenerBus to add the EventLoggingListener to eventLog event queue. With spark.eventLog.enabled disabled, _eventLogger is None (undefined).","title":" Starting EventLoggingListener (with Event Log Enabled)"},{"location":"SparkContext-creating-instance-internals/#contextcleaner","text":"_cleaner : Option [ ContextCleaner ] With spark.cleaner.referenceTracking configuration property enabled, SparkContext creates a ContextCleaner (with itself and the _shuffleDriverComponents ). SparkContext requests the ContextCleaner to start","title":" ContextCleaner"},{"location":"SparkContext-creating-instance-internals/#executorallocationmanager","text":"_executorAllocationManager : Option [ ExecutorAllocationManager ] SparkContext initializes _executorAllocationManager internal registry. SparkContext creates an ExecutorAllocationManager when: Dynamic Allocation of Executors is enabled (based on spark.dynamicAllocation.enabled configuration property and the master URL) SchedulerBackend is an ExecutorAllocationClient The ExecutorAllocationManager is requested to start .","title":" ExecutorAllocationManager"},{"location":"SparkContext-creating-instance-internals/#registering-user-defined-sparklisteners","text":"SparkContext registers user-defined listeners and starts SparkListenerEvent event delivery to the listeners .","title":"Registering User-Defined SparkListeners"},{"location":"SparkContext-creating-instance-internals/#postenvironmentupdate","text":"postEnvironmentUpdate is called that posts SparkListener.md#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] message on scheduler:LiveListenerBus.md[] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details.","title":" postEnvironmentUpdate"},{"location":"SparkContext-creating-instance-internals/#postapplicationstart","text":"SparkListener.md#SparkListenerApplicationStart[SparkListenerApplicationStart] message is posted to scheduler:LiveListenerBus.md[] (using the internal postApplicationStart method).","title":" postApplicationStart"},{"location":"SparkContext-creating-instance-internals/#poststarthook","text":"TaskScheduler scheduler:TaskScheduler.md#postStartHook[is notified that SparkContext is almost fully initialized]. NOTE: scheduler:TaskScheduler.md#postStartHook[TaskScheduler.postStartHook] does nothing by default, but custom implementations offer more advanced features, i.e. TaskSchedulerImpl scheduler:TaskSchedulerImpl.md#postStartHook[blocks the current thread until SchedulerBackend is ready]. There is also YarnClusterScheduler for Spark on YARN in cluster deploy mode.","title":" postStartHook"},{"location":"SparkContext-creating-instance-internals/#registering-metrics-sources","text":"SparkContext requests MetricsSystem to register metrics sources for the following services: DAGScheduler BlockManager ExecutorAllocationManager","title":"Registering Metrics Sources"},{"location":"SparkContext-creating-instance-internals/#adding-shutdown-hook","text":"SparkContext adds a shutdown hook (using ShutdownHookManager.addShutdownHook() ). SparkContext prints out the following DEBUG message to the logs: Adding shutdown hook CAUTION: FIXME ShutdownHookManager.addShutdownHook() Any non-fatal Exception leads to termination of the Spark context instance. CAUTION: FIXME What does NonFatal represent in Scala? CAUTION: FIXME Finish me","title":" Adding Shutdown Hook"},{"location":"SparkContext-creating-instance-internals/#initializing-nextshuffleid-and-nextrddid-internal-counters","text":"nextShuffleId and nextRddId start with 0 . CAUTION: FIXME Where are nextShuffleId and nextRddId used? A new instance of Spark context is created and ready for operation.","title":" Initializing nextShuffleId and nextRddId Internal Counters"},{"location":"SparkContext-creating-instance-internals/#loading-external-cluster-manager-for-url-getclustermanager-method","text":"getClusterManager ( url : String ): Option [ ExternalClusterManager ] getClusterManager loads scheduler:ExternalClusterManager.md[] that scheduler:ExternalClusterManager.md#canCreate[can handle the input url ]. If there are two or more external cluster managers that could handle url , a SparkException is thrown: Multiple Cluster Managers ([serviceLoaders]) registered for the url [url]. NOTE: getClusterManager uses Java's ++ https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load ] method. NOTE: getClusterManager is used to find a cluster manager for a master URL when SparkContext.md#createTaskScheduler[creating a SchedulerBackend and a TaskScheduler for the driver].","title":" Loading External Cluster Manager for URL (getClusterManager method)"},{"location":"SparkContext-creating-instance-internals/#setupandstartlistenerbus","text":"setupAndStartListenerBus (): Unit setupAndStartListenerBus is an internal method that reads configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property from the current SparkConf.md[SparkConf] to create and register SparkListenerInterface listeners. It expects that the class name represents a SparkListenerInterface listener with one of the following constructors (in this order): a single-argument constructor that accepts SparkConf.md[SparkConf] a zero-argument constructor setupAndStartListenerBus scheduler:LiveListenerBus.md#ListenerBus-addListener[registers every listener class]. You should see the following INFO message in the logs: INFO Registered listener [className] It scheduler:LiveListenerBus.md#start[starts LiveListenerBus] and records it in the internal _listenerBusStarted . When no single- SparkConf or zero-argument constructor could be found for a class name in configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property, a SparkException is thrown with the message: [className] did not have a zero-argument constructor or a single-argument constructor that accepts SparkConf. Note: if the class is defined inside of another Scala class, then its constructors may accept an implicit parameter that references the enclosing class; in this case, you must define the listener as a top-level class in order to prevent this extra parameter from breaking Spark's ability to find a valid constructor. Any exception while registering a SparkListenerInterface listener stops the SparkContext and a SparkException is thrown and the source exception's message. Exception when registering SparkListener Tip Set INFO logging level for org.apache.spark.SparkContext logger to see the extra listeners being registered. Registered listener pl.japila.spark.CustomSparkListener","title":" setupAndStartListenerBus"},{"location":"SparkContext/","text":"SparkContext \u00b6 SparkContext is the entry point to all of the components of Apache Spark (execution engine) and so the heart of a Spark application. In fact, you can consider an application a Spark application only when it uses a SparkContext (directly or indirectly). Important There should be one active SparkContext per JVM and Spark developers should use SparkContext.getOrCreate utility for sharing it (e.g. across threads). Creating Instance \u00b6 SparkContext takes the following to be created: SparkConf SparkContext is created (directly or indirectly using getOrCreate utility). While being created, SparkContext sets up core services and establishes a connection to a Spark execution environment . Local Properties \u00b6 localProperties : InheritableThreadLocal [ Properties ] SparkContext uses an InheritableThreadLocal ( Java ) of key-value pairs of thread-local properties to pass extra information from a parent thread (on the driver) to child threads. localProperties is meant to be used by developers using SparkContext.setLocalProperty and SparkContext.getLocalProperty . Local Properties are available using TaskContext.getLocalProperty . Local Properties are available to SparkListener s using the following events: SparkListenerJobStart SparkListenerStageSubmitted localProperties are passed down when SparkContext is requested for the following: Running Job (that in turn makes the local properties available to the DAGScheduler to run a job ) Running Approximate Job Submitting Job Submitting MapStage DAGScheduler passes down local properties when scheduling: ShuffleMapTask s ResultTask s TaskSet s Spark (Core) defines the following local properties. Name Default Value Setter callSite.long callSite.short SparkContext.setCallSite spark.job.description callSite.short SparkContext.setJobDescription ( SparkContext.setJobGroup ) spark.job.interruptOnCancel SparkContext.setJobGroup spark.jobGroup.id SparkContext.setJobGroup spark.scheduler.pool Services \u00b6 AppStatusStore ExecutorAllocationManager (optional) SchedulerBackend others ShuffleDriverComponents \u00b6 SparkContext creates a ShuffleDriverComponents when created . SparkContext loads the ShuffleDataIO that is in turn requested for the ShuffleDriverComponents . SparkContext requests the ShuffleDriverComponents to initialize . The ShuffleDriverComponents is used when: ShuffleDependency is created SparkContext creates the ContextCleaner (if enabled) SparkContext requests the ShuffleDriverComponents to clean up when stopping . Static Files \u00b6 addFile \u00b6 addFile ( path : String , recursive : Boolean ): Unit // recursive = false addFile ( path : String ): Unit addFile creates a Hadoop Path from the given path . For a no-schema path, addFile converts it to a canonical form. addFile prints out the following WARN message to the logs and exits. File with 'local' scheme is not supported to add to file server, since it is already available on every node. addFile ...FIXME In the end, addFile adds the file to the addedFiles internal registry (with the current timestamp): For new files, addFile prints out the following INFO message to the logs, fetches the file (to the root directory and without using the cache) and postEnvironmentUpdate . Added file [path] at [key] with timestamp [timestamp] For files that were already added, addFile prints out the following WARN message to the logs: The path [path] has been added already. Overwriting of added paths is not supported in the current version. addFile is used when: SparkContext is created listFiles \u00b6 listFiles (): Seq [ String ] listFiles is the files added . addedFiles Internal Registry \u00b6 addedFiles : Map [ String , Long ] addedFiles is a collection of static files by the timestamp the were added at. addedFiles is used when: SparkContext is requested to postEnvironmentUpdate and listFiles TaskSetManager is created (and resourceOffer ) files \u00b6 files : Seq [ String ] files is a collection of file paths defined by spark.files configuration property. Posting SparkListenerEnvironmentUpdate Event \u00b6 postEnvironmentUpdate (): Unit postEnvironmentUpdate ...FIXME postEnvironmentUpdate is used when: SparkContext is requested to addFile and addJar getOrCreate Utility \u00b6 getOrCreate (): SparkContext getOrCreate ( config : SparkConf ): SparkContext getOrCreate ...FIXME PluginContainer \u00b6 SparkContext creates a PluginContainer when created . PluginContainer is created (for the driver where SparkContext lives) using PluginContainer.apply utility. PluginContainer is then requested to registerMetrics with the applicationId . PluginContainer is requested to shutdown when SparkContext is requested to stop . Creating SchedulerBackend and TaskScheduler \u00b6 createTaskScheduler ( sc : SparkContext , master : String , deployMode : String ): ( SchedulerBackend , TaskScheduler ) createTaskScheduler creates a SchedulerBackend and a TaskScheduler for the given master URL and deployment mode. Internally, createTaskScheduler branches off per the given master URL ( master URL ) to select the requested implementations. createTaskScheduler accepts the following master URLs: local - local mode with 1 thread only local[n] or local[*] - local mode with n threads local[n, m] or local[*, m] -- local mode with n threads and m number of failures spark://hostname:port for Spark Standalone local-cluster[n, m, z] -- local cluster with n workers, m cores per worker, and z memory per worker Other URLs are simply handed over to getClusterManager to load an external cluster manager if available createTaskScheduler is used when SparkContext is created . Loading ExternalClusterManager \u00b6 getClusterManager ( url : String ): Option [ ExternalClusterManager ] getClusterManager uses Java's ServiceLoader to find and load an ExternalClusterManager that supports the given master URL. ExternalClusterManager Service Discovery For ServiceLoader to find ExternalClusterManager s, they have to be registered using the following file: META-INF/services/org.apache.spark.scheduler.ExternalClusterManager getClusterManager throws a SparkException when multiple cluster managers were found: Multiple external cluster managers registered for the url [url]: [serviceLoaders] getClusterManager is used when SparkContext is requested for a SchedulerBackend and TaskScheduler . Running Job Synchronously \u00b6 runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U ): Array [ U ] runJob [ T , U : ClassTag ]( rdd : RDD [ T ], processPartition : ( TaskContext , Iterator [ T ]) => U , resultHandler : ( Int , U ) => Unit ): Unit runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ]): Array [ U ] runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], resultHandler : ( Int , U ) => Unit ): Unit runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : Iterator [ T ] => U ): Array [ U ] runJob [ T , U : ClassTag ]( rdd : RDD [ T ], processPartition : Iterator [ T ] => U , resultHandler : ( Int , U ) => Unit ): Unit runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : Iterator [ T ] => U , partitions : Seq [ Int ]): Array [ U ] runJob finds the call site and cleans up the given func function. runJob prints out the following INFO message to the logs: Starting job: [callSite] With spark.logLineage enabled, runJob requests the given RDD for the recursive dependencies and prints out the following INFO message to the logs: RDD's recursive dependencies: [toDebugString] runJob requests the DAGScheduler to run a job . runJob requests the ConsoleProgressBar to finishAll if defined. In the end, runJob requests the given RDD to doCheckpoint . runJob throws an IllegalStateException when SparkContext is stopped : SparkContext has been shutdown Demo \u00b6 runJob is essentially executing a func function on all or a subset of partitions of an RDD and returning the result as an array (with elements being the results per partition). sc . setLocalProperty ( \"callSite.short\" , \"runJob Demo\" ) val partitionsNumber = 4 val rdd = sc . parallelize ( Seq ( \"hello world\" , \"nice to see you\" ), numSlices = partitionsNumber ) import org . apache . spark . TaskContext val func = ( t : TaskContext , ss : Iterator [ String ]) => 1 val result = sc . runJob ( rdd , func ) assert ( result . length == partitionsNumber ) sc . clearCallSite () Call Site \u00b6 getCallSite (): CallSite getCallSite ...FIXME getCallSite is used when: SparkContext is requested to broadcast , runJob , runApproximateJob , submitJob and submitMapStage AsyncRDDActions is requested to takeAsync RDD is created Closure Cleaning \u00b6 clean ( f : F , checkSerializable : Boolean = true ): F clean cleans up the given f closure (using ClosureCleaner.clean utility). Tip Enable DEBUG logging level for org.apache.spark.util.ClosureCleaner logger to see what happens inside the class. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG Refer to Logging . With DEBUG logging level you should see the following messages in the logs: +++ Cleaning closure [func] ([func.getClass.getName]) +++ + declared fields: [declaredFields.size] [field] ... +++ closure [func] ([func.getClass.getName]) is now cleaned +++ Logging \u00b6 Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.SparkContext=ALL Refer to Logging . To Be Reviewed \u00b6 SparkContext offers the following functions: Getting current status of a Spark application ** < > ** < > ** < > ** < > ** < > ** < > ** < > that specifies the number of spark-rdd-partitions.md[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** < > ** < > ** < > ** < > ** < > Setting Configuration ** < > ** Local Properties ** < > ** < > Creating Distributed Entities ** < > ** < > ** < > Accessing services, e.g. < >, < >, scheduler:LiveListenerBus.md[], storage:BlockManager.md[BlockManager], scheduler:SchedulerBackend.md[SchedulerBackends], shuffle:ShuffleManager.md[ShuffleManager] and the < >. < > < > < > < > < > < > < > < > < > < > TIP: Read the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]. Removing RDD Blocks from BlockManagerMaster \u00b6 unpersistRDD ( rddId : Int , blocking : Boolean = true ): Unit unpersistRDD requests BlockManagerMaster to storage:BlockManagerMaster.md#removeRdd[remove the blocks for the RDD] (given rddId ). NOTE: unpersistRDD uses SparkEnv core:SparkEnv.md#blockManager[to access the current BlockManager ] that is in turn used to storage:BlockManager.md#master[access the current BlockManagerMaster ]. unpersistRDD removes rddId from < > registry. In the end, unpersistRDD posts a SparkListener.md#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with rddId ) to < >. [NOTE] \u00b6 unpersistRDD is used when: ContextCleaner does core:ContextCleaner.md#doCleanupRDD[doCleanupRDD] SparkContext < > (i.e. marks an RDD as non-persistent) \u00b6 == [[applicationId]] Unique Identifier of Spark Application -- applicationId Method CAUTION: FIXME == [[postApplicationStart]] postApplicationStart Internal Method [source, scala] \u00b6 postApplicationStart(): Unit \u00b6 postApplicationStart ...FIXME postApplicationStart is used exclusively when SparkContext is created. == [[postApplicationEnd]] postApplicationEnd Method CAUTION: FIXME == [[clearActiveContext]] clearActiveContext Method CAUTION: FIXME == [[getPersistentRDDs]] Accessing persistent RDDs -- getPersistentRDDs Method [source, scala] \u00b6 getPersistentRDDs: Map[Int, RDD[_]] \u00b6 getPersistentRDDs returns the collection of RDDs that have marked themselves as persistent via spark-rdd-caching.md#cache[cache]. Internally, getPersistentRDDs returns < > internal registry. == [[cancelJob]] Cancelling Job -- cancelJob Method [source, scala] \u00b6 cancelJob(jobId: Int) \u00b6 cancelJob requests DAGScheduler scheduler:DAGScheduler.md#cancelJob[to cancel a Spark job]. == [[cancelStage]] Cancelling Stage -- cancelStage Methods [source, scala] \u00b6 cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit cancelStage simply requests DAGScheduler scheduler:DAGScheduler.md#cancelJob[to cancel a Spark stage] (with an optional reason ). NOTE: cancelStage is used when StagesTab spark-webui-StagesTab.md#handleKillRequest[handles a kill request] (from a user in web UI). Programmable Dynamic Allocation \u00b6 SparkContext offers the following methods as the developer API for Dynamic Allocation of Executors : < > < > < > (private!) < > === [[requestExecutors]] Requesting New Executors -- requestExecutors Method [source, scala] \u00b6 requestExecutors(numAdditionalExecutors: Int): Boolean \u00b6 requestExecutors requests numAdditionalExecutors executors from scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend]. === [[killExecutors]] Requesting to Kill Executors -- killExecutors Method [source, scala] \u00b6 killExecutors(executorIds: Seq[String]): Boolean \u00b6 CAUTION: FIXME === [[requestTotalExecutors]] Requesting Total Executors -- requestTotalExecutors Method [source, scala] \u00b6 requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a private[spark] method that scheduler:CoarseGrainedSchedulerBackend.md#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend]. NOTE: It works for scheduler:CoarseGrainedSchedulerBackend.md[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: Requesting executors is only supported in coarse-grained mode Executor IDs \u00b6 getExecutorIds is a private[spark] method that is part of ExecutorAllocationClient contract . It simply passes the call on to the current coarse-grained scheduler backend, i.e. calls getExecutorIds . Important It works for coarse-grained scheduler backends only. When called for other scheduler backends you should see the following WARN message in the logs: Requesting executors is only supported in coarse-grained mode CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!) === [[getOrCreate]] Getting Existing or Creating New SparkContext -- getOrCreate Methods [source, scala] \u00b6 getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext getOrCreate methods allow you to get the existing SparkContext or create a new one. [source, scala] \u00b6 import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate() // Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") val sc = SparkContext.getOrCreate(conf) The no-param getOrCreate method requires that the two mandatory Spark settings - < > and < > - are specified using spark-submit.md[spark-submit]. === [[constructors]] Constructors [source, scala] \u00b6 SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) You can create a SparkContext instance using the four constructors. [source, scala] \u00b6 import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") import org.apache.spark.SparkContext val sc = new SparkContext(conf) When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services): Running Spark version 2.0.0-SNAPSHOT NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores). == [[env]] Accessing Current SparkEnv -- env Method CAUTION: FIXME == [[getConf]] Getting Current SparkConf -- getConf Method [source, scala] \u00b6 getConf: SparkConf \u00b6 getConf returns the current SparkConf.md[SparkConf]. NOTE: Changing the SparkConf object does not change the current configuration (as the method returns a copy). == [[master]][[master-url]] Deployment Environment -- master Method [source, scala] \u00b6 master: String \u00b6 master method returns the current value of configuration-properties.md#spark.master[spark.master] which is the spark-deployment-environments.md[deployment environment] in use. == [[appName]] Application Name -- appName Method [source, scala] \u00b6 appName: String \u00b6 appName gives the value of the mandatory SparkConf.md#spark.app.name[spark.app.name] setting. NOTE: appName is used when spark-standalone.md#SparkDeploySchedulerBackend[ SparkDeploySchedulerBackend starts], spark-webui-SparkUI.md#createLiveUI[ SparkUI creates a web UI], when postApplicationStart is executed, and for Mesos and checkpointing in Spark Streaming. == [[applicationAttemptId]] Unique Identifier of Execution Attempt -- applicationAttemptId Method [source, scala] \u00b6 applicationAttemptId: Option[String] \u00b6 applicationAttemptId gives the unique identifier of the execution attempt of a Spark application. [NOTE] \u00b6 applicationAttemptId is used when: scheduler:ShuffleMapTask.md#creating-instance[ShuffleMapTask] and scheduler:ResultTask.md#creating-instance[ResultTask] are created * SparkContext < > \u00b6 == [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- getExecutorStorageStatus Method [source, scala] \u00b6 getExecutorStorageStatus: Array[StorageStatus] \u00b6 getExecutorStorageStatus storage:BlockManagerMaster.md#getStorageStatus[requests BlockManagerMaster for storage status] (of all storage:BlockManager.md[BlockManagers]). NOTE: getExecutorStorageStatus is a developer API. getExecutorStorageStatus is used when: SparkContext is requested for storage status of cached RDDs SparkStatusTracker is requested for known executors == [[deployMode]] Deploy Mode -- deployMode Method [source,scala] \u00b6 deployMode: String \u00b6 deployMode returns the current value of spark-deploy-mode.md[spark.submit.deployMode] setting or client if not set. == [[getSchedulingMode]] Scheduling Mode -- getSchedulingMode Method [source, scala] \u00b6 getSchedulingMode: SchedulingMode.SchedulingMode \u00b6 getSchedulingMode returns the current spark-scheduler-SchedulingMode.md[Scheduling Mode]. == [[getPoolForName]] Schedulable (Pool) by Name -- getPoolForName Method [source, scala] \u00b6 getPoolForName(pool: String): Option[Schedulable] \u00b6 getPoolForName returns a spark-scheduler-Schedulable.md[Schedulable] by the pool name, if one exists. NOTE: getPoolForName is part of the Developer's API and may change in the future. Internally, it requests the scheduler:TaskScheduler.md#rootPool[TaskScheduler for the root pool] and spark-scheduler-Pool.md#schedulableNameToSchedulable[looks up the Schedulable by the pool name]. It is exclusively used to spark-webui-PoolPage.md[show pool details in web UI (for a stage)]. == [[getAllPools]] All Schedulable Pools -- getAllPools Method [source, scala] \u00b6 getAllPools: Seq[Schedulable] \u00b6 getAllPools collects the spark-scheduler-Pool.md[Pools] in scheduler:TaskScheduler.md#contract[TaskScheduler.rootPool]. NOTE: TaskScheduler.rootPool is part of the scheduler:TaskScheduler.md#contract[TaskScheduler Contract]. NOTE: getAllPools is part of the Developer's API. CAUTION: FIXME Where is the method used? NOTE: getAllPools is used to calculate pool names for spark-webui-AllStagesPage.md#pool-names[Stages tab in web UI] with FAIR scheduling mode used. == [[defaultParallelism]] Default Level of Parallelism [source, scala] \u00b6 defaultParallelism: Int \u00b6 defaultParallelism requests < > for the scheduler:TaskScheduler.md#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism specifies the number of spark-rdd-partitions.md[partitions] in RDDs when created without specifying them explicitly by a user. [NOTE] \u00b6 defaultParallelism is used in < >, SparkContext.range and < > (as well as Spark Streaming's DStream.countByValue and DStream.countByValueAndWindow et al.). defaultParallelism is also used to instantiate rdd:HashPartitioner.md[HashPartitioner] and for the minimum number of partitions in rdd:HadoopRDD.md[HadoopRDDs]. \u00b6 == [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- taskScheduler Property [source, scala] \u00b6 taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit taskScheduler manages (i.e. reads or writes) <<_taskScheduler, _taskScheduler>> internal property. == [[version]] Getting Spark Version -- version Property [source, scala] \u00b6 version: String \u00b6 version returns the Spark version this SparkContext uses. == [[makeRDD]] makeRDD Method CAUTION: FIXME == [[submitJob]] Submitting Jobs Asynchronously -- submitJob Method [source, scala] \u00b6 submitJob T, U, R : SimpleFutureAction[R] submitJob submits a job in an asynchronous, non-blocking way to scheduler:DAGScheduler.md#submitJob[DAGScheduler]. It cleans the processPartition input function argument and returns an instance of spark-rdd-actions.md#FutureAction[SimpleFutureAction] that holds the JobWaiter instance. CAUTION: FIXME What are resultFunc ? It is used in: spark-rdd-actions.md#AsyncRDDActions[AsyncRDDActions] methods spark-streaming/spark-streaming.md[Spark Streaming] for spark-streaming/spark-streaming-receivertracker.md#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] == [[spark-configuration]] Spark Configuration CAUTION: FIXME == [[sparkcontext-and-rdd]] SparkContext and RDDs You use a Spark context to create RDDs (see < >). When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts. .A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png) == [[creating-rdds]][[parallelize]] Creating RDD -- parallelize Method SparkContext allows you to create many different RDDs from input sources like: Scala's collections, i.e. sc.parallelize(0 to 100) local or remote filesystems, i.e. sc.textFile(\"README.md\") Any Hadoop InputSource using sc.newAPIHadoopFile Read rdd:index.md#creating-rdds[Creating RDDs] in rdd:index.md[RDD - Resilient Distributed Dataset]. == [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- unpersist Method CAUTION: FIXME unpersist removes an RDD from the master's storage:BlockManager.md[Block Manager] (calls removeRdd(rddId: Int, blocking: Boolean) ) and the internal < > mapping. It finally posts SparkListener.md#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to listenerBus . == [[setCheckpointDir]] Setting Checkpoint Directory -- setCheckpointDir Method [source, scala] \u00b6 setCheckpointDir(directory: String) \u00b6 setCheckpointDir method is used to set up the checkpoint directory...FIXME CAUTION: FIXME == [[register]] Registering Accumulator -- register Methods [source, scala] \u00b6 register(acc: AccumulatorV2[ , _]): Unit register(acc: AccumulatorV2[ , _], name: String): Unit register registers the acc accumulator . You can optionally give an accumulator a name . TIP: You can create built-in accumulators for longs, doubles, and collection types using < >. Internally, register registers acc accumulator (with the current SparkContext). == [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators [source, scala] \u00b6 longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator T : CollectionAccumulator[T] You can use longAccumulator , doubleAccumulator or collectionAccumulator to create and register accumulators for simple and collection values. longAccumulator returns LongAccumulator with the zero value 0 . doubleAccumulator returns DoubleAccumulator with the zero value 0.0 . collectionAccumulator returns CollectionAccumulator with the zero value java.util.List[T] . scala> val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0) scala> val counter = sc.longAccumulator(\"counter\") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0) scala> counter.value res0: Long = 0 scala> sc.parallelize(0 to 9).foreach(n => counter.add(n)) scala> counter.value res3: Long = 45 The name input parameter allows you to give a name to an accumulator and have it displayed in spark-webui-StagePage.md#accumulators[Spark UI] (under Stages tab for a given stage). Tip You can register custom accumulators using register methods. == [[broadcast]] Creating Broadcast Variable -- broadcast Method [source, scala] \u00b6 broadcast T : Broadcast[T] broadcast method creates a Broadcast.md[]. It is a shared memory with value (as broadcast blocks) on the driver and later on all Spark executors. [source,plaintext] \u00b6 val sc: SparkContext = ??? scala> val hello = sc.broadcast(\"hello\") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0) Spark transfers the value to Spark executors once , and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png) Internally, broadcast requests BroadcastManager for a core:BroadcastManager.md#newBroadcast[new broadcast variable]. NOTE: The current BroadcastManager is available using core:SparkEnv.md#broadcastManager[ SparkEnv.broadcastManager ] attribute and is always core:BroadcastManager.md[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors). You should see the following INFO message in the logs: Created broadcast [id] from [callSite] If ContextCleaner is defined, the core:ContextCleaner.md#[new broadcast variable is registered for cleanup]. [NOTE] \u00b6 Spark does not support broadcasting RDDs. scala> sc.broadcast(sc.range(0, 10)) java.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result. at scala.Predef$.require(Predef.scala:224) at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392) ... 48 elided \u00b6 Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI. == [[jars]] Distribute JARs to workers The jar you specify with SparkContext.addJar will be copied to all the worker nodes. The configuration setting spark.jars is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. scala> sc.addJar(\"build.sbt\") 15/11/11 21:54:54 Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457 CAUTION: FIXME Why is HttpFileServer used for addJar? === SparkContext as Application-Wide Counter SparkContext keeps track of: [[nextShuffleId]] * shuffle ids using nextShuffleId internal counter for scheduler:ShuffleMapStage.md[registering shuffle dependencies] to shuffle:ShuffleManager.md[Shuffle Service]. == [[stop]][[stopping]] Stopping SparkContext -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the SparkContext. Internally, stop enables stopped internal flag. If already stopped, you should see the following INFO message in the logs: SparkContext already stopped. stop then does the following: Removes _shutdownHookRef from ShutdownHookManager < SparkListenerApplicationEnd >> (to < >) spark-webui-SparkUI.md#stop[Stops web UI] Requests MetricSystem to report metrics (from all registered sinks) core:ContextCleaner.md#stop[Stops ContextCleaner ] Requests ExecutorAllocationManager to stop If LiveListenerBus was started, scheduler:LiveListenerBus.md#stop[requests LiveListenerBus to stop] Requests spark-history-server:EventLoggingListener.md#stop[ EventLoggingListener to stop] Requests scheduler:DAGScheduler.md#stop[ DAGScheduler to stop] Requests rpc:index.md#stop[RpcEnv to stop HeartbeatReceiver endpoint] Requests ConsoleProgressBar to stop Clears the reference to TaskScheduler , i.e. _taskScheduler is null Requests core:SparkEnv.md#stop[ SparkEnv to stop] and clears SparkEnv Clears yarn/spark-yarn-client.md#SPARK_YARN_MODE[ SPARK_YARN_MODE flag] < > Ultimately, you should see the following INFO message in the logs: Successfully stopped SparkContext Registering SparkListener \u00b6 addSparkListener ( listener : SparkListenerInterface ): Unit addSparkListener registers a custom SparkListenerInterface . Note Custom listeners can also be registered declaratively using spark.extraListeners configuration property. == [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler By default, SparkContext uses ( private[spark] class) org.apache.spark.scheduler.DAGScheduler , but you can develop your own custom DAGScheduler implementation, and use ( private[spark] ) SparkContext.dagScheduler_=(ds: DAGScheduler) method to assign yours. It is also applicable to SchedulerBackend and TaskScheduler using schedulerBackend_=(sb: SchedulerBackend) and taskScheduler_=(ts: TaskScheduler) methods, respectively. CAUTION: FIXME Make it an advanced exercise. == [[events]] Events When a Spark context starts, it triggers SparkListener.md#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and SparkListener.md#SparkListenerApplicationStart[SparkListenerApplicationStart] messages. Refer to the section < >. == [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- setLogLevel Method [source, scala] \u00b6 setLogLevel(logLevel: String) \u00b6 setLogLevel allows you to set the root logging level in a Spark application, e.g. spark-shell.md[Spark shell]. Internally, setLogLevel calls ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel )] that it then uses to set using ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level )]. [TIP] \u00b6 You can directly set the logging level using ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger ()]. [source, scala] \u00b6 LogManager.getLogger(\"org\").setLevel(Level.OFF) \u00b6 ==== == [[hadoopConfiguration]] Hadoop Configuration While a < >, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration ] that is available as _hadoopConfiguration ). NOTE: spark-SparkHadoopUtil.md#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used. If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default Configuration object is returned. If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are both available, the following settings are set for the Hadoop configuration: fs.s3.awsAccessKeyId , fs.s3n.awsAccessKeyId , fs.s3a.access.key are set to the value of AWS_ACCESS_KEY_ID fs.s3.awsSecretAccessKey , fs.s3n.awsSecretAccessKey , and fs.s3a.secret.key are set to the value of AWS_SECRET_ACCESS_KEY Every spark.hadoop. setting becomes a setting of the configuration with the prefix spark.hadoop. removed for the key. The value of spark.buffer.size (default: 65536 ) is used as the value of io.file.buffer.size . == [[listenerBus]] listenerBus -- LiveListenerBus Event Bus listenerBus is a scheduler:LiveListenerBus.md[] object that acts as a mechanism to announce events to other services on the spark-driver.md[driver]. LiveListenerBus is created and started when SparkContext is created and, since it is a single-JVM event bus, is exclusively used on the driver. == [[startTime]] Time when SparkContext was Created -- startTime Property [source, scala] \u00b6 startTime: Long \u00b6 startTime is the time in milliseconds when < >. [source, scala] \u00b6 scala> sc.startTime res0: Long = 1464425605653 == [[submitMapStage]] Submitting ShuffleDependency for Execution -- submitMapStage Internal Method [source, scala] \u00b6 submitMapStage K, V, C : SimpleFutureAction[MapOutputStatistics] submitMapStage scheduler:DAGScheduler.md#submitMapStage[submits the input ShuffleDependency to DAGScheduler for execution] and returns a SimpleFutureAction . Internally, submitMapStage < > first and submits it with localProperties . NOTE: Interestingly, submitMapStage is used exclusively when Spark SQL's spark-sql-SparkPlan-ShuffleExchange.md[ShuffleExchange] physical operator is executed. NOTE: submitMapStage seems related to scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. == [[cancelJobGroup]] Cancelling Job Group -- cancelJobGroup Method [source, scala] \u00b6 cancelJobGroup(groupId: String) \u00b6 cancelJobGroup requests DAGScheduler scheduler:DAGScheduler.md#cancelJobGroup[to cancel a group of active Spark jobs]. NOTE: cancelJobGroup is used exclusively when SparkExecuteStatementOperation does cancel . == [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- cancelAllJobs Method CAUTION: FIXME NOTE: cancelAllJobs is used when spark-shell.md[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or SparkSQLCLIDriver is terminated. == [[cleaner]] ContextCleaner [source, scala] \u00b6 cleaner: Option[ContextCleaner] \u00b6 SparkContext may have a core:ContextCleaner.md[ContextCleaner] defined. ContextCleaner is created when SparkContext is created with configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition [source, scala] \u00b6 getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs simply scheduler:DAGScheduler.md#getPreferredLocs[requests DAGScheduler for the preferred locations for partition ]. NOTE: Preferred locations of a partition of a RDD are also called placement preferences or locality preferences . getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD. == [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- persistRDD Internal Method [source, scala] \u00b6 persistRDD(rdd: RDD[_]): Unit \u00b6 persistRDD registers rdd in < > internal registry. NOTE: persistRDD is used exclusively when RDD is rdd:index.md#persist-internal[persisted or locally checkpointed]. == [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- getRDDStorageInfo Methods [source, scala] \u00b6 getRDDStorageInfo: Array[RDDInfo] // <1> getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] // <2> <1> Part of Spark's Developer API that uses <2> filtering no RDDs getRDDStorageInfo takes all the RDDs (from < > registry) that match filter and creates a collection of storage:RDDInfo.md[RDDInfo] instances. getRDDStorageInfo ...FIXME In the end, getRDDStorageInfo gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than 0 ). NOTE: getRDDStorageInfo is used when RDD spark-rdd-lineage.md#toDebugString[is requested for RDD lineage graph]. == [[statusStore]] Accessing AppStatusStore [source, scala] \u00b6 statusStore: AppStatusStore \u00b6 statusStore gives the current core:AppStatusStore.md[]. statusStore is used when: SparkContext is requested to < > ConsoleProgressBar is requested to refresh SharedState (Spark SQL) is requested for a SQLAppStatusStore == [[uiWebUrl]] Requesting URL of web UI -- uiWebUrl Method [source, scala] \u00b6 uiWebUrl: Option[String] \u00b6 uiWebUrl requests the SparkUI for webUrl . == [[maxNumConcurrentTasks]] maxNumConcurrentTasks Method [source, scala] \u00b6 maxNumConcurrentTasks(): Int \u00b6 maxNumConcurrentTasks simply requests the < > for the scheduler:SchedulerBackend.md#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently]. NOTE: maxNumConcurrentTasks is used exclusively when DAGScheduler is requested to scheduler:DAGScheduler.md#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots]. == [[environment-variables]] Environment Variables .Environment Variables [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY | 1024 | Amount of memory to allocate for a Spark executor in MB. See executor:Executor.md#memory[Executor Memory]. [[SPARK_USER]] SPARK_USER The user who is running SparkContext. Available later as < >. === == [[addJar-internals]] addJar Method [source, scala] \u00b6 addJar(path: String): Unit \u00b6 addJar ...FIXME NOTE: addJar is used when...FIXME == [[runApproximateJob]] Running Approximate Job [source, scala] \u00b6 runApproximateJob T, U, R : PartialResult[R] runApproximateJob...FIXME runApproximateJob is used when: DoubleRDDFunctions is requested to meanApprox and sumApprox RDD is requested to countApprox and countByValueApprox == [[killTaskAttempt]] Killing Task [source, scala] \u00b6 killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean killTaskAttempt requests the < > to scheduler:DAGScheduler.md#killTaskAttempt[kill a task]. == [[checkpointFile]] checkpointFile Internal Method [source, scala] \u00b6 checkpointFile T: ClassTag : RDD[T] checkpointFile...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.SparkContext=ALL \u00b6 Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[checkpointDir]] Checkpoint Directory [source,scala] \u00b6 checkpointDir: Option[String] = None \u00b6 checkpointDir is...FIXME === [[persistentRdds]] persistentRdds Lookup Table Lookup table of persistent/cached RDDs per their ids. Used when SparkContext is requested to: < > < > < > < > Creating SparkEnv for Driver \u00b6 createSparkEnv ( conf : SparkConf , isLocal : Boolean , listenerBus : LiveListenerBus ): SparkEnv createSparkEnv uses the SparkEnv utility to create a SparkEnv for the driver (with the arguments and numDriverCores ). numDriverCores \u00b6 numDriverCores ( master : String , conf : SparkConf ): Int numDriverCores ...FIXME","title":"SparkContext"},{"location":"SparkContext/#sparkcontext","text":"SparkContext is the entry point to all of the components of Apache Spark (execution engine) and so the heart of a Spark application. In fact, you can consider an application a Spark application only when it uses a SparkContext (directly or indirectly). Important There should be one active SparkContext per JVM and Spark developers should use SparkContext.getOrCreate utility for sharing it (e.g. across threads).","title":"SparkContext"},{"location":"SparkContext/#creating-instance","text":"SparkContext takes the following to be created: SparkConf SparkContext is created (directly or indirectly using getOrCreate utility). While being created, SparkContext sets up core services and establishes a connection to a Spark execution environment .","title":"Creating Instance"},{"location":"SparkContext/#local-properties","text":"localProperties : InheritableThreadLocal [ Properties ] SparkContext uses an InheritableThreadLocal ( Java ) of key-value pairs of thread-local properties to pass extra information from a parent thread (on the driver) to child threads. localProperties is meant to be used by developers using SparkContext.setLocalProperty and SparkContext.getLocalProperty . Local Properties are available using TaskContext.getLocalProperty . Local Properties are available to SparkListener s using the following events: SparkListenerJobStart SparkListenerStageSubmitted localProperties are passed down when SparkContext is requested for the following: Running Job (that in turn makes the local properties available to the DAGScheduler to run a job ) Running Approximate Job Submitting Job Submitting MapStage DAGScheduler passes down local properties when scheduling: ShuffleMapTask s ResultTask s TaskSet s Spark (Core) defines the following local properties. Name Default Value Setter callSite.long callSite.short SparkContext.setCallSite spark.job.description callSite.short SparkContext.setJobDescription ( SparkContext.setJobGroup ) spark.job.interruptOnCancel SparkContext.setJobGroup spark.jobGroup.id SparkContext.setJobGroup spark.scheduler.pool","title":" Local Properties"},{"location":"SparkContext/#services","text":"AppStatusStore ExecutorAllocationManager (optional) SchedulerBackend others","title":"Services"},{"location":"SparkContext/#shuffledrivercomponents","text":"SparkContext creates a ShuffleDriverComponents when created . SparkContext loads the ShuffleDataIO that is in turn requested for the ShuffleDriverComponents . SparkContext requests the ShuffleDriverComponents to initialize . The ShuffleDriverComponents is used when: ShuffleDependency is created SparkContext creates the ContextCleaner (if enabled) SparkContext requests the ShuffleDriverComponents to clean up when stopping .","title":" ShuffleDriverComponents"},{"location":"SparkContext/#static-files","text":"","title":"Static Files"},{"location":"SparkContext/#addfile","text":"addFile ( path : String , recursive : Boolean ): Unit // recursive = false addFile ( path : String ): Unit addFile creates a Hadoop Path from the given path . For a no-schema path, addFile converts it to a canonical form. addFile prints out the following WARN message to the logs and exits. File with 'local' scheme is not supported to add to file server, since it is already available on every node. addFile ...FIXME In the end, addFile adds the file to the addedFiles internal registry (with the current timestamp): For new files, addFile prints out the following INFO message to the logs, fetches the file (to the root directory and without using the cache) and postEnvironmentUpdate . Added file [path] at [key] with timestamp [timestamp] For files that were already added, addFile prints out the following WARN message to the logs: The path [path] has been added already. Overwriting of added paths is not supported in the current version. addFile is used when: SparkContext is created","title":" addFile"},{"location":"SparkContext/#listfiles","text":"listFiles (): Seq [ String ] listFiles is the files added .","title":" listFiles"},{"location":"SparkContext/#addedfiles-internal-registry","text":"addedFiles : Map [ String , Long ] addedFiles is a collection of static files by the timestamp the were added at. addedFiles is used when: SparkContext is requested to postEnvironmentUpdate and listFiles TaskSetManager is created (and resourceOffer )","title":" addedFiles Internal Registry"},{"location":"SparkContext/#files","text":"files : Seq [ String ] files is a collection of file paths defined by spark.files configuration property.","title":" files"},{"location":"SparkContext/#posting-sparklistenerenvironmentupdate-event","text":"postEnvironmentUpdate (): Unit postEnvironmentUpdate ...FIXME postEnvironmentUpdate is used when: SparkContext is requested to addFile and addJar","title":" Posting SparkListenerEnvironmentUpdate Event"},{"location":"SparkContext/#getorcreate-utility","text":"getOrCreate (): SparkContext getOrCreate ( config : SparkConf ): SparkContext getOrCreate ...FIXME","title":" getOrCreate Utility"},{"location":"SparkContext/#plugincontainer","text":"SparkContext creates a PluginContainer when created . PluginContainer is created (for the driver where SparkContext lives) using PluginContainer.apply utility. PluginContainer is then requested to registerMetrics with the applicationId . PluginContainer is requested to shutdown when SparkContext is requested to stop .","title":" PluginContainer"},{"location":"SparkContext/#creating-schedulerbackend-and-taskscheduler","text":"createTaskScheduler ( sc : SparkContext , master : String , deployMode : String ): ( SchedulerBackend , TaskScheduler ) createTaskScheduler creates a SchedulerBackend and a TaskScheduler for the given master URL and deployment mode. Internally, createTaskScheduler branches off per the given master URL ( master URL ) to select the requested implementations. createTaskScheduler accepts the following master URLs: local - local mode with 1 thread only local[n] or local[*] - local mode with n threads local[n, m] or local[*, m] -- local mode with n threads and m number of failures spark://hostname:port for Spark Standalone local-cluster[n, m, z] -- local cluster with n workers, m cores per worker, and z memory per worker Other URLs are simply handed over to getClusterManager to load an external cluster manager if available createTaskScheduler is used when SparkContext is created .","title":" Creating SchedulerBackend and TaskScheduler"},{"location":"SparkContext/#loading-externalclustermanager","text":"getClusterManager ( url : String ): Option [ ExternalClusterManager ] getClusterManager uses Java's ServiceLoader to find and load an ExternalClusterManager that supports the given master URL. ExternalClusterManager Service Discovery For ServiceLoader to find ExternalClusterManager s, they have to be registered using the following file: META-INF/services/org.apache.spark.scheduler.ExternalClusterManager getClusterManager throws a SparkException when multiple cluster managers were found: Multiple external cluster managers registered for the url [url]: [serviceLoaders] getClusterManager is used when SparkContext is requested for a SchedulerBackend and TaskScheduler .","title":" Loading ExternalClusterManager"},{"location":"SparkContext/#running-job-synchronously","text":"runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U ): Array [ U ] runJob [ T , U : ClassTag ]( rdd : RDD [ T ], processPartition : ( TaskContext , Iterator [ T ]) => U , resultHandler : ( Int , U ) => Unit ): Unit runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ]): Array [ U ] runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], resultHandler : ( Int , U ) => Unit ): Unit runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : Iterator [ T ] => U ): Array [ U ] runJob [ T , U : ClassTag ]( rdd : RDD [ T ], processPartition : Iterator [ T ] => U , resultHandler : ( Int , U ) => Unit ): Unit runJob [ T , U : ClassTag ]( rdd : RDD [ T ], func : Iterator [ T ] => U , partitions : Seq [ Int ]): Array [ U ] runJob finds the call site and cleans up the given func function. runJob prints out the following INFO message to the logs: Starting job: [callSite] With spark.logLineage enabled, runJob requests the given RDD for the recursive dependencies and prints out the following INFO message to the logs: RDD's recursive dependencies: [toDebugString] runJob requests the DAGScheduler to run a job . runJob requests the ConsoleProgressBar to finishAll if defined. In the end, runJob requests the given RDD to doCheckpoint . runJob throws an IllegalStateException when SparkContext is stopped : SparkContext has been shutdown","title":" Running Job Synchronously"},{"location":"SparkContext/#demo","text":"runJob is essentially executing a func function on all or a subset of partitions of an RDD and returning the result as an array (with elements being the results per partition). sc . setLocalProperty ( \"callSite.short\" , \"runJob Demo\" ) val partitionsNumber = 4 val rdd = sc . parallelize ( Seq ( \"hello world\" , \"nice to see you\" ), numSlices = partitionsNumber ) import org . apache . spark . TaskContext val func = ( t : TaskContext , ss : Iterator [ String ]) => 1 val result = sc . runJob ( rdd , func ) assert ( result . length == partitionsNumber ) sc . clearCallSite ()","title":" Demo"},{"location":"SparkContext/#call-site","text":"getCallSite (): CallSite getCallSite ...FIXME getCallSite is used when: SparkContext is requested to broadcast , runJob , runApproximateJob , submitJob and submitMapStage AsyncRDDActions is requested to takeAsync RDD is created","title":" Call Site"},{"location":"SparkContext/#closure-cleaning","text":"clean ( f : F , checkSerializable : Boolean = true ): F clean cleans up the given f closure (using ClosureCleaner.clean utility). Tip Enable DEBUG logging level for org.apache.spark.util.ClosureCleaner logger to see what happens inside the class. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG Refer to Logging . With DEBUG logging level you should see the following messages in the logs: +++ Cleaning closure [func] ([func.getClass.getName]) +++ + declared fields: [declaredFields.size] [field] ... +++ closure [func] ([func.getClass.getName]) is now cleaned +++","title":" Closure Cleaning"},{"location":"SparkContext/#logging","text":"Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.SparkContext=ALL Refer to Logging .","title":"Logging"},{"location":"SparkContext/#to-be-reviewed","text":"SparkContext offers the following functions: Getting current status of a Spark application ** < > ** < > ** < > ** < > ** < > ** < > ** < > that specifies the number of spark-rdd-partitions.md[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** < > ** < > ** < > ** < > ** < > Setting Configuration ** < > ** Local Properties ** < > ** < > Creating Distributed Entities ** < > ** < > ** < > Accessing services, e.g. < >, < >, scheduler:LiveListenerBus.md[], storage:BlockManager.md[BlockManager], scheduler:SchedulerBackend.md[SchedulerBackends], shuffle:ShuffleManager.md[ShuffleManager] and the < >. < > < > < > < > < > < > < > < > < > < > TIP: Read the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ].","title":"To Be Reviewed"},{"location":"SparkContext/#removing-rdd-blocks-from-blockmanagermaster","text":"unpersistRDD ( rddId : Int , blocking : Boolean = true ): Unit unpersistRDD requests BlockManagerMaster to storage:BlockManagerMaster.md#removeRdd[remove the blocks for the RDD] (given rddId ). NOTE: unpersistRDD uses SparkEnv core:SparkEnv.md#blockManager[to access the current BlockManager ] that is in turn used to storage:BlockManager.md#master[access the current BlockManagerMaster ]. unpersistRDD removes rddId from < > registry. In the end, unpersistRDD posts a SparkListener.md#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with rddId ) to < >.","title":" Removing RDD Blocks from BlockManagerMaster"},{"location":"SparkContext/#note","text":"unpersistRDD is used when: ContextCleaner does core:ContextCleaner.md#doCleanupRDD[doCleanupRDD]","title":"[NOTE]"},{"location":"SparkContext/#sparkcontext-ie-marks-an-rdd-as-non-persistent","text":"== [[applicationId]] Unique Identifier of Spark Application -- applicationId Method CAUTION: FIXME == [[postApplicationStart]] postApplicationStart Internal Method","title":"SparkContext &lt;&gt; (i.e. marks an RDD as non-persistent)"},{"location":"SparkContext/#source-scala","text":"","title":"[source, scala]"},{"location":"SparkContext/#postapplicationstart-unit","text":"postApplicationStart ...FIXME postApplicationStart is used exclusively when SparkContext is created. == [[postApplicationEnd]] postApplicationEnd Method CAUTION: FIXME == [[clearActiveContext]] clearActiveContext Method CAUTION: FIXME == [[getPersistentRDDs]] Accessing persistent RDDs -- getPersistentRDDs Method","title":"postApplicationStart(): Unit"},{"location":"SparkContext/#source-scala_1","text":"","title":"[source, scala]"},{"location":"SparkContext/#getpersistentrdds-mapint-rdd_","text":"getPersistentRDDs returns the collection of RDDs that have marked themselves as persistent via spark-rdd-caching.md#cache[cache]. Internally, getPersistentRDDs returns < > internal registry. == [[cancelJob]] Cancelling Job -- cancelJob Method","title":"getPersistentRDDs: Map[Int, RDD[_]]"},{"location":"SparkContext/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkContext/#canceljobjobid-int","text":"cancelJob requests DAGScheduler scheduler:DAGScheduler.md#cancelJob[to cancel a Spark job]. == [[cancelStage]] Cancelling Stage -- cancelStage Methods","title":"cancelJob(jobId: Int)"},{"location":"SparkContext/#source-scala_3","text":"cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit cancelStage simply requests DAGScheduler scheduler:DAGScheduler.md#cancelJob[to cancel a Spark stage] (with an optional reason ). NOTE: cancelStage is used when StagesTab spark-webui-StagesTab.md#handleKillRequest[handles a kill request] (from a user in web UI).","title":"[source, scala]"},{"location":"SparkContext/#programmable-dynamic-allocation","text":"SparkContext offers the following methods as the developer API for Dynamic Allocation of Executors : < > < > < > (private!) < > === [[requestExecutors]] Requesting New Executors -- requestExecutors Method","title":" Programmable Dynamic Allocation"},{"location":"SparkContext/#source-scala_4","text":"","title":"[source, scala]"},{"location":"SparkContext/#requestexecutorsnumadditionalexecutors-int-boolean","text":"requestExecutors requests numAdditionalExecutors executors from scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend]. === [[killExecutors]] Requesting to Kill Executors -- killExecutors Method","title":"requestExecutors(numAdditionalExecutors: Int): Boolean"},{"location":"SparkContext/#source-scala_5","text":"","title":"[source, scala]"},{"location":"SparkContext/#killexecutorsexecutorids-seqstring-boolean","text":"CAUTION: FIXME === [[requestTotalExecutors]] Requesting Total Executors -- requestTotalExecutors Method","title":"killExecutors(executorIds: Seq[String]): Boolean"},{"location":"SparkContext/#source-scala_6","text":"requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a private[spark] method that scheduler:CoarseGrainedSchedulerBackend.md#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend]. NOTE: It works for scheduler:CoarseGrainedSchedulerBackend.md[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: Requesting executors is only supported in coarse-grained mode","title":"[source, scala]"},{"location":"SparkContext/#executor-ids","text":"getExecutorIds is a private[spark] method that is part of ExecutorAllocationClient contract . It simply passes the call on to the current coarse-grained scheduler backend, i.e. calls getExecutorIds . Important It works for coarse-grained scheduler backends only. When called for other scheduler backends you should see the following WARN message in the logs: Requesting executors is only supported in coarse-grained mode CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!) === [[getOrCreate]] Getting Existing or Creating New SparkContext -- getOrCreate Methods","title":" Executor IDs"},{"location":"SparkContext/#source-scala_7","text":"getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext getOrCreate methods allow you to get the existing SparkContext or create a new one.","title":"[source, scala]"},{"location":"SparkContext/#source-scala_8","text":"import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate() // Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") val sc = SparkContext.getOrCreate(conf) The no-param getOrCreate method requires that the two mandatory Spark settings - < > and < > - are specified using spark-submit.md[spark-submit]. === [[constructors]] Constructors","title":"[source, scala]"},{"location":"SparkContext/#source-scala_9","text":"SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) You can create a SparkContext instance using the four constructors.","title":"[source, scala]"},{"location":"SparkContext/#source-scala_10","text":"import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") import org.apache.spark.SparkContext val sc = new SparkContext(conf) When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services): Running Spark version 2.0.0-SNAPSHOT NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores). == [[env]] Accessing Current SparkEnv -- env Method CAUTION: FIXME == [[getConf]] Getting Current SparkConf -- getConf Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_11","text":"","title":"[source, scala]"},{"location":"SparkContext/#getconf-sparkconf","text":"getConf returns the current SparkConf.md[SparkConf]. NOTE: Changing the SparkConf object does not change the current configuration (as the method returns a copy). == [[master]][[master-url]] Deployment Environment -- master Method","title":"getConf: SparkConf"},{"location":"SparkContext/#source-scala_12","text":"","title":"[source, scala]"},{"location":"SparkContext/#master-string","text":"master method returns the current value of configuration-properties.md#spark.master[spark.master] which is the spark-deployment-environments.md[deployment environment] in use. == [[appName]] Application Name -- appName Method","title":"master: String"},{"location":"SparkContext/#source-scala_13","text":"","title":"[source, scala]"},{"location":"SparkContext/#appname-string","text":"appName gives the value of the mandatory SparkConf.md#spark.app.name[spark.app.name] setting. NOTE: appName is used when spark-standalone.md#SparkDeploySchedulerBackend[ SparkDeploySchedulerBackend starts], spark-webui-SparkUI.md#createLiveUI[ SparkUI creates a web UI], when postApplicationStart is executed, and for Mesos and checkpointing in Spark Streaming. == [[applicationAttemptId]] Unique Identifier of Execution Attempt -- applicationAttemptId Method","title":"appName: String"},{"location":"SparkContext/#source-scala_14","text":"","title":"[source, scala]"},{"location":"SparkContext/#applicationattemptid-optionstring","text":"applicationAttemptId gives the unique identifier of the execution attempt of a Spark application.","title":"applicationAttemptId: Option[String]"},{"location":"SparkContext/#note_1","text":"applicationAttemptId is used when: scheduler:ShuffleMapTask.md#creating-instance[ShuffleMapTask] and scheduler:ResultTask.md#creating-instance[ResultTask] are created","title":"[NOTE]"},{"location":"SparkContext/#sparkcontext_1","text":"== [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- getExecutorStorageStatus Method","title":"* SparkContext &lt;&gt;"},{"location":"SparkContext/#source-scala_15","text":"","title":"[source, scala]"},{"location":"SparkContext/#getexecutorstoragestatus-arraystoragestatus","text":"getExecutorStorageStatus storage:BlockManagerMaster.md#getStorageStatus[requests BlockManagerMaster for storage status] (of all storage:BlockManager.md[BlockManagers]). NOTE: getExecutorStorageStatus is a developer API. getExecutorStorageStatus is used when: SparkContext is requested for storage status of cached RDDs SparkStatusTracker is requested for known executors == [[deployMode]] Deploy Mode -- deployMode Method","title":"getExecutorStorageStatus: Array[StorageStatus]"},{"location":"SparkContext/#sourcescala","text":"","title":"[source,scala]"},{"location":"SparkContext/#deploymode-string","text":"deployMode returns the current value of spark-deploy-mode.md[spark.submit.deployMode] setting or client if not set. == [[getSchedulingMode]] Scheduling Mode -- getSchedulingMode Method","title":"deployMode: String"},{"location":"SparkContext/#source-scala_16","text":"","title":"[source, scala]"},{"location":"SparkContext/#getschedulingmode-schedulingmodeschedulingmode","text":"getSchedulingMode returns the current spark-scheduler-SchedulingMode.md[Scheduling Mode]. == [[getPoolForName]] Schedulable (Pool) by Name -- getPoolForName Method","title":"getSchedulingMode: SchedulingMode.SchedulingMode"},{"location":"SparkContext/#source-scala_17","text":"","title":"[source, scala]"},{"location":"SparkContext/#getpoolfornamepool-string-optionschedulable","text":"getPoolForName returns a spark-scheduler-Schedulable.md[Schedulable] by the pool name, if one exists. NOTE: getPoolForName is part of the Developer's API and may change in the future. Internally, it requests the scheduler:TaskScheduler.md#rootPool[TaskScheduler for the root pool] and spark-scheduler-Pool.md#schedulableNameToSchedulable[looks up the Schedulable by the pool name]. It is exclusively used to spark-webui-PoolPage.md[show pool details in web UI (for a stage)]. == [[getAllPools]] All Schedulable Pools -- getAllPools Method","title":"getPoolForName(pool: String): Option[Schedulable]"},{"location":"SparkContext/#source-scala_18","text":"","title":"[source, scala]"},{"location":"SparkContext/#getallpools-seqschedulable","text":"getAllPools collects the spark-scheduler-Pool.md[Pools] in scheduler:TaskScheduler.md#contract[TaskScheduler.rootPool]. NOTE: TaskScheduler.rootPool is part of the scheduler:TaskScheduler.md#contract[TaskScheduler Contract]. NOTE: getAllPools is part of the Developer's API. CAUTION: FIXME Where is the method used? NOTE: getAllPools is used to calculate pool names for spark-webui-AllStagesPage.md#pool-names[Stages tab in web UI] with FAIR scheduling mode used. == [[defaultParallelism]] Default Level of Parallelism","title":"getAllPools: Seq[Schedulable]"},{"location":"SparkContext/#source-scala_19","text":"","title":"[source, scala]"},{"location":"SparkContext/#defaultparallelism-int","text":"defaultParallelism requests < > for the scheduler:TaskScheduler.md#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism specifies the number of spark-rdd-partitions.md[partitions] in RDDs when created without specifying them explicitly by a user.","title":"defaultParallelism: Int"},{"location":"SparkContext/#note_2","text":"defaultParallelism is used in < >, SparkContext.range and < > (as well as Spark Streaming's DStream.countByValue and DStream.countByValueAndWindow et al.).","title":"[NOTE]"},{"location":"SparkContext/#defaultparallelism-is-also-used-to-instantiate-rddhashpartitionermdhashpartitioner-and-for-the-minimum-number-of-partitions-in-rddhadooprddmdhadooprdds","text":"== [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- taskScheduler Property","title":"defaultParallelism is also used to instantiate rdd:HashPartitioner.md[HashPartitioner] and for the minimum number of partitions in rdd:HadoopRDD.md[HadoopRDDs]."},{"location":"SparkContext/#source-scala_20","text":"taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit taskScheduler manages (i.e. reads or writes) <<_taskScheduler, _taskScheduler>> internal property. == [[version]] Getting Spark Version -- version Property","title":"[source, scala]"},{"location":"SparkContext/#source-scala_21","text":"","title":"[source, scala]"},{"location":"SparkContext/#version-string","text":"version returns the Spark version this SparkContext uses. == [[makeRDD]] makeRDD Method CAUTION: FIXME == [[submitJob]] Submitting Jobs Asynchronously -- submitJob Method","title":"version: String"},{"location":"SparkContext/#source-scala_22","text":"submitJob T, U, R : SimpleFutureAction[R] submitJob submits a job in an asynchronous, non-blocking way to scheduler:DAGScheduler.md#submitJob[DAGScheduler]. It cleans the processPartition input function argument and returns an instance of spark-rdd-actions.md#FutureAction[SimpleFutureAction] that holds the JobWaiter instance. CAUTION: FIXME What are resultFunc ? It is used in: spark-rdd-actions.md#AsyncRDDActions[AsyncRDDActions] methods spark-streaming/spark-streaming.md[Spark Streaming] for spark-streaming/spark-streaming-receivertracker.md#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] == [[spark-configuration]] Spark Configuration CAUTION: FIXME == [[sparkcontext-and-rdd]] SparkContext and RDDs You use a Spark context to create RDDs (see < >). When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts. .A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png) == [[creating-rdds]][[parallelize]] Creating RDD -- parallelize Method SparkContext allows you to create many different RDDs from input sources like: Scala's collections, i.e. sc.parallelize(0 to 100) local or remote filesystems, i.e. sc.textFile(\"README.md\") Any Hadoop InputSource using sc.newAPIHadoopFile Read rdd:index.md#creating-rdds[Creating RDDs] in rdd:index.md[RDD - Resilient Distributed Dataset]. == [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- unpersist Method CAUTION: FIXME unpersist removes an RDD from the master's storage:BlockManager.md[Block Manager] (calls removeRdd(rddId: Int, blocking: Boolean) ) and the internal < > mapping. It finally posts SparkListener.md#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to listenerBus . == [[setCheckpointDir]] Setting Checkpoint Directory -- setCheckpointDir Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_23","text":"","title":"[source, scala]"},{"location":"SparkContext/#setcheckpointdirdirectory-string","text":"setCheckpointDir method is used to set up the checkpoint directory...FIXME CAUTION: FIXME == [[register]] Registering Accumulator -- register Methods","title":"setCheckpointDir(directory: String)"},{"location":"SparkContext/#source-scala_24","text":"register(acc: AccumulatorV2[ , _]): Unit register(acc: AccumulatorV2[ , _], name: String): Unit register registers the acc accumulator . You can optionally give an accumulator a name . TIP: You can create built-in accumulators for longs, doubles, and collection types using < >. Internally, register registers acc accumulator (with the current SparkContext). == [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators","title":"[source, scala]"},{"location":"SparkContext/#source-scala_25","text":"longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator T : CollectionAccumulator[T] You can use longAccumulator , doubleAccumulator or collectionAccumulator to create and register accumulators for simple and collection values. longAccumulator returns LongAccumulator with the zero value 0 . doubleAccumulator returns DoubleAccumulator with the zero value 0.0 . collectionAccumulator returns CollectionAccumulator with the zero value java.util.List[T] . scala> val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0) scala> val counter = sc.longAccumulator(\"counter\") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0) scala> counter.value res0: Long = 0 scala> sc.parallelize(0 to 9).foreach(n => counter.add(n)) scala> counter.value res3: Long = 45 The name input parameter allows you to give a name to an accumulator and have it displayed in spark-webui-StagePage.md#accumulators[Spark UI] (under Stages tab for a given stage). Tip You can register custom accumulators using register methods. == [[broadcast]] Creating Broadcast Variable -- broadcast Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_26","text":"broadcast T : Broadcast[T] broadcast method creates a Broadcast.md[]. It is a shared memory with value (as broadcast blocks) on the driver and later on all Spark executors.","title":"[source, scala]"},{"location":"SparkContext/#sourceplaintext","text":"val sc: SparkContext = ??? scala> val hello = sc.broadcast(\"hello\") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0) Spark transfers the value to Spark executors once , and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png) Internally, broadcast requests BroadcastManager for a core:BroadcastManager.md#newBroadcast[new broadcast variable]. NOTE: The current BroadcastManager is available using core:SparkEnv.md#broadcastManager[ SparkEnv.broadcastManager ] attribute and is always core:BroadcastManager.md[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors). You should see the following INFO message in the logs: Created broadcast [id] from [callSite] If ContextCleaner is defined, the core:ContextCleaner.md#[new broadcast variable is registered for cleanup].","title":"[source,plaintext]"},{"location":"SparkContext/#note_3","text":"Spark does not support broadcasting RDDs.","title":"[NOTE]"},{"location":"SparkContext/#scala-scbroadcastscrange0-10-javalangillegalargumentexception-requirement-failed-can-not-directly-broadcast-rdds-instead-call-collect-and-broadcast-the-result-at-scalapredefrequirepredefscala224-at-orgapachesparksparkcontextbroadcastsparkcontextscala1392-48-elided","text":"Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI. == [[jars]] Distribute JARs to workers The jar you specify with SparkContext.addJar will be copied to all the worker nodes. The configuration setting spark.jars is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. scala> sc.addJar(\"build.sbt\") 15/11/11 21:54:54 Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457 CAUTION: FIXME Why is HttpFileServer used for addJar? === SparkContext as Application-Wide Counter SparkContext keeps track of: [[nextShuffleId]] * shuffle ids using nextShuffleId internal counter for scheduler:ShuffleMapStage.md[registering shuffle dependencies] to shuffle:ShuffleManager.md[Shuffle Service]. == [[stop]][[stopping]] Stopping SparkContext -- stop Method","title":"scala&gt; sc.broadcast(sc.range(0, 10))\njava.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\n  at scala.Predef$.require(Predef.scala:224)\n  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392)\n  ... 48 elided\n"},{"location":"SparkContext/#source-scala_27","text":"","title":"[source, scala]"},{"location":"SparkContext/#stop-unit","text":"stop stops the SparkContext. Internally, stop enables stopped internal flag. If already stopped, you should see the following INFO message in the logs: SparkContext already stopped. stop then does the following: Removes _shutdownHookRef from ShutdownHookManager < SparkListenerApplicationEnd >> (to < >) spark-webui-SparkUI.md#stop[Stops web UI] Requests MetricSystem to report metrics (from all registered sinks) core:ContextCleaner.md#stop[Stops ContextCleaner ] Requests ExecutorAllocationManager to stop If LiveListenerBus was started, scheduler:LiveListenerBus.md#stop[requests LiveListenerBus to stop] Requests spark-history-server:EventLoggingListener.md#stop[ EventLoggingListener to stop] Requests scheduler:DAGScheduler.md#stop[ DAGScheduler to stop] Requests rpc:index.md#stop[RpcEnv to stop HeartbeatReceiver endpoint] Requests ConsoleProgressBar to stop Clears the reference to TaskScheduler , i.e. _taskScheduler is null Requests core:SparkEnv.md#stop[ SparkEnv to stop] and clears SparkEnv Clears yarn/spark-yarn-client.md#SPARK_YARN_MODE[ SPARK_YARN_MODE flag] < > Ultimately, you should see the following INFO message in the logs: Successfully stopped SparkContext","title":"stop(): Unit"},{"location":"SparkContext/#registering-sparklistener","text":"addSparkListener ( listener : SparkListenerInterface ): Unit addSparkListener registers a custom SparkListenerInterface . Note Custom listeners can also be registered declaratively using spark.extraListeners configuration property. == [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler By default, SparkContext uses ( private[spark] class) org.apache.spark.scheduler.DAGScheduler , but you can develop your own custom DAGScheduler implementation, and use ( private[spark] ) SparkContext.dagScheduler_=(ds: DAGScheduler) method to assign yours. It is also applicable to SchedulerBackend and TaskScheduler using schedulerBackend_=(sb: SchedulerBackend) and taskScheduler_=(ts: TaskScheduler) methods, respectively. CAUTION: FIXME Make it an advanced exercise. == [[events]] Events When a Spark context starts, it triggers SparkListener.md#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and SparkListener.md#SparkListenerApplicationStart[SparkListenerApplicationStart] messages. Refer to the section < >. == [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- setLogLevel Method","title":" Registering SparkListener"},{"location":"SparkContext/#source-scala_28","text":"","title":"[source, scala]"},{"location":"SparkContext/#setloglevelloglevel-string","text":"setLogLevel allows you to set the root logging level in a Spark application, e.g. spark-shell.md[Spark shell]. Internally, setLogLevel calls ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel )] that it then uses to set using ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level )].","title":"setLogLevel(logLevel: String)"},{"location":"SparkContext/#tip","text":"You can directly set the logging level using ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger ()].","title":"[TIP]"},{"location":"SparkContext/#source-scala_29","text":"","title":"[source, scala]"},{"location":"SparkContext/#logmanagergetloggerorgsetlevelleveloff","text":"==== == [[hadoopConfiguration]] Hadoop Configuration While a < >, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration ] that is available as _hadoopConfiguration ). NOTE: spark-SparkHadoopUtil.md#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used. If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default Configuration object is returned. If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are both available, the following settings are set for the Hadoop configuration: fs.s3.awsAccessKeyId , fs.s3n.awsAccessKeyId , fs.s3a.access.key are set to the value of AWS_ACCESS_KEY_ID fs.s3.awsSecretAccessKey , fs.s3n.awsSecretAccessKey , and fs.s3a.secret.key are set to the value of AWS_SECRET_ACCESS_KEY Every spark.hadoop. setting becomes a setting of the configuration with the prefix spark.hadoop. removed for the key. The value of spark.buffer.size (default: 65536 ) is used as the value of io.file.buffer.size . == [[listenerBus]] listenerBus -- LiveListenerBus Event Bus listenerBus is a scheduler:LiveListenerBus.md[] object that acts as a mechanism to announce events to other services on the spark-driver.md[driver]. LiveListenerBus is created and started when SparkContext is created and, since it is a single-JVM event bus, is exclusively used on the driver. == [[startTime]] Time when SparkContext was Created -- startTime Property","title":"LogManager.getLogger(\"org\").setLevel(Level.OFF)"},{"location":"SparkContext/#source-scala_30","text":"","title":"[source, scala]"},{"location":"SparkContext/#starttime-long","text":"startTime is the time in milliseconds when < >.","title":"startTime: Long"},{"location":"SparkContext/#source-scala_31","text":"scala> sc.startTime res0: Long = 1464425605653 == [[submitMapStage]] Submitting ShuffleDependency for Execution -- submitMapStage Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_32","text":"submitMapStage K, V, C : SimpleFutureAction[MapOutputStatistics] submitMapStage scheduler:DAGScheduler.md#submitMapStage[submits the input ShuffleDependency to DAGScheduler for execution] and returns a SimpleFutureAction . Internally, submitMapStage < > first and submits it with localProperties . NOTE: Interestingly, submitMapStage is used exclusively when Spark SQL's spark-sql-SparkPlan-ShuffleExchange.md[ShuffleExchange] physical operator is executed. NOTE: submitMapStage seems related to scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. == [[cancelJobGroup]] Cancelling Job Group -- cancelJobGroup Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_33","text":"","title":"[source, scala]"},{"location":"SparkContext/#canceljobgroupgroupid-string","text":"cancelJobGroup requests DAGScheduler scheduler:DAGScheduler.md#cancelJobGroup[to cancel a group of active Spark jobs]. NOTE: cancelJobGroup is used exclusively when SparkExecuteStatementOperation does cancel . == [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- cancelAllJobs Method CAUTION: FIXME NOTE: cancelAllJobs is used when spark-shell.md[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or SparkSQLCLIDriver is terminated. == [[cleaner]] ContextCleaner","title":"cancelJobGroup(groupId: String)"},{"location":"SparkContext/#source-scala_34","text":"","title":"[source, scala]"},{"location":"SparkContext/#cleaner-optioncontextcleaner","text":"SparkContext may have a core:ContextCleaner.md[ContextCleaner] defined. ContextCleaner is created when SparkContext is created with configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition","title":"cleaner: Option[ContextCleaner]"},{"location":"SparkContext/#source-scala_35","text":"getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs simply scheduler:DAGScheduler.md#getPreferredLocs[requests DAGScheduler for the preferred locations for partition ]. NOTE: Preferred locations of a partition of a RDD are also called placement preferences or locality preferences . getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD. == [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- persistRDD Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_36","text":"","title":"[source, scala]"},{"location":"SparkContext/#persistrddrdd-rdd_-unit","text":"persistRDD registers rdd in < > internal registry. NOTE: persistRDD is used exclusively when RDD is rdd:index.md#persist-internal[persisted or locally checkpointed]. == [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- getRDDStorageInfo Methods","title":"persistRDD(rdd: RDD[_]): Unit"},{"location":"SparkContext/#source-scala_37","text":"getRDDStorageInfo: Array[RDDInfo] // <1> getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] // <2> <1> Part of Spark's Developer API that uses <2> filtering no RDDs getRDDStorageInfo takes all the RDDs (from < > registry) that match filter and creates a collection of storage:RDDInfo.md[RDDInfo] instances. getRDDStorageInfo ...FIXME In the end, getRDDStorageInfo gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than 0 ). NOTE: getRDDStorageInfo is used when RDD spark-rdd-lineage.md#toDebugString[is requested for RDD lineage graph]. == [[statusStore]] Accessing AppStatusStore","title":"[source, scala]"},{"location":"SparkContext/#source-scala_38","text":"","title":"[source, scala]"},{"location":"SparkContext/#statusstore-appstatusstore","text":"statusStore gives the current core:AppStatusStore.md[]. statusStore is used when: SparkContext is requested to < > ConsoleProgressBar is requested to refresh SharedState (Spark SQL) is requested for a SQLAppStatusStore == [[uiWebUrl]] Requesting URL of web UI -- uiWebUrl Method","title":"statusStore: AppStatusStore"},{"location":"SparkContext/#source-scala_39","text":"","title":"[source, scala]"},{"location":"SparkContext/#uiweburl-optionstring","text":"uiWebUrl requests the SparkUI for webUrl . == [[maxNumConcurrentTasks]] maxNumConcurrentTasks Method","title":"uiWebUrl: Option[String]"},{"location":"SparkContext/#source-scala_40","text":"","title":"[source, scala]"},{"location":"SparkContext/#maxnumconcurrenttasks-int","text":"maxNumConcurrentTasks simply requests the < > for the scheduler:SchedulerBackend.md#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently]. NOTE: maxNumConcurrentTasks is used exclusively when DAGScheduler is requested to scheduler:DAGScheduler.md#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots]. == [[environment-variables]] Environment Variables .Environment Variables [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY | 1024 | Amount of memory to allocate for a Spark executor in MB. See executor:Executor.md#memory[Executor Memory]. [[SPARK_USER]] SPARK_USER The user who is running SparkContext. Available later as < >. === == [[addJar-internals]] addJar Method","title":"maxNumConcurrentTasks(): Int"},{"location":"SparkContext/#source-scala_41","text":"","title":"[source, scala]"},{"location":"SparkContext/#addjarpath-string-unit","text":"addJar ...FIXME NOTE: addJar is used when...FIXME == [[runApproximateJob]] Running Approximate Job","title":"addJar(path: String): Unit"},{"location":"SparkContext/#source-scala_42","text":"runApproximateJob T, U, R : PartialResult[R] runApproximateJob...FIXME runApproximateJob is used when: DoubleRDDFunctions is requested to meanApprox and sumApprox RDD is requested to countApprox and countByValueApprox == [[killTaskAttempt]] Killing Task","title":"[source, scala]"},{"location":"SparkContext/#source-scala_43","text":"killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean killTaskAttempt requests the < > to scheduler:DAGScheduler.md#killTaskAttempt[kill a task]. == [[checkpointFile]] checkpointFile Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_44","text":"checkpointFile T: ClassTag : RDD[T] checkpointFile...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"SparkContext/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"SparkContext/#log4jloggerorgapachesparksparkcontextall","text":"Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[checkpointDir]] Checkpoint Directory","title":"log4j.logger.org.apache.spark.SparkContext=ALL"},{"location":"SparkContext/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"SparkContext/#checkpointdir-optionstring-none","text":"checkpointDir is...FIXME === [[persistentRdds]] persistentRdds Lookup Table Lookup table of persistent/cached RDDs per their ids. Used when SparkContext is requested to: < > < > < > < >","title":"checkpointDir: Option[String] = None"},{"location":"SparkContext/#creating-sparkenv-for-driver","text":"createSparkEnv ( conf : SparkConf , isLocal : Boolean , listenerBus : LiveListenerBus ): SparkEnv createSparkEnv uses the SparkEnv utility to create a SparkEnv for the driver (with the arguments and numDriverCores ).","title":" Creating SparkEnv for Driver"},{"location":"SparkContext/#numdrivercores","text":"numDriverCores ( master : String , conf : SparkConf ): Int numDriverCores ...FIXME","title":" numDriverCores"},{"location":"SparkEnv/","text":"SparkEnv \u00b6 SparkEnv is a handle to Spark Execution Environment with the core services of Apache Spark (that interact with each other to establish a distributed computing platform for a Spark application). There are two separate SparkEnv s of the driver and executors . Core Services \u00b6 Property Service blockManager BlockManager broadcastManager BroadcastManager closureSerializer Serializer conf SparkConf mapOutputTracker MapOutputTracker memoryManager MemoryManager metricsSystem MetricsSystem outputCommitCoordinator OutputCommitCoordinator rpcEnv RpcEnv securityManager SecurityManager serializer Serializer serializerManager SerializerManager shuffleManager ShuffleManager Creating Instance \u00b6 SparkEnv takes the following to be created: Executor ID RpcEnv Serializer Serializer SerializerManager MapOutputTracker ShuffleManager BroadcastManager BlockManager SecurityManager MetricsSystem MemoryManager OutputCommitCoordinator SparkConf SparkEnv is created using create utility. Driver's Temporary Directory \u00b6 driverTmpDir : Option [ String ] SparkEnv defines driverTmpDir internal registry for the driver to be used as the root directory of files added using SparkContext.addFile . driverTmpDir is undefined initially and is defined for the driver only when SparkEnv utility is used to create a \"base\" SparkEnv . Demo \u00b6 import org.apache.spark.SparkEnv // :pa -raw // BEGIN package org . apache . spark object BypassPrivateSpark { def driverTmpDir ( sparkEnv : SparkEnv ) = { sparkEnv . driverTmpDir } } // END val driverTmpDir = org . apache . spark . BypassPrivateSpark . driverTmpDir ( SparkEnv . get ). get The above is equivalent to the following snippet. import org . apache . spark . SparkFiles SparkFiles . getRootDirectory Creating SparkEnv for Driver \u00b6 createDriverEnv ( conf : SparkConf , isLocal : Boolean , listenerBus : LiveListenerBus , numCores : Int , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ): SparkEnv createDriverEnv creates a SparkEnv execution environment for the driver. createDriverEnv accepts an instance of SparkConf.md[SparkConf], spark-deployment-environments.md[whether it runs in local mode or not], scheduler:LiveListenerBus.md[], the number of cores to use for execution in local mode or 0 otherwise, and a OutputCommitCoordinator (default: none). createDriverEnv ensures that spark-driver.md#spark_driver_host[spark.driver.host] and spark-driver.md#spark_driver_port[spark.driver.port] settings are defined. It then passes the call straight on to the < > (with driver executor id, isDriver enabled, and the input parameters). createDriverEnv is used when SparkContext is created . Creating SparkEnv for Executor \u00b6 createExecutorEnv ( conf : SparkConf , executorId : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ): SparkEnv createExecutorEnv ( conf : SparkConf , executorId : String , bindAddress : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ): SparkEnv createExecutorEnv creates an executor's (execution) environment that is the Spark execution environment for an executor. createExecutorEnv simply < > (passing in all the input parameters) and < >. NOTE: The number of cores numCores is configured using --cores command-line option of CoarseGrainedExecutorBackend and is specific to a cluster manager. createExecutorEnv is used when CoarseGrainedExecutorBackend utility is requested to run . Creating \"Base\" SparkEnv \u00b6 create ( conf : SparkConf , executorId : String , bindAddress : String , advertiseAddress : String , port : Option [ Int ], isLocal : Boolean , numUsableCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], listenerBus : LiveListenerBus = null , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ): SparkEnv create creates the \"base\" SparkEnv (that is common across the driver and executors). create creates a RpcEnv as sparkDriver on the driver and sparkExecutor on executors. create creates a Serializer (based on spark.serializer configuration property). create prints out the following DEBUG message to the logs: Using serializer: [serializer] create creates a SerializerManager . create creates a JavaSerializer as the closure serializer. creates creates a BroadcastManager . creates creates a MapOutputTrackerMaster (on the driver) or a MapOutputTrackerWorker (on executors). creates registers or looks up a MapOutputTrackerMasterEndpoint under the name of MapOutputTracker . creates prints out the following INFO message to the logs (on the driver only): Registering MapOutputTracker creates creates a ShuffleManager (based on spark.shuffle.manager configuration property). create creates a UnifiedMemoryManager . With spark.shuffle.service.enabled configuration property enabled, create creates an ExternalBlockStoreClient . create creates a BlockManagerMaster . create creates a NettyBlockTransferService . create creates a BlockManager . create creates a MetricsSystem . create creates a OutputCommitCoordinator and registers or looks up a OutputCommitCoordinatorEndpoint under the name of OutputCommitCoordinator . create creates a SparkEnv (with all the services \"stitched\" together). Logging \u00b6 Enable ALL logging level for org.apache.spark.SparkEnv logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.SparkEnv=ALL Refer to Logging .","title":"SparkEnv"},{"location":"SparkEnv/#sparkenv","text":"SparkEnv is a handle to Spark Execution Environment with the core services of Apache Spark (that interact with each other to establish a distributed computing platform for a Spark application). There are two separate SparkEnv s of the driver and executors .","title":"SparkEnv"},{"location":"SparkEnv/#core-services","text":"Property Service blockManager BlockManager broadcastManager BroadcastManager closureSerializer Serializer conf SparkConf mapOutputTracker MapOutputTracker memoryManager MemoryManager metricsSystem MetricsSystem outputCommitCoordinator OutputCommitCoordinator rpcEnv RpcEnv securityManager SecurityManager serializer Serializer serializerManager SerializerManager shuffleManager ShuffleManager","title":" Core Services"},{"location":"SparkEnv/#creating-instance","text":"SparkEnv takes the following to be created: Executor ID RpcEnv Serializer Serializer SerializerManager MapOutputTracker ShuffleManager BroadcastManager BlockManager SecurityManager MetricsSystem MemoryManager OutputCommitCoordinator SparkConf SparkEnv is created using create utility.","title":"Creating Instance"},{"location":"SparkEnv/#drivers-temporary-directory","text":"driverTmpDir : Option [ String ] SparkEnv defines driverTmpDir internal registry for the driver to be used as the root directory of files added using SparkContext.addFile . driverTmpDir is undefined initially and is defined for the driver only when SparkEnv utility is used to create a \"base\" SparkEnv .","title":" Driver's Temporary Directory"},{"location":"SparkEnv/#demo","text":"import org.apache.spark.SparkEnv // :pa -raw // BEGIN package org . apache . spark object BypassPrivateSpark { def driverTmpDir ( sparkEnv : SparkEnv ) = { sparkEnv . driverTmpDir } } // END val driverTmpDir = org . apache . spark . BypassPrivateSpark . driverTmpDir ( SparkEnv . get ). get The above is equivalent to the following snippet. import org . apache . spark . SparkFiles SparkFiles . getRootDirectory","title":" Demo"},{"location":"SparkEnv/#creating-sparkenv-for-driver","text":"createDriverEnv ( conf : SparkConf , isLocal : Boolean , listenerBus : LiveListenerBus , numCores : Int , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ): SparkEnv createDriverEnv creates a SparkEnv execution environment for the driver. createDriverEnv accepts an instance of SparkConf.md[SparkConf], spark-deployment-environments.md[whether it runs in local mode or not], scheduler:LiveListenerBus.md[], the number of cores to use for execution in local mode or 0 otherwise, and a OutputCommitCoordinator (default: none). createDriverEnv ensures that spark-driver.md#spark_driver_host[spark.driver.host] and spark-driver.md#spark_driver_port[spark.driver.port] settings are defined. It then passes the call straight on to the < > (with driver executor id, isDriver enabled, and the input parameters). createDriverEnv is used when SparkContext is created .","title":" Creating SparkEnv for Driver"},{"location":"SparkEnv/#creating-sparkenv-for-executor","text":"createExecutorEnv ( conf : SparkConf , executorId : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ): SparkEnv createExecutorEnv ( conf : SparkConf , executorId : String , bindAddress : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ): SparkEnv createExecutorEnv creates an executor's (execution) environment that is the Spark execution environment for an executor. createExecutorEnv simply < > (passing in all the input parameters) and < >. NOTE: The number of cores numCores is configured using --cores command-line option of CoarseGrainedExecutorBackend and is specific to a cluster manager. createExecutorEnv is used when CoarseGrainedExecutorBackend utility is requested to run .","title":" Creating SparkEnv for Executor"},{"location":"SparkEnv/#creating-base-sparkenv","text":"create ( conf : SparkConf , executorId : String , bindAddress : String , advertiseAddress : String , port : Option [ Int ], isLocal : Boolean , numUsableCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], listenerBus : LiveListenerBus = null , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ): SparkEnv create creates the \"base\" SparkEnv (that is common across the driver and executors). create creates a RpcEnv as sparkDriver on the driver and sparkExecutor on executors. create creates a Serializer (based on spark.serializer configuration property). create prints out the following DEBUG message to the logs: Using serializer: [serializer] create creates a SerializerManager . create creates a JavaSerializer as the closure serializer. creates creates a BroadcastManager . creates creates a MapOutputTrackerMaster (on the driver) or a MapOutputTrackerWorker (on executors). creates registers or looks up a MapOutputTrackerMasterEndpoint under the name of MapOutputTracker . creates prints out the following INFO message to the logs (on the driver only): Registering MapOutputTracker creates creates a ShuffleManager (based on spark.shuffle.manager configuration property). create creates a UnifiedMemoryManager . With spark.shuffle.service.enabled configuration property enabled, create creates an ExternalBlockStoreClient . create creates a BlockManagerMaster . create creates a NettyBlockTransferService . create creates a BlockManager . create creates a MetricsSystem . create creates a OutputCommitCoordinator and registers or looks up a OutputCommitCoordinatorEndpoint under the name of OutputCommitCoordinator . create creates a SparkEnv (with all the services \"stitched\" together).","title":" Creating \"Base\" SparkEnv"},{"location":"SparkEnv/#logging","text":"Enable ALL logging level for org.apache.spark.SparkEnv logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.SparkEnv=ALL Refer to Logging .","title":"Logging"},{"location":"SparkFiles/","text":"SparkFiles \u00b6 SparkFiles is an utility to work with files added using SparkContext.addFile . Absolute Path of Added File \u00b6 get ( filename : String ): String get gets the absolute path of the given file in the root directory . Root Directory \u00b6 getRootDirectory (): String getRootDirectory requests the current SparkEnv for driverTmpDir (if defined) or defaults to the current directory ( . ). getRootDirectory is used when: SparkContext is requested to addFile Executor is requested to updateDependencies SparkFiles utility is requested to get the absolute path of a file","title":"SparkFiles"},{"location":"SparkFiles/#sparkfiles","text":"SparkFiles is an utility to work with files added using SparkContext.addFile .","title":"SparkFiles"},{"location":"SparkFiles/#absolute-path-of-added-file","text":"get ( filename : String ): String get gets the absolute path of the given file in the root directory .","title":" Absolute Path of Added File"},{"location":"SparkFiles/#root-directory","text":"getRootDirectory (): String getRootDirectory requests the current SparkEnv for driverTmpDir (if defined) or defaults to the current directory ( . ). getRootDirectory is used when: SparkContext is requested to addFile Executor is requested to updateDependencies SparkFiles utility is requested to get the absolute path of a file","title":" Root Directory"},{"location":"SparkHadoopWriter/","text":"SparkHadoopWriter Utility \u00b6 Writing Key-Value RDD Out (As Hadoop OutputFormat) \u00b6 write [ K , V : ClassTag ]( rdd : RDD [( K , V )], config : HadoopWriteConfigUtil [ K , V ]): Unit write runs a Spark job to write out partition records (for all partitions of the given key-value RDD ) with the given HadoopWriteConfigUtil and a HadoopMapReduceCommitProtocol committer. The number of writer tasks ( parallelism ) is the number of the partitions in the given key-value RDD . Internals \u00b6 Internally, write uses the id of the given RDD as the commitJobId . write creates a jobTrackerId with the current date. write requests the given HadoopWriteConfigUtil to create a Hadoop JobContext (for the jobTrackerId and commitJobId ). write requests the given HadoopWriteConfigUtil to initOutputFormat with the Hadoop JobContext . write requests the given HadoopWriteConfigUtil to assertConf . write requests the given HadoopWriteConfigUtil to create a HadoopMapReduceCommitProtocol committer for the commitJobId . write requests the HadoopMapReduceCommitProtocol to setupJob (with the jobContext ). write uses the SparkContext (of the given RDD) to run a Spark job asynchronously for the given RDD with the executeTask partition function. In the end, write requests the HadoopMapReduceCommitProtocol to commit the job and prints out the following INFO message to the logs: Job [getJobID] committed. Throwables \u00b6 In case of any Throwable , write prints out the following ERROR message to the logs: Aborting job [getJobID]. write requests the HadoopMapReduceCommitProtocol to abort the job and throws a SparkException : Job aborted. Usage \u00b6 write is used when: PairRDDFunctions.saveAsNewAPIHadoopDataset PairRDDFunctions.saveAsHadoopDataset Writing RDD Partition \u00b6 executeTask [ K , V : ClassTag ]( context : TaskContext , config : HadoopWriteConfigUtil [ K , V ], jobTrackerId : String , commitJobId : Int , sparkPartitionId : Int , sparkAttemptNumber : Int , committer : FileCommitProtocol , iterator : Iterator [( K , V )]): TaskCommitMessage Fixme Review Me executeTask requests the given HadoopWriteConfigUtil to create a TaskAttemptContext . executeTask requests the given FileCommitProtocol to set up a task with the TaskAttemptContext . executeTask requests the given HadoopWriteConfigUtil to initWriter (with the TaskAttemptContext and the given sparkPartitionId ). executeTask initHadoopOutputMetrics . executeTask writes all rows of the RDD partition (from the given Iterator[(K, V)] ). executeTask requests the given HadoopWriteConfigUtil to write . In the end, executeTask requests the given HadoopWriteConfigUtil to closeWriter and the given FileCommitProtocol to commit the task . executeTask updates metrics about writing data to external systems ( bytesWritten and recordsWritten ) every few records and at the end. In case of any errors, executeTask requests the given HadoopWriteConfigUtil to closeWriter and the given FileCommitProtocol to abort the task . In the end, executeTask prints out the following ERROR message to the logs: Task [taskAttemptID] aborted. executeTask is used when: SparkHadoopWriter utility is used to write Logging \u00b6 Enable ALL logging level for org.apache.spark.internal.io.SparkHadoopWriter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.internal.io.SparkHadoopWriter=ALL Refer to Logging .","title":"SparkHadoopWriter"},{"location":"SparkHadoopWriter/#sparkhadoopwriter-utility","text":"","title":"SparkHadoopWriter Utility"},{"location":"SparkHadoopWriter/#writing-key-value-rdd-out-as-hadoop-outputformat","text":"write [ K , V : ClassTag ]( rdd : RDD [( K , V )], config : HadoopWriteConfigUtil [ K , V ]): Unit write runs a Spark job to write out partition records (for all partitions of the given key-value RDD ) with the given HadoopWriteConfigUtil and a HadoopMapReduceCommitProtocol committer. The number of writer tasks ( parallelism ) is the number of the partitions in the given key-value RDD .","title":" Writing Key-Value RDD Out (As Hadoop OutputFormat)"},{"location":"SparkHadoopWriter/#internals","text":"Internally, write uses the id of the given RDD as the commitJobId . write creates a jobTrackerId with the current date. write requests the given HadoopWriteConfigUtil to create a Hadoop JobContext (for the jobTrackerId and commitJobId ). write requests the given HadoopWriteConfigUtil to initOutputFormat with the Hadoop JobContext . write requests the given HadoopWriteConfigUtil to assertConf . write requests the given HadoopWriteConfigUtil to create a HadoopMapReduceCommitProtocol committer for the commitJobId . write requests the HadoopMapReduceCommitProtocol to setupJob (with the jobContext ). write uses the SparkContext (of the given RDD) to run a Spark job asynchronously for the given RDD with the executeTask partition function. In the end, write requests the HadoopMapReduceCommitProtocol to commit the job and prints out the following INFO message to the logs: Job [getJobID] committed.","title":" Internals"},{"location":"SparkHadoopWriter/#throwables","text":"In case of any Throwable , write prints out the following ERROR message to the logs: Aborting job [getJobID]. write requests the HadoopMapReduceCommitProtocol to abort the job and throws a SparkException : Job aborted.","title":" Throwables"},{"location":"SparkHadoopWriter/#usage","text":"write is used when: PairRDDFunctions.saveAsNewAPIHadoopDataset PairRDDFunctions.saveAsHadoopDataset","title":" Usage"},{"location":"SparkHadoopWriter/#writing-rdd-partition","text":"executeTask [ K , V : ClassTag ]( context : TaskContext , config : HadoopWriteConfigUtil [ K , V ], jobTrackerId : String , commitJobId : Int , sparkPartitionId : Int , sparkAttemptNumber : Int , committer : FileCommitProtocol , iterator : Iterator [( K , V )]): TaskCommitMessage Fixme Review Me executeTask requests the given HadoopWriteConfigUtil to create a TaskAttemptContext . executeTask requests the given FileCommitProtocol to set up a task with the TaskAttemptContext . executeTask requests the given HadoopWriteConfigUtil to initWriter (with the TaskAttemptContext and the given sparkPartitionId ). executeTask initHadoopOutputMetrics . executeTask writes all rows of the RDD partition (from the given Iterator[(K, V)] ). executeTask requests the given HadoopWriteConfigUtil to write . In the end, executeTask requests the given HadoopWriteConfigUtil to closeWriter and the given FileCommitProtocol to commit the task . executeTask updates metrics about writing data to external systems ( bytesWritten and recordsWritten ) every few records and at the end. In case of any errors, executeTask requests the given HadoopWriteConfigUtil to closeWriter and the given FileCommitProtocol to abort the task . In the end, executeTask prints out the following ERROR message to the logs: Task [taskAttemptID] aborted. executeTask is used when: SparkHadoopWriter utility is used to write","title":" Writing RDD Partition"},{"location":"SparkHadoopWriter/#logging","text":"Enable ALL logging level for org.apache.spark.internal.io.SparkHadoopWriter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.internal.io.SparkHadoopWriter=ALL Refer to Logging .","title":"Logging"},{"location":"SparkListener/","text":"SparkListener \u00b6 SparkListener is an extension of the SparkListenerInterface abstraction for event listeners with a no-op implementation for callback methods. Implementations \u00b6 BarrierCoordinator SparkSession ( Spark SQL ) AppListingListener (Spark History Server) AppStatusListener BasicEventFilterBuilder (Spark History Server) EventLoggingListener (Spark History Server) ExecutionListenerBus ExecutorAllocationListener ExecutorMonitor HeartbeatReceiver HiveThriftServer2Listener (Spark Thrift Server) SpillListener SQLAppStatusListener ( Spark SQL ) SQLEventFilterBuilder StatsReportListener StreamingQueryListenerBus ( Spark Structured Streaming )","title":"SparkListener"},{"location":"SparkListener/#sparklistener","text":"SparkListener is an extension of the SparkListenerInterface abstraction for event listeners with a no-op implementation for callback methods.","title":"SparkListener"},{"location":"SparkListener/#implementations","text":"BarrierCoordinator SparkSession ( Spark SQL ) AppListingListener (Spark History Server) AppStatusListener BasicEventFilterBuilder (Spark History Server) EventLoggingListener (Spark History Server) ExecutionListenerBus ExecutorAllocationListener ExecutorMonitor HeartbeatReceiver HiveThriftServer2Listener (Spark Thrift Server) SpillListener SQLAppStatusListener ( Spark SQL ) SQLEventFilterBuilder StatsReportListener StreamingQueryListenerBus ( Spark Structured Streaming )","title":"Implementations"},{"location":"SparkListenerBus/","text":"SparkListenerBus \u00b6 SparkListenerBus is an extension of the ListenerBus abstraction for event buses for SparkListenerInterface s to be notified about SparkListenerEvent s. Posting Event to SparkListener \u00b6 doPostEvent ( listener : SparkListenerInterface , event : SparkListenerEvent ): Unit doPostEvent is part of the ListenerBus abstraction. doPostEvent notifies the given SparkListenerInterface about the SparkListenerEvent . doPostEvent calls an event-specific method of SparkListenerInterface or falls back to onOtherEvent . Implementations \u00b6 AsyncEventQueue ReplayListenerBus","title":"SparkListenerBus"},{"location":"SparkListenerBus/#sparklistenerbus","text":"SparkListenerBus is an extension of the ListenerBus abstraction for event buses for SparkListenerInterface s to be notified about SparkListenerEvent s.","title":"SparkListenerBus"},{"location":"SparkListenerBus/#posting-event-to-sparklistener","text":"doPostEvent ( listener : SparkListenerInterface , event : SparkListenerEvent ): Unit doPostEvent is part of the ListenerBus abstraction. doPostEvent notifies the given SparkListenerInterface about the SparkListenerEvent . doPostEvent calls an event-specific method of SparkListenerInterface or falls back to onOtherEvent .","title":" Posting Event to SparkListener"},{"location":"SparkListenerBus/#implementations","text":"AsyncEventQueue ReplayListenerBus","title":"Implementations"},{"location":"SparkListenerEvent/","text":"SparkListenerEvent \u00b6 SparkListenerEvent is an abstraction of scheduling events . Dispatching SparkListenerEvents \u00b6 SparkListenerBus in general (and AsyncEventQueue in particular) are event buses used to dispatch SparkListenerEvent s to registered SparkListener s. LiveListenerBus is an event bus to dispatch SparkListenerEvent s to registered SparkListener s. Spark History Server \u00b6 Once logged, Spark History Server uses JsonProtocol utility to sparkEventFromJson . Contract \u00b6 logEvent \u00b6 logEvent : Boolean logEvent controls whether EventLoggingListener should save the event to an event log. Default: true logEvent is used when: EventLoggingListener is requested to handle \"other\" events Implementations \u00b6 SparkListenerApplicationEnd \u00b6 SparkListenerApplicationStart \u00b6 SparkListenerBlockManagerAdded \u00b6 SparkListenerBlockManagerRemoved \u00b6 SparkListenerBlockUpdated \u00b6 SparkListenerEnvironmentUpdate \u00b6 SparkListenerExecutorAdded \u00b6 SparkListenerExecutorBlacklisted \u00b6 SparkListenerExecutorBlacklistedForStage \u00b6 SparkListenerExecutorMetricsUpdate \u00b6 SparkListenerExecutorRemoved \u00b6 SparkListenerExecutorUnblacklisted \u00b6 SparkListenerJobEnd \u00b6 SparkListenerJobStart \u00b6 SparkListenerLogStart \u00b6 SparkListenerNodeBlacklisted \u00b6 SparkListenerNodeBlacklistedForStage \u00b6 SparkListenerNodeUnblacklisted \u00b6 SparkListenerSpeculativeTaskSubmitted \u00b6 SparkListenerStageCompleted \u00b6 SparkListenerStageExecutorMetrics \u00b6 SparkListenerStageSubmitted \u00b6 SparkListenerTaskEnd \u00b6 SparkListenerTaskEnd SparkListenerTaskGettingResult \u00b6 SparkListenerTaskStart \u00b6 SparkListenerUnpersistRDD \u00b6","title":"SparkListenerEvent"},{"location":"SparkListenerEvent/#sparklistenerevent","text":"SparkListenerEvent is an abstraction of scheduling events .","title":"SparkListenerEvent"},{"location":"SparkListenerEvent/#dispatching-sparklistenerevents","text":"SparkListenerBus in general (and AsyncEventQueue in particular) are event buses used to dispatch SparkListenerEvent s to registered SparkListener s. LiveListenerBus is an event bus to dispatch SparkListenerEvent s to registered SparkListener s.","title":"Dispatching SparkListenerEvents"},{"location":"SparkListenerEvent/#spark-history-server","text":"Once logged, Spark History Server uses JsonProtocol utility to sparkEventFromJson .","title":"Spark History Server"},{"location":"SparkListenerEvent/#contract","text":"","title":"Contract"},{"location":"SparkListenerEvent/#logevent","text":"logEvent : Boolean logEvent controls whether EventLoggingListener should save the event to an event log. Default: true logEvent is used when: EventLoggingListener is requested to handle \"other\" events","title":" logEvent"},{"location":"SparkListenerEvent/#implementations","text":"","title":"Implementations"},{"location":"SparkListenerEvent/#sparklistenerapplicationend","text":"","title":" SparkListenerApplicationEnd"},{"location":"SparkListenerEvent/#sparklistenerapplicationstart","text":"","title":" SparkListenerApplicationStart"},{"location":"SparkListenerEvent/#sparklistenerblockmanageradded","text":"","title":" SparkListenerBlockManagerAdded"},{"location":"SparkListenerEvent/#sparklistenerblockmanagerremoved","text":"","title":" SparkListenerBlockManagerRemoved"},{"location":"SparkListenerEvent/#sparklistenerblockupdated","text":"","title":" SparkListenerBlockUpdated"},{"location":"SparkListenerEvent/#sparklistenerenvironmentupdate","text":"","title":" SparkListenerEnvironmentUpdate"},{"location":"SparkListenerEvent/#sparklistenerexecutoradded","text":"","title":" SparkListenerExecutorAdded"},{"location":"SparkListenerEvent/#sparklistenerexecutorblacklisted","text":"","title":" SparkListenerExecutorBlacklisted"},{"location":"SparkListenerEvent/#sparklistenerexecutorblacklistedforstage","text":"","title":" SparkListenerExecutorBlacklistedForStage"},{"location":"SparkListenerEvent/#sparklistenerexecutormetricsupdate","text":"","title":" SparkListenerExecutorMetricsUpdate"},{"location":"SparkListenerEvent/#sparklistenerexecutorremoved","text":"","title":" SparkListenerExecutorRemoved"},{"location":"SparkListenerEvent/#sparklistenerexecutorunblacklisted","text":"","title":" SparkListenerExecutorUnblacklisted"},{"location":"SparkListenerEvent/#sparklistenerjobend","text":"","title":" SparkListenerJobEnd"},{"location":"SparkListenerEvent/#sparklistenerjobstart","text":"","title":" SparkListenerJobStart"},{"location":"SparkListenerEvent/#sparklistenerlogstart","text":"","title":" SparkListenerLogStart"},{"location":"SparkListenerEvent/#sparklistenernodeblacklisted","text":"","title":" SparkListenerNodeBlacklisted"},{"location":"SparkListenerEvent/#sparklistenernodeblacklistedforstage","text":"","title":" SparkListenerNodeBlacklistedForStage"},{"location":"SparkListenerEvent/#sparklistenernodeunblacklisted","text":"","title":" SparkListenerNodeUnblacklisted"},{"location":"SparkListenerEvent/#sparklistenerspeculativetasksubmitted","text":"","title":" SparkListenerSpeculativeTaskSubmitted"},{"location":"SparkListenerEvent/#sparklistenerstagecompleted","text":"","title":" SparkListenerStageCompleted"},{"location":"SparkListenerEvent/#sparklistenerstageexecutormetrics","text":"","title":" SparkListenerStageExecutorMetrics"},{"location":"SparkListenerEvent/#sparklistenerstagesubmitted","text":"","title":" SparkListenerStageSubmitted"},{"location":"SparkListenerEvent/#sparklistenertaskend","text":"SparkListenerTaskEnd","title":" SparkListenerTaskEnd"},{"location":"SparkListenerEvent/#sparklistenertaskgettingresult","text":"","title":" SparkListenerTaskGettingResult"},{"location":"SparkListenerEvent/#sparklistenertaskstart","text":"","title":" SparkListenerTaskStart"},{"location":"SparkListenerEvent/#sparklistenerunpersistrdd","text":"","title":" SparkListenerUnpersistRDD"},{"location":"SparkListenerInterface/","text":"SparkListenerInterface \u00b6 SparkListenerInterface is an abstraction of event listeners (that SparkListenerBus notifies about scheduling events ). SparkListenerInterface is a way to intercept scheduling events from the Spark Scheduler that are emitted over the course of execution of a Spark application. SparkListenerInterface is used heavily to manage communication between internal components in the distributed environment for a Spark application (e.g. web UI , event persistence for History Server , dynamic allocation of executors , keeping track of executors ). SparkListenerInterface can be registered in a Spark application using SparkContext.addSparkListener method or spark.extraListeners configuration property. Tip Enable INFO logging level for org.apache.spark.SparkContext logger to see what and when custom Spark listeners are registered. onApplicationEnd \u00b6 onApplicationEnd ( applicationEnd : SparkListenerApplicationEnd ): Unit Used when: SparkListenerBus is requested to post a SparkListenerApplicationEnd event onApplicationStart \u00b6 onApplicationStart ( applicationStart : SparkListenerApplicationStart ): Unit Used when: SparkListenerBus is requested to post a SparkListenerApplicationStart event onBlockManagerAdded \u00b6 onBlockManagerAdded ( blockManagerAdded : SparkListenerBlockManagerAdded ): Unit Used when: SparkListenerBus is requested to post a SparkListenerBlockManagerAdded event onBlockManagerRemoved \u00b6 onBlockManagerRemoved ( blockManagerRemoved : SparkListenerBlockManagerRemoved ): Unit Used when: SparkListenerBus is requested to post a SparkListenerBlockManagerRemoved event onBlockUpdated \u00b6 onBlockUpdated ( blockUpdated : SparkListenerBlockUpdated ): Unit Used when: SparkListenerBus is requested to post a SparkListenerBlockUpdated event onEnvironmentUpdate \u00b6 onEnvironmentUpdate ( environmentUpdate : SparkListenerEnvironmentUpdate ): Unit Used when: SparkListenerBus is requested to post a SparkListenerEnvironmentUpdate event onExecutorAdded \u00b6 onExecutorAdded ( executorAdded : SparkListenerExecutorAdded ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorAdded event onExecutorBlacklisted \u00b6 onExecutorBlacklisted ( executorBlacklisted : SparkListenerExecutorBlacklisted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorBlacklisted event onExecutorBlacklistedForStage \u00b6 onExecutorBlacklistedForStage ( executorBlacklistedForStage : SparkListenerExecutorBlacklistedForStage ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorBlacklistedForStage event onExecutorMetricsUpdate \u00b6 onExecutorMetricsUpdate ( executorMetricsUpdate : SparkListenerExecutorMetricsUpdate ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorMetricsUpdate event onExecutorRemoved \u00b6 onExecutorRemoved ( executorRemoved : SparkListenerExecutorRemoved ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorRemoved event onExecutorUnblacklisted \u00b6 onExecutorUnblacklisted ( executorUnblacklisted : SparkListenerExecutorUnblacklisted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorUnblacklisted event onJobEnd \u00b6 onJobEnd ( jobEnd : SparkListenerJobEnd ): Unit Used when: SparkListenerBus is requested to post a SparkListenerJobEnd event onJobStart \u00b6 onJobStart ( jobStart : SparkListenerJobStart ): Unit Used when: SparkListenerBus is requested to post a SparkListenerJobStart event onNodeBlacklisted \u00b6 onNodeBlacklisted ( nodeBlacklisted : SparkListenerNodeBlacklisted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerNodeBlacklisted event onNodeBlacklistedForStage \u00b6 onNodeBlacklistedForStage ( nodeBlacklistedForStage : SparkListenerNodeBlacklistedForStage ): Unit Used when: SparkListenerBus is requested to post a SparkListenerNodeBlacklistedForStage event onNodeUnblacklisted \u00b6 onNodeUnblacklisted ( nodeUnblacklisted : SparkListenerNodeUnblacklisted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerNodeUnblacklisted event onOtherEvent \u00b6 onOtherEvent ( event : SparkListenerEvent ): Unit Used when: SparkListenerBus is requested to post a custom SparkListenerEvent onSpeculativeTaskSubmitted \u00b6 onSpeculativeTaskSubmitted ( speculativeTask : SparkListenerSpeculativeTaskSubmitted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerSpeculativeTaskSubmitted event onStageCompleted \u00b6 onStageCompleted ( stageCompleted : SparkListenerStageCompleted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerStageCompleted event onStageExecutorMetrics \u00b6 onStageExecutorMetrics ( executorMetrics : SparkListenerStageExecutorMetrics ): Unit Used when: SparkListenerBus is requested to post a SparkListenerStageExecutorMetrics event onStageSubmitted \u00b6 onStageSubmitted ( stageSubmitted : SparkListenerStageSubmitted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerStageSubmitted event onTaskEnd \u00b6 onTaskEnd ( taskEnd : SparkListenerTaskEnd ): Unit Used when: SparkListenerBus is requested to post a SparkListenerTaskEnd event onTaskGettingResult \u00b6 onTaskGettingResult ( taskGettingResult : SparkListenerTaskGettingResult ): Unit Used when: SparkListenerBus is requested to post a SparkListenerTaskGettingResult event onTaskStart \u00b6 onTaskStart ( taskStart : SparkListenerTaskStart ): Unit Used when: SparkListenerBus is requested to post a SparkListenerTaskStart event onUnpersistRDD \u00b6 onUnpersistRDD ( unpersistRDD : SparkListenerUnpersistRDD ): Unit Used when: SparkListenerBus is requested to post a SparkListenerUnpersistRDD event Implementations \u00b6 EventFilterBuilder SparkFirehoseListener SparkListener","title":"SparkListenerInterface"},{"location":"SparkListenerInterface/#sparklistenerinterface","text":"SparkListenerInterface is an abstraction of event listeners (that SparkListenerBus notifies about scheduling events ). SparkListenerInterface is a way to intercept scheduling events from the Spark Scheduler that are emitted over the course of execution of a Spark application. SparkListenerInterface is used heavily to manage communication between internal components in the distributed environment for a Spark application (e.g. web UI , event persistence for History Server , dynamic allocation of executors , keeping track of executors ). SparkListenerInterface can be registered in a Spark application using SparkContext.addSparkListener method or spark.extraListeners configuration property. Tip Enable INFO logging level for org.apache.spark.SparkContext logger to see what and when custom Spark listeners are registered.","title":"SparkListenerInterface"},{"location":"SparkListenerInterface/#onapplicationend","text":"onApplicationEnd ( applicationEnd : SparkListenerApplicationEnd ): Unit Used when: SparkListenerBus is requested to post a SparkListenerApplicationEnd event","title":" onApplicationEnd"},{"location":"SparkListenerInterface/#onapplicationstart","text":"onApplicationStart ( applicationStart : SparkListenerApplicationStart ): Unit Used when: SparkListenerBus is requested to post a SparkListenerApplicationStart event","title":" onApplicationStart"},{"location":"SparkListenerInterface/#onblockmanageradded","text":"onBlockManagerAdded ( blockManagerAdded : SparkListenerBlockManagerAdded ): Unit Used when: SparkListenerBus is requested to post a SparkListenerBlockManagerAdded event","title":" onBlockManagerAdded"},{"location":"SparkListenerInterface/#onblockmanagerremoved","text":"onBlockManagerRemoved ( blockManagerRemoved : SparkListenerBlockManagerRemoved ): Unit Used when: SparkListenerBus is requested to post a SparkListenerBlockManagerRemoved event","title":" onBlockManagerRemoved"},{"location":"SparkListenerInterface/#onblockupdated","text":"onBlockUpdated ( blockUpdated : SparkListenerBlockUpdated ): Unit Used when: SparkListenerBus is requested to post a SparkListenerBlockUpdated event","title":" onBlockUpdated"},{"location":"SparkListenerInterface/#onenvironmentupdate","text":"onEnvironmentUpdate ( environmentUpdate : SparkListenerEnvironmentUpdate ): Unit Used when: SparkListenerBus is requested to post a SparkListenerEnvironmentUpdate event","title":" onEnvironmentUpdate"},{"location":"SparkListenerInterface/#onexecutoradded","text":"onExecutorAdded ( executorAdded : SparkListenerExecutorAdded ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorAdded event","title":" onExecutorAdded"},{"location":"SparkListenerInterface/#onexecutorblacklisted","text":"onExecutorBlacklisted ( executorBlacklisted : SparkListenerExecutorBlacklisted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorBlacklisted event","title":" onExecutorBlacklisted"},{"location":"SparkListenerInterface/#onexecutorblacklistedforstage","text":"onExecutorBlacklistedForStage ( executorBlacklistedForStage : SparkListenerExecutorBlacklistedForStage ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorBlacklistedForStage event","title":" onExecutorBlacklistedForStage"},{"location":"SparkListenerInterface/#onexecutormetricsupdate","text":"onExecutorMetricsUpdate ( executorMetricsUpdate : SparkListenerExecutorMetricsUpdate ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorMetricsUpdate event","title":" onExecutorMetricsUpdate"},{"location":"SparkListenerInterface/#onexecutorremoved","text":"onExecutorRemoved ( executorRemoved : SparkListenerExecutorRemoved ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorRemoved event","title":" onExecutorRemoved"},{"location":"SparkListenerInterface/#onexecutorunblacklisted","text":"onExecutorUnblacklisted ( executorUnblacklisted : SparkListenerExecutorUnblacklisted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerExecutorUnblacklisted event","title":" onExecutorUnblacklisted"},{"location":"SparkListenerInterface/#onjobend","text":"onJobEnd ( jobEnd : SparkListenerJobEnd ): Unit Used when: SparkListenerBus is requested to post a SparkListenerJobEnd event","title":" onJobEnd"},{"location":"SparkListenerInterface/#onjobstart","text":"onJobStart ( jobStart : SparkListenerJobStart ): Unit Used when: SparkListenerBus is requested to post a SparkListenerJobStart event","title":" onJobStart"},{"location":"SparkListenerInterface/#onnodeblacklisted","text":"onNodeBlacklisted ( nodeBlacklisted : SparkListenerNodeBlacklisted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerNodeBlacklisted event","title":" onNodeBlacklisted"},{"location":"SparkListenerInterface/#onnodeblacklistedforstage","text":"onNodeBlacklistedForStage ( nodeBlacklistedForStage : SparkListenerNodeBlacklistedForStage ): Unit Used when: SparkListenerBus is requested to post a SparkListenerNodeBlacklistedForStage event","title":" onNodeBlacklistedForStage"},{"location":"SparkListenerInterface/#onnodeunblacklisted","text":"onNodeUnblacklisted ( nodeUnblacklisted : SparkListenerNodeUnblacklisted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerNodeUnblacklisted event","title":" onNodeUnblacklisted"},{"location":"SparkListenerInterface/#onotherevent","text":"onOtherEvent ( event : SparkListenerEvent ): Unit Used when: SparkListenerBus is requested to post a custom SparkListenerEvent","title":" onOtherEvent"},{"location":"SparkListenerInterface/#onspeculativetasksubmitted","text":"onSpeculativeTaskSubmitted ( speculativeTask : SparkListenerSpeculativeTaskSubmitted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerSpeculativeTaskSubmitted event","title":" onSpeculativeTaskSubmitted"},{"location":"SparkListenerInterface/#onstagecompleted","text":"onStageCompleted ( stageCompleted : SparkListenerStageCompleted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerStageCompleted event","title":" onStageCompleted"},{"location":"SparkListenerInterface/#onstageexecutormetrics","text":"onStageExecutorMetrics ( executorMetrics : SparkListenerStageExecutorMetrics ): Unit Used when: SparkListenerBus is requested to post a SparkListenerStageExecutorMetrics event","title":" onStageExecutorMetrics"},{"location":"SparkListenerInterface/#onstagesubmitted","text":"onStageSubmitted ( stageSubmitted : SparkListenerStageSubmitted ): Unit Used when: SparkListenerBus is requested to post a SparkListenerStageSubmitted event","title":" onStageSubmitted"},{"location":"SparkListenerInterface/#ontaskend","text":"onTaskEnd ( taskEnd : SparkListenerTaskEnd ): Unit Used when: SparkListenerBus is requested to post a SparkListenerTaskEnd event","title":" onTaskEnd"},{"location":"SparkListenerInterface/#ontaskgettingresult","text":"onTaskGettingResult ( taskGettingResult : SparkListenerTaskGettingResult ): Unit Used when: SparkListenerBus is requested to post a SparkListenerTaskGettingResult event","title":" onTaskGettingResult"},{"location":"SparkListenerInterface/#ontaskstart","text":"onTaskStart ( taskStart : SparkListenerTaskStart ): Unit Used when: SparkListenerBus is requested to post a SparkListenerTaskStart event","title":" onTaskStart"},{"location":"SparkListenerInterface/#onunpersistrdd","text":"onUnpersistRDD ( unpersistRDD : SparkListenerUnpersistRDD ): Unit Used when: SparkListenerBus is requested to post a SparkListenerUnpersistRDD event","title":" onUnpersistRDD"},{"location":"SparkListenerInterface/#implementations","text":"EventFilterBuilder SparkFirehoseListener SparkListener","title":"Implementations"},{"location":"SparkListenerTaskEnd/","text":"SparkListenerTaskEnd \u00b6 SparkListenerTaskEnd is a SparkListenerEvent . SparkListenerTaskEnd is posted (and created ) when: DAGScheduler is requested to postTaskEnd SparkListenerTaskEnd is intercepted using SparkListenerInterface.onTaskEnd Creating Instance \u00b6 SparkListenerTaskEnd takes the following to be created: Stage ID Stage Attempt ID Task Type TaskEndReason TaskInfo ExecutorMetrics TaskMetrics","title":"SparkListenerTaskEnd"},{"location":"SparkListenerTaskEnd/#sparklistenertaskend","text":"SparkListenerTaskEnd is a SparkListenerEvent . SparkListenerTaskEnd is posted (and created ) when: DAGScheduler is requested to postTaskEnd SparkListenerTaskEnd is intercepted using SparkListenerInterface.onTaskEnd","title":"SparkListenerTaskEnd"},{"location":"SparkListenerTaskEnd/#creating-instance","text":"SparkListenerTaskEnd takes the following to be created: Stage ID Stage Attempt ID Task Type TaskEndReason TaskInfo ExecutorMetrics TaskMetrics","title":"Creating Instance"},{"location":"SparkStatusTracker/","text":"SparkStatusTracker \u00b6 Creating Instance \u00b6 SparkStatusTracker takes the following when created: [[sc]] SparkContext.md[] [[store]] AppStatusStore SparkStatusTracker is created for SparkContext .","title":"SparkStatusTracker"},{"location":"SparkStatusTracker/#sparkstatustracker","text":"","title":"SparkStatusTracker"},{"location":"SparkStatusTracker/#creating-instance","text":"SparkStatusTracker takes the following when created: [[sc]] SparkContext.md[] [[store]] AppStatusStore SparkStatusTracker is created for SparkContext .","title":"Creating Instance"},{"location":"SpillListener/","text":"SpillListener \u00b6 SpillListener is a SparkListener that intercepts ( listens to ) the following events for detecting spills in jobs: onTaskEnd onStageCompleted SpillListener is used for testing only. Creating Instance \u00b6 SpillListener takes no input arguments to be created. SpillListener is created when TestUtils is requested to assertSpilled and assertNotSpilled . onTaskEnd Callback \u00b6 onTaskEnd ( taskEnd : SparkListenerTaskEnd ): Unit onTaskEnd ...FIXME onTaskEnd is part of the SparkListener abstraction. onStageCompleted Callback \u00b6 onStageCompleted ( stageComplete : SparkListenerStageCompleted ): Unit onStageCompleted ...FIXME onStageCompleted is part of the SparkListener abstraction.","title":"SpillListener"},{"location":"SpillListener/#spilllistener","text":"SpillListener is a SparkListener that intercepts ( listens to ) the following events for detecting spills in jobs: onTaskEnd onStageCompleted SpillListener is used for testing only.","title":"SpillListener"},{"location":"SpillListener/#creating-instance","text":"SpillListener takes no input arguments to be created. SpillListener is created when TestUtils is requested to assertSpilled and assertNotSpilled .","title":"Creating Instance"},{"location":"SpillListener/#ontaskend-callback","text":"onTaskEnd ( taskEnd : SparkListenerTaskEnd ): Unit onTaskEnd ...FIXME onTaskEnd is part of the SparkListener abstraction.","title":" onTaskEnd Callback"},{"location":"SpillListener/#onstagecompleted-callback","text":"onStageCompleted ( stageComplete : SparkListenerStageCompleted ): Unit onStageCompleted ...FIXME onStageCompleted is part of the SparkListener abstraction.","title":" onStageCompleted Callback"},{"location":"StatsReportListener/","text":"StatsReportListener \u2014 Logging Summary Statistics \u00b6 org.apache.spark.scheduler.StatsReportListener (see https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.StatsReportListener[the listener's scaladoc]) is a SparkListener.md[] that logs summary statistics when each stage completes. StatsReportListener listens to SparkListenerTaskEnd and SparkListenerStageCompleted events and prints them out at INFO logging level. [TIP] \u00b6 Enable INFO logging level for org.apache.spark.scheduler.StatsReportListener logger to see Spark events. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.StatsReportListener=INFO Refer to spark-logging.md[Logging]. \u00b6 === [[onStageCompleted]] Intercepting Stage Completed Events -- onStageCompleted Callback CAUTION: FIXME === [[example]] Example $ ./bin/spark-shell -c spark.extraListeners=org.apache.spark.scheduler.StatsReportListener ... INFO SparkContext: Registered listener org.apache.spark.scheduler.StatsReportListener ... scala> spark.read.text(\"README.md\").count ... INFO StatsReportListener: Finished stage: Stage(0, 0); Name: 'count at <console>:24'; Status: succeeded; numTasks: 1; Took: 212 msec INFO StatsReportListener: task runtime:(count: 1, mean: 198.000000, stdev: 0.000000, max: 198.000000, min: 198.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 59.000000, stdev: 0.000000, max: 59.000000, min: 59.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: task result size:(count: 1, mean: 1885.000000, stdev: 0.000000, max: 1885.000000, min: 1885.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 73.737374, stdev: 0.000000, max: 73.737374, min: 73.737374) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % INFO StatsReportListener: other time pct: (count: 1, mean: 26.262626, stdev: 0.000000, max: 26.262626, min: 26.262626) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % INFO StatsReportListener: Finished stage: Stage(1, 0); Name: 'count at <console>:24'; Status: succeeded; numTasks: 1; Took: 34 msec INFO StatsReportListener: task runtime:(count: 1, mean: 33.000000, stdev: 0.000000, max: 33.000000, min: 33.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: task result size:(count: 1, mean: 1960.000000, stdev: 0.000000, max: 1960.000000, min: 1960.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 75.757576, stdev: 0.000000, max: 75.757576, min: 75.757576) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 76 % 76 % 76 % 76 % 76 % 76 % 76 % 76 % 76 % INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % INFO StatsReportListener: other time pct: (count: 1, mean: 24.242424, stdev: 0.000000, max: 24.242424, min: 24.242424) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 24 % 24 % 24 % 24 % 24 % 24 % 24 % 24 % 24 % res0: Long = 99","title":"StatsReportListener"},{"location":"StatsReportListener/#statsreportlistener-logging-summary-statistics","text":"org.apache.spark.scheduler.StatsReportListener (see https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.StatsReportListener[the listener's scaladoc]) is a SparkListener.md[] that logs summary statistics when each stage completes. StatsReportListener listens to SparkListenerTaskEnd and SparkListenerStageCompleted events and prints them out at INFO logging level.","title":"StatsReportListener &mdash; Logging Summary Statistics"},{"location":"StatsReportListener/#tip","text":"Enable INFO logging level for org.apache.spark.scheduler.StatsReportListener logger to see Spark events. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.StatsReportListener=INFO","title":"[TIP]"},{"location":"StatsReportListener/#refer-to-spark-loggingmdlogging","text":"=== [[onStageCompleted]] Intercepting Stage Completed Events -- onStageCompleted Callback CAUTION: FIXME === [[example]] Example $ ./bin/spark-shell -c spark.extraListeners=org.apache.spark.scheduler.StatsReportListener ... INFO SparkContext: Registered listener org.apache.spark.scheduler.StatsReportListener ... scala> spark.read.text(\"README.md\").count ... INFO StatsReportListener: Finished stage: Stage(0, 0); Name: 'count at <console>:24'; Status: succeeded; numTasks: 1; Took: 212 msec INFO StatsReportListener: task runtime:(count: 1, mean: 198.000000, stdev: 0.000000, max: 198.000000, min: 198.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 59.000000, stdev: 0.000000, max: 59.000000, min: 59.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: task result size:(count: 1, mean: 1885.000000, stdev: 0.000000, max: 1885.000000, min: 1885.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 73.737374, stdev: 0.000000, max: 73.737374, min: 73.737374) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % INFO StatsReportListener: other time pct: (count: 1, mean: 26.262626, stdev: 0.000000, max: 26.262626, min: 26.262626) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % INFO StatsReportListener: Finished stage: Stage(1, 0); Name: 'count at <console>:24'; Status: succeeded; numTasks: 1; Took: 34 msec INFO StatsReportListener: task runtime:(count: 1, mean: 33.000000, stdev: 0.000000, max: 33.000000, min: 33.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: task result size:(count: 1, mean: 1960.000000, stdev: 0.000000, max: 1960.000000, min: 1960.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 75.757576, stdev: 0.000000, max: 75.757576, min: 75.757576) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 76 % 76 % 76 % 76 % 76 % 76 % 76 % 76 % 76 % INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % INFO StatsReportListener: other time pct: (count: 1, mean: 24.242424, stdev: 0.000000, max: 24.242424, min: 24.242424) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 24 % 24 % 24 % 24 % 24 % 24 % 24 % 24 % 24 % res0: Long = 99","title":"Refer to spark-logging.md[Logging]."},{"location":"TaskCompletionListener/","text":"TaskCompletionListener \u00b6 TaskCompletionListener is an extension of the EventListener ( Java ) abstraction for task listeners that can be notified on task completion . Contract \u00b6 onTaskCompletion \u00b6 onTaskCompletion ( context : TaskContext ): Unit Used when: TaskContextImpl is requested to addTaskCompletionListener (and a task has already completed) and markTaskCompleted ShuffleFetchCompletionListener is requested to onComplete","title":"TaskCompletionListener"},{"location":"TaskCompletionListener/#taskcompletionlistener","text":"TaskCompletionListener is an extension of the EventListener ( Java ) abstraction for task listeners that can be notified on task completion .","title":"TaskCompletionListener"},{"location":"TaskCompletionListener/#contract","text":"","title":"Contract"},{"location":"TaskCompletionListener/#ontaskcompletion","text":"onTaskCompletion ( context : TaskContext ): Unit Used when: TaskContextImpl is requested to addTaskCompletionListener (and a task has already completed) and markTaskCompleted ShuffleFetchCompletionListener is requested to onComplete","title":" onTaskCompletion"},{"location":"TaskFailureListener/","text":"TaskFailureListener \u00b6 TaskFailureListener is an extension of the EventListener ( Java ) abstraction for task listeners that can be notified on task failure . Contract \u00b6 onTaskFailure \u00b6 onTaskFailure ( context : TaskContext , error : Throwable ): Unit Used when: TaskContextImpl is requested to addTaskFailureListener (and a task has already failed) and markTaskFailed","title":"TaskFailureListener"},{"location":"TaskFailureListener/#taskfailurelistener","text":"TaskFailureListener is an extension of the EventListener ( Java ) abstraction for task listeners that can be notified on task failure .","title":"TaskFailureListener"},{"location":"TaskFailureListener/#contract","text":"","title":"Contract"},{"location":"TaskFailureListener/#ontaskfailure","text":"onTaskFailure ( context : TaskContext , error : Throwable ): Unit Used when: TaskContextImpl is requested to addTaskFailureListener (and a task has already failed) and markTaskFailed","title":" onTaskFailure"},{"location":"Utils/","text":"Utils Utility \u00b6 Local URI Scheme \u00b6 Utils defines a local URI scheme for files that are locally available on worker nodes in the cluster. The local URL scheme is used when: Utils is used to isLocalUri Client (Spark on YARN) is used isLocalUri \u00b6 isLocalUri ( uri : String ): Boolean isLocalUri is true when the URI is a local: URI (the given uri starts with local: scheme). isLocalUri is used when: FIXME getCurrentUserName \u00b6 getCurrentUserName (): String getCurrentUserName computes the user name who has started the SparkContext.md[SparkContext] instance. NOTE: It is later available as SparkContext.md#sparkUser[SparkContext.sparkUser]. Internally, it reads SparkContext.md#SPARK_USER[SPARK_USER] environment variable and, if not set, reverts to Hadoop Security API's UserGroupInformation.getCurrentUser().getShortUserName() . NOTE: It is another place where Spark relies on Hadoop API for its operation. localHostName \u00b6 localHostName (): String localHostName computes the local host name. It starts by checking SPARK_LOCAL_HOSTNAME environment variable for the value. If it is not defined, it uses SPARK_LOCAL_IP to find the name (using InetAddress.getByName ). If it is not defined either, it calls InetAddress.getLocalHost for the name. NOTE: Utils.localHostName is executed while SparkContext.md#creating-instance[ SparkContext is created] and also to compute the default value of spark-driver.md#spark_driver_host[spark.driver.host Spark property]. getUserJars \u00b6 getUserJars ( conf : SparkConf ): Seq [ String ] getUserJars is the spark.jars configuration property with non-empty entries. getUserJars is used when: SparkContext is created extractHostPortFromSparkUrl \u00b6 extractHostPortFromSparkUrl ( sparkUrl : String ): ( String , Int ) extractHostPortFromSparkUrl creates a Java URI with the input sparkUrl and takes the host and port parts. extractHostPortFromSparkUrl asserts that the input sparkURL uses spark scheme. extractHostPortFromSparkUrl throws a SparkException for unparseable spark URLs: Invalid master URL: [sparkUrl] extractHostPortFromSparkUrl is used when: StandaloneSubmitRequestServlet is requested to buildDriverDescription RpcAddress is requested to extract an RpcAddress from a Spark master URL isDynamicAllocationEnabled \u00b6 isDynamicAllocationEnabled ( conf : SparkConf ): Boolean isDynamicAllocationEnabled is true when the following hold: spark.dynamicAllocation.enabled configuration property is true spark.master is non- local isDynamicAllocationEnabled is used when: SparkContext is created (to start an ExecutorAllocationManager ) DAGScheduler is requested to checkBarrierStageWithDynamicAllocation SchedulerBackendUtils is requested to getInitialTargetExecutorNumber StandaloneSchedulerBackend (Spark Standalone) is requested to start ExecutorPodsAllocator (Spark on Kubernetes) is requested to onNewSnapshots ApplicationMaster (Spark on YARN) is created checkAndGetK8sMasterUrl \u00b6 checkAndGetK8sMasterUrl ( rawMasterURL : String ): String checkAndGetK8sMasterUrl ...FIXME checkAndGetK8sMasterUrl is used when: SparkSubmit is requested to prepareSubmitEnvironment (for Kubernetes cluster manager) getLocalDir \u00b6 getLocalDir ( conf : SparkConf ): String getLocalDir ...FIXME getLocalDir is used when: Utils is requested to < > SparkEnv is core:SparkEnv.md#create[created] (on the driver) spark-shell.md[spark-shell] is launched Spark on YARN's Client is requested to spark-yarn-client.md#prepareLocalResources[prepareLocalResources] and spark-yarn-client.md#createConfArchive[create ++ spark_conf .zip++ archive with configuration files and Spark configuration] PySpark's PythonBroadcast is requested to readObject PySpark's EvalPythonExec is requested to doExecute Fetching File \u00b6 fetchFile ( url : String , targetDir : File , conf : SparkConf , securityMgr : SecurityManager , hadoopConf : Configuration , timestamp : Long , useCache : Boolean ): File fetchFile ...FIXME fetchFile is used when: SparkContext is requested to SparkContext.md#addFile[addFile] Executor is requested to executor:Executor.md#updateDependencies[updateDependencies] Spark Standalone's DriverRunner is requested to downloadUserJar getOrCreateLocalRootDirs \u00b6 getOrCreateLocalRootDirs ( conf : SparkConf ): Array [ String ] getOrCreateLocalRootDirs ...FIXME getOrCreateLocalRootDirs is used when: Utils is requested to < > Worker is requested to spark-standalone-worker.md#receive[handle a LaunchExecutor message] getOrCreateLocalRootDirsImpl \u00b6 getOrCreateLocalRootDirsImpl ( conf : SparkConf ): Array [ String ] getOrCreateLocalRootDirsImpl ...FIXME getOrCreateLocalRootDirsImpl is used when Utils is requested to getOrCreateLocalRootDirs","title":"Utils"},{"location":"Utils/#utils-utility","text":"","title":"Utils Utility"},{"location":"Utils/#local-uri-scheme","text":"Utils defines a local URI scheme for files that are locally available on worker nodes in the cluster. The local URL scheme is used when: Utils is used to isLocalUri Client (Spark on YARN) is used","title":" Local URI Scheme"},{"location":"Utils/#islocaluri","text":"isLocalUri ( uri : String ): Boolean isLocalUri is true when the URI is a local: URI (the given uri starts with local: scheme). isLocalUri is used when: FIXME","title":" isLocalUri"},{"location":"Utils/#getcurrentusername","text":"getCurrentUserName (): String getCurrentUserName computes the user name who has started the SparkContext.md[SparkContext] instance. NOTE: It is later available as SparkContext.md#sparkUser[SparkContext.sparkUser]. Internally, it reads SparkContext.md#SPARK_USER[SPARK_USER] environment variable and, if not set, reverts to Hadoop Security API's UserGroupInformation.getCurrentUser().getShortUserName() . NOTE: It is another place where Spark relies on Hadoop API for its operation.","title":" getCurrentUserName"},{"location":"Utils/#localhostname","text":"localHostName (): String localHostName computes the local host name. It starts by checking SPARK_LOCAL_HOSTNAME environment variable for the value. If it is not defined, it uses SPARK_LOCAL_IP to find the name (using InetAddress.getByName ). If it is not defined either, it calls InetAddress.getLocalHost for the name. NOTE: Utils.localHostName is executed while SparkContext.md#creating-instance[ SparkContext is created] and also to compute the default value of spark-driver.md#spark_driver_host[spark.driver.host Spark property].","title":" localHostName"},{"location":"Utils/#getuserjars","text":"getUserJars ( conf : SparkConf ): Seq [ String ] getUserJars is the spark.jars configuration property with non-empty entries. getUserJars is used when: SparkContext is created","title":" getUserJars"},{"location":"Utils/#extracthostportfromsparkurl","text":"extractHostPortFromSparkUrl ( sparkUrl : String ): ( String , Int ) extractHostPortFromSparkUrl creates a Java URI with the input sparkUrl and takes the host and port parts. extractHostPortFromSparkUrl asserts that the input sparkURL uses spark scheme. extractHostPortFromSparkUrl throws a SparkException for unparseable spark URLs: Invalid master URL: [sparkUrl] extractHostPortFromSparkUrl is used when: StandaloneSubmitRequestServlet is requested to buildDriverDescription RpcAddress is requested to extract an RpcAddress from a Spark master URL","title":" extractHostPortFromSparkUrl"},{"location":"Utils/#isdynamicallocationenabled","text":"isDynamicAllocationEnabled ( conf : SparkConf ): Boolean isDynamicAllocationEnabled is true when the following hold: spark.dynamicAllocation.enabled configuration property is true spark.master is non- local isDynamicAllocationEnabled is used when: SparkContext is created (to start an ExecutorAllocationManager ) DAGScheduler is requested to checkBarrierStageWithDynamicAllocation SchedulerBackendUtils is requested to getInitialTargetExecutorNumber StandaloneSchedulerBackend (Spark Standalone) is requested to start ExecutorPodsAllocator (Spark on Kubernetes) is requested to onNewSnapshots ApplicationMaster (Spark on YARN) is created","title":" isDynamicAllocationEnabled"},{"location":"Utils/#checkandgetk8smasterurl","text":"checkAndGetK8sMasterUrl ( rawMasterURL : String ): String checkAndGetK8sMasterUrl ...FIXME checkAndGetK8sMasterUrl is used when: SparkSubmit is requested to prepareSubmitEnvironment (for Kubernetes cluster manager)","title":" checkAndGetK8sMasterUrl"},{"location":"Utils/#getlocaldir","text":"getLocalDir ( conf : SparkConf ): String getLocalDir ...FIXME getLocalDir is used when: Utils is requested to < > SparkEnv is core:SparkEnv.md#create[created] (on the driver) spark-shell.md[spark-shell] is launched Spark on YARN's Client is requested to spark-yarn-client.md#prepareLocalResources[prepareLocalResources] and spark-yarn-client.md#createConfArchive[create ++ spark_conf .zip++ archive with configuration files and Spark configuration] PySpark's PythonBroadcast is requested to readObject PySpark's EvalPythonExec is requested to doExecute","title":" getLocalDir"},{"location":"Utils/#fetching-file","text":"fetchFile ( url : String , targetDir : File , conf : SparkConf , securityMgr : SecurityManager , hadoopConf : Configuration , timestamp : Long , useCache : Boolean ): File fetchFile ...FIXME fetchFile is used when: SparkContext is requested to SparkContext.md#addFile[addFile] Executor is requested to executor:Executor.md#updateDependencies[updateDependencies] Spark Standalone's DriverRunner is requested to downloadUserJar","title":" Fetching File"},{"location":"Utils/#getorcreatelocalrootdirs","text":"getOrCreateLocalRootDirs ( conf : SparkConf ): Array [ String ] getOrCreateLocalRootDirs ...FIXME getOrCreateLocalRootDirs is used when: Utils is requested to < > Worker is requested to spark-standalone-worker.md#receive[handle a LaunchExecutor message]","title":" getOrCreateLocalRootDirs"},{"location":"Utils/#getorcreatelocalrootdirsimpl","text":"getOrCreateLocalRootDirsImpl ( conf : SparkConf ): Array [ String ] getOrCreateLocalRootDirsImpl ...FIXME getOrCreateLocalRootDirsImpl is used when Utils is requested to getOrCreateLocalRootDirs","title":" getOrCreateLocalRootDirsImpl"},{"location":"architecture/","text":"= Spark Architecture Spark uses a master/worker architecture . There is a spark-driver.md[driver] that talks to a single coordinator called spark-master.md[master] that manages spark-workers.md[workers] in which executor:Executor.md[executors] run. .Spark architecture image::driver-sparkcontext-clustermanager-workers-executors.png[align=\"center\"] The driver and the executors run in their own Java processes. You can run them all on the same ( horizontal cluster ) or separate machines ( vertical cluster ) or in a mixed machine configuration. .Spark architecture in detail image::sparkapp-sparkcontext-master-slaves.png[align=\"center\"] Physical machines are called hosts or nodes .","title":"Architecture"},{"location":"barrier-execution-mode/","text":"= Barrier Execution Mode Barrier Execution Mode is...FIXME See https://jira.apache.org/jira/browse/SPARK-24374[SPIP : Barrier Execution Mode] and https://jira.apache.org/jira/browse/SPARK-24582[Design Doc]. NOTE: The barrier execution mode is experimental and it only handles limited scenarios. In case of a task failure, instead of only restarting the failed task, Spark will abort the entire stage and re-launch all tasks for this stage. Use < > transformation to mark the current stage as a < >. [[barrier]] [source, scala] barrier(): RDDBarrier[T] \u00b6 barrier simply creates a < > that comes with the barrier-aware < > transformation. [[mapPartitions]] [source, scala] mapPartitions S: ClassTag : RDD[S] mapPartitions is simply changes the regular < > transformation to create a rdd:MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. Task has a scheduler:Task.md#isBarrier[isBarrier] flag that says whether this task belongs to a barrier stage (default: false ). Spark must launch all the tasks at the same time for a < >. An RDD is in a < >, if at least one of its parent RDD(s), or itself, are mapped from an RDDBarrier . rdd:ShuffledRDD.md[ShuffledRDD] has the rdd:RDD.md#isBarrier[isBarrier] flag always disabled ( false ). rdd:MapPartitionsRDD.md[MapPartitionsRDD] is the only one RDD that can have the rdd:RDD.md#isBarrier_[isBarrier] flag enabled. rdd:spark-RDDBarrier.md#mapPartitions[RDDBarrier.mapPartitions] is the only transformation that creates a rdd:MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. == [[barrier-stage]] Barrier Stage Barrier Stage is a scheduler:Stage.md[stage] that...FIXME","title":"barrier-execution-mode"},{"location":"barrier-execution-mode/#barrier-rddbarriert","text":"barrier simply creates a < > that comes with the barrier-aware < > transformation. [[mapPartitions]] [source, scala] mapPartitions S: ClassTag : RDD[S] mapPartitions is simply changes the regular < > transformation to create a rdd:MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. Task has a scheduler:Task.md#isBarrier[isBarrier] flag that says whether this task belongs to a barrier stage (default: false ). Spark must launch all the tasks at the same time for a < >. An RDD is in a < >, if at least one of its parent RDD(s), or itself, are mapped from an RDDBarrier . rdd:ShuffledRDD.md[ShuffledRDD] has the rdd:RDD.md#isBarrier[isBarrier] flag always disabled ( false ). rdd:MapPartitionsRDD.md[MapPartitionsRDD] is the only one RDD that can have the rdd:RDD.md#isBarrier_[isBarrier] flag enabled. rdd:spark-RDDBarrier.md#mapPartitions[RDDBarrier.mapPartitions] is the only transformation that creates a rdd:MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. == [[barrier-stage]] Barrier Stage Barrier Stage is a scheduler:Stage.md[stage] that...FIXME","title":"barrier(): RDDBarrier[T]"},{"location":"configuration-properties/","text":"Spark Configuration Properties \u00b6 spark.app.id \u00b6 Unique identifier of a Spark application that Spark uses to uniquely identify metric sources . Default: TaskScheduler.applicationId() Set when SparkContext is created spark.cleaner.referenceTracking \u00b6 Controls whether to enable ContextCleaner Default: true spark.diskStore.subDirectories \u00b6 Number of subdirectories inside each path listed in spark.local.dir for hashing block files into. Default: 64 Used by BlockManager and DiskBlockManager spark.driver.host \u00b6 Address of the driver (endpoints) Default: Utils.localCanonicalHostName spark.driver.maxResultSize \u00b6 Maximum size of task results (in bytes) Default: 1g Used when: TaskRunner is requested to run a task (and decide on the type of a serialized task result ) TaskSetManager is requested to check available memory for task results spark.driver.port \u00b6 Port of the driver (endpoints) Default: 0 spark.executor.cores \u00b6 Number of CPU cores for Executor Default: 1 spark.executor.id \u00b6 Default: (undefined) spark.executor.metrics.fileSystemSchemes \u00b6 A comma-separated list of the file system schemes to report in executor metrics Default: file,hdfs spark.extraListeners \u00b6 A comma-separated list of fully-qualified class names of SparkListener s (to be registered when SparkContext is created) Default: (empty) spark.file.transferTo \u00b6 Controls whether to use Java FileChannel s (Java NIO) for copying data between two Java FileInputStream s to improve copy performance Default: true Used when: BypassMergeSortShuffleWriter and UnsafeShuffleWriter are created spark.files \u00b6 The files to be added to a Spark application (that can be defined directly as a configuration property or indirectly using --files option of spark-submit script) Default: (empty) Used when: SparkContext is created spark.jars \u00b6 Default: (empty) spark.kryo.pool \u00b6 Default: true Used when: KryoSerializer is created spark.kryo.unsafe \u00b6 Whether KryoSerializer should use Unsafe-based IO for serialization Default: false spark.local.dir \u00b6 A comma-separated list of directories that are used as a temporary storage for \"scratch\" space (incl. map output files and RDDs that get stored on disk). This should be on a fast, local disk in your system. Default: /tmp spark.logConf \u00b6 Default: false spark.logLineage \u00b6 Default: false spark.master \u00b6 Master URL of the cluster manager to connect the Spark application to spark.memory.offHeap.enabled \u00b6 Controls whether Tungsten memory will be allocated on the JVM heap ( false ) or off-heap ( true / using sun.misc.Unsafe ). Default: false When enabled, spark.memory.offHeap.size must be greater than 0 . Used when: MemoryManager is requested for tungstenMemoryMode spark.memory.offHeap.size \u00b6 Maximum memory (in bytes) for off-heap memory allocation Default: 0 This setting has no impact on heap memory usage, so if your executors' total memory consumption must fit within some hard limit then be sure to shrink your JVM heap size accordingly. Must not be negative and be set to a positive value when spark.memory.offHeap.enabled is enabled spark.memory.storageFraction \u00b6 Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by spark.memory.fraction. The higher this is, the less working memory may be available to execution and tasks may spill to disk more often. Leaving this at the default value is recommended Default: 0.5 Must be in [0,1) Used when: UnifiedMemoryManager is created MemoryManager is created spark.network.maxRemoteBlockSizeFetchToMem \u00b6 Remote block will be fetched to disk when size of the block is above this threshold in bytes This is to avoid a giant request takes too much memory. Note this configuration will affect both shuffle fetch and block manager remote block fetch. With an external shuffle service use at least 2.3.0 Default: 200m Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task NettyBlockTransferService is requested to uploadBlock BlockManager is requested to fetchRemoteManagedBuffer spark.network.timeout \u00b6 Network timeout (in seconds) to use for RPC remote endpoint lookup Default: 120s spark.network.timeoutInterval \u00b6 (in millis) Default: spark.storage.blockManagerTimeoutIntervalMs spark.rdd.compress \u00b6 Controls whether to compress RDD partitions when stored serialized Default: false spark.reducer.maxBlocksInFlightPerAddress \u00b6 Maximum number of remote blocks being fetched per reduce task from a given host port When a large number of blocks are being requested from a given address in a single fetch or simultaneously, this could crash the serving executor or a Node Manager. This is especially useful to reduce the load on the Node Manager when external shuffle is enabled. You can mitigate the issue by setting it to a lower value. Default: (unlimited) Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task spark.reducer.maxReqsInFlight \u00b6 Maximum number of remote requests to fetch blocks at any given point When the number of hosts in the cluster increase, it might lead to very large number of inbound connections to one or more nodes, causing the workers to fail under load. By allowing it to limit the number of fetch requests, this scenario can be mitigated Default: (unlimited) Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task spark.reducer.maxSizeInFlight \u00b6 Maximum size of all map outputs to fetch simultaneously from each reduce task (in MiB unless otherwise specified) Since each output requires us to create a buffer to receive it, this represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory Default: 48m Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task spark.repl.class.uri \u00b6 Controls whether to compress RDD partitions when stored serialized Default: false spark.rpc.lookupTimeout \u00b6 Default Endpoint Lookup Timeout Default: 120s spark.rpc.message.maxSize \u00b6 Maximum allowed message size for RPC communication (in MB unless specified) Default: 128 Must be below 2047MB ( Int.MaxValue / 1024 / 1024 ) Used when: CoarseGrainedSchedulerBackend is requested to launch tasks RpcUtils is requested for the maximum message size Executor is created MapOutputTrackerMaster is created (and makes sure that spark.shuffle.mapOutput.minSizeForBroadcast is below the threshold) spark.scheduler.minRegisteredResourcesRatio \u00b6 Minimum ratio of (registered resources / total expected resources) before submitting tasks Default: (undefined) spark.scheduler.revive.interval \u00b6 Revive Interval that is the time (in millis) between resource offers revives Default: 1s Used when: DriverEndpoint is requested to onStart spark.serializer \u00b6 The fully-qualified class name of the Serializer (of the driver and executors ) Default: org.apache.spark.serializer.JavaSerializer Used when: SparkEnv utility is used to create a SparkEnv SparkConf is requested to registerKryoClasses (as a side-effect) spark.shuffle.compress \u00b6 Controls whether to compress shuffle output when stored Default: true spark.shuffle.detectCorrupt \u00b6 Controls corruption detection in fetched blocks Default: true Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task spark.shuffle.detectCorrupt.useExtraMemory \u00b6 If enabled, part of a compressed/encrypted stream will be de-compressed/de-crypted by using extra memory to detect early corruption. Any IOException thrown will cause the task to be retried once and if it fails again with same exception, then FetchFailedException will be thrown to retry previous stage Default: false Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task spark.shuffle.file.buffer \u00b6 Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise specified. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files. Default: 32k Must be greater than 0 and less than or equal to 2097151 ( (Integer.MAX_VALUE - 15) / 1024 ) Used when the following are created: BypassMergeSortShuffleWriter ShuffleExternalSorter UnsafeShuffleWriter ExternalAppendOnlyMap ExternalSorter spark.shuffle.manager \u00b6 A fully-qualified class name or the alias of the ShuffleManager in a Spark application Default: sort Supported aliases: sort tungsten-sort Used when SparkEnv object is requested to create a \"base\" SparkEnv for a driver or an executor spark.shuffle.mapOutput.parallelAggregationThreshold \u00b6 (internal) Multi-thread is used when the number of mappers * shuffle partitions is greater than or equal to this threshold. Note that the actual parallelism is calculated by number of mappers * shuffle partitions / this threshold + 1, so this threshold should be positive. Default: 10000000 Used when: MapOutputTrackerMaster is requested for the statistics of a ShuffleDependency spark.shuffle.minNumPartitionsToHighlyCompress \u00b6 (internal) Minimum number of partitions (threshold) for MapStatus utility to prefer a HighlyCompressedMapStatus (over CompressedMapStatus ) (for ShuffleWriters ). Default: 2000 Must be a positive integer (above 0 ) spark.shuffle.readHostLocalDisk \u00b6 If enabled (with spark.shuffle.useOldFetchProtocol disabled and spark.shuffle.service.enabled enabled), shuffle blocks requested from those block managers which are running on the same host are read from the disk directly instead of being fetched as remote blocks over the network. Default: true spark.shuffle.registration.maxAttempts \u00b6 How many attempts to register a BlockManager with External Shuffle Service Default: 3 Used when BlockManager is requested to register with External Shuffle Server spark.shuffle.sort.bypassMergeThreshold \u00b6 Maximum number of reduce partitions below which SortShuffleManager avoids merge-sorting data for no map-side aggregation Default: 200 Used when: SortShuffleWriter utility is used to shouldBypassMergeSort ShuffleExchangeExec ( Spark SQL ) physical operator is requested to prepareShuffleDependency spark.shuffle.sort.io.plugin.class \u00b6 Name of the class to use for shuffle IO Default: LocalDiskShuffleDataIO spark.shuffle.spill.initialMemoryThreshold \u00b6 Initial threshold for the size of an in-memory collection Default: 5MB Used by Spillable spark.shuffle.spill.numElementsForceSpillThreshold \u00b6 (internal) The maximum number of elements in memory before forcing the shuffle sorter to spill. Default: Integer.MAX_VALUE The default value is to never force the sorter to spill, until Spark reaches some limitations, like the max page size limitation for the pointer array in the sorter. Used when: ShuffleExternalSorter is created Spillable is created Spark SQL's SortBasedAggregator is requested for an UnsafeKVExternalSorter Spark SQL's ObjectAggregationMap is requested to dumpToExternalSorter Spark SQL's UnsafeExternalRowSorter is created Spark SQL's UnsafeFixedWidthAggregationMap is requested for an UnsafeKVExternalSorter spark.shuffle.sync \u00b6 Controls whether DiskBlockObjectWriter should force outstanding writes to disk while committing a single atomic block (i.e. all operating system buffers should synchronize with the disk to ensure that all changes to a file are in fact recorded in the storage) Default: false Used when BlockManager is requested for a DiskBlockObjectWriter spark.shuffle.useOldFetchProtocol \u00b6 Whether to use the old protocol while doing the shuffle block fetching. It is only enabled while we need the compatibility in the scenario of new Spark version job fetching shuffle blocks from old version external shuffle service. Default: false spark.speculation \u00b6 Controls Speculative Execution of Tasks Default: false spark.speculation.interval \u00b6 The time interval to use before checking for speculative tasks in Speculative Execution of Tasks . Default: 100ms spark.speculation.multiplier \u00b6 Default: 1.5 spark.speculation.quantile \u00b6 The percentage of tasks that has not finished yet at which to start speculation in Speculative Execution of Tasks . Default: 0.75 spark.storage.blockManagerSlaveTimeoutMs \u00b6 (in millis) Default: spark.network.timeout spark.storage.blockManagerTimeoutIntervalMs \u00b6 (in millis) Default: 60s spark.storage.localDiskByExecutors.cacheSize \u00b6 The max number of executors for which the local dirs are stored. This size is both applied for the driver and both for the executors side to avoid having an unbounded store. This cache will be used to avoid the network in case of fetching disk persisted RDD blocks or shuffle blocks (when spark.shuffle.readHostLocalDisk is set) from the same host. Default: 1000 spark.storage.replication.policy \u00b6 Default: RandomBlockReplicationPolicy spark.task.cpus \u00b6 The number of CPU cores to schedule ( allocate ) to a task Default: 1 Used when: ExecutorAllocationManager is created TaskSchedulerImpl is created AppStatusListener is requested to handle a SparkListenerEnvironmentUpdate event SparkContext utility is used to create a TaskScheduler ResourceProfile is requested to getDefaultTaskResources LocalityPreferredContainerPlacementStrategy is requested to numExecutorsPending spark.task.maxDirectResultSize \u00b6 Maximum size of a task result (in bytes) to be sent to the driver as a DirectTaskResult Default: 1048576B ( 1L << 20 ) Used when: TaskRunner is requested to run a task (and decide on the type of a serialized task result ) spark.task.maxFailures \u00b6 Number of failures of a single task (of a TaskSet ) before giving up on the entire TaskSet and then the job Default: 4 spark.plugins \u00b6 A comma-separated list of class names implementing org.apache.spark.api.plugin.SparkPlugin to load into a Spark application. Default: (empty) Since: 3.0.0 Set when SparkContext is created spark.plugins.defaultList \u00b6 FIXME spark.ui.showConsoleProgress \u00b6 Controls whether to enable ConsoleProgressBar and show the progress bar in the console Default: false == [[properties]] Properties [cols=\"1m,1\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.blockManager.port a| [[spark.blockManager.port]][[BLOCK_MANAGER_PORT]] Port to use for block managers to listen on when a more specific setting is not provided (i.e. < > for the driver). Default: 0 In Spark on Kubernetes the default port is 7079 | spark.default.parallelism a| [[spark.default.parallelism]] Number of partitions to use for rdd:HashPartitioner.md[HashPartitioner] spark.default.parallelism corresponds to scheduler:SchedulerBackend.md#defaultParallelism[default parallelism] of a scheduler backend and is as follows: The number of threads for local/spark-LocalSchedulerBackend.md[LocalSchedulerBackend]. the number of CPU cores in spark-mesos.md#defaultParallelism[Spark on Mesos] and defaults to 8 . Maximum of totalCoreCount and 2 in scheduler:CoarseGrainedSchedulerBackend.md#defaultParallelism[CoarseGrainedSchedulerBackend]. | spark.driver.blockManager.port a| [[spark.driver.blockManager.port]][[DRIVER_BLOCK_MANAGER_PORT]] Port the storage:BlockManager.md[block manager] on the driver listens on Default: < > | spark.executor.extraClassPath a| [[spark.executor.extraClassPath]][[EXECUTOR_CLASS_PATH]] User-defined class path for executors , i.e. URLs representing user-defined class path entries that are added to an executor's class path. URLs are separated by system-dependent path separator, i.e. : on Unix-like systems and ; on Microsoft Windows. Default: (empty) Used when: Spark Standalone's StandaloneSchedulerBackend is requested to spark-standalone:StandaloneSchedulerBackend.md#start[start] (and creates a command for executor:CoarseGrainedExecutorBackend.md[]) Spark local's LocalSchedulerBackend is requested for the spark-local:spark-LocalSchedulerBackend.md#getUserClasspath[user-defined class path for executors] Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to spark-on-mesos:spark-mesos-MesosCoarseGrainedSchedulerBackend.md#createCommand[create a command for CoarseGrainedExecutorBackend] Spark on Mesos' MesosFineGrainedSchedulerBackend is requested to create a command for MesosExecutorBackend Spark on Kubernetes' BasicExecutorFeatureStep is requested to configurePod Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareEnvironment[prepareEnvironment] (for CoarseGrainedExecutorBackend ) | spark.executor.extraJavaOptions a| [[spark.executor.extraJavaOptions]] Extra Java options of an executor:Executor.md[] Used when Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareCommand[prepare the command to launch CoarseGrainedExecutorBackend in a YARN container] | spark.executor.extraLibraryPath a| [[spark.executor.extraLibraryPath]] Extra library paths separated by system-dependent path separator, i.e. : on Unix/MacOS systems and ; on Microsoft Windows Used when Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareCommand[prepare the command to launch CoarseGrainedExecutorBackend in a YARN container] | spark.executor.uri a| [[spark.executor.uri]] Equivalent to SPARK_EXECUTOR_URI | spark.executor.logs.rolling.time.interval a| [[spark.executor.logs.rolling.time.interval]] | spark.executor.logs.rolling.strategy a| [[spark.executor.logs.rolling.strategy]] | spark.executor.logs.rolling.maxRetainedFiles a| [[spark.executor.logs.rolling.maxRetainedFiles]] | spark.executor.logs.rolling.maxSize a| [[spark.executor.logs.rolling.maxSize]] | spark.executor.heartbeatInterval a| [[spark.executor.heartbeatInterval]] Interval after which an executor:Executor.md[] reports heartbeat and metrics for active tasks to the driver Default: 10s Refer to executor:Executor.md#heartbeats-and-active-task-metrics[Sending heartbeats and partial metrics for active tasks] | spark.executor.heartbeat.maxFailures a| [[spark.executor.heartbeat.maxFailures]] Number of times an executor:Executor.md[] will try to send heartbeats to the driver before it gives up and exits (with exit code 56 ). Default: 60 NOTE: Introduced in https://issues.apache.org/jira/browse/SPARK-13522[SPARK-13522 Executor should kill itself when it's unable to heartbeat to the driver more than N times]. | spark.executor.instances a| [[spark.executor.instances]] Number of executor:Executor.md[] in use Default: 0 | spark.executor.userClassPathFirst a| [[spark.executor.userClassPathFirst]] Flag to control whether to load classes in user jars before those in Spark jars Default: false | spark.executor.memory a| [[spark.executor.memory]] Amount of memory to use for an executor:Executor.md[] Default: 1g Equivalent to SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable. Refer to executor:Executor.md#memory[Executor Memory -- spark.executor.memory or SPARK_EXECUTOR_MEMORY settings] | spark.executor.port a| [[spark.executor.port]] | spark.launcher.port a| [[spark.launcher.port]] | spark.launcher.secret a| [[spark.launcher.secret]] | spark.locality.wait a| [[spark.locality.wait]] For locality-aware delay scheduling for PROCESS_LOCAL , NODE_LOCAL , and RACK_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocalities] when locality-specific setting is not set. Default: 3s | spark.locality.wait.node a| [[spark.locality.wait.node]] Scheduling delay for NODE_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.locality.wait.process a| [[spark.locality.wait.process]] Scheduling delay for PROCESS_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.locality.wait.rack a| [[spark.locality.wait.rack]] Scheduling delay for RACK_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.logging.exceptionPrintInterval a| [[spark.logging.exceptionPrintInterval]] How frequently to reprint duplicate exceptions in full (in millis). Default: 10000 | spark.scheduler.allocation.file a| [[spark.scheduler.allocation.file]] Path to the configuration file of < > Default: fairscheduler.xml (on a Spark application's class path) | spark.scheduler.executorTaskBlacklistTime a| [[spark.scheduler.executorTaskBlacklistTime]] How long to wait before a task can be re-launched on the executor where it once failed. It is to prevent repeated task failures due to executor failures. Default: 0L | spark.scheduler.mode a| [[spark.scheduler.mode]][[SCHEDULER_MODE_PROPERTY]] Scheduling Mode of the scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl], i.e. case-insensitive name of the spark-scheduler-SchedulingMode.md[scheduling mode] that TaskSchedulerImpl uses to choose between the < > for task scheduling (of tasks of jobs submitted for execution to the same SparkContext ) Default: FIFO Supported values: FAIR for fair sharing (of cluster resources) FIFO (default) for queueing jobs one after another Task scheduling is an algorithm that is used to assign cluster resources (CPU cores and memory) to tasks (that are part of jobs with one or more stages). Fair sharing allows for executing tasks of different jobs at the same time (that were all submitted to the same SparkContext ). In FIFO scheduling mode a single SparkContext can submit a single job for execution only (regardless of how many cluster resources the job really use which could lead to a inefficient utilization of cluster resources and a longer execution of the Spark application overall). Scheduling mode is particularly useful in multi-tenant environments in which a single SparkContext could be shared across different users (to make a cluster resource utilization more efficient). | spark.starvation.timeout a| [[spark.starvation.timeout]] Threshold above which Spark warns a user that an initial TaskSet may be starved Default: 15s | spark.storage.exceptionOnPinLeak a| [[spark.storage.exceptionOnPinLeak]] | spark.unsafe.exceptionOnMemoryLeak a| [[spark.unsafe.exceptionOnMemoryLeak]] |=== == [[spark.memory.fraction]] spark.memory.fraction spark.memory.fraction is the fraction of JVM heap space used for execution and storage. Default: 0.6 == [[spark.shuffle.spill.batchSize]] spark.shuffle.spill.batchSize Size of object batches when reading or writing from serializers. Default: 10000 Used by shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap] and shuffle:ExternalSorter.md[ExternalSorter] == [[spark.shuffle.mapOutput.dispatcher.numThreads]] spark.shuffle.mapOutput.dispatcher.numThreads Default: 8 == [[spark.shuffle.mapOutput.minSizeForBroadcast]] spark.shuffle.mapOutput.minSizeForBroadcast Size of serialized shuffle map output statuses when scheduler:MapOutputTrackerMaster.md#MessageLoop[MapOutputTrackerMaster] uses to determine whether to use a broadcast variable to send them to executors Default: 512k Must be below spark.rpc.message.maxSize (to prevent sending an RPC message that is too large) == [[spark.shuffle.reduceLocality.enabled]] spark.shuffle.reduceLocality.enabled Enables locality preferences for reduce tasks Default: true When enabled ( true ), MapOutputTrackerMaster will scheduler:MapOutputTrackerMaster.md#getPreferredLocationsForShuffle[compute the preferred hosts] on which to run a given map output partition in a given shuffle, i.e. the nodes that the most outputs for that partition are on. == [[spark.shuffle.sort.initialBufferSize]] spark.shuffle.sort.initialBufferSize Initial buffer size for sorting Default: shuffle:UnsafeShuffleWriter.md#DEFAULT_INITIAL_SORT_BUFFER_SIZE[4096] Used exclusively when UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#open[open] (and creates a shuffle:ShuffleExternalSorter.md[ShuffleExternalSorter]) == [[spark.shuffle.unsafe.file.output.buffer]] spark.shuffle.unsafe.file.output.buffer The file system for this buffer size after each partition is written in unsafe shuffle writer. In KiB unless otherwise specified. Default: 32k Must be greater than 0 and less than or equal to 2097151 ( (Integer.MAX_VALUE - 15) / 1024 ) == [[spark.scheduler.maxRegisteredResourcesWaitingTime]] spark.scheduler.maxRegisteredResourcesWaitingTime Time to wait for sufficient resources available Default: 30s == [[spark.shuffle.unsafe.fastMergeEnabled]] spark.shuffle.unsafe.fastMergeEnabled Enables fast merge strategy for UnsafeShuffleWriter to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spill files]. Default: true == [[spark.shuffle.spill.compress]] spark.shuffle.spill.compress Controls whether to compress shuffle output temporarily spilled to disk. Default: true == [[spark.block.failures.beforeLocationRefresh]] spark.block.failures.beforeLocationRefresh Default: 5 == [[spark.io.encryption.enabled]] spark.io.encryption.enabled Controls whether to use IO encryption Default: false == [[spark.closure.serializer]] spark.closure.serializer serializer:Serializer.md[Serializer] Default: org.apache.spark.serializer.JavaSerializer == [[spark.io.compression.codec]] spark.io.compression.codec The default io:CompressionCodec.md[CompressionCodec] Default: lz4 == [[spark.io.compression.lz4.blockSize]] spark.io.compression.lz4.blockSize The block size of the io:CompressionCodec.md#LZ4CompressionCodec[LZ4CompressionCodec] Default: 32k == [[spark.io.compression.snappy.blockSize]] spark.io.compression.snappy.blockSize The block size of the io:CompressionCodec.md#SnappyCompressionCodec[SnappyCompressionCodec] Default: 32k == [[spark.io.compression.zstd.bufferSize]] spark.io.compression.zstd.bufferSize The buffer size of the BufferedOutputStream of the io:CompressionCodec.md#ZStdCompressionCodec[ZStdCompressionCodec] Default: 32k The buffer is used to avoid the overhead of excessive JNI calls while compressing or uncompressing small amount of data == [[spark.io.compression.zstd.level]] spark.io.compression.zstd.level The compression level of the io:CompressionCodec.md#ZStdCompressionCodec[ZStdCompressionCodec] Default: 1 The default level is the fastest of all with reasonably high compression ratio == [[spark.buffer.size]] spark.buffer.size Default: 65536 == [[spark.cleaner.referenceTracking.cleanCheckpoints]] spark.cleaner.referenceTracking.cleanCheckpoints Enables cleaning checkpoint files when a checkpointed reference is out of scope Default: false == [[spark.cleaner.periodicGC.interval]] spark.cleaner.periodicGC.interval Controls how often to trigger a garbage collection Default: 30min == [[spark.cleaner.referenceTracking.blocking]] spark.cleaner.referenceTracking.blocking Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by < >) Default: true == [[spark.cleaner.referenceTracking.blocking.shuffle]] spark.cleaner.referenceTracking.blocking.shuffle Controls whether the cleaning thread should block on shuffle cleanup tasks. Default: false == [[spark.broadcast.blockSize]] spark.broadcast.blockSize The size of a block (in kB unless the unit is specified) Default: 4m Used when core:TorrentBroadcast.md#writeBlocks[ TorrentBroadcast stores brodcast blocks to BlockManager ] == [[spark.broadcast.compress]] spark.broadcast.compress Controls broadcast compression Default: true Used when core:TorrentBroadcast.md#creating-instance[ TorrentBroadcast is created] and later when core:TorrentBroadcast.md#writeBlocks[it stores broadcast blocks to BlockManager ]. Also in serializer:SerializerManager.md#settings[SerializerManager]. == [[spark.app.name]] spark.app.name Application Name Default: (undefined) == [[spark.rpc.numRetries]] spark.rpc.numRetries Number of attempts to send a message to and receive a response from a remote endpoint. Default: 3 == [[spark.rpc.retry.wait]] spark.rpc.retry.wait Time to wait between retries. Default: 3s == [[spark.rpc.askTimeout]] spark.rpc.askTimeout Timeout for RPC ask calls Default: 120s == [[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Initial per-task memory size needed to store a block in memory. Default: 1024 * 1024 Must be at most the storage:MemoryStore.md#maxMemory[total amount of memory available for storage] Used when MemoryStore is requested to storage:MemoryStore.md#putIterator[putIterator] and storage:MemoryStore.md#putIteratorAsBytes[putIteratorAsBytes]","title":"Configuration Properties"},{"location":"configuration-properties/#spark-configuration-properties","text":"","title":"Spark Configuration Properties"},{"location":"configuration-properties/#sparkappid","text":"Unique identifier of a Spark application that Spark uses to uniquely identify metric sources . Default: TaskScheduler.applicationId() Set when SparkContext is created","title":" spark.app.id"},{"location":"configuration-properties/#sparkcleanerreferencetracking","text":"Controls whether to enable ContextCleaner Default: true","title":" spark.cleaner.referenceTracking"},{"location":"configuration-properties/#sparkdiskstoresubdirectories","text":"Number of subdirectories inside each path listed in spark.local.dir for hashing block files into. Default: 64 Used by BlockManager and DiskBlockManager","title":" spark.diskStore.subDirectories"},{"location":"configuration-properties/#sparkdriverhost","text":"Address of the driver (endpoints) Default: Utils.localCanonicalHostName","title":" spark.driver.host"},{"location":"configuration-properties/#sparkdrivermaxresultsize","text":"Maximum size of task results (in bytes) Default: 1g Used when: TaskRunner is requested to run a task (and decide on the type of a serialized task result ) TaskSetManager is requested to check available memory for task results","title":" spark.driver.maxResultSize"},{"location":"configuration-properties/#sparkdriverport","text":"Port of the driver (endpoints) Default: 0","title":" spark.driver.port"},{"location":"configuration-properties/#sparkexecutorcores","text":"Number of CPU cores for Executor Default: 1","title":" spark.executor.cores"},{"location":"configuration-properties/#sparkexecutorid","text":"Default: (undefined)","title":" spark.executor.id"},{"location":"configuration-properties/#sparkexecutormetricsfilesystemschemes","text":"A comma-separated list of the file system schemes to report in executor metrics Default: file,hdfs","title":" spark.executor.metrics.fileSystemSchemes"},{"location":"configuration-properties/#sparkextralisteners","text":"A comma-separated list of fully-qualified class names of SparkListener s (to be registered when SparkContext is created) Default: (empty)","title":" spark.extraListeners"},{"location":"configuration-properties/#sparkfiletransferto","text":"Controls whether to use Java FileChannel s (Java NIO) for copying data between two Java FileInputStream s to improve copy performance Default: true Used when: BypassMergeSortShuffleWriter and UnsafeShuffleWriter are created","title":" spark.file.transferTo"},{"location":"configuration-properties/#sparkfiles","text":"The files to be added to a Spark application (that can be defined directly as a configuration property or indirectly using --files option of spark-submit script) Default: (empty) Used when: SparkContext is created","title":" spark.files"},{"location":"configuration-properties/#sparkjars","text":"Default: (empty)","title":" spark.jars"},{"location":"configuration-properties/#sparkkryopool","text":"Default: true Used when: KryoSerializer is created","title":" spark.kryo.pool"},{"location":"configuration-properties/#sparkkryounsafe","text":"Whether KryoSerializer should use Unsafe-based IO for serialization Default: false","title":" spark.kryo.unsafe"},{"location":"configuration-properties/#sparklocaldir","text":"A comma-separated list of directories that are used as a temporary storage for \"scratch\" space (incl. map output files and RDDs that get stored on disk). This should be on a fast, local disk in your system. Default: /tmp","title":" spark.local.dir"},{"location":"configuration-properties/#sparklogconf","text":"Default: false","title":" spark.logConf"},{"location":"configuration-properties/#sparkloglineage","text":"Default: false","title":" spark.logLineage"},{"location":"configuration-properties/#sparkmaster","text":"Master URL of the cluster manager to connect the Spark application to","title":" spark.master"},{"location":"configuration-properties/#sparkmemoryoffheapenabled","text":"Controls whether Tungsten memory will be allocated on the JVM heap ( false ) or off-heap ( true / using sun.misc.Unsafe ). Default: false When enabled, spark.memory.offHeap.size must be greater than 0 . Used when: MemoryManager is requested for tungstenMemoryMode","title":" spark.memory.offHeap.enabled"},{"location":"configuration-properties/#sparkmemoryoffheapsize","text":"Maximum memory (in bytes) for off-heap memory allocation Default: 0 This setting has no impact on heap memory usage, so if your executors' total memory consumption must fit within some hard limit then be sure to shrink your JVM heap size accordingly. Must not be negative and be set to a positive value when spark.memory.offHeap.enabled is enabled","title":" spark.memory.offHeap.size"},{"location":"configuration-properties/#sparkmemorystoragefraction","text":"Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by spark.memory.fraction. The higher this is, the less working memory may be available to execution and tasks may spill to disk more often. Leaving this at the default value is recommended Default: 0.5 Must be in [0,1) Used when: UnifiedMemoryManager is created MemoryManager is created","title":" spark.memory.storageFraction"},{"location":"configuration-properties/#sparknetworkmaxremoteblocksizefetchtomem","text":"Remote block will be fetched to disk when size of the block is above this threshold in bytes This is to avoid a giant request takes too much memory. Note this configuration will affect both shuffle fetch and block manager remote block fetch. With an external shuffle service use at least 2.3.0 Default: 200m Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task NettyBlockTransferService is requested to uploadBlock BlockManager is requested to fetchRemoteManagedBuffer","title":" spark.network.maxRemoteBlockSizeFetchToMem"},{"location":"configuration-properties/#sparknetworktimeout","text":"Network timeout (in seconds) to use for RPC remote endpoint lookup Default: 120s","title":" spark.network.timeout"},{"location":"configuration-properties/#sparknetworktimeoutinterval","text":"(in millis) Default: spark.storage.blockManagerTimeoutIntervalMs","title":" spark.network.timeoutInterval"},{"location":"configuration-properties/#sparkrddcompress","text":"Controls whether to compress RDD partitions when stored serialized Default: false","title":" spark.rdd.compress"},{"location":"configuration-properties/#sparkreducermaxblocksinflightperaddress","text":"Maximum number of remote blocks being fetched per reduce task from a given host port When a large number of blocks are being requested from a given address in a single fetch or simultaneously, this could crash the serving executor or a Node Manager. This is especially useful to reduce the load on the Node Manager when external shuffle is enabled. You can mitigate the issue by setting it to a lower value. Default: (unlimited) Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task","title":" spark.reducer.maxBlocksInFlightPerAddress"},{"location":"configuration-properties/#sparkreducermaxreqsinflight","text":"Maximum number of remote requests to fetch blocks at any given point When the number of hosts in the cluster increase, it might lead to very large number of inbound connections to one or more nodes, causing the workers to fail under load. By allowing it to limit the number of fetch requests, this scenario can be mitigated Default: (unlimited) Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task","title":" spark.reducer.maxReqsInFlight"},{"location":"configuration-properties/#sparkreducermaxsizeinflight","text":"Maximum size of all map outputs to fetch simultaneously from each reduce task (in MiB unless otherwise specified) Since each output requires us to create a buffer to receive it, this represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory Default: 48m Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task","title":" spark.reducer.maxSizeInFlight"},{"location":"configuration-properties/#sparkreplclassuri","text":"Controls whether to compress RDD partitions when stored serialized Default: false","title":" spark.repl.class.uri"},{"location":"configuration-properties/#sparkrpclookuptimeout","text":"Default Endpoint Lookup Timeout Default: 120s","title":" spark.rpc.lookupTimeout"},{"location":"configuration-properties/#sparkrpcmessagemaxsize","text":"Maximum allowed message size for RPC communication (in MB unless specified) Default: 128 Must be below 2047MB ( Int.MaxValue / 1024 / 1024 ) Used when: CoarseGrainedSchedulerBackend is requested to launch tasks RpcUtils is requested for the maximum message size Executor is created MapOutputTrackerMaster is created (and makes sure that spark.shuffle.mapOutput.minSizeForBroadcast is below the threshold)","title":" spark.rpc.message.maxSize"},{"location":"configuration-properties/#sparkschedulerminregisteredresourcesratio","text":"Minimum ratio of (registered resources / total expected resources) before submitting tasks Default: (undefined)","title":" spark.scheduler.minRegisteredResourcesRatio"},{"location":"configuration-properties/#sparkschedulerreviveinterval","text":"Revive Interval that is the time (in millis) between resource offers revives Default: 1s Used when: DriverEndpoint is requested to onStart","title":" spark.scheduler.revive.interval"},{"location":"configuration-properties/#sparkserializer","text":"The fully-qualified class name of the Serializer (of the driver and executors ) Default: org.apache.spark.serializer.JavaSerializer Used when: SparkEnv utility is used to create a SparkEnv SparkConf is requested to registerKryoClasses (as a side-effect)","title":" spark.serializer"},{"location":"configuration-properties/#sparkshufflecompress","text":"Controls whether to compress shuffle output when stored Default: true","title":" spark.shuffle.compress"},{"location":"configuration-properties/#sparkshuffledetectcorrupt","text":"Controls corruption detection in fetched blocks Default: true Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task","title":" spark.shuffle.detectCorrupt"},{"location":"configuration-properties/#sparkshuffledetectcorruptuseextramemory","text":"If enabled, part of a compressed/encrypted stream will be de-compressed/de-crypted by using extra memory to detect early corruption. Any IOException thrown will cause the task to be retried once and if it fails again with same exception, then FetchFailedException will be thrown to retry previous stage Default: false Used when: BlockStoreShuffleReader is requested to read combined records for a reduce task","title":" spark.shuffle.detectCorrupt.useExtraMemory"},{"location":"configuration-properties/#sparkshufflefilebuffer","text":"Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise specified. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files. Default: 32k Must be greater than 0 and less than or equal to 2097151 ( (Integer.MAX_VALUE - 15) / 1024 ) Used when the following are created: BypassMergeSortShuffleWriter ShuffleExternalSorter UnsafeShuffleWriter ExternalAppendOnlyMap ExternalSorter","title":" spark.shuffle.file.buffer"},{"location":"configuration-properties/#sparkshufflemanager","text":"A fully-qualified class name or the alias of the ShuffleManager in a Spark application Default: sort Supported aliases: sort tungsten-sort Used when SparkEnv object is requested to create a \"base\" SparkEnv for a driver or an executor","title":" spark.shuffle.manager"},{"location":"configuration-properties/#sparkshufflemapoutputparallelaggregationthreshold","text":"(internal) Multi-thread is used when the number of mappers * shuffle partitions is greater than or equal to this threshold. Note that the actual parallelism is calculated by number of mappers * shuffle partitions / this threshold + 1, so this threshold should be positive. Default: 10000000 Used when: MapOutputTrackerMaster is requested for the statistics of a ShuffleDependency","title":" spark.shuffle.mapOutput.parallelAggregationThreshold"},{"location":"configuration-properties/#sparkshuffleminnumpartitionstohighlycompress","text":"(internal) Minimum number of partitions (threshold) for MapStatus utility to prefer a HighlyCompressedMapStatus (over CompressedMapStatus ) (for ShuffleWriters ). Default: 2000 Must be a positive integer (above 0 )","title":" spark.shuffle.minNumPartitionsToHighlyCompress"},{"location":"configuration-properties/#sparkshufflereadhostlocaldisk","text":"If enabled (with spark.shuffle.useOldFetchProtocol disabled and spark.shuffle.service.enabled enabled), shuffle blocks requested from those block managers which are running on the same host are read from the disk directly instead of being fetched as remote blocks over the network. Default: true","title":" spark.shuffle.readHostLocalDisk"},{"location":"configuration-properties/#sparkshuffleregistrationmaxattempts","text":"How many attempts to register a BlockManager with External Shuffle Service Default: 3 Used when BlockManager is requested to register with External Shuffle Server","title":" spark.shuffle.registration.maxAttempts"},{"location":"configuration-properties/#sparkshufflesortbypassmergethreshold","text":"Maximum number of reduce partitions below which SortShuffleManager avoids merge-sorting data for no map-side aggregation Default: 200 Used when: SortShuffleWriter utility is used to shouldBypassMergeSort ShuffleExchangeExec ( Spark SQL ) physical operator is requested to prepareShuffleDependency","title":" spark.shuffle.sort.bypassMergeThreshold"},{"location":"configuration-properties/#sparkshufflesortiopluginclass","text":"Name of the class to use for shuffle IO Default: LocalDiskShuffleDataIO","title":" spark.shuffle.sort.io.plugin.class"},{"location":"configuration-properties/#sparkshufflespillinitialmemorythreshold","text":"Initial threshold for the size of an in-memory collection Default: 5MB Used by Spillable","title":" spark.shuffle.spill.initialMemoryThreshold"},{"location":"configuration-properties/#sparkshufflespillnumelementsforcespillthreshold","text":"(internal) The maximum number of elements in memory before forcing the shuffle sorter to spill. Default: Integer.MAX_VALUE The default value is to never force the sorter to spill, until Spark reaches some limitations, like the max page size limitation for the pointer array in the sorter. Used when: ShuffleExternalSorter is created Spillable is created Spark SQL's SortBasedAggregator is requested for an UnsafeKVExternalSorter Spark SQL's ObjectAggregationMap is requested to dumpToExternalSorter Spark SQL's UnsafeExternalRowSorter is created Spark SQL's UnsafeFixedWidthAggregationMap is requested for an UnsafeKVExternalSorter","title":" spark.shuffle.spill.numElementsForceSpillThreshold"},{"location":"configuration-properties/#sparkshufflesync","text":"Controls whether DiskBlockObjectWriter should force outstanding writes to disk while committing a single atomic block (i.e. all operating system buffers should synchronize with the disk to ensure that all changes to a file are in fact recorded in the storage) Default: false Used when BlockManager is requested for a DiskBlockObjectWriter","title":" spark.shuffle.sync"},{"location":"configuration-properties/#sparkshuffleuseoldfetchprotocol","text":"Whether to use the old protocol while doing the shuffle block fetching. It is only enabled while we need the compatibility in the scenario of new Spark version job fetching shuffle blocks from old version external shuffle service. Default: false","title":" spark.shuffle.useOldFetchProtocol"},{"location":"configuration-properties/#sparkspeculation","text":"Controls Speculative Execution of Tasks Default: false","title":" spark.speculation"},{"location":"configuration-properties/#sparkspeculationinterval","text":"The time interval to use before checking for speculative tasks in Speculative Execution of Tasks . Default: 100ms","title":" spark.speculation.interval"},{"location":"configuration-properties/#sparkspeculationmultiplier","text":"Default: 1.5","title":" spark.speculation.multiplier"},{"location":"configuration-properties/#sparkspeculationquantile","text":"The percentage of tasks that has not finished yet at which to start speculation in Speculative Execution of Tasks . Default: 0.75","title":" spark.speculation.quantile"},{"location":"configuration-properties/#sparkstorageblockmanagerslavetimeoutms","text":"(in millis) Default: spark.network.timeout","title":" spark.storage.blockManagerSlaveTimeoutMs"},{"location":"configuration-properties/#sparkstorageblockmanagertimeoutintervalms","text":"(in millis) Default: 60s","title":" spark.storage.blockManagerTimeoutIntervalMs"},{"location":"configuration-properties/#sparkstoragelocaldiskbyexecutorscachesize","text":"The max number of executors for which the local dirs are stored. This size is both applied for the driver and both for the executors side to avoid having an unbounded store. This cache will be used to avoid the network in case of fetching disk persisted RDD blocks or shuffle blocks (when spark.shuffle.readHostLocalDisk is set) from the same host. Default: 1000","title":" spark.storage.localDiskByExecutors.cacheSize"},{"location":"configuration-properties/#sparkstoragereplicationpolicy","text":"Default: RandomBlockReplicationPolicy","title":" spark.storage.replication.policy"},{"location":"configuration-properties/#sparktaskcpus","text":"The number of CPU cores to schedule ( allocate ) to a task Default: 1 Used when: ExecutorAllocationManager is created TaskSchedulerImpl is created AppStatusListener is requested to handle a SparkListenerEnvironmentUpdate event SparkContext utility is used to create a TaskScheduler ResourceProfile is requested to getDefaultTaskResources LocalityPreferredContainerPlacementStrategy is requested to numExecutorsPending","title":" spark.task.cpus"},{"location":"configuration-properties/#sparktaskmaxdirectresultsize","text":"Maximum size of a task result (in bytes) to be sent to the driver as a DirectTaskResult Default: 1048576B ( 1L << 20 ) Used when: TaskRunner is requested to run a task (and decide on the type of a serialized task result )","title":" spark.task.maxDirectResultSize"},{"location":"configuration-properties/#sparktaskmaxfailures","text":"Number of failures of a single task (of a TaskSet ) before giving up on the entire TaskSet and then the job Default: 4","title":" spark.task.maxFailures"},{"location":"configuration-properties/#sparkplugins","text":"A comma-separated list of class names implementing org.apache.spark.api.plugin.SparkPlugin to load into a Spark application. Default: (empty) Since: 3.0.0 Set when SparkContext is created","title":" spark.plugins"},{"location":"configuration-properties/#sparkpluginsdefaultlist","text":"FIXME","title":" spark.plugins.defaultList"},{"location":"configuration-properties/#sparkuishowconsoleprogress","text":"Controls whether to enable ConsoleProgressBar and show the progress bar in the console Default: false == [[properties]] Properties [cols=\"1m,1\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.blockManager.port a| [[spark.blockManager.port]][[BLOCK_MANAGER_PORT]] Port to use for block managers to listen on when a more specific setting is not provided (i.e. < > for the driver). Default: 0 In Spark on Kubernetes the default port is 7079 | spark.default.parallelism a| [[spark.default.parallelism]] Number of partitions to use for rdd:HashPartitioner.md[HashPartitioner] spark.default.parallelism corresponds to scheduler:SchedulerBackend.md#defaultParallelism[default parallelism] of a scheduler backend and is as follows: The number of threads for local/spark-LocalSchedulerBackend.md[LocalSchedulerBackend]. the number of CPU cores in spark-mesos.md#defaultParallelism[Spark on Mesos] and defaults to 8 . Maximum of totalCoreCount and 2 in scheduler:CoarseGrainedSchedulerBackend.md#defaultParallelism[CoarseGrainedSchedulerBackend]. | spark.driver.blockManager.port a| [[spark.driver.blockManager.port]][[DRIVER_BLOCK_MANAGER_PORT]] Port the storage:BlockManager.md[block manager] on the driver listens on Default: < > | spark.executor.extraClassPath a| [[spark.executor.extraClassPath]][[EXECUTOR_CLASS_PATH]] User-defined class path for executors , i.e. URLs representing user-defined class path entries that are added to an executor's class path. URLs are separated by system-dependent path separator, i.e. : on Unix-like systems and ; on Microsoft Windows. Default: (empty) Used when: Spark Standalone's StandaloneSchedulerBackend is requested to spark-standalone:StandaloneSchedulerBackend.md#start[start] (and creates a command for executor:CoarseGrainedExecutorBackend.md[]) Spark local's LocalSchedulerBackend is requested for the spark-local:spark-LocalSchedulerBackend.md#getUserClasspath[user-defined class path for executors] Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to spark-on-mesos:spark-mesos-MesosCoarseGrainedSchedulerBackend.md#createCommand[create a command for CoarseGrainedExecutorBackend] Spark on Mesos' MesosFineGrainedSchedulerBackend is requested to create a command for MesosExecutorBackend Spark on Kubernetes' BasicExecutorFeatureStep is requested to configurePod Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareEnvironment[prepareEnvironment] (for CoarseGrainedExecutorBackend ) | spark.executor.extraJavaOptions a| [[spark.executor.extraJavaOptions]] Extra Java options of an executor:Executor.md[] Used when Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareCommand[prepare the command to launch CoarseGrainedExecutorBackend in a YARN container] | spark.executor.extraLibraryPath a| [[spark.executor.extraLibraryPath]] Extra library paths separated by system-dependent path separator, i.e. : on Unix/MacOS systems and ; on Microsoft Windows Used when Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareCommand[prepare the command to launch CoarseGrainedExecutorBackend in a YARN container] | spark.executor.uri a| [[spark.executor.uri]] Equivalent to SPARK_EXECUTOR_URI | spark.executor.logs.rolling.time.interval a| [[spark.executor.logs.rolling.time.interval]] | spark.executor.logs.rolling.strategy a| [[spark.executor.logs.rolling.strategy]] | spark.executor.logs.rolling.maxRetainedFiles a| [[spark.executor.logs.rolling.maxRetainedFiles]] | spark.executor.logs.rolling.maxSize a| [[spark.executor.logs.rolling.maxSize]] | spark.executor.heartbeatInterval a| [[spark.executor.heartbeatInterval]] Interval after which an executor:Executor.md[] reports heartbeat and metrics for active tasks to the driver Default: 10s Refer to executor:Executor.md#heartbeats-and-active-task-metrics[Sending heartbeats and partial metrics for active tasks] | spark.executor.heartbeat.maxFailures a| [[spark.executor.heartbeat.maxFailures]] Number of times an executor:Executor.md[] will try to send heartbeats to the driver before it gives up and exits (with exit code 56 ). Default: 60 NOTE: Introduced in https://issues.apache.org/jira/browse/SPARK-13522[SPARK-13522 Executor should kill itself when it's unable to heartbeat to the driver more than N times]. | spark.executor.instances a| [[spark.executor.instances]] Number of executor:Executor.md[] in use Default: 0 | spark.executor.userClassPathFirst a| [[spark.executor.userClassPathFirst]] Flag to control whether to load classes in user jars before those in Spark jars Default: false | spark.executor.memory a| [[spark.executor.memory]] Amount of memory to use for an executor:Executor.md[] Default: 1g Equivalent to SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable. Refer to executor:Executor.md#memory[Executor Memory -- spark.executor.memory or SPARK_EXECUTOR_MEMORY settings] | spark.executor.port a| [[spark.executor.port]] | spark.launcher.port a| [[spark.launcher.port]] | spark.launcher.secret a| [[spark.launcher.secret]] | spark.locality.wait a| [[spark.locality.wait]] For locality-aware delay scheduling for PROCESS_LOCAL , NODE_LOCAL , and RACK_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocalities] when locality-specific setting is not set. Default: 3s | spark.locality.wait.node a| [[spark.locality.wait.node]] Scheduling delay for NODE_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.locality.wait.process a| [[spark.locality.wait.process]] Scheduling delay for PROCESS_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.locality.wait.rack a| [[spark.locality.wait.rack]] Scheduling delay for RACK_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.logging.exceptionPrintInterval a| [[spark.logging.exceptionPrintInterval]] How frequently to reprint duplicate exceptions in full (in millis). Default: 10000 | spark.scheduler.allocation.file a| [[spark.scheduler.allocation.file]] Path to the configuration file of < > Default: fairscheduler.xml (on a Spark application's class path) | spark.scheduler.executorTaskBlacklistTime a| [[spark.scheduler.executorTaskBlacklistTime]] How long to wait before a task can be re-launched on the executor where it once failed. It is to prevent repeated task failures due to executor failures. Default: 0L | spark.scheduler.mode a| [[spark.scheduler.mode]][[SCHEDULER_MODE_PROPERTY]] Scheduling Mode of the scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl], i.e. case-insensitive name of the spark-scheduler-SchedulingMode.md[scheduling mode] that TaskSchedulerImpl uses to choose between the < > for task scheduling (of tasks of jobs submitted for execution to the same SparkContext ) Default: FIFO Supported values: FAIR for fair sharing (of cluster resources) FIFO (default) for queueing jobs one after another Task scheduling is an algorithm that is used to assign cluster resources (CPU cores and memory) to tasks (that are part of jobs with one or more stages). Fair sharing allows for executing tasks of different jobs at the same time (that were all submitted to the same SparkContext ). In FIFO scheduling mode a single SparkContext can submit a single job for execution only (regardless of how many cluster resources the job really use which could lead to a inefficient utilization of cluster resources and a longer execution of the Spark application overall). Scheduling mode is particularly useful in multi-tenant environments in which a single SparkContext could be shared across different users (to make a cluster resource utilization more efficient). | spark.starvation.timeout a| [[spark.starvation.timeout]] Threshold above which Spark warns a user that an initial TaskSet may be starved Default: 15s | spark.storage.exceptionOnPinLeak a| [[spark.storage.exceptionOnPinLeak]] | spark.unsafe.exceptionOnMemoryLeak a| [[spark.unsafe.exceptionOnMemoryLeak]] |=== == [[spark.memory.fraction]] spark.memory.fraction spark.memory.fraction is the fraction of JVM heap space used for execution and storage. Default: 0.6 == [[spark.shuffle.spill.batchSize]] spark.shuffle.spill.batchSize Size of object batches when reading or writing from serializers. Default: 10000 Used by shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap] and shuffle:ExternalSorter.md[ExternalSorter] == [[spark.shuffle.mapOutput.dispatcher.numThreads]] spark.shuffle.mapOutput.dispatcher.numThreads Default: 8 == [[spark.shuffle.mapOutput.minSizeForBroadcast]] spark.shuffle.mapOutput.minSizeForBroadcast Size of serialized shuffle map output statuses when scheduler:MapOutputTrackerMaster.md#MessageLoop[MapOutputTrackerMaster] uses to determine whether to use a broadcast variable to send them to executors Default: 512k Must be below spark.rpc.message.maxSize (to prevent sending an RPC message that is too large) == [[spark.shuffle.reduceLocality.enabled]] spark.shuffle.reduceLocality.enabled Enables locality preferences for reduce tasks Default: true When enabled ( true ), MapOutputTrackerMaster will scheduler:MapOutputTrackerMaster.md#getPreferredLocationsForShuffle[compute the preferred hosts] on which to run a given map output partition in a given shuffle, i.e. the nodes that the most outputs for that partition are on. == [[spark.shuffle.sort.initialBufferSize]] spark.shuffle.sort.initialBufferSize Initial buffer size for sorting Default: shuffle:UnsafeShuffleWriter.md#DEFAULT_INITIAL_SORT_BUFFER_SIZE[4096] Used exclusively when UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#open[open] (and creates a shuffle:ShuffleExternalSorter.md[ShuffleExternalSorter]) == [[spark.shuffle.unsafe.file.output.buffer]] spark.shuffle.unsafe.file.output.buffer The file system for this buffer size after each partition is written in unsafe shuffle writer. In KiB unless otherwise specified. Default: 32k Must be greater than 0 and less than or equal to 2097151 ( (Integer.MAX_VALUE - 15) / 1024 ) == [[spark.scheduler.maxRegisteredResourcesWaitingTime]] spark.scheduler.maxRegisteredResourcesWaitingTime Time to wait for sufficient resources available Default: 30s == [[spark.shuffle.unsafe.fastMergeEnabled]] spark.shuffle.unsafe.fastMergeEnabled Enables fast merge strategy for UnsafeShuffleWriter to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spill files]. Default: true == [[spark.shuffle.spill.compress]] spark.shuffle.spill.compress Controls whether to compress shuffle output temporarily spilled to disk. Default: true == [[spark.block.failures.beforeLocationRefresh]] spark.block.failures.beforeLocationRefresh Default: 5 == [[spark.io.encryption.enabled]] spark.io.encryption.enabled Controls whether to use IO encryption Default: false == [[spark.closure.serializer]] spark.closure.serializer serializer:Serializer.md[Serializer] Default: org.apache.spark.serializer.JavaSerializer == [[spark.io.compression.codec]] spark.io.compression.codec The default io:CompressionCodec.md[CompressionCodec] Default: lz4 == [[spark.io.compression.lz4.blockSize]] spark.io.compression.lz4.blockSize The block size of the io:CompressionCodec.md#LZ4CompressionCodec[LZ4CompressionCodec] Default: 32k == [[spark.io.compression.snappy.blockSize]] spark.io.compression.snappy.blockSize The block size of the io:CompressionCodec.md#SnappyCompressionCodec[SnappyCompressionCodec] Default: 32k == [[spark.io.compression.zstd.bufferSize]] spark.io.compression.zstd.bufferSize The buffer size of the BufferedOutputStream of the io:CompressionCodec.md#ZStdCompressionCodec[ZStdCompressionCodec] Default: 32k The buffer is used to avoid the overhead of excessive JNI calls while compressing or uncompressing small amount of data == [[spark.io.compression.zstd.level]] spark.io.compression.zstd.level The compression level of the io:CompressionCodec.md#ZStdCompressionCodec[ZStdCompressionCodec] Default: 1 The default level is the fastest of all with reasonably high compression ratio == [[spark.buffer.size]] spark.buffer.size Default: 65536 == [[spark.cleaner.referenceTracking.cleanCheckpoints]] spark.cleaner.referenceTracking.cleanCheckpoints Enables cleaning checkpoint files when a checkpointed reference is out of scope Default: false == [[spark.cleaner.periodicGC.interval]] spark.cleaner.periodicGC.interval Controls how often to trigger a garbage collection Default: 30min == [[spark.cleaner.referenceTracking.blocking]] spark.cleaner.referenceTracking.blocking Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by < >) Default: true == [[spark.cleaner.referenceTracking.blocking.shuffle]] spark.cleaner.referenceTracking.blocking.shuffle Controls whether the cleaning thread should block on shuffle cleanup tasks. Default: false == [[spark.broadcast.blockSize]] spark.broadcast.blockSize The size of a block (in kB unless the unit is specified) Default: 4m Used when core:TorrentBroadcast.md#writeBlocks[ TorrentBroadcast stores brodcast blocks to BlockManager ] == [[spark.broadcast.compress]] spark.broadcast.compress Controls broadcast compression Default: true Used when core:TorrentBroadcast.md#creating-instance[ TorrentBroadcast is created] and later when core:TorrentBroadcast.md#writeBlocks[it stores broadcast blocks to BlockManager ]. Also in serializer:SerializerManager.md#settings[SerializerManager]. == [[spark.app.name]] spark.app.name Application Name Default: (undefined) == [[spark.rpc.numRetries]] spark.rpc.numRetries Number of attempts to send a message to and receive a response from a remote endpoint. Default: 3 == [[spark.rpc.retry.wait]] spark.rpc.retry.wait Time to wait between retries. Default: 3s == [[spark.rpc.askTimeout]] spark.rpc.askTimeout Timeout for RPC ask calls Default: 120s == [[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Initial per-task memory size needed to store a block in memory. Default: 1024 * 1024 Must be at most the storage:MemoryStore.md#maxMemory[total amount of memory available for storage] Used when MemoryStore is requested to storage:MemoryStore.md#putIterator[putIterator] and storage:MemoryStore.md#putIteratorAsBytes[putIteratorAsBytes]","title":" spark.ui.showConsoleProgress"},{"location":"data-locality/","text":"= Data Locality / Placement Spark relies on data locality , aka data placement or proximity to data source , that makes Spark jobs sensitive to where the data is located. It is therefore important to have yarn/README.md[Spark running on Hadoop YARN cluster] if the data comes from HDFS. In yarn/README.md[Spark on YARN] Spark tries to place tasks alongside HDFS blocks. With HDFS the Spark driver contacts NameNode about the DataNodes (ideally local) containing the various blocks of a file or directory as well as their locations (represented as InputSplits ), and then schedules the work to the SparkWorkers. Spark's compute nodes / workers should be running on storage nodes. Concept of locality-aware scheduling . Spark tries to execute tasks as close to the data as possible to minimize data transfer (over the wire). .Locality Level in the Spark UI image::sparkui-stages-locality-level.png[] There are the following task localities (consult https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskLocality$[org.apache.spark.scheduler.TaskLocality ] object): PROCESS_LOCAL NODE_LOCAL NO_PREF RACK_LOCAL ANY Task location can either be a host or a pair of a host and an executor.","title":"Data Locality"},{"location":"driver/","text":"Driver \u00b6 A Spark driver ( aka an application's driver process ) is a JVM process that hosts SparkContext.md[SparkContext] for a Spark application. It is the master node in a Spark application. It is the cockpit of jobs and tasks execution (using scheduler:DAGScheduler.md[DAGScheduler] and scheduler:TaskScheduler.md[Task Scheduler]). It hosts spark-webui.md[Web UI] for the environment. .Driver with the services image::spark-driver.png[align=\"center\"] It splits a Spark application into tasks and schedules them to run on executors. A driver is where the task scheduler lives and spawns tasks across workers. A driver coordinates workers and overall execution of tasks. NOTE: spark-shell.md[Spark shell] is a Spark application and the driver. It creates a SparkContext that is available as sc . Driver requires the additional services (beside the common ones like shuffle:ShuffleManager.md[], memory:MemoryManager.md[], storage:BlockTransferService.md[], core:BroadcastManager.md[]): Listener Bus rpc:index.md[] scheduler:MapOutputTrackerMaster.md[] with the name MapOutputTracker storage:BlockManagerMaster.md[] with the name BlockManagerMaster MetricsSystem with the name driver OutputCommitCoordinator CAUTION: FIXME Diagram of RpcEnv for a driver (and later executors). Perhaps it should be in the notes about RpcEnv? High-level control flow of work Your Spark application runs as long as the Spark driver. ** Once the driver terminates, so does your Spark application. Creates SparkContext , RDD 's, and executes transformations and actions Launches scheduler:Task.md[tasks] === [[driver-memory]] Driver's Memory It can be set first using spark-submit.md#command-line-options[spark-submit's --driver-memory ] command-line option or < > and falls back to spark-submit.md#environment-variables[SPARK_DRIVER_MEMORY] if not set earlier. NOTE: It is printed out to the standard error output in spark-submit.md#verbose-mode[spark-submit's verbose mode]. === [[driver-memory]] Driver's Cores It can be set first using spark-submit.md#driver-cores[spark-submit's --driver-cores ] command-line option for spark-deploy-mode.md#cluster[ cluster deploy mode]. NOTE: In spark-deploy-mode.md#client[ client deploy mode] the driver's memory corresponds to the memory of the JVM process the Spark application runs on. NOTE: It is printed out to the standard error output in spark-submit.md#verbose-mode[spark-submit's verbose mode]. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_driver_blockManager_port]] spark.driver.blockManager.port | storage:BlockManager.md#spark_blockManager_port[spark.blockManager.port] | Port to use for the storage:BlockManager.md[BlockManager] on the driver. More precisely, spark.driver.blockManager.port is used when core:SparkEnv.md#NettyBlockTransferService[ NettyBlockTransferService is created] (while SparkEnv is created for the driver). | [[spark_driver_memory]] spark.driver.memory | 1g | The driver's memory size (in MiBs). Refer to < >. | [[spark_driver_cores]] spark.driver.cores | 1 | The number of CPU cores assigned to the driver in spark-deploy-mode.md#cluster[cluster deploy mode]. NOTE: When yarn/spark-yarn-client.md#creating-instance[Client is created] (for Spark on YARN in cluster mode only), it sets the number of cores for ApplicationManager using spark.driver.cores . Refer to < >. | [[spark_driver_extraLibraryPath]] spark.driver.extraLibraryPath | | | [[spark_driver_extraJavaOptions]] spark.driver.extraJavaOptions | | Additional JVM options for the driver. | [[spark.driver.appUIAddress]] spark.driver.appUIAddress spark.driver.appUIAddress is used exclusively in yarn/README.md[Spark on YARN]. It is set when yarn/spark-yarn-client-yarnclientschedulerbackend.md#start[YarnClientSchedulerBackend starts] to yarn/spark-yarn-applicationmaster.md#runExecutorLauncher[run ExecutorLauncher] (and yarn/spark-yarn-applicationmaster.md#registerAM[register ApplicationMaster] for the Spark application). | [[spark_driver_libraryPath]] spark.driver.libraryPath | | |=== ==== [[spark_driver_extraClassPath]] spark.driver.extraClassPath spark.driver.extraClassPath system property sets the additional classpath entries (e.g. jars and directories) that should be added to the driver's classpath in spark-deploy-mode.md#cluster[ cluster deploy mode]. [NOTE] \u00b6 For spark-deploy-mode.md#client[ client deploy mode] you can use a properties file or command line to set spark.driver.extraClassPath . Do not use SparkConf.md[SparkConf] since it is too late for client deploy mode given the JVM has already been set up to start a Spark application. Refer to spark-class.md#buildSparkSubmitCommand[ buildSparkSubmitCommand Internal Method] for the very low-level details of how it is handled internally. \u00b6 spark.driver.extraClassPath uses a OS-specific path separator. NOTE: Use spark-submit 's spark-submit.md#driver-class-path[ --driver-class-path command-line option] on command line to override spark.driver.extraClassPath from a spark-properties.md#spark-defaults-conf[Spark properties file].","title":"Driver"},{"location":"driver/#driver","text":"A Spark driver ( aka an application's driver process ) is a JVM process that hosts SparkContext.md[SparkContext] for a Spark application. It is the master node in a Spark application. It is the cockpit of jobs and tasks execution (using scheduler:DAGScheduler.md[DAGScheduler] and scheduler:TaskScheduler.md[Task Scheduler]). It hosts spark-webui.md[Web UI] for the environment. .Driver with the services image::spark-driver.png[align=\"center\"] It splits a Spark application into tasks and schedules them to run on executors. A driver is where the task scheduler lives and spawns tasks across workers. A driver coordinates workers and overall execution of tasks. NOTE: spark-shell.md[Spark shell] is a Spark application and the driver. It creates a SparkContext that is available as sc . Driver requires the additional services (beside the common ones like shuffle:ShuffleManager.md[], memory:MemoryManager.md[], storage:BlockTransferService.md[], core:BroadcastManager.md[]): Listener Bus rpc:index.md[] scheduler:MapOutputTrackerMaster.md[] with the name MapOutputTracker storage:BlockManagerMaster.md[] with the name BlockManagerMaster MetricsSystem with the name driver OutputCommitCoordinator CAUTION: FIXME Diagram of RpcEnv for a driver (and later executors). Perhaps it should be in the notes about RpcEnv? High-level control flow of work Your Spark application runs as long as the Spark driver. ** Once the driver terminates, so does your Spark application. Creates SparkContext , RDD 's, and executes transformations and actions Launches scheduler:Task.md[tasks] === [[driver-memory]] Driver's Memory It can be set first using spark-submit.md#command-line-options[spark-submit's --driver-memory ] command-line option or < > and falls back to spark-submit.md#environment-variables[SPARK_DRIVER_MEMORY] if not set earlier. NOTE: It is printed out to the standard error output in spark-submit.md#verbose-mode[spark-submit's verbose mode]. === [[driver-memory]] Driver's Cores It can be set first using spark-submit.md#driver-cores[spark-submit's --driver-cores ] command-line option for spark-deploy-mode.md#cluster[ cluster deploy mode]. NOTE: In spark-deploy-mode.md#client[ client deploy mode] the driver's memory corresponds to the memory of the JVM process the Spark application runs on. NOTE: It is printed out to the standard error output in spark-submit.md#verbose-mode[spark-submit's verbose mode]. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_driver_blockManager_port]] spark.driver.blockManager.port | storage:BlockManager.md#spark_blockManager_port[spark.blockManager.port] | Port to use for the storage:BlockManager.md[BlockManager] on the driver. More precisely, spark.driver.blockManager.port is used when core:SparkEnv.md#NettyBlockTransferService[ NettyBlockTransferService is created] (while SparkEnv is created for the driver). | [[spark_driver_memory]] spark.driver.memory | 1g | The driver's memory size (in MiBs). Refer to < >. | [[spark_driver_cores]] spark.driver.cores | 1 | The number of CPU cores assigned to the driver in spark-deploy-mode.md#cluster[cluster deploy mode]. NOTE: When yarn/spark-yarn-client.md#creating-instance[Client is created] (for Spark on YARN in cluster mode only), it sets the number of cores for ApplicationManager using spark.driver.cores . Refer to < >. | [[spark_driver_extraLibraryPath]] spark.driver.extraLibraryPath | | | [[spark_driver_extraJavaOptions]] spark.driver.extraJavaOptions | | Additional JVM options for the driver. | [[spark.driver.appUIAddress]] spark.driver.appUIAddress spark.driver.appUIAddress is used exclusively in yarn/README.md[Spark on YARN]. It is set when yarn/spark-yarn-client-yarnclientschedulerbackend.md#start[YarnClientSchedulerBackend starts] to yarn/spark-yarn-applicationmaster.md#runExecutorLauncher[run ExecutorLauncher] (and yarn/spark-yarn-applicationmaster.md#registerAM[register ApplicationMaster] for the Spark application). | [[spark_driver_libraryPath]] spark.driver.libraryPath | | |=== ==== [[spark_driver_extraClassPath]] spark.driver.extraClassPath spark.driver.extraClassPath system property sets the additional classpath entries (e.g. jars and directories) that should be added to the driver's classpath in spark-deploy-mode.md#cluster[ cluster deploy mode].","title":"Driver"},{"location":"driver/#note","text":"For spark-deploy-mode.md#client[ client deploy mode] you can use a properties file or command line to set spark.driver.extraClassPath . Do not use SparkConf.md[SparkConf] since it is too late for client deploy mode given the JVM has already been set up to start a Spark application.","title":"[NOTE]"},{"location":"driver/#refer-to-spark-classmdbuildsparksubmitcommandbuildsparksubmitcommand-internal-method-for-the-very-low-level-details-of-how-it-is-handled-internally","text":"spark.driver.extraClassPath uses a OS-specific path separator. NOTE: Use spark-submit 's spark-submit.md#driver-class-path[ --driver-class-path command-line option] on command line to override spark.driver.extraClassPath from a spark-properties.md#spark-defaults-conf[Spark properties file].","title":"Refer to spark-class.md#buildSparkSubmitCommand[buildSparkSubmitCommand Internal Method] for the very low-level details of how it is handled internally."},{"location":"local-properties/","text":"Local Properties \u00b6 SparkContext.setLocalProperty lets you set key-value pairs that will be propagated down to tasks and can be accessed there using TaskContext.getLocalProperty . Creating Logical Job Groups \u00b6 One of the purposes of local properties is to create logical groups of Spark jobs by means of properties that (regardless of the threads used to submit the jobs) makes the separate jobs launched from different threads belong to a single logical group. A common use case for the local property concept is to set a local property in a thread, say spark-scheduler-FairSchedulableBuilder.md[spark.scheduler.pool], after which all jobs submitted within the thread will be grouped, say into a pool by FAIR job scheduler. val data = sc . parallelize ( 0 to 9 ) sc . setLocalProperty ( \"spark.scheduler.pool\" , \"myPool\" ) // these two jobs (one per action) will run in the myPool pool data . count data . collect sc . setLocalProperty ( \"spark.scheduler.pool\" , null ) // this job will run in the default pool data . count","title":"Local Properties"},{"location":"local-properties/#local-properties","text":"SparkContext.setLocalProperty lets you set key-value pairs that will be propagated down to tasks and can be accessed there using TaskContext.getLocalProperty .","title":"Local Properties"},{"location":"local-properties/#creating-logical-job-groups","text":"One of the purposes of local properties is to create logical groups of Spark jobs by means of properties that (regardless of the threads used to submit the jobs) makes the separate jobs launched from different threads belong to a single logical group. A common use case for the local property concept is to set a local property in a thread, say spark-scheduler-FairSchedulableBuilder.md[spark.scheduler.pool], after which all jobs submitted within the thread will be grouped, say into a pool by FAIR job scheduler. val data = sc . parallelize ( 0 to 9 ) sc . setLocalProperty ( \"spark.scheduler.pool\" , \"myPool\" ) // these two jobs (one per action) will run in the myPool pool data . count data . collect sc . setLocalProperty ( \"spark.scheduler.pool\" , null ) // this job will run in the default pool data . count","title":"Creating Logical Job Groups"},{"location":"master/","text":"== Master A master is a running Spark instance that connects to a cluster manager for resources. The master acquires cluster nodes to run executors. CAUTION: FIXME Add it to the Spark architecture figure above.","title":"Master"},{"location":"overview/","text":"Apache Spark \u00b6 Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run rdd:index.md[(lazy) transformations] of these RDDs to some other form (shape), and finally perform rdd:index.md[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, spark-streaming/spark-streaming.md[Spark Streaming], spark-mllib/spark-mllib.md[Spark MLlib] and spark-graphx.md[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called rdd:index.md[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers spark-shell.md[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use spark-standalone.md[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via spark-streaming/spark-streaming.md[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , varia/spark-hadoop.md[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or spark-shell.md[the Spark shell], or the many Spark Application Frameworks leveraging the concept of rdd:index.md[RDD], i.e. Spark SQL, spark-streaming/spark-streaming.md[Spark Streaming], spark-mllib/spark-mllib.md[Spark MLlib] and spark-graphx.md[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (spark-mllib/spark-mllib.md[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using spark-shell.md[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any spark-cluster.md[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, spark-streaming/spark-streaming.md[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or spark-streaming/spark-streaming.md[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with spark-sql-thrift-server.md[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a scheduler:DAGScheduler.md[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can spark-rdd-caching.md[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was varia/spark-hadoop.md[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like varia/spark-hadoop.md[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so rdd:index.md[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Overview"},{"location":"overview/#apache-spark","text":"Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run rdd:index.md[(lazy) transformations] of these RDDs to some other form (shape), and finally perform rdd:index.md[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, spark-streaming/spark-streaming.md[Spark Streaming], spark-mllib/spark-mllib.md[Spark MLlib] and spark-graphx.md[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called rdd:index.md[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers spark-shell.md[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use spark-standalone.md[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via spark-streaming/spark-streaming.md[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , varia/spark-hadoop.md[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or spark-shell.md[the Spark shell], or the many Spark Application Frameworks leveraging the concept of rdd:index.md[RDD], i.e. Spark SQL, spark-streaming/spark-streaming.md[Spark Streaming], spark-mllib/spark-mllib.md[Spark MLlib] and spark-graphx.md[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (spark-mllib/spark-mllib.md[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using spark-shell.md[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any spark-cluster.md[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, spark-streaming/spark-streaming.md[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or spark-streaming/spark-streaming.md[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with spark-sql-thrift-server.md[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a scheduler:DAGScheduler.md[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can spark-rdd-caching.md[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was varia/spark-hadoop.md[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like varia/spark-hadoop.md[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so rdd:index.md[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Apache Spark"},{"location":"spark-building-from-sources/","text":"== Building Apache Spark from Sources You can download pre-packaged versions of Apache Spark from http://spark.apache.org/downloads.html[the project's web site]. The packages are built for a different Hadoop versions for Scala 2.11. NOTE: Since https://github.com/apache/spark/commit/289373b28cd2546165187de2e6a9185a1257b1e7[[SPARK-6363 ][BUILD] Make Scala 2.11 the default Scala version] the default version of Scala in Apache Spark is 2.11 . The build process for Scala 2.11 takes less than 15 minutes (on a decent machine like my shiny MacBook Pro with 8 cores and 16 GB RAM) and is so simple that it's unlikely to refuse the urge to do it yourself. You can use < > or < > as the build command. === [[sbt]] Using sbt as the build tool The build command with sbt as the build tool is as follows: ./build/sbt -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean assembly Using Java 8 to build Spark using sbt takes ca 10 minutes. \u279c spark git:(master) \u2717 ./build/sbt -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean assembly ... [success] Total time: 496 s, completed Dec 7, 2015 8:24:41 PM === [[profiles]] Build Profiles CAUTION: FIXME Describe yarn profile and others ==== [[hive-thriftserver]] hive-thriftserver Maven profile for Spark Thrift Server CAUTION: FIXME TIP: Read spark-sql-thrift-server.md[Thrift JDBC/ODBC Server -- Spark Thrift Server (STS)]. === [[maven]] Using Apache Maven as the build tool The build command with Apache Maven is as follows: $ ./build/mvn -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean install After a couple of minutes your freshly baked distro is ready to fly! I'm using Oracle Java 8 to build Spark. \u279c spark git:(master) \u2717 java -version java version \"1.8.0_102\" Java(TM) SE Runtime Environment (build 1.8.0_102-b14) Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode) \u279c spark git:(master) \u2717 ./build/mvn -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean install Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0 Using `mvn` from path: /usr/local/bin/mvn Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0 [INFO] Scanning for projects... [INFO] ------------------------------------------------------------------------ [INFO] Reactor Build Order: [INFO] [INFO] Spark Project Parent POM [INFO] Spark Project Tags [INFO] Spark Project Sketch [INFO] Spark Project Networking [INFO] Spark Project Shuffle Streaming Service [INFO] Spark Project Unsafe [INFO] Spark Project Launcher [INFO] Spark Project Core [INFO] Spark Project GraphX [INFO] Spark Project Streaming [INFO] Spark Project Catalyst [INFO] Spark Project SQL [INFO] Spark Project ML Local Library [INFO] Spark Project ML Library [INFO] Spark Project Tools [INFO] Spark Project Hive [INFO] Spark Project REPL [INFO] Spark Project YARN Shuffle Service [INFO] Spark Project YARN [INFO] Spark Project Hive Thrift Server [INFO] Spark Project Assembly [INFO] Spark Project External Flume Sink [INFO] Spark Project External Flume [INFO] Spark Project External Flume Assembly [INFO] Spark Integration for Kafka 0.8 [INFO] Spark Project Examples [INFO] Spark Project External Kafka Assembly [INFO] Spark Integration for Kafka 0.10 [INFO] Spark Integration for Kafka 0.10 Assembly [INFO] Spark Project Java 8 Tests [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building Spark Project Parent POM 2.0.0-SNAPSHOT [INFO] ------------------------------------------------------------------------ ... [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] Spark Project Parent POM ........................... SUCCESS [ 4.186 s] [INFO] Spark Project Tags ................................. SUCCESS [ 4.893 s] [INFO] Spark Project Sketch ............................... SUCCESS [ 5.066 s] [INFO] Spark Project Networking ........................... SUCCESS [ 11.108 s] [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 7.051 s] [INFO] Spark Project Unsafe ............................... SUCCESS [ 7.650 s] [INFO] Spark Project Launcher ............................. SUCCESS [ 9.905 s] [INFO] Spark Project Core ................................. SUCCESS [02:09 min] [INFO] Spark Project GraphX ............................... SUCCESS [ 19.317 s] [INFO] Spark Project Streaming ............................ SUCCESS [ 42.077 s] [INFO] Spark Project Catalyst ............................. SUCCESS [01:32 min] [INFO] Spark Project SQL .................................. SUCCESS [01:47 min] [INFO] Spark Project ML Local Library ..................... SUCCESS [ 10.049 s] [INFO] Spark Project ML Library ........................... SUCCESS [01:36 min] [INFO] Spark Project Tools ................................ SUCCESS [ 3.520 s] [INFO] Spark Project Hive ................................. SUCCESS [ 52.528 s] [INFO] Spark Project REPL ................................. SUCCESS [ 7.243 s] [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [ 7.898 s] [INFO] Spark Project YARN ................................. SUCCESS [ 15.380 s] [INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 24.876 s] [INFO] Spark Project Assembly ............................. SUCCESS [ 2.971 s] [INFO] Spark Project External Flume Sink .................. SUCCESS [ 7.377 s] [INFO] Spark Project External Flume ....................... SUCCESS [ 10.752 s] [INFO] Spark Project External Flume Assembly .............. SUCCESS [ 1.695 s] [INFO] Spark Integration for Kafka 0.8 .................... SUCCESS [ 13.013 s] [INFO] Spark Project Examples ............................. SUCCESS [ 31.728 s] [INFO] Spark Project External Kafka Assembly .............. SUCCESS [ 3.472 s] [INFO] Spark Integration for Kafka 0.10 ................... SUCCESS [ 12.297 s] [INFO] Spark Integration for Kafka 0.10 Assembly .......... SUCCESS [ 3.789 s] [INFO] Spark Project Java 8 Tests ......................... SUCCESS [ 4.267 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 12:29 min [INFO] Finished at: 2016-07-07T22:29:56+02:00 [INFO] Final Memory: 110M/913M [INFO] ------------------------------------------------------------------------ Please note the messages that say the version of Spark ( Building Spark Project Parent POM 2.0.0-SNAPSHOT ), Scala version ( maven-clean-plugin:2.6.1:clean (default-clean) @ spark-parent_2.11 ) and the Spark modules built. The above command gives you the latest version of Apache Spark 2.0.0-SNAPSHOT built for Scala 2.11.8 (see https://github.com/apache/spark/blob/master/pom.xml#L2640-L2674[the configuration of scala-2.11 profile]). TIP: You can also know the version of Spark using ./bin/spark-shell --version . === [[make-distribution]] Making Distribution ./make-distribution.sh is the shell script to make a distribution. It uses the same profiles as for sbt and Maven. Use --tgz option to have a tar gz version of the Spark distribution. \u279c spark git:(master) \u2717 ./make-distribution.sh --tgz -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests Once finished, you will have the distribution in the current directory, i.e. spark-2.0.0-SNAPSHOT-bin-2.7.2.tgz .","title":"Building from Sources"},{"location":"spark-debugging/","text":"== Debugging Spark === Using spark-shell and IntelliJ IDEA Start spark-shell with SPARK_SUBMIT_OPTS environment variable that configures the JVM's JDWP. SPARK_SUBMIT_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\" ./bin/spark-shell Attach IntelliJ IDEA to the JVM process using Run > Attach to Local Process menu. === Using sbt Use sbt -jvm-debug 5005 , connect to the remote JVM at the port 5005 using IntelliJ IDEA, place breakpoints on the desired lines of the source code of Spark. \u279c sparkme-app sbt -jvm-debug 5005 Listening for transport dt_socket at address: 5005 ... Run Spark context and the breakpoints get triggered. scala> val sc = new SparkContext(conf) 15/11/14 22:58:46 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT TIP: Read https://www.jetbrains.com/idea/help/debugging-2.html[Debugging ] chapter in IntelliJ IDEA 15.0 Help.","title":"spark-debugging"},{"location":"spark-deploy-mode/","text":"== Deploy Mode Deploy mode specifies the location of where spark-driver.md[driver] executes in the spark-deployment-environments.md[deployment environment]. Deploy mode can be one of the following options: client (default) - the driver runs on the machine that the Spark application was launched. cluster - the driver runs on a random node in a cluster. NOTE: cluster deploy mode is only available for spark-cluster.md[non-local cluster deployments]. You can control the deploy mode of a Spark application using spark-submit.md#deploy-mode[spark-submit's --deploy-mode command-line option] or < spark.submit.deployMode Spark property>>. NOTE: spark.submit.deployMode setting can be client or cluster . === [[client]] Client Deploy Mode CAUTION: FIXME === [[cluster]] Cluster Deploy Mode CAUTION: FIXME === [[spark.submit.deployMode]] spark.submit.deployMode spark.submit.deployMode (default: client ) can be client or cluster .","title":"Deploy Mode"},{"location":"spark-deployment-environments/","text":"Deployment Environments \u00b6 Spark Deployment Environments ( Run Modes ): local/spark-local.md[local] spark-cluster.md[clustered] ** spark-standalone.md[Spark Standalone] ** Spark on Apache Mesos ** yarn/README.md[Spark on Hadoop YARN] A Spark application is composed of the driver and executors that can run locally (on a single JVM) or using cluster resources (like CPU, RAM and disk that are managed by a cluster manager). NOTE: You can specify where to run the driver using the spark-deploy-mode.md[deploy mode] (using --deploy-mode option of spark-submit or spark.submit.deployMode Spark property). == [[master-urls]] Master URLs Spark supports the following master URLs (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L2583-L2592[private object SparkMasterRegex]): local , local[N] and local[{asterisk}] for local/spark-local.md#masterURL[Spark local] local[N, maxRetries] for local/spark-local.md#masterURL[Spark local-with-retries] local-cluster[N, cores, memory] for simulating a Spark cluster of N executors (threads), cores CPUs and memory locally (aka Spark local-cluster ) spark://host:port,host1:port1,... for connecting to spark-standalone.md[Spark Standalone cluster(s)] mesos:// for spark-mesos/spark-mesos.md[Spark on Mesos cluster] yarn for yarn/README.md[Spark on YARN] You can specify the master URL of a Spark application as follows: spark-submit.md[spark-submit's --master command-line option], SparkConf.md#spark.master[ spark.master Spark property], When creating a SparkContext.md#getOrCreate[ SparkContext (using setMaster method)], When creating a spark-sql-sparksession-builder.md[ SparkSession (using master method of the builder interface)].","title":"Deployment Environments"},{"location":"spark-deployment-environments/#deployment-environments","text":"Spark Deployment Environments ( Run Modes ): local/spark-local.md[local] spark-cluster.md[clustered] ** spark-standalone.md[Spark Standalone] ** Spark on Apache Mesos ** yarn/README.md[Spark on Hadoop YARN] A Spark application is composed of the driver and executors that can run locally (on a single JVM) or using cluster resources (like CPU, RAM and disk that are managed by a cluster manager). NOTE: You can specify where to run the driver using the spark-deploy-mode.md[deploy mode] (using --deploy-mode option of spark-submit or spark.submit.deployMode Spark property). == [[master-urls]] Master URLs Spark supports the following master URLs (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L2583-L2592[private object SparkMasterRegex]): local , local[N] and local[{asterisk}] for local/spark-local.md#masterURL[Spark local] local[N, maxRetries] for local/spark-local.md#masterURL[Spark local-with-retries] local-cluster[N, cores, memory] for simulating a Spark cluster of N executors (threads), cores CPUs and memory locally (aka Spark local-cluster ) spark://host:port,host1:port1,... for connecting to spark-standalone.md[Spark Standalone cluster(s)] mesos:// for spark-mesos/spark-mesos.md[Spark on Mesos cluster] yarn for yarn/README.md[Spark on YARN] You can specify the master URL of a Spark application as follows: spark-submit.md[spark-submit's --master command-line option], SparkConf.md#spark.master[ spark.master Spark property], When creating a SparkContext.md#getOrCreate[ SparkContext (using setMaster method)], When creating a spark-sql-sparksession-builder.md[ SparkSession (using master method of the builder interface)].","title":"Deployment Environments"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Logging Levels \u00b6 The valid logging levels are log4j's Levels (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data) conf/log4j.properties \u00b6 You can set up the default logging for Spark shell in conf/log4j.properties . Use conf/log4j.properties.template as a starting point. Setting Default Log Level Programatically \u00b6 Refer to Setting Default Log Level Programatically in SparkContext -- Entry Point to Spark Core . Setting Log Levels in Spark Applications \u00b6 In standalone Spark applications or while in Spark Shell session, use the following: import org.apache.log4j.{Level, Logger} Logger.getLogger(classOf[RackResolver]).getLevel Logger.getLogger(\"org\").setLevel(Level.OFF) Logger.getLogger(\"akka\").setLevel(Level.OFF) sbt \u00b6 When running a Spark application from within sbt using run task, you can use the following build.sbt to configure logging levels: fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties Disabling Logging \u00b6 Use the following conf/log4j.properties to disable logging completely: log4j.logger.org=OFF","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging.","title":"Logging"},{"location":"spark-logging/#logging-levels","text":"The valid logging levels are log4j's Levels (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data)","title":" Logging Levels"},{"location":"spark-logging/#conflog4jproperties","text":"You can set up the default logging for Spark shell in conf/log4j.properties . Use conf/log4j.properties.template as a starting point.","title":"conf/log4j.properties"},{"location":"spark-logging/#setting-default-log-level-programatically","text":"Refer to Setting Default Log Level Programatically in SparkContext -- Entry Point to Spark Core .","title":" Setting Default Log Level Programatically"},{"location":"spark-logging/#setting-log-levels-in-spark-applications","text":"In standalone Spark applications or while in Spark Shell session, use the following: import org.apache.log4j.{Level, Logger} Logger.getLogger(classOf[RackResolver]).getLevel Logger.getLogger(\"org\").setLevel(Level.OFF) Logger.getLogger(\"akka\").setLevel(Level.OFF)","title":" Setting Log Levels in Spark Applications"},{"location":"spark-logging/#sbt","text":"When running a Spark application from within sbt using run task, you can use the following build.sbt to configure logging levels: fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties","title":"sbt"},{"location":"spark-logging/#disabling-logging","text":"Use the following conf/log4j.properties to disable logging completely: log4j.logger.org=OFF","title":"Disabling Logging"},{"location":"spark-properties/","text":"Spark Properties and spark-defaults.conf Properties File \u00b6 Spark properties are the means of tuning the execution environment of a Spark application. The default Spark properties file is < $SPARK_HOME/conf/spark-defaults.conf >> that could be overriden using spark-submit with the spark-submit.md#properties-file[--properties-file] command-line option. .Environment Variables [options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | SPARK_CONF_DIR | $\\{SPARK_HOME}/conf | Spark's configuration directory (with spark-defaults.conf ) |=== TIP: Read the official documentation of Apache Spark on http://spark.apache.org/docs/latest/configuration.html[Spark Configuration]. === [[spark-defaults-conf]] spark-defaults.conf -- Default Spark Properties File spark-defaults.conf (under SPARK_CONF_DIR or $SPARK_HOME/conf ) is the default properties file with the Spark properties of your Spark applications. NOTE: spark-defaults.conf is loaded by spark-AbstractCommandBuilder.md#loadPropertiesFile[AbstractCommandBuilder's loadPropertiesFile internal method]. === [[getDefaultPropertiesFile]] Calculating Path of Default Spark Properties -- Utils.getDefaultPropertiesFile method [source, scala] \u00b6 getDefaultPropertiesFile(env: Map[String, String] = sys.env): String \u00b6 getDefaultPropertiesFile calculates the absolute path to spark-defaults.conf properties file that can be either in directory specified by SPARK_CONF_DIR environment variable or $SPARK_HOME/conf directory. NOTE: getDefaultPropertiesFile is part of private[spark] org.apache.spark.util.Utils object.","title":"Spark Properties"},{"location":"spark-properties/#spark-properties-and-spark-defaultsconf-properties-file","text":"Spark properties are the means of tuning the execution environment of a Spark application. The default Spark properties file is < $SPARK_HOME/conf/spark-defaults.conf >> that could be overriden using spark-submit with the spark-submit.md#properties-file[--properties-file] command-line option. .Environment Variables [options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | SPARK_CONF_DIR | $\\{SPARK_HOME}/conf | Spark's configuration directory (with spark-defaults.conf ) |=== TIP: Read the official documentation of Apache Spark on http://spark.apache.org/docs/latest/configuration.html[Spark Configuration]. === [[spark-defaults-conf]] spark-defaults.conf -- Default Spark Properties File spark-defaults.conf (under SPARK_CONF_DIR or $SPARK_HOME/conf ) is the default properties file with the Spark properties of your Spark applications. NOTE: spark-defaults.conf is loaded by spark-AbstractCommandBuilder.md#loadPropertiesFile[AbstractCommandBuilder's loadPropertiesFile internal method]. === [[getDefaultPropertiesFile]] Calculating Path of Default Spark Properties -- Utils.getDefaultPropertiesFile method","title":"Spark Properties and spark-defaults.conf Properties File"},{"location":"spark-properties/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-properties/#getdefaultpropertiesfileenv-mapstring-string-sysenv-string","text":"getDefaultPropertiesFile calculates the absolute path to spark-defaults.conf properties file that can be either in directory specified by SPARK_CONF_DIR environment variable or $SPARK_HOME/conf directory. NOTE: getDefaultPropertiesFile is part of private[spark] org.apache.spark.util.Utils object.","title":"getDefaultPropertiesFile(env: Map[String, String] = sys.env): String"},{"location":"spark-tips-and-tricks-access-private-members-spark-shell/","text":"== Access private members in Scala in Spark shell If you ever wanted to use private[spark] members in Spark using the Scala programming language, e.g. toy with org.apache.spark.scheduler.DAGScheduler or similar, you will have to use the following trick in Spark shell - use :paste -raw as described in https://issues.scala-lang.org/browse/SI-5299[REPL : support for package definition]. Open spark-shell and execute :paste -raw that allows you to enter any valid Scala code, including package . The following snippet shows how to access private[spark] member DAGScheduler.RESUBMIT_TIMEOUT : scala> :paste -raw // Entering paste mode (ctrl-D to finish) package org.apache.spark object spark { def test = { import org.apache.spark.scheduler._ println(DAGScheduler.RESUBMIT_TIMEOUT == 200) } } scala> spark.test true scala> sc.version res0: String = 1.6.0-SNAPSHOT","title":"Access private members in Scala in Spark shell"},{"location":"spark-tips-and-tricks-running-spark-windows/","text":"== Running Spark Applications on Windows Running Spark applications on Windows in general is no different than running it on other operating systems like Linux or macOS. NOTE: A Spark application could be spark-shell.md[spark-shell] or your own custom Spark application. What makes the huge difference between the operating systems is Hadoop that is used internally for file system access in Spark. You may run into few minor issues when you are on Windows due to the way Hadoop works with Windows' POSIX-incompatible NTFS filesystem. NOTE: You do not have to install Apache Hadoop to work with Spark or run Spark applications. TIP: Read the Apache Hadoop project's https://wiki.apache.org/hadoop/WindowsProblems[Problems running Hadoop on Windows]. Among the issues is the infamous java.io.IOException when running Spark Shell (below a stacktrace from Spark 2.0.2 on Windows 10 so the line numbers may be different in your case). 16/12/26 21:34:11 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries. at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379) at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394) at org.apache.hadoop.util.Shell.<clinit>(Shell.java:387) at org.apache.hadoop.hive.conf.HiveConf$ConfVars.findHadoopBinary(HiveConf.java:2327) at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:365) at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:105) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.spark.util.Utils$.classForName(Utils.scala:228) at org.apache.spark.sql.SparkSession$.hiveClassesArePresent(SparkSession.scala:963) at org.apache.spark.repl.Main$.createSparkSession(Main.scala:91) [NOTE] \u00b6 You need to have Administrator rights on your laptop. All the following commands must be executed in a command-line window ( cmd ) ran as Administrator, i.e. using Run as administrator option while executing cmd . Read the official document in Microsoft TechNet -- ++ https://technet.microsoft.com/en-us/library/cc947813(v=ws.10).aspx++[Start a Command Prompt as an Administrator]. \u00b6 Download winutils.exe binary from https://github.com/steveloughran/winutils repository. NOTE: You should select the version of Hadoop the Spark distribution was compiled with, e.g. use hadoop-2.7.1 for Spark 2 ( https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe[here is the direct link to winutils.exe binary]). Save winutils.exe binary to a directory of your choice, e.g. c:\\hadoop\\bin . Set HADOOP_HOME to reflect the directory with winutils.exe (without bin ). set HADOOP_HOME=c:\\hadoop Set PATH environment variable to include %HADOOP_HOME%\\bin as follows: set PATH=%HADOOP_HOME%\\bin;%PATH% TIP: Define HADOOP_HOME and PATH environment variables in Control Panel so any Windows program would use them. Create C:\\tmp\\hive directory. [NOTE] \u00b6 c:\\tmp\\hive directory is the default value of https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.scratchdir [ hive.exec.scratchdir configuration property] in Hive 0.14.0 and later and Spark uses a custom build of Hive 1.2.1. You can change hive.exec.scratchdir configuration property to another directory as described in < hive.exec.scratchdir Configuration Property>> in this document. \u00b6 Execute the following command in cmd that you started using the option Run as administrator . winutils.exe chmod -R 777 C:\\tmp\\hive Check the permissions (that is one of the commands that are executed under the covers): winutils.exe ls -F C:\\tmp\\hive Open spark-shell and observe the output (perhaps with few WARN messages that you can simply disregard). As a verification step, execute the following line to display the content of a DataFrame : [source, scala] \u00b6 scala> spark.range(1).withColumn(\"status\", lit(\"All seems fine. Congratulations!\")).show(false) +---+--------------------------------+ |id |status | +---+--------------------------------+ |0 |All seems fine. Congratulations!| +---+--------------------------------+ [NOTE] \u00b6 Disregard WARN messages when you start spark-shell . They are harmless. 16/12/26 22:05:41 WARN General: Plugin (Bundle) \"org.datanucleus\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/C:/spark-2.0.2-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/C:/spark-2.0.2-bin-hadoop2.7/bin/../jars/datanucleus-core- 3.2.10.jar.\" 16/12/26 22:05:41 WARN General: Plugin (Bundle) \"org.datanucleus.api.jdo\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/C:/spark-2.0.2-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/C:/spark-2.0.2-bin- hadoop2.7/bin/../jars/datanucleus-api-jdo-3.2.6.jar.\" 16/12/26 22:05:41 WARN General: Plugin (Bundle) \"org.datanucleus.store.rdbms\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/C:/spark-2.0.2-bin-hadoop2.7/bin/../jars/datanucleus-rdbms-3.2.9.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/C:/spark-2.0.2-bin- hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar.\" \u00b6 If you see the above output, you're done. You should now be able to run Spark applications on your Windows. Congrats! === [[changing-hive.exec.scratchdir]] Changing hive.exec.scratchdir Configuration Property Create a hive-site.xml file with the following content: <configuration> <property> <name>hive.exec.scratchdir</name> <value>/tmp/mydir</value> <description>Scratch space for Hive jobs</description> </property> </configuration> Start a Spark application, e.g. spark-shell , with HADOOP_CONF_DIR environment variable set to the directory with hive-site.xml . HADOOP_CONF_DIR=conf ./bin/spark-shell","title":"Running Spark Applications on Windows"},{"location":"spark-tips-and-tricks-running-spark-windows/#note","text":"You need to have Administrator rights on your laptop. All the following commands must be executed in a command-line window ( cmd ) ran as Administrator, i.e. using Run as administrator option while executing cmd .","title":"[NOTE]"},{"location":"spark-tips-and-tricks-running-spark-windows/#read-the-official-document-in-microsoft-technet-httpstechnetmicrosoftcomen-uslibrarycc947813vws10aspxstart-a-command-prompt-as-an-administrator","text":"Download winutils.exe binary from https://github.com/steveloughran/winutils repository. NOTE: You should select the version of Hadoop the Spark distribution was compiled with, e.g. use hadoop-2.7.1 for Spark 2 ( https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe[here is the direct link to winutils.exe binary]). Save winutils.exe binary to a directory of your choice, e.g. c:\\hadoop\\bin . Set HADOOP_HOME to reflect the directory with winutils.exe (without bin ). set HADOOP_HOME=c:\\hadoop Set PATH environment variable to include %HADOOP_HOME%\\bin as follows: set PATH=%HADOOP_HOME%\\bin;%PATH% TIP: Define HADOOP_HOME and PATH environment variables in Control Panel so any Windows program would use them. Create C:\\tmp\\hive directory.","title":"Read the official document in Microsoft TechNet -- ++https://technet.microsoft.com/en-us/library/cc947813(v=ws.10).aspx++[Start a Command Prompt as an Administrator]."},{"location":"spark-tips-and-tricks-running-spark-windows/#note_1","text":"c:\\tmp\\hive directory is the default value of https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.scratchdir [ hive.exec.scratchdir configuration property] in Hive 0.14.0 and later and Spark uses a custom build of Hive 1.2.1.","title":"[NOTE]"},{"location":"spark-tips-and-tricks-running-spark-windows/#you-can-change-hiveexecscratchdir-configuration-property-to-another-directory-as-described-in-wzxhzdk27-configuration-property-in-this-document","text":"Execute the following command in cmd that you started using the option Run as administrator . winutils.exe chmod -R 777 C:\\tmp\\hive Check the permissions (that is one of the commands that are executed under the covers): winutils.exe ls -F C:\\tmp\\hive Open spark-shell and observe the output (perhaps with few WARN messages that you can simply disregard). As a verification step, execute the following line to display the content of a DataFrame :","title":"You can change hive.exec.scratchdir configuration property to another directory as described in &lt;\u0002wzxhzdk:27\u0003 Configuration Property>&gt; in this document."},{"location":"spark-tips-and-tricks-running-spark-windows/#source-scala","text":"scala> spark.range(1).withColumn(\"status\", lit(\"All seems fine. Congratulations!\")).show(false) +---+--------------------------------+ |id |status | +---+--------------------------------+ |0 |All seems fine. Congratulations!| +---+--------------------------------+","title":"[source, scala]"},{"location":"spark-tips-and-tricks-running-spark-windows/#note_2","text":"Disregard WARN messages when you start spark-shell . They are harmless.","title":"[NOTE]"},{"location":"spark-tips-and-tricks-running-spark-windows/#161226-220541-warn-general-plugin-bundle-orgdatanucleus-is-already-registered-ensure-you-dont-have-multiple-jar-versions-of-the-same-plugin-in-the-classpath-the-url-filecspark-202-bin-hadoop27jarsdatanucleus-core-3210jar-is-already-registered-and-you-are-trying-to-register-an-identical-plugin-located-at-url-filecspark-202-bin-hadoop27binjarsdatanucleus-core-3210jar-161226-220541-warn-general-plugin-bundle-orgdatanucleusapijdo-is-already-registered-ensure-you-dont-have-multiple-jar-versions-of-the-same-plugin-in-the-classpath-the-url-filecspark-202-bin-hadoop27jarsdatanucleus-api-jdo-326jar-is-already-registered-and-you-are-trying-to-register-an-identical-plugin-located-at-url-filecspark-202-bin-hadoop27binjarsdatanucleus-api-jdo-326jar-161226-220541-warn-general-plugin-bundle-orgdatanucleusstorerdbms-is-already-registered-ensure-you-dont-have-multiple-jar-versions-of-the-same-plugin-in-the-classpath-the-url-filecspark-202-bin-hadoop27binjarsdatanucleus-rdbms-329jar-is-already-registered-and-you-are-trying-to-register-an-identical-plugin-located-at-url-filecspark-202-bin-hadoop27jarsdatanucleus-rdbms-329jar","text":"If you see the above output, you're done. You should now be able to run Spark applications on your Windows. Congrats! === [[changing-hive.exec.scratchdir]] Changing hive.exec.scratchdir Configuration Property Create a hive-site.xml file with the following content: <configuration> <property> <name>hive.exec.scratchdir</name> <value>/tmp/mydir</value> <description>Scratch space for Hive jobs</description> </property> </configuration> Start a Spark application, e.g. spark-shell , with HADOOP_CONF_DIR environment variable set to the directory with hive-site.xml . HADOOP_CONF_DIR=conf ./bin/spark-shell","title":"16/12/26 22:05:41 WARN General: Plugin (Bundle) &quot;org.datanucleus&quot; is already registered. Ensure you dont have multiple JAR versions of\nthe same plugin in the classpath. The URL &quot;file:/C:/spark-2.0.2-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar&quot; is already registered,\nand you are trying to register an identical plugin located at URL &quot;file:/C:/spark-2.0.2-bin-hadoop2.7/bin/../jars/datanucleus-core-\n3.2.10.jar.&quot;\n16/12/26 22:05:41 WARN General: Plugin (Bundle) &quot;org.datanucleus.api.jdo&quot; is already registered. Ensure you dont have multiple JAR\nversions of the same plugin in the classpath. The URL &quot;file:/C:/spark-2.0.2-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar&quot; is already\nregistered, and you are trying to register an identical plugin located at URL &quot;file:/C:/spark-2.0.2-bin-\nhadoop2.7/bin/../jars/datanucleus-api-jdo-3.2.6.jar.&quot;\n16/12/26 22:05:41 WARN General: Plugin (Bundle) &quot;org.datanucleus.store.rdbms&quot; is already registered. Ensure you dont have multiple JAR\nversions of the same plugin in the classpath. The URL &quot;file:/C:/spark-2.0.2-bin-hadoop2.7/bin/../jars/datanucleus-rdbms-3.2.9.jar&quot; is\nalready registered, and you are trying to register an identical plugin located at URL &quot;file:/C:/spark-2.0.2-bin-\nhadoop2.7/jars/datanucleus-rdbms-3.2.9.jar.&quot;\n"},{"location":"spark-tips-and-tricks-sparkexception-task-not-serializable/","text":"== org.apache.spark.SparkException: Task not serializable When you run into org.apache.spark.SparkException: Task not serializable exception, it means that you use a reference to an instance of a non-serializable class inside a transformation. See the following example: \u279c spark git:(master) \u2717 ./bin/spark-shell Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 1.6.0-SNAPSHOT /_/ Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66) Type in expressions to have them evaluated. Type :help for more information. scala> class NotSerializable(val num: Int) defined class NotSerializable scala> val notSerializable = new NotSerializable(10) notSerializable: NotSerializable = NotSerializable@2700f556 scala> sc.parallelize(0 to 10).map(_ => notSerializable.num).count org.apache.spark.SparkException: Task not serializable at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304) at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294) at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122) at org.apache.spark.SparkContext.clean(SparkContext.scala:2055) at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:318) at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:317) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111) at org.apache.spark.rdd.RDD.withScope(RDD.scala:310) at org.apache.spark.rdd.RDD.map(RDD.scala:317) ... 48 elided Caused by: java.io.NotSerializableException: NotSerializable Serialization stack: - object not serializable (class: NotSerializable, value: NotSerializable@2700f556) - field (class: $iw, name: notSerializable, type: class NotSerializable) - object (class $iw, $iw@10e542f3) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@729feae8) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@5fc3b20b) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@36dab184) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@5eb974) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@79c514e4) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@5aeaee3) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@2be9425f) - field (class: $line18.$read, name: $iw, type: class $iw) - object (class $line18.$read, $line18.$read@6311640d) - field (class: $iw, name: $line18$read, type: class $line18.$read) - object (class $iw, $iw@c9cd06e) - field (class: $iw, name: $outer, type: class $iw) - object (class $iw, $iw@6565691a) - field (class: $anonfun$1, name: $outer, type: class $iw) - object (class $anonfun$1, <function1>) at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40) at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47) at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101) at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301) ... 57 more === Further reading https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/troubleshooting/javaionotserializableexception.html[Job aborted due to stage failure: Task not serializable] https://issues.apache.org/jira/browse/SPARK-5307[Add utility to help with NotSerializableException debugging] http://stackoverflow.com/q/22592811/1305344[Task not serializable: java.io.NotSerializableException when calling function outside closure only on classes not objects]","title":"Task not serializable Exception"},{"location":"spark-tips-and-tricks/","text":"= Spark Tips and Tricks == [[SPARK_PRINT_LAUNCH_COMMAND]] Print Launch Command of Spark Scripts SPARK_PRINT_LAUNCH_COMMAND environment variable controls whether the Spark launch command is printed out to the standard error output, i.e. System.err , or not. Spark Command: [here comes the command] ======================================== All the Spark shell scripts use org.apache.spark.launcher.Main class internally that checks SPARK_PRINT_LAUNCH_COMMAND and when set (to any value) will print out the entire command line to launch it. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Java/JavaVirtualMachines/Current/Contents/Home/bin/java -cp /Users/jacek/dev/oss/spark/conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-1.6.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar -Dscala.usejavacp=true -Xms1g -Xmx1g org.apache.spark.deploy.SparkSubmit --master spark://localhost:7077 --class org.apache.spark.repl.Main --name Spark shell spark-shell ======================================== == Show Spark version in Spark shell In spark-shell, use sc.version or org.apache.spark.SPARK_VERSION to know the Spark version: scala> sc.version res0: String = 1.6.0-SNAPSHOT scala> org.apache.spark.SPARK_VERSION res1: String = 1.6.0-SNAPSHOT == Resolving local host name When you face networking issues when Spark can't resolve your local hostname or IP address, use the preferred SPARK_LOCAL_HOSTNAME environment variable as the custom host name or SPARK_LOCAL_IP as the custom IP that is going to be later resolved to a hostname. Spark checks them out before using http://docs.oracle.com/javase/8/docs/api/java/net/InetAddress.html#getLocalHost--[java.net.InetAddress.getLocalHost ()] (consult https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L759[org.apache.spark.util.Utils.findLocalInetAddress ()] method). You may see the following WARN messages in the logs when Spark finished the resolving process: WARN Your hostname, [hostname] resolves to a loopback address: [host-address]; using... WARN Set SPARK_LOCAL_IP if you need to bind to another address == [[spark-standalone-windows]] Starting standalone Master and workers on Windows 7 Windows 7 users can use spark-class.md[spark-class] to start spark-standalone.md[Spark Standalone] as there are no launch scripts for the Windows platform. $ ./bin/spark-class org.apache.spark.deploy.master.Master -h localhost $ ./bin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077","title":"Spark Tips and Tricks"},{"location":"speculative-execution-of-tasks/","text":"Speculative Execution of Tasks \u00b6 Speculative tasks (also speculatable tasks or task strugglers ) are tasks that run slower than most (FIXME the setting) of the all tasks in a job. Speculative execution of tasks is a health-check procedure that checks for tasks to be speculated , i.e. running slower in a stage than the median of all successfully completed tasks in a taskset (FIXME the setting). Such slow tasks will be re-submitted to another worker. It will not stop the slow tasks, but run a new copy in parallel. The thread starts as TaskSchedulerImpl starts in spark-cluster.md[clustered deployment modes] with configuration-properties.md#spark.speculation[spark.speculation] enabled. It executes periodically every configuration-properties.md#spark.speculation.interval[spark.speculation.interval] after the initial spark.speculation.interval passes. When enabled, you should see the following INFO message in the logs: [source,plaintext] \u00b6 Starting speculative execution thread \u00b6 It works as scheduler:TaskSchedulerImpl.md#task-scheduler-speculation[ task-scheduler-speculation daemon thread pool] (using j.u.c.ScheduledThreadPoolExecutor with core pool size of 1). The job with speculatable tasks should finish while speculative tasks are running, and it will leave these tasks running - no KILL command yet. It uses checkSpeculatableTasks method that asks rootPool to check for speculatable tasks. If there are any, SchedulerBackend is called for scheduler:SchedulerBackend.md#reviveOffers[reviveOffers]. CAUTION: FIXME How does Spark handle repeated results of speculative tasks since there are copies launched?","title":"Speculative Execution of Tasks"},{"location":"speculative-execution-of-tasks/#speculative-execution-of-tasks","text":"Speculative tasks (also speculatable tasks or task strugglers ) are tasks that run slower than most (FIXME the setting) of the all tasks in a job. Speculative execution of tasks is a health-check procedure that checks for tasks to be speculated , i.e. running slower in a stage than the median of all successfully completed tasks in a taskset (FIXME the setting). Such slow tasks will be re-submitted to another worker. It will not stop the slow tasks, but run a new copy in parallel. The thread starts as TaskSchedulerImpl starts in spark-cluster.md[clustered deployment modes] with configuration-properties.md#spark.speculation[spark.speculation] enabled. It executes periodically every configuration-properties.md#spark.speculation.interval[spark.speculation.interval] after the initial spark.speculation.interval passes. When enabled, you should see the following INFO message in the logs:","title":"Speculative Execution of Tasks"},{"location":"speculative-execution-of-tasks/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"speculative-execution-of-tasks/#starting-speculative-execution-thread","text":"It works as scheduler:TaskSchedulerImpl.md#task-scheduler-speculation[ task-scheduler-speculation daemon thread pool] (using j.u.c.ScheduledThreadPoolExecutor with core pool size of 1). The job with speculatable tasks should finish while speculative tasks are running, and it will leave these tasks running - no KILL command yet. It uses checkSpeculatableTasks method that asks rootPool to check for speculatable tasks. If there are any, SchedulerBackend is called for scheduler:SchedulerBackend.md#reviveOffers[reviveOffers]. CAUTION: FIXME How does Spark handle repeated results of speculative tasks since there are copies launched?","title":"Starting speculative execution thread"},{"location":"tags/","text":"Tags \u00b6 The following is a list of the tags used in The Internals of Apache Spark online book. Read up on Setting up tags to learn more. DeveloperApi \u00b6 SparkEnv SparkListener StatsReportListener TaskCompletionListener TaskFailureListener ShuffleReadMetrics ShuffleWriteMetrics TaskMetrics SparkPlugin StorageLevel","title":"Tags"},{"location":"tags/#tags","text":"The following is a list of the tags used in The Internals of Apache Spark online book. Read up on Setting up tags to learn more.","title":"Tags"},{"location":"tags/#developerapi","text":"SparkEnv SparkListener StatsReportListener TaskCompletionListener TaskFailureListener ShuffleReadMetrics ShuffleWriteMetrics TaskMetrics SparkPlugin StorageLevel","title":"DeveloperApi"},{"location":"workers/","text":"== Workers Workers (aka slaves ) are running Spark instances where executors live to execute tasks. They are the compute nodes in Spark. CAUTION: FIXME Are workers perhaps part of Spark Standalone only? CAUTION: FIXME How many executors are spawned per worker? A worker receives serialized tasks that it runs in a thread pool. It hosts a local storage:BlockManager.md[Block Manager] that serves blocks to other workers in a Spark cluster. Workers communicate among themselves using their Block Manager instances. CAUTION: FIXME Diagram of a driver with workers as boxes. Explain task execution in Spark and understand Spark\u2019s underlying execution model. New vocabulary often faced in Spark UI SparkContext.md[When you create SparkContext], each worker starts an executor. This is a separate process (JVM), and it loads your jar, too. The executors connect back to your driver program. Now the driver can send them commands, like flatMap , map and reduceByKey . When the driver quits, the executors shut down. A new process is not started for each step. A new process is started on each worker when the SparkContext is constructed. The executor deserializes the command (this is possible because it has loaded your jar), and executes it on a partition. Shortly speaking, an application in Spark is executed in three steps: Create RDD graph, i.e. DAG (directed acyclic graph) of RDDs to represent entire computation. Create stage graph, i.e. a DAG of stages that is a logical execution plan based on the RDD graph. Stages are created by breaking the RDD graph at shuffle boundaries. Based on the plan, schedule and execute tasks on workers. exercises/spark-examples-wordcount-spark-shell.md[In the WordCount example], the RDD graph is as follows: file -> lines -> words -> per-word count -> global word count -> output Based on this graph, two stages are created. The stage creation rule is based on the idea of pipelining as many rdd:index.md[narrow transformations] as possible. RDD operations with \"narrow\" dependencies, like map() and filter() , are pipelined together into one set of tasks in each stage. In the end, every stage will only have shuffle dependencies on other stages, and may compute multiple operations inside it. In the WordCount example, the narrow transformation finishes at per-word count. Therefore, you get two stages: file -> lines -> words -> per-word count global word count -> output Once stages are defined, Spark will generate scheduler:Task.md[tasks] from scheduler:Stage.md[stages]. The first stage will create scheduler:ShuffleMapTask.md[ShuffleMapTask]s with the last stage creating scheduler:ResultTask.md[ResultTask]s because in the last stage, one action operation is included to produce results. The number of tasks to be generated depends on how your files are distributed. Suppose that you have 3 three different files in three different nodes, the first stage will generate 3 tasks: one task per partition. Therefore, you should not map your steps to tasks directly. A task belongs to a stage, and is related to a partition. The number of tasks being generated in each stage will be equal to the number of partitions. === [[Cleanup]] Cleanup CAUTION: FIXME === [[settings]] Settings spark.worker.cleanup.enabled (default: false ) < > enabled.","title":"Workers"},{"location":"accumulators/","text":"Accumulators \u00b6 Accumulators are shared variables that accumulate values from executors on the driver using associative and commutative \"add\" operation. The main abstraction is AccumulatorV2 . Accumulators are registered ( created ) using SparkContext with or without a name. Only named accumulators are displayed in web UI . DAGScheduler is responsible for updating accumulators (from partial values from tasks running on executors every heartbeat). Accumulators are serializable so they can safely be referenced in the code executed in executors and then safely send over the wire for execution. // on the driver val counter = sc . longAccumulator ( \"counter\" ) sc . parallelize ( 1 to 9 ). foreach { x => // on executors counter . add ( x ) } // on the driver println ( counter . value ) Tip Learn more about Accumulators in the official documentation of Apache Spark. Further Reading \u00b6 Performance and Scalability of Broadcast in Spark","title":"Accumulators"},{"location":"accumulators/#accumulators","text":"Accumulators are shared variables that accumulate values from executors on the driver using associative and commutative \"add\" operation. The main abstraction is AccumulatorV2 . Accumulators are registered ( created ) using SparkContext with or without a name. Only named accumulators are displayed in web UI . DAGScheduler is responsible for updating accumulators (from partial values from tasks running on executors every heartbeat). Accumulators are serializable so they can safely be referenced in the code executed in executors and then safely send over the wire for execution. // on the driver val counter = sc . longAccumulator ( \"counter\" ) sc . parallelize ( 1 to 9 ). foreach { x => // on executors counter . add ( x ) } // on the driver println ( counter . value ) Tip Learn more about Accumulators in the official documentation of Apache Spark.","title":"Accumulators"},{"location":"accumulators/#further-reading","text":"Performance and Scalability of Broadcast in Spark","title":"Further Reading"},{"location":"accumulators/AccumulableInfo/","text":"AccumulableInfo \u00b6 AccumulableInfo represents an update to an AccumulatorV2 . AccumulableInfo is used to transfer accumulator updates from executors to the driver every executor heartbeat or when a task finishes. Creating Instance \u00b6 AccumulableInfo takes the following to be created: Accumulator ID Name Partial Update Partial Value internal flag countFailedValues flag Metadata (default: None ) AccumulableInfo is created when: AccumulatorV2 is requested to convert itself to an AccumulableInfo JsonProtocol is requested to accumulableInfoFromJson SQLMetric ( Spark SQL ) is requested to convert itself to an AccumulableInfo internal Flag \u00b6 internal : Boolean AccumulableInfo is given an internal flag when created . internal flag denotes whether this accumulator is internal. internal is used when: LiveEntityHelpers is requested for newAccumulatorInfos JsonProtocol is requested to accumulableInfoToJson","title":"AccumulableInfo"},{"location":"accumulators/AccumulableInfo/#accumulableinfo","text":"AccumulableInfo represents an update to an AccumulatorV2 . AccumulableInfo is used to transfer accumulator updates from executors to the driver every executor heartbeat or when a task finishes.","title":"AccumulableInfo"},{"location":"accumulators/AccumulableInfo/#creating-instance","text":"AccumulableInfo takes the following to be created: Accumulator ID Name Partial Update Partial Value internal flag countFailedValues flag Metadata (default: None ) AccumulableInfo is created when: AccumulatorV2 is requested to convert itself to an AccumulableInfo JsonProtocol is requested to accumulableInfoFromJson SQLMetric ( Spark SQL ) is requested to convert itself to an AccumulableInfo","title":"Creating Instance"},{"location":"accumulators/AccumulableInfo/#internal-flag","text":"internal : Boolean AccumulableInfo is given an internal flag when created . internal flag denotes whether this accumulator is internal. internal is used when: LiveEntityHelpers is requested for newAccumulatorInfos JsonProtocol is requested to accumulableInfoToJson","title":" internal Flag"},{"location":"accumulators/AccumulatorContext/","text":"== [[AccumulatorContext]] AccumulatorContext AccumulatorContext is a private[spark] internal object used to track accumulators by Spark itself using an internal originals lookup table. Spark uses the AccumulatorContext object to register and unregister accumulators. The originals lookup table maps accumulator identifier to the accumulator itself. Every accumulator has its own unique accumulator id that is assigned using the internal nextId counter. === [[register]] register Method CAUTION: FIXME === [[newId]] newId Method CAUTION: FIXME === [[AccumulatorContext-SQL_ACCUM_IDENTIFIER]] AccumulatorContext.SQL_ACCUM_IDENTIFIER AccumulatorContext.SQL_ACCUM_IDENTIFIER is an internal identifier for Spark SQL's internal accumulators. The value is sql and Spark uses it to distinguish spark-sql-SparkPlan.md#SQLMetric[Spark SQL metrics] from others.","title":"AccumulatorContext"},{"location":"accumulators/AccumulatorSource/","text":"AccumulatorSource \u00b6 AccumulatorSource is...FIXME","title":"AccumulatorSource"},{"location":"accumulators/AccumulatorSource/#accumulatorsource","text":"AccumulatorSource is...FIXME","title":"AccumulatorSource"},{"location":"accumulators/AccumulatorV2/","text":"AccumulatorV2 \u00b6 AccumulatorV2[IN, OUT] is an abstraction of accumulators AccumulatorV2 is a Java Serializable . Contract \u00b6 Adding Value \u00b6 add ( v : IN ): Unit Accumulates ( adds ) the given v value to this accumulator Copying Accumulator \u00b6 copy (): AccumulatorV2 [ IN , OUT ] Is Zero Value \u00b6 isZero : Boolean Merging Updates \u00b6 merge ( other : AccumulatorV2 [ IN , OUT ]): Unit Resetting Accumulator \u00b6 reset (): Unit Value \u00b6 value : OUT The current value of this accumulator Used when: TaskRunner is requested to collectAccumulatorsAndResetStatusOnFailure AccumulatorSource is requested to register DAGScheduler is requested to update accumulators TaskSchedulerImpl is requested to executorHeartbeatReceived TaskSetManager is requested to handleSuccessfulTask JsonProtocol is requested to taskEndReasonFromJson others Implementations \u00b6 AggregatingAccumulator ( Spark SQL ) CollectionAccumulator DoubleAccumulator EventTimeStatsAccum ( Spark Structured Streaming ) LongAccumulator SetAccumulator (Spark SQL) SQLMetric ( Spark SQL ) Converting this Accumulator to AccumulableInfo \u00b6 toInfo ( update : Option [ Any ], value : Option [ Any ]): AccumulableInfo toInfo determines whether the accumulator is internal based on the name (and whether it uses the internal.metrics prefix) and uses it to create an AccumulableInfo . toInfo is used when: TaskRunner is requested to collectAccumulatorsAndResetStatusOnFailure DAGScheduler is requested to updateAccumulators TaskSchedulerImpl is requested to executorHeartbeatReceived JsonProtocol is requested to taskEndReasonFromJson SQLAppStatusListener ( Spark SQL ) is requested to handle a SparkListenerTaskEnd event ( onTaskEnd ) Registering Accumulator \u00b6 register ( sc : SparkContext , name : Option [ String ] = None , countFailedValues : Boolean = false ): Unit register ...FIXME register is used when: SparkContext is requested to register an accumulator TaskMetrics is requested to register task accumulators CollectMetricsExec ( Spark SQL ) is requested for an AggregatingAccumulator SQLMetrics ( Spark SQL ) is used to create a performance metric Serializing AccumulatorV2 \u00b6 writeReplace (): Any writeReplace is part of the Serializable ( Java ) abstraction (to designate an alternative object to be used when writing an object to the stream). writeReplace ...FIXME Deserializing AccumulatorV2 \u00b6 readObject ( in : ObjectInputStream ): Unit readObject is part of the Serializable ( Java ) abstraction (for special handling during deserialization). readObject reads the non-static and non-transient fields of the AccumulatorV2 from the given ObjectInputStream . If the atDriverSide internal flag is turned on, readObject turns it off (to indicate readObject is executed on an executor). Otherwise, atDriverSide internal flag is turned on. readObject requests the active TaskContext to register this accumulator .","title":"AccumulatorV2"},{"location":"accumulators/AccumulatorV2/#accumulatorv2","text":"AccumulatorV2[IN, OUT] is an abstraction of accumulators AccumulatorV2 is a Java Serializable .","title":"AccumulatorV2"},{"location":"accumulators/AccumulatorV2/#contract","text":"","title":"Contract"},{"location":"accumulators/AccumulatorV2/#adding-value","text":"add ( v : IN ): Unit Accumulates ( adds ) the given v value to this accumulator","title":" Adding Value"},{"location":"accumulators/AccumulatorV2/#copying-accumulator","text":"copy (): AccumulatorV2 [ IN , OUT ]","title":" Copying Accumulator"},{"location":"accumulators/AccumulatorV2/#is-zero-value","text":"isZero : Boolean","title":" Is Zero Value"},{"location":"accumulators/AccumulatorV2/#merging-updates","text":"merge ( other : AccumulatorV2 [ IN , OUT ]): Unit","title":" Merging Updates"},{"location":"accumulators/AccumulatorV2/#resetting-accumulator","text":"reset (): Unit","title":" Resetting Accumulator"},{"location":"accumulators/AccumulatorV2/#value","text":"value : OUT The current value of this accumulator Used when: TaskRunner is requested to collectAccumulatorsAndResetStatusOnFailure AccumulatorSource is requested to register DAGScheduler is requested to update accumulators TaskSchedulerImpl is requested to executorHeartbeatReceived TaskSetManager is requested to handleSuccessfulTask JsonProtocol is requested to taskEndReasonFromJson others","title":" Value"},{"location":"accumulators/AccumulatorV2/#implementations","text":"AggregatingAccumulator ( Spark SQL ) CollectionAccumulator DoubleAccumulator EventTimeStatsAccum ( Spark Structured Streaming ) LongAccumulator SetAccumulator (Spark SQL) SQLMetric ( Spark SQL )","title":"Implementations"},{"location":"accumulators/AccumulatorV2/#converting-this-accumulator-to-accumulableinfo","text":"toInfo ( update : Option [ Any ], value : Option [ Any ]): AccumulableInfo toInfo determines whether the accumulator is internal based on the name (and whether it uses the internal.metrics prefix) and uses it to create an AccumulableInfo . toInfo is used when: TaskRunner is requested to collectAccumulatorsAndResetStatusOnFailure DAGScheduler is requested to updateAccumulators TaskSchedulerImpl is requested to executorHeartbeatReceived JsonProtocol is requested to taskEndReasonFromJson SQLAppStatusListener ( Spark SQL ) is requested to handle a SparkListenerTaskEnd event ( onTaskEnd )","title":" Converting this Accumulator to AccumulableInfo"},{"location":"accumulators/AccumulatorV2/#registering-accumulator","text":"register ( sc : SparkContext , name : Option [ String ] = None , countFailedValues : Boolean = false ): Unit register ...FIXME register is used when: SparkContext is requested to register an accumulator TaskMetrics is requested to register task accumulators CollectMetricsExec ( Spark SQL ) is requested for an AggregatingAccumulator SQLMetrics ( Spark SQL ) is used to create a performance metric","title":" Registering Accumulator"},{"location":"accumulators/AccumulatorV2/#serializing-accumulatorv2","text":"writeReplace (): Any writeReplace is part of the Serializable ( Java ) abstraction (to designate an alternative object to be used when writing an object to the stream). writeReplace ...FIXME","title":" Serializing AccumulatorV2"},{"location":"accumulators/AccumulatorV2/#deserializing-accumulatorv2","text":"readObject ( in : ObjectInputStream ): Unit readObject is part of the Serializable ( Java ) abstraction (for special handling during deserialization). readObject reads the non-static and non-transient fields of the AccumulatorV2 from the given ObjectInputStream . If the atDriverSide internal flag is turned on, readObject turns it off (to indicate readObject is executed on an executor). Otherwise, atDriverSide internal flag is turned on. readObject requests the active TaskContext to register this accumulator .","title":" Deserializing AccumulatorV2"},{"location":"accumulators/InternalAccumulator/","text":"InternalAccumulator \u00b6 InternalAccumulator is an utility with field names for internal accumulators. internal.metrics Prefix \u00b6 internal.metrics. is the prefix of metrics that are considered internal and should not be displayed in web UI. internal.metrics. is used when: AccumulatorV2 is requested to convert itself to AccumulableInfo and writeReplace JsonProtocol is requested to accumValueToJson and accumValueFromJson","title":"InternalAccumulator"},{"location":"accumulators/InternalAccumulator/#internalaccumulator","text":"InternalAccumulator is an utility with field names for internal accumulators.","title":"InternalAccumulator"},{"location":"accumulators/InternalAccumulator/#internalmetrics-prefix","text":"internal.metrics. is the prefix of metrics that are considered internal and should not be displayed in web UI. internal.metrics. is used when: AccumulatorV2 is requested to convert itself to AccumulableInfo and writeReplace JsonProtocol is requested to accumValueToJson and accumValueFromJson","title":" internal.metrics Prefix"},{"location":"core/BlockFetchStarter/","text":"BlockFetchStarter \u00b6 BlockFetchStarter is the < > of...FIXME...to < >. [[contract]] [[createAndStart]] [source, java] void createAndStart(String[] blockIds, BlockFetchingListener listener) throws IOException, InterruptedException; createAndStart is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is 0 ) RetryingBlockFetcher is requested to core:RetryingBlockFetcher.md#fetchAllOutstanding[fetchAllOutstanding]","title":"BlockFetchStarter"},{"location":"core/BlockFetchStarter/#blockfetchstarter","text":"BlockFetchStarter is the < > of...FIXME...to < >. [[contract]] [[createAndStart]] [source, java] void createAndStart(String[] blockIds, BlockFetchingListener listener) throws IOException, InterruptedException; createAndStart is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is 0 ) RetryingBlockFetcher is requested to core:RetryingBlockFetcher.md#fetchAllOutstanding[fetchAllOutstanding]","title":"BlockFetchStarter"},{"location":"core/BlockFetchingListener/","text":"BlockFetchingListener \u00b6 BlockFetchingListener is an extension of the EventListener ( Java ) abstraction that want to be notified about block fetch success and failures . BlockFetchingListener is used to create a OneForOneBlockFetcher , OneForOneBlockPusher and RetryingBlockFetcher . Contract \u00b6 onBlockFetchFailure \u00b6 void onBlockFetchFailure ( String blockId , Throwable exception ) onBlockFetchSuccess \u00b6 void onBlockFetchSuccess ( String blockId , ManagedBuffer data ) Implementations \u00b6 \"Unnamed\" in ShuffleBlockFetcherIterator \"Unnamed\" in BlockTransferService RetryingBlockFetchListener","title":"BlockFetchingListener"},{"location":"core/BlockFetchingListener/#blockfetchinglistener","text":"BlockFetchingListener is an extension of the EventListener ( Java ) abstraction that want to be notified about block fetch success and failures . BlockFetchingListener is used to create a OneForOneBlockFetcher , OneForOneBlockPusher and RetryingBlockFetcher .","title":"BlockFetchingListener"},{"location":"core/BlockFetchingListener/#contract","text":"","title":"Contract"},{"location":"core/BlockFetchingListener/#onblockfetchfailure","text":"void onBlockFetchFailure ( String blockId , Throwable exception )","title":" onBlockFetchFailure"},{"location":"core/BlockFetchingListener/#onblockfetchsuccess","text":"void onBlockFetchSuccess ( String blockId , ManagedBuffer data )","title":" onBlockFetchSuccess"},{"location":"core/BlockFetchingListener/#implementations","text":"\"Unnamed\" in ShuffleBlockFetcherIterator \"Unnamed\" in BlockTransferService RetryingBlockFetchListener","title":"Implementations"},{"location":"core/BroadcastFactory/","text":"= BroadcastFactory BroadcastFactory is an < > for < > that core:BroadcastManager.md[BroadcastManager] uses for Broadcast.md[]. NOTE: As of https://issues.apache.org/jira/browse/SPARK-12588[Spark 2.0], it is no longer possible to plug a custom BroadcastFactory in, and core:TorrentBroadcastFactory.md[TorrentBroadcastFactory] is the one and only known implementation. == [[contract]] Contract === [[initialize]] initialize Method [source,scala] \u00b6 initialize( isDriver: Boolean, conf: SparkConf, securityMgr: SecurityManager): Unit Used when BroadcastManager is BroadcastManager.md#creating-instance[created]. === [[newBroadcast]] newBroadcast Method [source,scala] \u00b6 newBroadcast T: ClassTag : Broadcast[T] Used when BroadcastManager is requested for a BroadcastManager.md#newBroadcast[new broadcast variable]. === [[stop]] stop Method [source,scala] \u00b6 stop(): Unit \u00b6 Used when BroadcastManager is requested to BroadcastManager.md#stop[stop]. === [[unbroadcast]] unbroadcast Method [source,scala] \u00b6 unbroadcast( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit Used when BroadcastManager is requested to BroadcastManager.md#unbroadcast[unbroadcast a broadcast variable]. == [[implementations]] Available BroadcastFactories core:TorrentBroadcastFactory.md[TorrentBroadcastFactory] is the default and only known BroadcastFactory in Apache Spark.","title":"BroadcastFactory"},{"location":"core/BroadcastFactory/#sourcescala","text":"initialize( isDriver: Boolean, conf: SparkConf, securityMgr: SecurityManager): Unit Used when BroadcastManager is BroadcastManager.md#creating-instance[created]. === [[newBroadcast]] newBroadcast Method","title":"[source,scala]"},{"location":"core/BroadcastFactory/#sourcescala_1","text":"newBroadcast T: ClassTag : Broadcast[T] Used when BroadcastManager is requested for a BroadcastManager.md#newBroadcast[new broadcast variable]. === [[stop]] stop Method","title":"[source,scala]"},{"location":"core/BroadcastFactory/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"core/BroadcastFactory/#stop-unit","text":"Used when BroadcastManager is requested to BroadcastManager.md#stop[stop]. === [[unbroadcast]] unbroadcast Method","title":"stop(): Unit"},{"location":"core/BroadcastFactory/#sourcescala_3","text":"unbroadcast( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit Used when BroadcastManager is requested to BroadcastManager.md#unbroadcast[unbroadcast a broadcast variable]. == [[implementations]] Available BroadcastFactories core:TorrentBroadcastFactory.md[TorrentBroadcastFactory] is the default and only known BroadcastFactory in Apache Spark.","title":"[source,scala]"},{"location":"core/BroadcastManager/","text":"BroadcastManager \u00b6 Creating Instance \u00b6 BroadcastManager takes the following to be created: isDriver flag SparkConf SecurityManager While being created, BroadcastManager is requested to initialize . BroadcastManager is created when: SparkEnv utility is used to create a base SparkEnv (for the driver and executors) Initializing \u00b6 initialize (): Unit Unless initialized already, initialize creates a TorrentBroadcastFactory and requests it to initialize itself . TorrentBroadcastFactory \u00b6 BroadcastManager manages a BroadcastFactory : Creates and initializes it when created (and requested to initialize ) Stops it when stopped BroadcastManager uses the BroadcastFactory in newBroadcast and unbroadcast . Creating Broadcast Variable \u00b6 newBroadcast [ T : ClassTag ]( value_ : T , isLocal : Boolean ): Broadcast [ T ] newBroadcast requests the BroadcastFactory for a new broadcast variable (with the next available broadcast ID ). newBroadcast is used when: SparkContext is requested for a new broadcast variable MapOutputTracker utility is used to serializeMapStatuses Unique Identifiers of Broadcast Variables \u00b6 BroadcastManager tracks broadcast variables and assigns unique and continuous identifiers. MapOutputTrackerMaster \u00b6 BroadcastManager is used to create a MapOutputTrackerMaster","title":"BroadcastManager"},{"location":"core/BroadcastManager/#broadcastmanager","text":"","title":"BroadcastManager"},{"location":"core/BroadcastManager/#creating-instance","text":"BroadcastManager takes the following to be created: isDriver flag SparkConf SecurityManager While being created, BroadcastManager is requested to initialize . BroadcastManager is created when: SparkEnv utility is used to create a base SparkEnv (for the driver and executors)","title":"Creating Instance"},{"location":"core/BroadcastManager/#initializing","text":"initialize (): Unit Unless initialized already, initialize creates a TorrentBroadcastFactory and requests it to initialize itself .","title":" Initializing"},{"location":"core/BroadcastManager/#torrentbroadcastfactory","text":"BroadcastManager manages a BroadcastFactory : Creates and initializes it when created (and requested to initialize ) Stops it when stopped BroadcastManager uses the BroadcastFactory in newBroadcast and unbroadcast .","title":" TorrentBroadcastFactory"},{"location":"core/BroadcastManager/#creating-broadcast-variable","text":"newBroadcast [ T : ClassTag ]( value_ : T , isLocal : Boolean ): Broadcast [ T ] newBroadcast requests the BroadcastFactory for a new broadcast variable (with the next available broadcast ID ). newBroadcast is used when: SparkContext is requested for a new broadcast variable MapOutputTracker utility is used to serializeMapStatuses","title":" Creating Broadcast Variable"},{"location":"core/BroadcastManager/#unique-identifiers-of-broadcast-variables","text":"BroadcastManager tracks broadcast variables and assigns unique and continuous identifiers.","title":" Unique Identifiers of Broadcast Variables"},{"location":"core/BroadcastManager/#mapoutputtrackermaster","text":"BroadcastManager is used to create a MapOutputTrackerMaster","title":" MapOutputTrackerMaster"},{"location":"core/CleanerListener/","text":"= CleanerListener CleanerListener is an abstraction of listeners that can be core:ContextCleaner.md#attachListener[registered with ContextCleaner] to be informed when < >, < >, < >, < > and < > are cleaned. == [[rddCleaned]] rddCleaned Callback Method [source, scala] \u00b6 rddCleaned( rddId: Int): Unit rddCleaned is used when...FIXME == [[broadcastCleaned]] broadcastCleaned Callback Method [source, scala] \u00b6 broadcastCleaned( broadcastId: Long): Unit broadcastCleaned is used when...FIXME == [[shuffleCleaned]] shuffleCleaned Callback Method [source, scala] \u00b6 shuffleCleaned( shuffleId: Int, blocking: Boolean): Unit shuffleCleaned is used when...FIXME == [[accumCleaned]] accumCleaned Callback Method [source, scala] \u00b6 accumCleaned( accId: Long): Unit accumCleaned is used when...FIXME == [[checkpointCleaned]] checkpointCleaned Callback Method [source, scala] \u00b6 checkpointCleaned( rddId: Long): Unit checkpointCleaned is used when...FIXME","title":"CleanerListener"},{"location":"core/CleanerListener/#source-scala","text":"rddCleaned( rddId: Int): Unit rddCleaned is used when...FIXME == [[broadcastCleaned]] broadcastCleaned Callback Method","title":"[source, scala]"},{"location":"core/CleanerListener/#source-scala_1","text":"broadcastCleaned( broadcastId: Long): Unit broadcastCleaned is used when...FIXME == [[shuffleCleaned]] shuffleCleaned Callback Method","title":"[source, scala]"},{"location":"core/CleanerListener/#source-scala_2","text":"shuffleCleaned( shuffleId: Int, blocking: Boolean): Unit shuffleCleaned is used when...FIXME == [[accumCleaned]] accumCleaned Callback Method","title":"[source, scala]"},{"location":"core/CleanerListener/#source-scala_3","text":"accumCleaned( accId: Long): Unit accumCleaned is used when...FIXME == [[checkpointCleaned]] checkpointCleaned Callback Method","title":"[source, scala]"},{"location":"core/CleanerListener/#source-scala_4","text":"checkpointCleaned( rddId: Long): Unit checkpointCleaned is used when...FIXME","title":"[source, scala]"},{"location":"core/ContextCleaner/","text":"ContextCleaner \u00b6 ContextCleaner is a Spark service that is responsible for < > ( cleanup ) of < >, < >, < >, < > and < > that is aimed at reducing the memory requirements of long-running data-heavy Spark applications. Creating Instance \u00b6 ContextCleaner takes the following to be created: [[sc]] SparkContext.md[] ContextCleaner is created and requested to start when SparkContext is created with configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[cleaningThread]] Spark Context Cleaner Cleaning Thread ContextCleaner uses a daemon thread Spark Context Cleaner to clean RDD, shuffle, and broadcast states. The Spark Context Cleaner thread is started when ContextCleaner is requested to < >. == [[listeners]][[attachListener]] CleanerListeners ContextCleaner allows attaching core:CleanerListener.md[CleanerListeners] to be informed when objects are cleaned using attachListener method. [source,scala] \u00b6 attachListener( listener: CleanerListener): Unit == [[doCleanupRDD]] doCleanupRDD Method [source, scala] \u00b6 doCleanupRDD( rddId: Int, blocking: Boolean): Unit doCleanupRDD...FIXME doCleanupRDD is used when ContextCleaner is requested to < > for a CleanRDD. == [[keepCleaning]] keepCleaning Internal Method [source, scala] \u00b6 keepCleaning(): Unit \u00b6 keepCleaning runs indefinitely until ContextCleaner is requested to < >. keepCleaning...FIXME keepCleaning prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Got cleaning task [task] \u00b6 keepCleaning is used in < > that is started once when ContextCleaner is requested to < >. == [[registerRDDCheckpointDataForCleanup]] registerRDDCheckpointDataForCleanup Method [source, scala] \u00b6 registerRDDCheckpointDataForCleanup T : Unit registerRDDCheckpointDataForCleanup...FIXME registerRDDCheckpointDataForCleanup is used when ContextCleaner is requested to < > (with configuration-properties.md#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled). == [[registerBroadcastForCleanup]] registerBroadcastForCleanup Method [source, scala] \u00b6 registerBroadcastForCleanup T : Unit registerBroadcastForCleanup...FIXME registerBroadcastForCleanup is used when SparkContext is used to SparkContext.md#broadcast[create a broadcast variable]. == [[registerRDDForCleanup]] registerRDDForCleanup Method [source, scala] \u00b6 registerRDDForCleanup( rdd: RDD[_]): Unit registerRDDForCleanup...FIXME registerRDDForCleanup is used for rdd:RDD.md#persist[RDD.persist] operation. == [[registerAccumulatorForCleanup]] registerAccumulatorForCleanup Method [source, scala] \u00b6 registerAccumulatorForCleanup( a: AccumulatorV2[_, _]): Unit registerAccumulatorForCleanup...FIXME registerAccumulatorForCleanup is used when AccumulatorV2 is requested to register. == [[stop]] Stopping ContextCleaner [source, scala] \u00b6 stop(): Unit \u00b6 stop...FIXME stop is used when SparkContext is requested to SparkContext.md#stop[stop]. == [[start]] Starting ContextCleaner [source, scala] \u00b6 start(): Unit \u00b6 start starts the < > and an action to request the JVM garbage collector (using System.gc() ) on regular basis per configuration-properties.md#spark.cleaner.periodicGC.interval[spark.cleaner.periodicGC.interval] configuration property. The action to request the JVM GC is scheduled on < >. start is used when SparkContext is created. == [[periodicGCService]] periodicGCService Single-Thread Executor Service periodicGCService is an internal single-thread {java-javadoc-url}/java/util/concurrent/ScheduledExecutorService.html[executor service] with the name context-cleaner-periodic-gc to request the JVM garbage collector. The periodic runs are started when < > and stopped when < >. == [[registerShuffleForCleanup]] Registering ShuffleDependency for Cleanup [source, scala] \u00b6 registerShuffleForCleanup( shuffleDependency: ShuffleDependency[_, _, _]): Unit registerShuffleForCleanup registers the given ShuffleDependency for cleanup. Internally, registerShuffleForCleanup simply executes < > for the given ShuffleDependency. registerShuffleForCleanup is used when ShuffleDependency is created. == [[registerForCleanup]] Registering Object Reference For Cleanup [source, scala] \u00b6 registerForCleanup( objectForCleanup: AnyRef, task: CleanupTask): Unit registerForCleanup adds the input objectForCleanup to the < > internal queue. Despite the widest-possible AnyRef type of the input objectForCleanup , the type is really CleanupTaskWeakReference which is a custom Java's {java-javadoc-url}/java/lang/ref/WeakReference.html[java.lang.ref.WeakReference]. registerForCleanup is used when ContextCleaner is requested to < >, < >, < >, < >, and < >. == [[doCleanupShuffle]] Shuffle Cleanup [source, scala] \u00b6 doCleanupShuffle( shuffleId: Int, blocking: Boolean): Unit doCleanupShuffle performs a shuffle cleanup which is to remove the shuffle from the current scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] and storage:BlockManagerMaster.md[BlockManagerMaster]. doCleanupShuffle also notifies core:CleanerListener.md[CleanerListeners]. Internally, when executed, doCleanupShuffle prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Cleaning shuffle [id] \u00b6 doCleanupShuffle uses core:SparkEnv.md[SparkEnv] to access the core:SparkEnv.md#mapOutputTracker[MapOutputTracker] to scheduler:MapOutputTracker.md#unregisterShuffle[unregister the given shuffle]. doCleanupShuffle uses core:SparkEnv.md[SparkEnv] to access the core:SparkEnv.md#blockManager[BlockManagerMaster] to storage:BlockManagerMaster.md#removeShuffle[remove the shuffle blocks] (for the given shuffleId). doCleanupShuffle informs all registered < > that core:CleanerListener.md#shuffleCleaned[shuffle was cleaned]. In the end, doCleanupShuffle prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Cleaned shuffle [id] \u00b6 In case of any exception, doCleanupShuffle prints out the following ERROR message to the logs and the exception itself: [source,plaintext] \u00b6 Error cleaning shuffle [id] \u00b6 doCleanupShuffle is used when ContextCleaner is requested to < > and (interestingly) while fitting an ALSModel (in Spark MLlib). == [[logging]] Logging Enable ALL logging level for org.apache.spark.ContextCleaner logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.ContextCleaner=ALL \u00b6 Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[referenceBuffer]] referenceBuffer === [[referenceQueue]] referenceQueue","title":"ContextCleaner"},{"location":"core/ContextCleaner/#contextcleaner","text":"ContextCleaner is a Spark service that is responsible for < > ( cleanup ) of < >, < >, < >, < > and < > that is aimed at reducing the memory requirements of long-running data-heavy Spark applications.","title":"ContextCleaner"},{"location":"core/ContextCleaner/#creating-instance","text":"ContextCleaner takes the following to be created: [[sc]] SparkContext.md[] ContextCleaner is created and requested to start when SparkContext is created with configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[cleaningThread]] Spark Context Cleaner Cleaning Thread ContextCleaner uses a daemon thread Spark Context Cleaner to clean RDD, shuffle, and broadcast states. The Spark Context Cleaner thread is started when ContextCleaner is requested to < >. == [[listeners]][[attachListener]] CleanerListeners ContextCleaner allows attaching core:CleanerListener.md[CleanerListeners] to be informed when objects are cleaned using attachListener method.","title":"Creating Instance"},{"location":"core/ContextCleaner/#sourcescala","text":"attachListener( listener: CleanerListener): Unit == [[doCleanupRDD]] doCleanupRDD Method","title":"[source,scala]"},{"location":"core/ContextCleaner/#source-scala","text":"doCleanupRDD( rddId: Int, blocking: Boolean): Unit doCleanupRDD...FIXME doCleanupRDD is used when ContextCleaner is requested to < > for a CleanRDD. == [[keepCleaning]] keepCleaning Internal Method","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/ContextCleaner/#keepcleaning-unit","text":"keepCleaning runs indefinitely until ContextCleaner is requested to < >. keepCleaning...FIXME keepCleaning prints out the following DEBUG message to the logs:","title":"keepCleaning(): Unit"},{"location":"core/ContextCleaner/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#got-cleaning-task-task","text":"keepCleaning is used in < > that is started once when ContextCleaner is requested to < >. == [[registerRDDCheckpointDataForCleanup]] registerRDDCheckpointDataForCleanup Method","title":"Got cleaning task [task]"},{"location":"core/ContextCleaner/#source-scala_2","text":"registerRDDCheckpointDataForCleanup T : Unit registerRDDCheckpointDataForCleanup...FIXME registerRDDCheckpointDataForCleanup is used when ContextCleaner is requested to < > (with configuration-properties.md#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled). == [[registerBroadcastForCleanup]] registerBroadcastForCleanup Method","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_3","text":"registerBroadcastForCleanup T : Unit registerBroadcastForCleanup...FIXME registerBroadcastForCleanup is used when SparkContext is used to SparkContext.md#broadcast[create a broadcast variable]. == [[registerRDDForCleanup]] registerRDDForCleanup Method","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_4","text":"registerRDDForCleanup( rdd: RDD[_]): Unit registerRDDForCleanup...FIXME registerRDDForCleanup is used for rdd:RDD.md#persist[RDD.persist] operation. == [[registerAccumulatorForCleanup]] registerAccumulatorForCleanup Method","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_5","text":"registerAccumulatorForCleanup( a: AccumulatorV2[_, _]): Unit registerAccumulatorForCleanup...FIXME registerAccumulatorForCleanup is used when AccumulatorV2 is requested to register. == [[stop]] Stopping ContextCleaner","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_6","text":"","title":"[source, scala]"},{"location":"core/ContextCleaner/#stop-unit","text":"stop...FIXME stop is used when SparkContext is requested to SparkContext.md#stop[stop]. == [[start]] Starting ContextCleaner","title":"stop(): Unit"},{"location":"core/ContextCleaner/#source-scala_7","text":"","title":"[source, scala]"},{"location":"core/ContextCleaner/#start-unit","text":"start starts the < > and an action to request the JVM garbage collector (using System.gc() ) on regular basis per configuration-properties.md#spark.cleaner.periodicGC.interval[spark.cleaner.periodicGC.interval] configuration property. The action to request the JVM GC is scheduled on < >. start is used when SparkContext is created. == [[periodicGCService]] periodicGCService Single-Thread Executor Service periodicGCService is an internal single-thread {java-javadoc-url}/java/util/concurrent/ScheduledExecutorService.html[executor service] with the name context-cleaner-periodic-gc to request the JVM garbage collector. The periodic runs are started when < > and stopped when < >. == [[registerShuffleForCleanup]] Registering ShuffleDependency for Cleanup","title":"start(): Unit"},{"location":"core/ContextCleaner/#source-scala_8","text":"registerShuffleForCleanup( shuffleDependency: ShuffleDependency[_, _, _]): Unit registerShuffleForCleanup registers the given ShuffleDependency for cleanup. Internally, registerShuffleForCleanup simply executes < > for the given ShuffleDependency. registerShuffleForCleanup is used when ShuffleDependency is created. == [[registerForCleanup]] Registering Object Reference For Cleanup","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_9","text":"registerForCleanup( objectForCleanup: AnyRef, task: CleanupTask): Unit registerForCleanup adds the input objectForCleanup to the < > internal queue. Despite the widest-possible AnyRef type of the input objectForCleanup , the type is really CleanupTaskWeakReference which is a custom Java's {java-javadoc-url}/java/lang/ref/WeakReference.html[java.lang.ref.WeakReference]. registerForCleanup is used when ContextCleaner is requested to < >, < >, < >, < >, and < >. == [[doCleanupShuffle]] Shuffle Cleanup","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_10","text":"doCleanupShuffle( shuffleId: Int, blocking: Boolean): Unit doCleanupShuffle performs a shuffle cleanup which is to remove the shuffle from the current scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] and storage:BlockManagerMaster.md[BlockManagerMaster]. doCleanupShuffle also notifies core:CleanerListener.md[CleanerListeners]. Internally, when executed, doCleanupShuffle prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"core/ContextCleaner/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#cleaning-shuffle-id","text":"doCleanupShuffle uses core:SparkEnv.md[SparkEnv] to access the core:SparkEnv.md#mapOutputTracker[MapOutputTracker] to scheduler:MapOutputTracker.md#unregisterShuffle[unregister the given shuffle]. doCleanupShuffle uses core:SparkEnv.md[SparkEnv] to access the core:SparkEnv.md#blockManager[BlockManagerMaster] to storage:BlockManagerMaster.md#removeShuffle[remove the shuffle blocks] (for the given shuffleId). doCleanupShuffle informs all registered < > that core:CleanerListener.md#shuffleCleaned[shuffle was cleaned]. In the end, doCleanupShuffle prints out the following DEBUG message to the logs:","title":"Cleaning shuffle [id]"},{"location":"core/ContextCleaner/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#cleaned-shuffle-id","text":"In case of any exception, doCleanupShuffle prints out the following ERROR message to the logs and the exception itself:","title":"Cleaned shuffle [id]"},{"location":"core/ContextCleaner/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#error-cleaning-shuffle-id","text":"doCleanupShuffle is used when ContextCleaner is requested to < > and (interestingly) while fitting an ALSModel (in Spark MLlib). == [[logging]] Logging Enable ALL logging level for org.apache.spark.ContextCleaner logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Error cleaning shuffle [id]"},{"location":"core/ContextCleaner/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#log4jloggerorgapachesparkcontextcleanerall","text":"Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[referenceBuffer]] referenceBuffer === [[referenceQueue]] referenceQueue","title":"log4j.logger.org.apache.spark.ContextCleaner=ALL"},{"location":"core/InMemoryStore/","text":"InMemoryStore \u00b6 InMemoryStore is a KVStore . Creating Instance \u00b6 InMemoryStore takes no arguments to be created. InMemoryStore is created when: FsHistoryProvider is created and requested to createInMemoryStore AppStatusStore utility is used to create an AppStatusStore for a live Spark application","title":"InMemoryStore"},{"location":"core/InMemoryStore/#inmemorystore","text":"InMemoryStore is a KVStore .","title":"InMemoryStore"},{"location":"core/InMemoryStore/#creating-instance","text":"InMemoryStore takes no arguments to be created. InMemoryStore is created when: FsHistoryProvider is created and requested to createInMemoryStore AppStatusStore utility is used to create an AppStatusStore for a live Spark application","title":"Creating Instance"},{"location":"core/KVStore/","text":"KVStore \u00b6 KVStore is an abstraction of key-value stores . KVStore is a Java Closeable . Contract \u00b6 count \u00b6 long count ( Class <?> type ) long count ( Class <?> type , String index , Object indexedValue ) delete \u00b6 void delete ( Class <?> type , Object naturalKey ) getMetadata \u00b6 < T > T getMetadata ( Class < T > klass ) read \u00b6 < T > T read ( Class < T > klass , Object naturalKey ) removeAllByIndexValues \u00b6 < T > boolean removeAllByIndexValues ( Class < T > klass , String index , Collection <?> indexValues ) setMetadata \u00b6 void setMetadata ( Object value ) view \u00b6 < T > KVStoreView < T > view ( Class < T > type ) KVStoreView over entities of the given type write \u00b6 void write ( Object value ) Implementations \u00b6 ElementTrackingStore InMemoryStore LevelDB","title":"KVStore"},{"location":"core/KVStore/#kvstore","text":"KVStore is an abstraction of key-value stores . KVStore is a Java Closeable .","title":"KVStore"},{"location":"core/KVStore/#contract","text":"","title":"Contract"},{"location":"core/KVStore/#count","text":"long count ( Class <?> type ) long count ( Class <?> type , String index , Object indexedValue )","title":" count"},{"location":"core/KVStore/#delete","text":"void delete ( Class <?> type , Object naturalKey )","title":" delete"},{"location":"core/KVStore/#getmetadata","text":"< T > T getMetadata ( Class < T > klass )","title":" getMetadata"},{"location":"core/KVStore/#read","text":"< T > T read ( Class < T > klass , Object naturalKey )","title":" read"},{"location":"core/KVStore/#removeallbyindexvalues","text":"< T > boolean removeAllByIndexValues ( Class < T > klass , String index , Collection <?> indexValues )","title":" removeAllByIndexValues"},{"location":"core/KVStore/#setmetadata","text":"void setMetadata ( Object value )","title":" setMetadata"},{"location":"core/KVStore/#view","text":"< T > KVStoreView < T > view ( Class < T > type ) KVStoreView over entities of the given type","title":" view"},{"location":"core/KVStore/#write","text":"void write ( Object value )","title":" write"},{"location":"core/KVStore/#implementations","text":"ElementTrackingStore InMemoryStore LevelDB","title":"Implementations"},{"location":"core/LevelDB/","text":"LevelDB \u00b6 LevelDB is a KVStore for FsHistoryProvider . Creating Instance \u00b6 LevelDB takes the following to be created: Path KVStoreSerializer LevelDB is created when: KVUtils utility is used to open (a LevelDB store)","title":"LevelDB"},{"location":"core/LevelDB/#leveldb","text":"LevelDB is a KVStore for FsHistoryProvider .","title":"LevelDB"},{"location":"core/LevelDB/#creating-instance","text":"LevelDB takes the following to be created: Path KVStoreSerializer LevelDB is created when: KVUtils utility is used to open (a LevelDB store)","title":"Creating Instance"},{"location":"core/RetryingBlockFetcher/","text":"RetryingBlockFetcher \u00b6 RetryingBlockFetcher is...FIXME RetryingBlockFetcher is < > and immediately < > when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default) RetryingBlockFetcher uses a < > to core:BlockFetchStarter.md#createAndStart[createAndStart] when requested to < > and later < >. [[outstandingBlocksIds]] RetryingBlockFetcher uses outstandingBlocksIds internal registry of outstanding block IDs to fetch that is initially the < > when < >. At < >, RetryingBlockFetcher prints out the following INFO message to the logs (with the number of < >): Retrying fetch ([retryCount]/[maxRetries]) for [size] outstanding blocks after [retryWaitTime] ms On < > and < >, < > removes the block ID from < >. [[currentListener]] RetryingBlockFetcher uses a < > to remove block IDs from the < > internal registry. == [[creating-instance]] Creating RetryingBlockFetcher Instance RetryingBlockFetcher takes the following when created: [[conf]] network:TransportConf.md[] [[fetchStarter]] core:BlockFetchStarter.md[] [[blockIds]] Block IDs to fetch [[listener]] core:BlockFetchingListener.md[] == [[start]] Starting RetryingBlockFetcher -- start Method [source, java] \u00b6 void start() \u00b6 start simply < >. start is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default) == [[initiateRetry]] initiateRetry Internal Method [source, java] \u00b6 synchronized void initiateRetry() \u00b6 initiateRetry ...FIXME [NOTE] \u00b6 initiateRetry is used when: RetryingBlockFetcher is requested to < > * RetryingBlockFetchListener is requested to < > \u00b6 == [[fetchAllOutstanding]] fetchAllOutstanding Internal Method [source, java] \u00b6 void fetchAllOutstanding() \u00b6 fetchAllOutstanding requests < > to core:BlockFetchStarter.md#createAndStart[createAndStart] for the < >. NOTE: fetchAllOutstanding is used when RetryingBlockFetcher is requested to < > and < >. == [[RetryingBlockFetchListener]] RetryingBlockFetchListener RetryingBlockFetchListener is a core:BlockFetchingListener.md[] that < > uses to remove block IDs from the < > internal registry. === [[RetryingBlockFetchListener-onBlockFetchSuccess]] onBlockFetchSuccess Method [source, scala] \u00b6 void onBlockFetchSuccess(String blockId, ManagedBuffer data) \u00b6 NOTE: onBlockFetchSuccess is part of core:BlockFetchingListener.md#onBlockFetchSuccess[BlockFetchingListener Contract]. onBlockFetchSuccess ...FIXME === [[RetryingBlockFetchListener-onBlockFetchFailure]] onBlockFetchFailure Method [source, scala] \u00b6 void onBlockFetchFailure(String blockId, Throwable exception) \u00b6 NOTE: onBlockFetchFailure is part of core:BlockFetchingListener.md#onBlockFetchFailure[BlockFetchingListener Contract]. onBlockFetchFailure ...FIXME","title":"RetryingBlockFetcher"},{"location":"core/RetryingBlockFetcher/#retryingblockfetcher","text":"RetryingBlockFetcher is...FIXME RetryingBlockFetcher is < > and immediately < > when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default) RetryingBlockFetcher uses a < > to core:BlockFetchStarter.md#createAndStart[createAndStart] when requested to < > and later < >. [[outstandingBlocksIds]] RetryingBlockFetcher uses outstandingBlocksIds internal registry of outstanding block IDs to fetch that is initially the < > when < >. At < >, RetryingBlockFetcher prints out the following INFO message to the logs (with the number of < >): Retrying fetch ([retryCount]/[maxRetries]) for [size] outstanding blocks after [retryWaitTime] ms On < > and < >, < > removes the block ID from < >. [[currentListener]] RetryingBlockFetcher uses a < > to remove block IDs from the < > internal registry. == [[creating-instance]] Creating RetryingBlockFetcher Instance RetryingBlockFetcher takes the following when created: [[conf]] network:TransportConf.md[] [[fetchStarter]] core:BlockFetchStarter.md[] [[blockIds]] Block IDs to fetch [[listener]] core:BlockFetchingListener.md[] == [[start]] Starting RetryingBlockFetcher -- start Method","title":"RetryingBlockFetcher"},{"location":"core/RetryingBlockFetcher/#source-java","text":"","title":"[source, java]"},{"location":"core/RetryingBlockFetcher/#void-start","text":"start simply < >. start is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default) == [[initiateRetry]] initiateRetry Internal Method","title":"void start()"},{"location":"core/RetryingBlockFetcher/#source-java_1","text":"","title":"[source, java]"},{"location":"core/RetryingBlockFetcher/#synchronized-void-initiateretry","text":"initiateRetry ...FIXME","title":"synchronized void initiateRetry()"},{"location":"core/RetryingBlockFetcher/#note","text":"initiateRetry is used when: RetryingBlockFetcher is requested to < >","title":"[NOTE]"},{"location":"core/RetryingBlockFetcher/#retryingblockfetchlistener-is-requested-to","text":"== [[fetchAllOutstanding]] fetchAllOutstanding Internal Method","title":"* RetryingBlockFetchListener is requested to &lt;&gt;"},{"location":"core/RetryingBlockFetcher/#source-java_2","text":"","title":"[source, java]"},{"location":"core/RetryingBlockFetcher/#void-fetchalloutstanding","text":"fetchAllOutstanding requests < > to core:BlockFetchStarter.md#createAndStart[createAndStart] for the < >. NOTE: fetchAllOutstanding is used when RetryingBlockFetcher is requested to < > and < >. == [[RetryingBlockFetchListener]] RetryingBlockFetchListener RetryingBlockFetchListener is a core:BlockFetchingListener.md[] that < > uses to remove block IDs from the < > internal registry. === [[RetryingBlockFetchListener-onBlockFetchSuccess]] onBlockFetchSuccess Method","title":"void fetchAllOutstanding()"},{"location":"core/RetryingBlockFetcher/#source-scala","text":"","title":"[source, scala]"},{"location":"core/RetryingBlockFetcher/#void-onblockfetchsuccessstring-blockid-managedbuffer-data","text":"NOTE: onBlockFetchSuccess is part of core:BlockFetchingListener.md#onBlockFetchSuccess[BlockFetchingListener Contract]. onBlockFetchSuccess ...FIXME === [[RetryingBlockFetchListener-onBlockFetchFailure]] onBlockFetchFailure Method","title":"void onBlockFetchSuccess(String blockId, ManagedBuffer data)"},{"location":"core/RetryingBlockFetcher/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/RetryingBlockFetcher/#void-onblockfetchfailurestring-blockid-throwable-exception","text":"NOTE: onBlockFetchFailure is part of core:BlockFetchingListener.md#onBlockFetchFailure[BlockFetchingListener Contract]. onBlockFetchFailure ...FIXME","title":"void onBlockFetchFailure(String blockId, Throwable exception)"},{"location":"core/TorrentBroadcast/","text":"= TorrentBroadcast TorrentBroadcast is a Broadcast.md[] that uses a BitTorrent-like protocol for broadcast blocks distribution. .TorrentBroadcast -- Broadcasting using BitTorrent image::sparkcontext-broadcast-bittorrent.png[align=\"center\"] When a SparkContext.md#broadcast[broadcast variable is created (using SparkContext.broadcast )] on the driver, a < >. [source, scala] \u00b6 // On the driver val sc: SparkContext = ??? val anyScalaValue = ??? val b = sc.broadcast(anyScalaValue) // \u2190 TorrentBroadcast is created A broadcast variable is stored on the driver's storage:BlockManager.md[BlockManager] as a single value and separately as broadcast blocks (after it was < >). The broadcast block size is the value of core:BroadcastManager.md#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property. .TorrentBroadcast puts broadcast and the chunks to driver's BlockManager image::sparkcontext-broadcast-bittorrent-newBroadcast.png[align=\"center\"] NOTE: TorrentBroadcast-based broadcast variables are created using core:TorrentBroadcastFactory.md[TorrentBroadcastFactory]. == [[creating-instance]] Creating Instance TorrentBroadcast takes the following to be created: [[obj]] Object (the value) to be broadcast [[id]] ID TorrentBroadcast is created when TorrentBroadcastFactory is requested for a core:TorrentBroadcastFactory.md#newBroadcast[new broadcast variable]. == [[_value]] Transient Lazy Broadcast Value [source, scala] \u00b6 _value: T \u00b6 TorrentBroadcast uses _value internal registry for the value that is < > on demand (and cached afterwards). _value is a @transient private lazy val and uses two Scala language features: It is not serialized when the TorrentBroadcast is serialized to be sent over the wire to executors (and has to be < > afterwards) It is lazily instantiated when first requested and cached afterwards == [[numBlocks]] numBlocks Internal Value TorrentBroadcast uses numBlocks internal value for the total number of blocks it contains. It is < > when TorrentBroadcast is < >. == [[getValue]] Getting Value of Broadcast Variable [source, scala] \u00b6 def getValue(): T \u00b6 getValue returns the <<_value, _value>>. getValue is part of the Broadcast.md#getValue[Broadcast] abstraction. == [[broadcastId]] BroadcastBlockId TorrentBroadcast uses a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for...FIXME == [[readBroadcastBlock]] readBroadcastBlock Internal Method [source, scala] \u00b6 readBroadcastBlock(): T \u00b6 readBroadcastBlock SparkEnv.md#get[uses the SparkEnv] to access SparkEnv.md#broadcastManager[BroadcastManager] that is requested for BroadcastManager.md#cachedValues[cached broadcast values]. readBroadcastBlock looks up the < > in the cached broadcast values and returns it if found. If not found, readBroadcastBlock requests the SparkEnv for the core:SparkEnv.md#conf[SparkConf] and < >. readBroadcastBlock SparkEnv.md#get[uses the SparkEnv] to access SparkEnv.md#blockManager[BlockManager]. readBroadcastBlock requests the BlockManager for storage:BlockManager.md#getLocalValues[getLocalValues]. If the broadcast data was available locally, readBroadcastBlock < > for the broadcast and returns the value. If however the broadcast data was not found locally, you should see the following INFO message in the logs: [source,plaintext] \u00b6 Started reading broadcast variable [id] \u00b6 readBroadcastBlock < > of the broadcast. You should see the following INFO message in the logs: [source,plaintext] \u00b6 Reading broadcast variable [id] took [usedTimeMs] \u00b6 readBroadcastBlock < ByteBuffer blocks>> NOTE: readBroadcastBlock uses the core:SparkEnv.md#serializer[current Serializer ] and the internal io:CompressionCodec.md[CompressionCodec] to bring all the blocks together as one single broadcast variable. readBroadcastBlock storage:BlockManager.md#putSingle[stores the broadcast variable with MEMORY_AND_DISK storage level to the local BlockManager ]. When storing the broadcast variable was unsuccessful, a SparkException is thrown. [source,plaintext] \u00b6 Failed to store [broadcastId] in BlockManager \u00b6 The broadcast variable is returned. readBroadcastBlock is used when TorrentBroadcast is requested for the <<_value, broadcast value>>. == [[setConf]] setConf Internal Method [source, scala] \u00b6 setConf( conf: SparkConf): Unit setConf uses the input conf SparkConf.md[SparkConf] to set compression codec and the block size. Internally, setConf reads core:BroadcastManager.md#spark.broadcast.compress[spark.broadcast.compress] configuration property and if enabled (which it is by default) sets a io:CompressionCodec.md#createCodec[CompressionCodec] (as an internal compressionCodec property). setConf also reads core:BroadcastManager.md#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property and sets the block size (as the internal blockSize property). setConf is executed when < > or < >. == [[writeBlocks]] Storing Broadcast and Blocks to BlockManager [source, scala] \u00b6 writeBlocks( value: T): Int writeBlocks stores the given value (that is the < >) and the blocks in storage:BlockManager.md[]. writeBlocks returns the < > (was divided into). Internally, writeBlocks uses the core:SparkEnv.md#get[SparkEnv] to access core:SparkEnv.md#blockManager[BlockManager]. writeBlocks requests the BlockManager to storage:BlockManager.md#putSingle[putSingle] (with MEMORY_AND_DISK storage level). writeBlocks < > the given value (of the < >, the system core:SparkEnv.md#serializer[Serializer], and the optional < >). For every block, writeBlocks creates a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for the < > and piece[index] identifier, and requests the BlockManager to storage:BlockManager.md#putBytes[putBytes] (with MEMORY_AND_DISK_SER storage level). The entire broadcast value is stored in the local BlockManager with MEMORY_AND_DISK storage level whereas the blocks with MEMORY_AND_DISK_SER storage level. With < > writeBlocks...FIXME In case of an error while storing the value or the blocks, writeBlocks throws a SparkException: [source,plaintext] \u00b6 Failed to store [pieceId] of [broadcastId] in local BlockManager \u00b6 writeBlocks is used when TorrentBroadcast is < > for the < > internal registry (that happens on the driver only). == [[blockifyObject]] Chunking Broadcast Variable Into Blocks [source, scala] \u00b6 blockifyObject T : Array[ByteBuffer] blockifyObject divides (aka blockifies ) the input obj value into blocks ( ByteBuffer chunks). blockifyObject uses the given serializer:Serializer.md[] to write the value in a serialized format to a ChunkedByteBufferOutputStream of the given blockSize size with the optional io:CompressionCodec.md[CompressionCodec]. blockifyObject is used when TorrentBroadcast is requested to < >. == [[doUnpersist]] doUnpersist Method [source, scala] \u00b6 doUnpersist(blocking: Boolean): Unit \u00b6 doUnpersist < >. NOTE: doUnpersist is part of the Broadcast.md#contract[ Broadcast Variable Contract] and is executed from < > method. == [[doDestroy]] doDestroy Method [source, scala] \u00b6 doDestroy(blocking: Boolean): Unit \u00b6 doDestroy < >, i.e. the driver and executors. NOTE: doDestroy is executed when Broadcast.md#destroy-internal[ Broadcast removes the persisted data and metadata related to a broadcast variable]. == [[unpersist]] unpersist Utility [source, scala] \u00b6 unpersist( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit unpersist removes all broadcast blocks from executors and, with the given removeFromDriver flag enabled, from the driver. When executed, unpersist prints out the following DEBUG message in the logs: [source,plaintext] \u00b6 Unpersisting TorrentBroadcast [id] \u00b6 unpersist requests storage:BlockManagerMaster.md#removeBroadcast[ BlockManagerMaster to remove the id broadcast]. NOTE: unpersist uses core:SparkEnv.md#blockManager[ SparkEnv to get the BlockManagerMaster ] (through blockManager property). unpersist is used when: TorrentBroadcast is requested to < > and < > TorrentBroadcastFactory is requested to TorrentBroadcastFactory.md#unbroadcast[unbroadcast] == [[readBlocks]] Reading Broadcast Blocks [source, scala] \u00b6 readBlocks(): Array[BlockData] \u00b6 readBlocks creates a local array of storage:BlockData.md[]s for < > elements (that is later modified and returned). readBlocks uses the core:SparkEnv.md[] to access core:SparkEnv.md#blockManager[BlockManager] (that is later used to fetch local or remote blocks). For every block (randomly-chosen by block ID between 0 and < >), readBlocks creates a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for the < > (of the broadcast variable) and the chunk identified by the piece prefix followed by the ID. readBlocks prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Reading piece [pieceId] of [broadcastId] \u00b6 readBlocks first tries to look up the piece locally by requesting the BlockManager to storage:BlockManager.md#getLocalBytes[getLocalBytes] and, if found, stores the reference in the local block array (for the piece ID) and < > for the chunk. If not found locally, readBlocks requests the BlockManager to storage:BlockManager.md#getRemoteBytes[getRemoteBytes]. readBlocks...FIXME readBlocks throws a SparkException for blocks neither available locally nor remotely: [source,plaintext] \u00b6 Failed to get [pieceId] of [broadcastId] \u00b6 readBlocks is used when TorrentBroadcast is requested to < >. == [[unBlockifyObject]] unBlockifyObject Utility [source, scala] \u00b6 unBlockifyObject T: ClassTag : T unBlockifyObject...FIXME unBlockifyObject is used when TorrentBroadcast is requested to < >. == [[releaseLock]] releaseLock Internal Method [source, scala] \u00b6 releaseLock( blockId: BlockId): Unit releaseLock...FIXME releaseLock is used when TorrentBroadcast is requested to < > and < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.broadcast.TorrentBroadcast logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.broadcast.TorrentBroadcast=ALL \u00b6 Refer to spark-logging.md[Logging].","title":"TorrentBroadcast"},{"location":"core/TorrentBroadcast/#source-scala","text":"// On the driver val sc: SparkContext = ??? val anyScalaValue = ??? val b = sc.broadcast(anyScalaValue) // \u2190 TorrentBroadcast is created A broadcast variable is stored on the driver's storage:BlockManager.md[BlockManager] as a single value and separately as broadcast blocks (after it was < >). The broadcast block size is the value of core:BroadcastManager.md#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property. .TorrentBroadcast puts broadcast and the chunks to driver's BlockManager image::sparkcontext-broadcast-bittorrent-newBroadcast.png[align=\"center\"] NOTE: TorrentBroadcast-based broadcast variables are created using core:TorrentBroadcastFactory.md[TorrentBroadcastFactory]. == [[creating-instance]] Creating Instance TorrentBroadcast takes the following to be created: [[obj]] Object (the value) to be broadcast [[id]] ID TorrentBroadcast is created when TorrentBroadcastFactory is requested for a core:TorrentBroadcastFactory.md#newBroadcast[new broadcast variable]. == [[_value]] Transient Lazy Broadcast Value","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#_value-t","text":"TorrentBroadcast uses _value internal registry for the value that is < > on demand (and cached afterwards). _value is a @transient private lazy val and uses two Scala language features: It is not serialized when the TorrentBroadcast is serialized to be sent over the wire to executors (and has to be < > afterwards) It is lazily instantiated when first requested and cached afterwards == [[numBlocks]] numBlocks Internal Value TorrentBroadcast uses numBlocks internal value for the total number of blocks it contains. It is < > when TorrentBroadcast is < >. == [[getValue]] Getting Value of Broadcast Variable","title":"_value: T"},{"location":"core/TorrentBroadcast/#source-scala_2","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#def-getvalue-t","text":"getValue returns the <<_value, _value>>. getValue is part of the Broadcast.md#getValue[Broadcast] abstraction. == [[broadcastId]] BroadcastBlockId TorrentBroadcast uses a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for...FIXME == [[readBroadcastBlock]] readBroadcastBlock Internal Method","title":"def getValue(): T"},{"location":"core/TorrentBroadcast/#source-scala_3","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#readbroadcastblock-t","text":"readBroadcastBlock SparkEnv.md#get[uses the SparkEnv] to access SparkEnv.md#broadcastManager[BroadcastManager] that is requested for BroadcastManager.md#cachedValues[cached broadcast values]. readBroadcastBlock looks up the < > in the cached broadcast values and returns it if found. If not found, readBroadcastBlock requests the SparkEnv for the core:SparkEnv.md#conf[SparkConf] and < >. readBroadcastBlock SparkEnv.md#get[uses the SparkEnv] to access SparkEnv.md#blockManager[BlockManager]. readBroadcastBlock requests the BlockManager for storage:BlockManager.md#getLocalValues[getLocalValues]. If the broadcast data was available locally, readBroadcastBlock < > for the broadcast and returns the value. If however the broadcast data was not found locally, you should see the following INFO message in the logs:","title":"readBroadcastBlock(): T"},{"location":"core/TorrentBroadcast/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#started-reading-broadcast-variable-id","text":"readBroadcastBlock < > of the broadcast. You should see the following INFO message in the logs:","title":"Started reading broadcast variable [id]"},{"location":"core/TorrentBroadcast/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#reading-broadcast-variable-id-took-usedtimems","text":"readBroadcastBlock < ByteBuffer blocks>> NOTE: readBroadcastBlock uses the core:SparkEnv.md#serializer[current Serializer ] and the internal io:CompressionCodec.md[CompressionCodec] to bring all the blocks together as one single broadcast variable. readBroadcastBlock storage:BlockManager.md#putSingle[stores the broadcast variable with MEMORY_AND_DISK storage level to the local BlockManager ]. When storing the broadcast variable was unsuccessful, a SparkException is thrown.","title":"Reading broadcast variable [id] took [usedTimeMs]"},{"location":"core/TorrentBroadcast/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#failed-to-store-broadcastid-in-blockmanager","text":"The broadcast variable is returned. readBroadcastBlock is used when TorrentBroadcast is requested for the <<_value, broadcast value>>. == [[setConf]] setConf Internal Method","title":"Failed to store [broadcastId] in BlockManager"},{"location":"core/TorrentBroadcast/#source-scala_4","text":"setConf( conf: SparkConf): Unit setConf uses the input conf SparkConf.md[SparkConf] to set compression codec and the block size. Internally, setConf reads core:BroadcastManager.md#spark.broadcast.compress[spark.broadcast.compress] configuration property and if enabled (which it is by default) sets a io:CompressionCodec.md#createCodec[CompressionCodec] (as an internal compressionCodec property). setConf also reads core:BroadcastManager.md#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property and sets the block size (as the internal blockSize property). setConf is executed when < > or < >. == [[writeBlocks]] Storing Broadcast and Blocks to BlockManager","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source-scala_5","text":"writeBlocks( value: T): Int writeBlocks stores the given value (that is the < >) and the blocks in storage:BlockManager.md[]. writeBlocks returns the < > (was divided into). Internally, writeBlocks uses the core:SparkEnv.md#get[SparkEnv] to access core:SparkEnv.md#blockManager[BlockManager]. writeBlocks requests the BlockManager to storage:BlockManager.md#putSingle[putSingle] (with MEMORY_AND_DISK storage level). writeBlocks < > the given value (of the < >, the system core:SparkEnv.md#serializer[Serializer], and the optional < >). For every block, writeBlocks creates a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for the < > and piece[index] identifier, and requests the BlockManager to storage:BlockManager.md#putBytes[putBytes] (with MEMORY_AND_DISK_SER storage level). The entire broadcast value is stored in the local BlockManager with MEMORY_AND_DISK storage level whereas the blocks with MEMORY_AND_DISK_SER storage level. With < > writeBlocks...FIXME In case of an error while storing the value or the blocks, writeBlocks throws a SparkException:","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#failed-to-store-pieceid-of-broadcastid-in-local-blockmanager","text":"writeBlocks is used when TorrentBroadcast is < > for the < > internal registry (that happens on the driver only). == [[blockifyObject]] Chunking Broadcast Variable Into Blocks","title":"Failed to store [pieceId] of [broadcastId] in local BlockManager"},{"location":"core/TorrentBroadcast/#source-scala_6","text":"blockifyObject T : Array[ByteBuffer] blockifyObject divides (aka blockifies ) the input obj value into blocks ( ByteBuffer chunks). blockifyObject uses the given serializer:Serializer.md[] to write the value in a serialized format to a ChunkedByteBufferOutputStream of the given blockSize size with the optional io:CompressionCodec.md[CompressionCodec]. blockifyObject is used when TorrentBroadcast is requested to < >. == [[doUnpersist]] doUnpersist Method","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source-scala_7","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#dounpersistblocking-boolean-unit","text":"doUnpersist < >. NOTE: doUnpersist is part of the Broadcast.md#contract[ Broadcast Variable Contract] and is executed from < > method. == [[doDestroy]] doDestroy Method","title":"doUnpersist(blocking: Boolean): Unit"},{"location":"core/TorrentBroadcast/#source-scala_8","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#dodestroyblocking-boolean-unit","text":"doDestroy < >, i.e. the driver and executors. NOTE: doDestroy is executed when Broadcast.md#destroy-internal[ Broadcast removes the persisted data and metadata related to a broadcast variable]. == [[unpersist]] unpersist Utility","title":"doDestroy(blocking: Boolean): Unit"},{"location":"core/TorrentBroadcast/#source-scala_9","text":"unpersist( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit unpersist removes all broadcast blocks from executors and, with the given removeFromDriver flag enabled, from the driver. When executed, unpersist prints out the following DEBUG message in the logs:","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#unpersisting-torrentbroadcast-id","text":"unpersist requests storage:BlockManagerMaster.md#removeBroadcast[ BlockManagerMaster to remove the id broadcast]. NOTE: unpersist uses core:SparkEnv.md#blockManager[ SparkEnv to get the BlockManagerMaster ] (through blockManager property). unpersist is used when: TorrentBroadcast is requested to < > and < > TorrentBroadcastFactory is requested to TorrentBroadcastFactory.md#unbroadcast[unbroadcast] == [[readBlocks]] Reading Broadcast Blocks","title":"Unpersisting TorrentBroadcast [id]"},{"location":"core/TorrentBroadcast/#source-scala_10","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#readblocks-arrayblockdata","text":"readBlocks creates a local array of storage:BlockData.md[]s for < > elements (that is later modified and returned). readBlocks uses the core:SparkEnv.md[] to access core:SparkEnv.md#blockManager[BlockManager] (that is later used to fetch local or remote blocks). For every block (randomly-chosen by block ID between 0 and < >), readBlocks creates a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for the < > (of the broadcast variable) and the chunk identified by the piece prefix followed by the ID. readBlocks prints out the following DEBUG message to the logs:","title":"readBlocks(): Array[BlockData]"},{"location":"core/TorrentBroadcast/#sourceplaintext_5","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#reading-piece-pieceid-of-broadcastid","text":"readBlocks first tries to look up the piece locally by requesting the BlockManager to storage:BlockManager.md#getLocalBytes[getLocalBytes] and, if found, stores the reference in the local block array (for the piece ID) and < > for the chunk. If not found locally, readBlocks requests the BlockManager to storage:BlockManager.md#getRemoteBytes[getRemoteBytes]. readBlocks...FIXME readBlocks throws a SparkException for blocks neither available locally nor remotely:","title":"Reading piece [pieceId] of [broadcastId]"},{"location":"core/TorrentBroadcast/#sourceplaintext_6","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#failed-to-get-pieceid-of-broadcastid","text":"readBlocks is used when TorrentBroadcast is requested to < >. == [[unBlockifyObject]] unBlockifyObject Utility","title":"Failed to get [pieceId] of [broadcastId]"},{"location":"core/TorrentBroadcast/#source-scala_11","text":"unBlockifyObject T: ClassTag : T unBlockifyObject...FIXME unBlockifyObject is used when TorrentBroadcast is requested to < >. == [[releaseLock]] releaseLock Internal Method","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source-scala_12","text":"releaseLock( blockId: BlockId): Unit releaseLock...FIXME releaseLock is used when TorrentBroadcast is requested to < > and < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.broadcast.TorrentBroadcast logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source","text":"","title":"[source]"},{"location":"core/TorrentBroadcast/#log4jloggerorgapachesparkbroadcasttorrentbroadcastall","text":"Refer to spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.broadcast.TorrentBroadcast=ALL"},{"location":"core/TorrentBroadcastFactory/","text":"= TorrentBroadcastFactory TorrentBroadcastFactory is a core:BroadcastFactory.md[BroadcastFactory] of core:TorrentBroadcast.md[TorrentBroadcast]s (for BitTorrent-like Broadcast.md[]s). NOTE: As of https://issues.apache.org/jira/browse/SPARK-12588[Spark 2.0] TorrentBroadcastFactory is is the one and only known core:BroadcastFactory.md[BroadcastFactory]. == [[creating-instance]] Creating Instance TorrentBroadcastFactory takes no arguments to be created. TorrentBroadcastFactory is created for BroadcastManager.md#broadcastFactory[BroadcastManager]. == [[newBroadcast]] Creating Broadcast Variable (TorrentBroadcast) [source,scala] \u00b6 newBroadcast T: ClassTag : Broadcast[T] newBroadcast creates a core:TorrentBroadcast.md[] (for the given value_ and id and ignoring the isLocal flag). newBroadcast is part of the BroadcastFactory.md#newBroadcast[BroadcastFactory] abstraction. == [[unbroadcast]] Unbroadcasting Broadcast Variable [source,scala] \u00b6 unbroadcast( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit unbroadcast core:TorrentBroadcast.md#unpersist[removes all persisted state associated with the TorrentBroadcast] (by the given id). unbroadcast is part of the BroadcastFactory.md#unbroadcast[BroadcastFactory] abstraction. == [[initialize]] Initializing TorrentBroadcastFactory [source,scala] \u00b6 initialize( isDriver: Boolean, conf: SparkConf, securityMgr: SecurityManager): Unit initialize does nothing. initialize is part of the BroadcastFactory.md#initialize[BroadcastFactory] abstraction. == [[stop]] Stopping TorrentBroadcastFactory [source,scala] \u00b6 stop(): Unit \u00b6 stop does nothing. stop is part of the BroadcastFactory.md#stop[BroadcastFactory] abstraction.","title":"TorrentBroadcastFactory"},{"location":"core/TorrentBroadcastFactory/#sourcescala","text":"newBroadcast T: ClassTag : Broadcast[T] newBroadcast creates a core:TorrentBroadcast.md[] (for the given value_ and id and ignoring the isLocal flag). newBroadcast is part of the BroadcastFactory.md#newBroadcast[BroadcastFactory] abstraction. == [[unbroadcast]] Unbroadcasting Broadcast Variable","title":"[source,scala]"},{"location":"core/TorrentBroadcastFactory/#sourcescala_1","text":"unbroadcast( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit unbroadcast core:TorrentBroadcast.md#unpersist[removes all persisted state associated with the TorrentBroadcast] (by the given id). unbroadcast is part of the BroadcastFactory.md#unbroadcast[BroadcastFactory] abstraction. == [[initialize]] Initializing TorrentBroadcastFactory","title":"[source,scala]"},{"location":"core/TorrentBroadcastFactory/#sourcescala_2","text":"initialize( isDriver: Boolean, conf: SparkConf, securityMgr: SecurityManager): Unit initialize does nothing. initialize is part of the BroadcastFactory.md#initialize[BroadcastFactory] abstraction. == [[stop]] Stopping TorrentBroadcastFactory","title":"[source,scala]"},{"location":"core/TorrentBroadcastFactory/#sourcescala_3","text":"","title":"[source,scala]"},{"location":"core/TorrentBroadcastFactory/#stop-unit","text":"stop does nothing. stop is part of the BroadcastFactory.md#stop[BroadcastFactory] abstraction.","title":"stop(): Unit"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: DiskBlockManager and Block Data","title":"Demos"},{"location":"demo/#demos","text":"The following demos are available: DiskBlockManager and Block Data","title":"Demos"},{"location":"demo/diskblockmanager-and-block-data/","text":"Demo: DiskBlockManager and Block Data \u00b6 The demo shows how Spark stores data blocks on local disk (using DiskBlockManager and DiskStore among the services). Configure Local Directories \u00b6 Spark uses spark.local.dir configuration property for one or more local directories to store data blocks. Start spark-shell with the property set to a directory of your choice (say local-dirs ). Use one directory for easier monitoring. $SPARK_HOME/bin/spark-shell --conf spark.local.dir=local-dirs When started, Spark will create a proper directory layout. You are interested in blockmgr-[uuid] directory. \"Create\" Data Blocks \u00b6 Execute the following Spark application that forces persisting ( caching ) data to disk. import org.apache.spark.storage.StorageLevel spark.range(2).persist(StorageLevel.DISK_ONLY).count Observe Block Files \u00b6 Go to the blockmgr-[uuid] directory and observe the block files. There should be a few. Do you know how many and why? $ tree local-dirs/blockmgr-b7167b5a-ae8d-404b-8de2-1a0fb101fe00/ local-dirs/blockmgr-b7167b5a-ae8d-404b-8de2-1a0fb101fe00/ \u251c\u2500\u2500 00 \u251c\u2500\u2500 04 \u2502 \u2514\u2500\u2500 shuffle_0_8_0.data \u251c\u2500\u2500 06 \u251c\u2500\u2500 08 \u2502 \u2514\u2500\u2500 shuffle_0_8_0.index ... \u251c\u2500\u2500 37 \u2502 \u2514\u2500\u2500 shuffle_0_7_0.index \u251c\u2500\u2500 38 \u2502 \u2514\u2500\u2500 shuffle_0_4_0.data \u251c\u2500\u2500 39 \u2502 \u2514\u2500\u2500 shuffle_0_9_0.index \u2514\u2500\u2500 3a \u2514\u2500\u2500 shuffle_0_6_0.data 47 directories, 48 files Use web UI \u00b6 Open http://localhost:4040 and switch to Storage tab (at http://localhost:4040/storage/ ). You should see one RDD cached. Click the link in RDD Name column and review the information. Enable Logging \u00b6 Enable ALL logging level for org.apache.spark.storage.DiskStore and org.apache.spark.storage.DiskBlockManager loggers to have an even deeper insight on the block storage internals. log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL log4j.logger.org.apache.spark.storage.DiskStore=ALL","title":"DiskBlockManager and Block Data"},{"location":"demo/diskblockmanager-and-block-data/#demo-diskblockmanager-and-block-data","text":"The demo shows how Spark stores data blocks on local disk (using DiskBlockManager and DiskStore among the services).","title":"Demo: DiskBlockManager and Block Data"},{"location":"demo/diskblockmanager-and-block-data/#configure-local-directories","text":"Spark uses spark.local.dir configuration property for one or more local directories to store data blocks. Start spark-shell with the property set to a directory of your choice (say local-dirs ). Use one directory for easier monitoring. $SPARK_HOME/bin/spark-shell --conf spark.local.dir=local-dirs When started, Spark will create a proper directory layout. You are interested in blockmgr-[uuid] directory.","title":"Configure Local Directories"},{"location":"demo/diskblockmanager-and-block-data/#create-data-blocks","text":"Execute the following Spark application that forces persisting ( caching ) data to disk. import org.apache.spark.storage.StorageLevel spark.range(2).persist(StorageLevel.DISK_ONLY).count","title":"\"Create\" Data Blocks"},{"location":"demo/diskblockmanager-and-block-data/#observe-block-files","text":"Go to the blockmgr-[uuid] directory and observe the block files. There should be a few. Do you know how many and why? $ tree local-dirs/blockmgr-b7167b5a-ae8d-404b-8de2-1a0fb101fe00/ local-dirs/blockmgr-b7167b5a-ae8d-404b-8de2-1a0fb101fe00/ \u251c\u2500\u2500 00 \u251c\u2500\u2500 04 \u2502 \u2514\u2500\u2500 shuffle_0_8_0.data \u251c\u2500\u2500 06 \u251c\u2500\u2500 08 \u2502 \u2514\u2500\u2500 shuffle_0_8_0.index ... \u251c\u2500\u2500 37 \u2502 \u2514\u2500\u2500 shuffle_0_7_0.index \u251c\u2500\u2500 38 \u2502 \u2514\u2500\u2500 shuffle_0_4_0.data \u251c\u2500\u2500 39 \u2502 \u2514\u2500\u2500 shuffle_0_9_0.index \u2514\u2500\u2500 3a \u2514\u2500\u2500 shuffle_0_6_0.data 47 directories, 48 files","title":"Observe Block Files"},{"location":"demo/diskblockmanager-and-block-data/#use-web-ui","text":"Open http://localhost:4040 and switch to Storage tab (at http://localhost:4040/storage/ ). You should see one RDD cached. Click the link in RDD Name column and review the information.","title":"Use web UI"},{"location":"demo/diskblockmanager-and-block-data/#enable-logging","text":"Enable ALL logging level for org.apache.spark.storage.DiskStore and org.apache.spark.storage.DiskBlockManager loggers to have an even deeper insight on the block storage internals. log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL log4j.logger.org.apache.spark.storage.DiskStore=ALL","title":"Enable Logging"},{"location":"dynamic-allocation/","text":"Dynamic Allocation of Executors \u00b6 Dynamic Allocation of Executors ( Dynamic Resource Allocation or Elastic Scaling ) is a Spark service for adding and removing Spark executors dynamically on demand to match workload. Unlike the \"traditional\" static allocation where a Spark application reserves CPU and memory resources upfront (irrespective of how much it may eventually use), in dynamic allocation you get as much as needed and no more. It scales the number of executors up and down based on workload, i.e. idle executors are removed, and when there are pending tasks waiting for executors to be launched on, dynamic allocation requests them. Dynamic Allocation is enabled (and SparkContext creates an ExecutorAllocationManager ) when: spark.dynamicAllocation.enabled configuration property is enabled spark.master is non- local SchedulerBackend is an ExecutorAllocationClient ExecutorAllocationManager is the heart of Dynamic Resource Allocation. When enabled, it is recommended to use the External Shuffle Service . Dynamic Allocation comes with the policy of scaling executors up and down as follows: Scale Up Policy requests new executors when there are pending tasks and increases the number of executors exponentially since executors start slow and Spark application may need slightly more. Scale Down Policy removes executors that have been idle for spark.dynamicAllocation.executorIdleTimeout seconds. Performance Metrics \u00b6 ExecutorAllocationManagerSource metric source is used to report performance metrics. SparkContext.killExecutors \u00b6 SparkContext.killExecutors is unsupported with Dynamic Allocation enabled. Programmable Dynamic Allocation \u00b6 SparkContext offers a developer API to scale executors up or down . Getting Initial Number of Executors for Dynamic Allocation \u00b6 getDynamicAllocationInitialExecutors ( conf : SparkConf ): Int getDynamicAllocationInitialExecutors first makes sure that < > is equal or greater than < >. NOTE: < > falls back to < > if not set. Why to print the WARN message to the logs? If not, you should see the following WARN message in the logs: spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs. getDynamicAllocationInitialExecutors makes sure that executor:Executor.md#spark.executor.instances[spark.executor.instances] is greater than < >. NOTE: Both executor:Executor.md#spark.executor.instances[spark.executor.instances] and < > fall back to 0 when no defined explicitly. If not, you should see the following WARN message in the logs: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs. getDynamicAllocationInitialExecutors sets the initial number of executors to be the maximum of: spark.dynamicAllocation.minExecutors spark.dynamicAllocation.initialExecutors spark.executor.instances 0 You should see the following INFO message in the logs: Using initial executors = [initialExecutors], max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances getDynamicAllocationInitialExecutors is used when ExecutorAllocationManager is requested to set the initial number of executors . Resources \u00b6 Documentation \u00b6 Dynamic Allocation in the official documentation of Apache Spark Dynamic allocation in the documentation of Cloudera Data Platform (CDP) Slides \u00b6 Dynamic Allocation in Spark by Databricks","title":"Dynamic Allocation of Executors"},{"location":"dynamic-allocation/#dynamic-allocation-of-executors","text":"Dynamic Allocation of Executors ( Dynamic Resource Allocation or Elastic Scaling ) is a Spark service for adding and removing Spark executors dynamically on demand to match workload. Unlike the \"traditional\" static allocation where a Spark application reserves CPU and memory resources upfront (irrespective of how much it may eventually use), in dynamic allocation you get as much as needed and no more. It scales the number of executors up and down based on workload, i.e. idle executors are removed, and when there are pending tasks waiting for executors to be launched on, dynamic allocation requests them. Dynamic Allocation is enabled (and SparkContext creates an ExecutorAllocationManager ) when: spark.dynamicAllocation.enabled configuration property is enabled spark.master is non- local SchedulerBackend is an ExecutorAllocationClient ExecutorAllocationManager is the heart of Dynamic Resource Allocation. When enabled, it is recommended to use the External Shuffle Service . Dynamic Allocation comes with the policy of scaling executors up and down as follows: Scale Up Policy requests new executors when there are pending tasks and increases the number of executors exponentially since executors start slow and Spark application may need slightly more. Scale Down Policy removes executors that have been idle for spark.dynamicAllocation.executorIdleTimeout seconds.","title":"Dynamic Allocation of Executors"},{"location":"dynamic-allocation/#performance-metrics","text":"ExecutorAllocationManagerSource metric source is used to report performance metrics.","title":"Performance Metrics"},{"location":"dynamic-allocation/#sparkcontextkillexecutors","text":"SparkContext.killExecutors is unsupported with Dynamic Allocation enabled.","title":"SparkContext.killExecutors"},{"location":"dynamic-allocation/#programmable-dynamic-allocation","text":"SparkContext offers a developer API to scale executors up or down .","title":"Programmable Dynamic Allocation"},{"location":"dynamic-allocation/#getting-initial-number-of-executors-for-dynamic-allocation","text":"getDynamicAllocationInitialExecutors ( conf : SparkConf ): Int getDynamicAllocationInitialExecutors first makes sure that < > is equal or greater than < >. NOTE: < > falls back to < > if not set. Why to print the WARN message to the logs? If not, you should see the following WARN message in the logs: spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs. getDynamicAllocationInitialExecutors makes sure that executor:Executor.md#spark.executor.instances[spark.executor.instances] is greater than < >. NOTE: Both executor:Executor.md#spark.executor.instances[spark.executor.instances] and < > fall back to 0 when no defined explicitly. If not, you should see the following WARN message in the logs: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs. getDynamicAllocationInitialExecutors sets the initial number of executors to be the maximum of: spark.dynamicAllocation.minExecutors spark.dynamicAllocation.initialExecutors spark.executor.instances 0 You should see the following INFO message in the logs: Using initial executors = [initialExecutors], max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances getDynamicAllocationInitialExecutors is used when ExecutorAllocationManager is requested to set the initial number of executors .","title":" Getting Initial Number of Executors for Dynamic Allocation"},{"location":"dynamic-allocation/#resources","text":"","title":"Resources"},{"location":"dynamic-allocation/#documentation","text":"Dynamic Allocation in the official documentation of Apache Spark Dynamic allocation in the documentation of Cloudera Data Platform (CDP)","title":"Documentation"},{"location":"dynamic-allocation/#slides","text":"Dynamic Allocation in Spark by Databricks","title":"Slides"},{"location":"dynamic-allocation/ExecutorAllocationClient/","text":"ExecutorAllocationClient \u00b6 ExecutorAllocationClient is an abstraction of schedulers that can communicate with a cluster manager to request or kill executors. Contract \u00b6 Active Executor IDs \u00b6 getExecutorIds (): Seq [ String ] Used when: SparkContext is requested for active executors isExecutorActive \u00b6 isExecutorActive ( id : String ): Boolean Whether a given executor (by ID) is active (and can be used to execute tasks) Used when: FIXME Killing Executors \u00b6 killExecutors ( executorIds : Seq [ String ], adjustTargetNumExecutors : Boolean , countFailures : Boolean , force : Boolean = false ): Seq [ String ] Requests a cluster manager to kill given executors and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when: ExecutorAllocationClient is requested to kill an executor ExecutorAllocationManager is requested to removeExecutors SparkContext is requested to kill executors and killAndReplaceExecutor BlacklistTracker is requested to kill an executor DriverEndpoint is requested to handle a KillExecutorsOnHost message Killing Executors on Host \u00b6 killExecutorsOnHost ( host : String ): Boolean Used when: BlacklistTracker is requested to kill executors on a blacklisted node Requesting Additional Executors \u00b6 requestExecutors ( numAdditionalExecutors : Int ): Boolean Requests additional executors from a cluster manager and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when: SparkContext is requested for additional executors Updating Total Executors \u00b6 requestTotalExecutors ( resourceProfileIdToNumExecutors : Map [ Int , Int ], numLocalityAwareTasksPerResourceProfileId : Map [ Int , Int ], hostToLocalTaskCount : Map [ Int , Map [ String , Int ]]): Boolean Updates a cluster manager with the exact number of executors desired. Returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when: SparkContext is requested to update the number of total executors ExecutorAllocationManager is requested to start , updateAndSyncNumExecutorsTarget , addExecutors , removeExecutors Implementations \u00b6 CoarseGrainedSchedulerBackend KubernetesClusterSchedulerBackend ( Spark on Kubernetes ) MesosCoarseGrainedSchedulerBackend StandaloneSchedulerBackend YarnSchedulerBackend Killing Single Executor \u00b6 killExecutor ( executorId : String ): Boolean killExecutor kill the given executor . killExecutor is used when: ExecutorAllocationManager removes an executor . SparkContext is requested to kill executors . Decommissioning Executors \u00b6 decommissionExecutors ( executorsAndDecomInfo : Array [( String , ExecutorDecommissionInfo )], adjustTargetNumExecutors : Boolean , triggeredByExecutor : Boolean ): Seq [ String ] decommissionExecutors kills the given executors. decommissionExecutors is used when: ExecutorAllocationClient is requested to decommission a single executor ExecutorAllocationManager is requested to remove executors StandaloneSchedulerBackend is requested to executorDecommissioned Decommissioning Single Executor \u00b6 decommissionExecutor ( executorId : String , decommissionInfo : ExecutorDecommissionInfo , adjustTargetNumExecutors : Boolean , triggeredByExecutor : Boolean = false ): Boolean decommissionExecutor ...FIXME decommissionExecutor is used when: DriverEndpoint is requested to handle a ExecutorDecommissioning message","title":"ExecutorAllocationClient"},{"location":"dynamic-allocation/ExecutorAllocationClient/#executorallocationclient","text":"ExecutorAllocationClient is an abstraction of schedulers that can communicate with a cluster manager to request or kill executors.","title":"ExecutorAllocationClient"},{"location":"dynamic-allocation/ExecutorAllocationClient/#contract","text":"","title":"Contract"},{"location":"dynamic-allocation/ExecutorAllocationClient/#active-executor-ids","text":"getExecutorIds (): Seq [ String ] Used when: SparkContext is requested for active executors","title":" Active Executor IDs"},{"location":"dynamic-allocation/ExecutorAllocationClient/#isexecutoractive","text":"isExecutorActive ( id : String ): Boolean Whether a given executor (by ID) is active (and can be used to execute tasks) Used when: FIXME","title":" isExecutorActive"},{"location":"dynamic-allocation/ExecutorAllocationClient/#killing-executors","text":"killExecutors ( executorIds : Seq [ String ], adjustTargetNumExecutors : Boolean , countFailures : Boolean , force : Boolean = false ): Seq [ String ] Requests a cluster manager to kill given executors and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when: ExecutorAllocationClient is requested to kill an executor ExecutorAllocationManager is requested to removeExecutors SparkContext is requested to kill executors and killAndReplaceExecutor BlacklistTracker is requested to kill an executor DriverEndpoint is requested to handle a KillExecutorsOnHost message","title":" Killing Executors"},{"location":"dynamic-allocation/ExecutorAllocationClient/#killing-executors-on-host","text":"killExecutorsOnHost ( host : String ): Boolean Used when: BlacklistTracker is requested to kill executors on a blacklisted node","title":" Killing Executors on Host"},{"location":"dynamic-allocation/ExecutorAllocationClient/#requesting-additional-executors","text":"requestExecutors ( numAdditionalExecutors : Int ): Boolean Requests additional executors from a cluster manager and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when: SparkContext is requested for additional executors","title":" Requesting Additional Executors"},{"location":"dynamic-allocation/ExecutorAllocationClient/#updating-total-executors","text":"requestTotalExecutors ( resourceProfileIdToNumExecutors : Map [ Int , Int ], numLocalityAwareTasksPerResourceProfileId : Map [ Int , Int ], hostToLocalTaskCount : Map [ Int , Map [ String , Int ]]): Boolean Updates a cluster manager with the exact number of executors desired. Returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when: SparkContext is requested to update the number of total executors ExecutorAllocationManager is requested to start , updateAndSyncNumExecutorsTarget , addExecutors , removeExecutors","title":" Updating Total Executors"},{"location":"dynamic-allocation/ExecutorAllocationClient/#implementations","text":"CoarseGrainedSchedulerBackend KubernetesClusterSchedulerBackend ( Spark on Kubernetes ) MesosCoarseGrainedSchedulerBackend StandaloneSchedulerBackend YarnSchedulerBackend","title":"Implementations"},{"location":"dynamic-allocation/ExecutorAllocationClient/#killing-single-executor","text":"killExecutor ( executorId : String ): Boolean killExecutor kill the given executor . killExecutor is used when: ExecutorAllocationManager removes an executor . SparkContext is requested to kill executors .","title":" Killing Single Executor"},{"location":"dynamic-allocation/ExecutorAllocationClient/#decommissioning-executors","text":"decommissionExecutors ( executorsAndDecomInfo : Array [( String , ExecutorDecommissionInfo )], adjustTargetNumExecutors : Boolean , triggeredByExecutor : Boolean ): Seq [ String ] decommissionExecutors kills the given executors. decommissionExecutors is used when: ExecutorAllocationClient is requested to decommission a single executor ExecutorAllocationManager is requested to remove executors StandaloneSchedulerBackend is requested to executorDecommissioned","title":" Decommissioning Executors"},{"location":"dynamic-allocation/ExecutorAllocationClient/#decommissioning-single-executor","text":"decommissionExecutor ( executorId : String , decommissionInfo : ExecutorDecommissionInfo , adjustTargetNumExecutors : Boolean , triggeredByExecutor : Boolean = false ): Boolean decommissionExecutor ...FIXME decommissionExecutor is used when: DriverEndpoint is requested to handle a ExecutorDecommissioning message","title":" Decommissioning Single Executor"},{"location":"dynamic-allocation/ExecutorAllocationListener/","text":"ExecutorAllocationListener \u00b6 ExecutorAllocationListener is a SparkListener.md[] that intercepts events about stages, tasks, and executors, i.e. onStageSubmitted, onStageCompleted, onTaskStart, onTaskEnd, onExecutorAdded, and onExecutorRemoved. Using the events ExecutorAllocationManager can manage the pool of dynamically managed executors. Internal Class ExecutorAllocationListener is an internal class of ExecutorAllocationManager with full access to internal registries.","title":"ExecutorAllocationListener"},{"location":"dynamic-allocation/ExecutorAllocationListener/#executorallocationlistener","text":"ExecutorAllocationListener is a SparkListener.md[] that intercepts events about stages, tasks, and executors, i.e. onStageSubmitted, onStageCompleted, onTaskStart, onTaskEnd, onExecutorAdded, and onExecutorRemoved. Using the events ExecutorAllocationManager can manage the pool of dynamically managed executors. Internal Class ExecutorAllocationListener is an internal class of ExecutorAllocationManager with full access to internal registries.","title":"ExecutorAllocationListener"},{"location":"dynamic-allocation/ExecutorAllocationManager/","text":"ExecutorAllocationManager \u00b6 ExecutorAllocationManager can be used to dynamically allocate executors based on processing workload. ExecutorAllocationManager intercepts Spark events using the internal ExecutorAllocationListener that keeps track of the workload. Creating Instance \u00b6 ExecutorAllocationManager takes the following to be created: ExecutorAllocationClient LiveListenerBus SparkConf ContextCleaner (default: None ) Clock (default: SystemClock ) ExecutorAllocationManager is created (and started) when SparkContext is created (with Dynamic Allocation of Executors enabled) Validating Configuration \u00b6 validateSettings (): Unit validateSettings makes sure that the settings for dynamic allocation are correct. validateSettings throws a SparkException when the following are not met: spark.dynamicAllocation.minExecutors must be positive spark.dynamicAllocation.maxExecutors must be 0 or greater spark.dynamicAllocation.minExecutors must be less than or equal to spark.dynamicAllocation.maxExecutors spark.dynamicAllocation.executorIdleTimeout must be greater than 0 spark.shuffle.service.enabled must be enabled. The number of tasks per core, i.e. spark.executor.cores divided by spark.task.cpus , is not zero. Performance Metrics \u00b6 ExecutorAllocationManager uses ExecutorAllocationManagerSource for performance metrics. ExecutorMonitor \u00b6 ExecutorAllocationManager creates an ExecutorMonitor when created . ExecutorMonitor is added to the management queue (of LiveListenerBus ) when ExecutorAllocationManager is started . ExecutorMonitor is attached (to the ContextCleaner ) when ExecutorAllocationManager is started . ExecutorMonitor is requested to reset when ExecutorAllocationManager is requested to reset . ExecutorMonitor is used for the performance metrics: numberExecutorsPendingToRemove (based on pendingRemovalCount ) numberAllExecutors (based on executorCount ) ExecutorMonitor is used for the following: timedOutExecutors when ExecutorAllocationManager is requested to schedule executorCount when ExecutorAllocationManager is requested to addExecutors executorCount , pendingRemovalCount and executorsKilled when ExecutorAllocationManager is requested to removeExecutors ExecutorAllocationListener \u00b6 ExecutorAllocationManager creates an ExecutorAllocationListener when created to intercept Spark events that impact the allocation policy. ExecutorAllocationListener is added to the management queue (of LiveListenerBus ) when ExecutorAllocationManager is started . ExecutorAllocationListener is used to calculate the maximum number of executors needed . spark.dynamicAllocation.executorAllocationRatio \u00b6 ExecutorAllocationManager uses spark.dynamicAllocation.executorAllocationRatio configuration property for maxNumExecutorsNeeded . tasksPerExecutorForFullParallelism \u00b6 ExecutorAllocationManager uses spark.executor.cores and spark.task.cpus configuration properties for the number of tasks that can be submitted to an executor for full parallelism. Used when: maxNumExecutorsNeeded Maximum Number of Executors Needed \u00b6 maxNumExecutorsNeeded (): Int maxNumExecutorsNeeded requests the ExecutorAllocationListener for the number of pending and running tasks. maxNumExecutorsNeeded is the smallest integer value that is greater than or equal to the multiplication of the total number of pending and running tasks by executorAllocationRatio divided by tasksPerExecutorForFullParallelism . maxNumExecutorsNeeded is used for: updateAndSyncNumExecutorsTarget numberMaxNeededExecutors performance metric ExecutorAllocationClient \u00b6 ExecutorAllocationManager is given an ExecutorAllocationClient when created . Starting ExecutorAllocationManager \u00b6 start (): Unit start requests the LiveListenerBus to add to the management queue : ExecutorAllocationListener ExecutorMonitor start requests the ContextCleaner (if defined) to attach the ExecutorMonitor . creates a scheduleTask (a Java Runnable ) for schedule when started. start requests the ScheduledExecutorService to schedule the scheduleTask every 100 ms. Note The schedule delay of 100 is not configurable. start requests the ExecutorAllocationClient to request the total executors with the following: numExecutorsTarget localityAwareTasks hostToLocalTaskCount start is used when SparkContext is created . Scheduling Executors \u00b6 schedule (): Unit schedule requests the ExecutorMonitor for timedOutExecutors . If there are executors to be removed, schedule turns the initializing internal flag off. schedule updateAndSyncNumExecutorsTarget with the current time. In the end, schedule removes the executors to be removed if there are any. updateAndSyncNumExecutorsTarget \u00b6 updateAndSyncNumExecutorsTarget ( now : Long ): Int updateAndSyncNumExecutorsTarget maxNumExecutorsNeeded . updateAndSyncNumExecutorsTarget ...FIXME Stopping ExecutorAllocationManager \u00b6 stop (): Unit stop shuts down < >. Note stop waits 10 seconds for the termination to be complete. stop is used when SparkContext is requested to stop spark-dynamic-executor-allocation Allocation Executor \u00b6 spark-dynamic-executor-allocation allocation executor is a...FIXME ExecutorAllocationManagerSource \u00b6 ExecutorAllocationManagerSource Removing Executors \u00b6 removeExecutors ( executors : Seq [( String , Int )]): Seq [ String ] removeExecutors ...FIXME removeExecutors is used when: ExecutorAllocationManager is requested to schedule executors Logging \u00b6 Enable ALL logging level for org.apache.spark.ExecutorAllocationManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ExecutorAllocationManager=ALL Refer to Logging .","title":"ExecutorAllocationManager"},{"location":"dynamic-allocation/ExecutorAllocationManager/#executorallocationmanager","text":"ExecutorAllocationManager can be used to dynamically allocate executors based on processing workload. ExecutorAllocationManager intercepts Spark events using the internal ExecutorAllocationListener that keeps track of the workload.","title":"ExecutorAllocationManager"},{"location":"dynamic-allocation/ExecutorAllocationManager/#creating-instance","text":"ExecutorAllocationManager takes the following to be created: ExecutorAllocationClient LiveListenerBus SparkConf ContextCleaner (default: None ) Clock (default: SystemClock ) ExecutorAllocationManager is created (and started) when SparkContext is created (with Dynamic Allocation of Executors enabled)","title":"Creating Instance"},{"location":"dynamic-allocation/ExecutorAllocationManager/#validating-configuration","text":"validateSettings (): Unit validateSettings makes sure that the settings for dynamic allocation are correct. validateSettings throws a SparkException when the following are not met: spark.dynamicAllocation.minExecutors must be positive spark.dynamicAllocation.maxExecutors must be 0 or greater spark.dynamicAllocation.minExecutors must be less than or equal to spark.dynamicAllocation.maxExecutors spark.dynamicAllocation.executorIdleTimeout must be greater than 0 spark.shuffle.service.enabled must be enabled. The number of tasks per core, i.e. spark.executor.cores divided by spark.task.cpus , is not zero.","title":" Validating Configuration"},{"location":"dynamic-allocation/ExecutorAllocationManager/#performance-metrics","text":"ExecutorAllocationManager uses ExecutorAllocationManagerSource for performance metrics.","title":"Performance Metrics"},{"location":"dynamic-allocation/ExecutorAllocationManager/#executormonitor","text":"ExecutorAllocationManager creates an ExecutorMonitor when created . ExecutorMonitor is added to the management queue (of LiveListenerBus ) when ExecutorAllocationManager is started . ExecutorMonitor is attached (to the ContextCleaner ) when ExecutorAllocationManager is started . ExecutorMonitor is requested to reset when ExecutorAllocationManager is requested to reset . ExecutorMonitor is used for the performance metrics: numberExecutorsPendingToRemove (based on pendingRemovalCount ) numberAllExecutors (based on executorCount ) ExecutorMonitor is used for the following: timedOutExecutors when ExecutorAllocationManager is requested to schedule executorCount when ExecutorAllocationManager is requested to addExecutors executorCount , pendingRemovalCount and executorsKilled when ExecutorAllocationManager is requested to removeExecutors","title":" ExecutorMonitor"},{"location":"dynamic-allocation/ExecutorAllocationManager/#executorallocationlistener","text":"ExecutorAllocationManager creates an ExecutorAllocationListener when created to intercept Spark events that impact the allocation policy. ExecutorAllocationListener is added to the management queue (of LiveListenerBus ) when ExecutorAllocationManager is started . ExecutorAllocationListener is used to calculate the maximum number of executors needed .","title":" ExecutorAllocationListener"},{"location":"dynamic-allocation/ExecutorAllocationManager/#sparkdynamicallocationexecutorallocationratio","text":"ExecutorAllocationManager uses spark.dynamicAllocation.executorAllocationRatio configuration property for maxNumExecutorsNeeded .","title":" spark.dynamicAllocation.executorAllocationRatio"},{"location":"dynamic-allocation/ExecutorAllocationManager/#tasksperexecutorforfullparallelism","text":"ExecutorAllocationManager uses spark.executor.cores and spark.task.cpus configuration properties for the number of tasks that can be submitted to an executor for full parallelism. Used when: maxNumExecutorsNeeded","title":" tasksPerExecutorForFullParallelism"},{"location":"dynamic-allocation/ExecutorAllocationManager/#maximum-number-of-executors-needed","text":"maxNumExecutorsNeeded (): Int maxNumExecutorsNeeded requests the ExecutorAllocationListener for the number of pending and running tasks. maxNumExecutorsNeeded is the smallest integer value that is greater than or equal to the multiplication of the total number of pending and running tasks by executorAllocationRatio divided by tasksPerExecutorForFullParallelism . maxNumExecutorsNeeded is used for: updateAndSyncNumExecutorsTarget numberMaxNeededExecutors performance metric","title":" Maximum Number of Executors Needed"},{"location":"dynamic-allocation/ExecutorAllocationManager/#executorallocationclient","text":"ExecutorAllocationManager is given an ExecutorAllocationClient when created .","title":" ExecutorAllocationClient"},{"location":"dynamic-allocation/ExecutorAllocationManager/#starting-executorallocationmanager","text":"start (): Unit start requests the LiveListenerBus to add to the management queue : ExecutorAllocationListener ExecutorMonitor start requests the ContextCleaner (if defined) to attach the ExecutorMonitor . creates a scheduleTask (a Java Runnable ) for schedule when started. start requests the ScheduledExecutorService to schedule the scheduleTask every 100 ms. Note The schedule delay of 100 is not configurable. start requests the ExecutorAllocationClient to request the total executors with the following: numExecutorsTarget localityAwareTasks hostToLocalTaskCount start is used when SparkContext is created .","title":" Starting ExecutorAllocationManager"},{"location":"dynamic-allocation/ExecutorAllocationManager/#scheduling-executors","text":"schedule (): Unit schedule requests the ExecutorMonitor for timedOutExecutors . If there are executors to be removed, schedule turns the initializing internal flag off. schedule updateAndSyncNumExecutorsTarget with the current time. In the end, schedule removes the executors to be removed if there are any.","title":" Scheduling Executors"},{"location":"dynamic-allocation/ExecutorAllocationManager/#updateandsyncnumexecutorstarget","text":"updateAndSyncNumExecutorsTarget ( now : Long ): Int updateAndSyncNumExecutorsTarget maxNumExecutorsNeeded . updateAndSyncNumExecutorsTarget ...FIXME","title":" updateAndSyncNumExecutorsTarget"},{"location":"dynamic-allocation/ExecutorAllocationManager/#stopping-executorallocationmanager","text":"stop (): Unit stop shuts down < >. Note stop waits 10 seconds for the termination to be complete. stop is used when SparkContext is requested to stop","title":" Stopping ExecutorAllocationManager"},{"location":"dynamic-allocation/ExecutorAllocationManager/#spark-dynamic-executor-allocation-allocation-executor","text":"spark-dynamic-executor-allocation allocation executor is a...FIXME","title":" spark-dynamic-executor-allocation Allocation Executor"},{"location":"dynamic-allocation/ExecutorAllocationManager/#executorallocationmanagersource","text":"ExecutorAllocationManagerSource","title":" ExecutorAllocationManagerSource"},{"location":"dynamic-allocation/ExecutorAllocationManager/#removing-executors","text":"removeExecutors ( executors : Seq [( String , Int )]): Seq [ String ] removeExecutors ...FIXME removeExecutors is used when: ExecutorAllocationManager is requested to schedule executors","title":" Removing Executors"},{"location":"dynamic-allocation/ExecutorAllocationManager/#logging","text":"Enable ALL logging level for org.apache.spark.ExecutorAllocationManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ExecutorAllocationManager=ALL Refer to Logging .","title":"Logging"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/","text":"ExecutorAllocationManagerSource \u00b6 ExecutorAllocationManagerSource is a metric source for Dynamic Allocation of Executors . Source Name \u00b6 ExecutorAllocationManagerSource is registered under the name ExecutorAllocationManager . Gauges \u00b6 numberExecutorsToAdd \u00b6 executors/numberExecutorsToAdd for numExecutorsToAdd numberExecutorsPendingToRemove \u00b6 executors/numberExecutorsPendingToRemove for pendingRemovalCount numberAllExecutors \u00b6 executors/numberAllExecutors for executorCount numberTargetExecutors \u00b6 executors/numberTargetExecutors for numExecutorsTarget numberMaxNeededExecutors \u00b6 executors/numberMaxNeededExecutors for maxNumExecutorsNeeded","title":"ExecutorAllocationManagerSource"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/#executorallocationmanagersource","text":"ExecutorAllocationManagerSource is a metric source for Dynamic Allocation of Executors .","title":"ExecutorAllocationManagerSource"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/#source-name","text":"ExecutorAllocationManagerSource is registered under the name ExecutorAllocationManager .","title":" Source Name"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/#gauges","text":"","title":"Gauges"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/#numberexecutorstoadd","text":"executors/numberExecutorsToAdd for numExecutorsToAdd","title":" numberExecutorsToAdd"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/#numberexecutorspendingtoremove","text":"executors/numberExecutorsPendingToRemove for pendingRemovalCount","title":" numberExecutorsPendingToRemove"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/#numberallexecutors","text":"executors/numberAllExecutors for executorCount","title":" numberAllExecutors"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/#numbertargetexecutors","text":"executors/numberTargetExecutors for numExecutorsTarget","title":" numberTargetExecutors"},{"location":"dynamic-allocation/ExecutorAllocationManagerSource/#numbermaxneededexecutors","text":"executors/numberMaxNeededExecutors for maxNumExecutorsNeeded","title":" numberMaxNeededExecutors"},{"location":"dynamic-allocation/ExecutorMonitor/","text":"ExecutorMonitor \u00b6 ExecutorMonitor is a SparkListener and a CleanerListener . Creating Instance \u00b6 ExecutorMonitor takes the following to be created: SparkConf ExecutorAllocationClient LiveListenerBus Clock ExecutorMonitor is created when: ExecutorAllocationManager is created shuffleIds Registry \u00b6 shuffleIds : Set [ Int ] ExecutorMonitor uses a mutable HashSet to track shuffle IDs...FIXME shuffleIds is initialized only when shuffleTrackingEnabled is enabled. shuffleIds is used by Tracker internal class for the following: updateTimeout , addShuffle , removeShuffle and updateActiveShuffles Executors Registry \u00b6 executors : ConcurrentHashMap [ String , Tracker ] ExecutorMonitor uses a Java ConcurrentHashMap to track available executors. An executor is added when (via ensureExecutorIsTracked ): onBlockUpdated onExecutorAdded onTaskStart An executor is removed when onExecutorRemoved . All executors are removed when reset . executors is used when: onOtherEvent ( cleanupShuffle ) executorCount executorsKilled onUnpersistRDD onTaskEnd onJobStart onJobEnd pendingRemovalCount timedOutExecutors fetchFromShuffleSvcEnabled Flag \u00b6 fetchFromShuffleSvcEnabled : Boolean ExecutorMonitor initializes fetchFromShuffleSvcEnabled internal flag based on the values of spark.shuffle.service.enabled and spark.shuffle.service.fetch.rdd.enabled configuration properties. fetchFromShuffleSvcEnabled is enabled ( true ) when the aforementioned configuration properties are. fetchFromShuffleSvcEnabled is used when: onBlockUpdated shuffleTrackingEnabled Flag \u00b6 shuffleTrackingEnabled : Boolean ExecutorMonitor initializes shuffleTrackingEnabled internal flag based on the values of spark.shuffle.service.enabled and spark.dynamicAllocation.shuffleTracking.enabled configuration properties. shuffleTrackingEnabled is enabled ( true ) when the following holds: spark.shuffle.service.enabled is disabled spark.dynamicAllocation.shuffleTracking.enabled is enabled When enabled, shuffleTrackingEnabled is used to skip execution of the following (making them noops): onJobStart onJobEnd When disabled, shuffleTrackingEnabled is used for the following: onTaskEnd shuffleCleaned shuffleIds spark.dynamicAllocation.cachedExecutorIdleTimeout \u00b6 ExecutorMonitor reads spark.dynamicAllocation.cachedExecutorIdleTimeout configuration property for Tracker to updateTimeout . onBlockUpdated \u00b6 onBlockUpdated ( event : SparkListenerBlockUpdated ): Unit onBlockUpdated is part of the SparkListenerInterface abstraction. onBlockUpdated ...FIXME onExecutorAdded \u00b6 onExecutorAdded ( event : SparkListenerExecutorAdded ): Unit onExecutorAdded is part of the SparkListenerInterface abstraction. onExecutorAdded ...FIXME onExecutorRemoved \u00b6 onExecutorRemoved ( event : SparkListenerExecutorRemoved ): Unit onExecutorRemoved is part of the SparkListenerInterface abstraction. onExecutorRemoved ...FIXME onJobEnd \u00b6 onJobEnd ( event : SparkListenerJobEnd ): Unit onJobEnd is part of the SparkListenerInterface abstraction. onJobEnd ...FIXME onJobStart \u00b6 onJobStart ( event : SparkListenerJobStart ): Unit onJobStart is part of the SparkListenerInterface abstraction. Note onJobStart does nothing and simply returns when the shuffleTrackingEnabled flag is turned off ( false ). onJobStart requests the input SparkListenerJobStart for the StageInfo s and converts...FIXME onOtherEvent \u00b6 onOtherEvent ( event : SparkListenerEvent ): Unit onOtherEvent is part of the SparkListenerInterface abstraction. onOtherEvent ...FIXME cleanupShuffle \u00b6 cleanupShuffle ( id : Int ): Unit cleanupShuffle ...FIXME cleanupShuffle is used when onOtherEvent onTaskEnd \u00b6 onTaskEnd ( event : SparkListenerTaskEnd ): Unit onTaskEnd is part of the SparkListenerInterface abstraction. onTaskEnd ...FIXME onTaskStart \u00b6 onTaskStart ( event : SparkListenerTaskStart ): Unit onTaskStart is part of the SparkListenerInterface abstraction. onTaskStart ...FIXME onUnpersistRDD \u00b6 onUnpersistRDD ( event : SparkListenerUnpersistRDD ): Unit onUnpersistRDD is part of the SparkListenerInterface abstraction. onUnpersistRDD ...FIXME reset \u00b6 reset (): Unit reset ...FIXME reset is used when: FIXME shuffleCleaned \u00b6 shuffleCleaned ( shuffleId : Int ): Unit shuffleCleaned is part of the CleanerListener abstraction. shuffleCleaned ...FIXME timedOutExecutors \u00b6 timedOutExecutors (): Seq [ String ] timedOutExecutors ( when : Long ): Seq [ String ] timedOutExecutors ...FIXME timedOutExecutors is used when: ExecutorAllocationManager is requested to schedule executorCount \u00b6 executorCount : Int executorCount ...FIXME executorCount is used when: ExecutorAllocationManager is requested to addExecutors and removeExecutors ExecutorAllocationManagerSource is requested for numberAllExecutors performance metric pendingRemovalCount \u00b6 pendingRemovalCount : Int pendingRemovalCount ...FIXME pendingRemovalCount is used when: ExecutorAllocationManager is requested to removeExecutors ExecutorAllocationManagerSource is requested for numberExecutorsPendingToRemove performance metric executorsKilled \u00b6 executorsKilled ( ids : Seq [ String ]): Unit executorsKilled ...FIXME executorsKilled is used when: ExecutorAllocationManager is requested to removeExecutors ensureExecutorIsTracked \u00b6 ensureExecutorIsTracked ( id : String , resourceProfileId : Int ): Tracker ensureExecutorIsTracked ...FIXME ensureExecutorIsTracked is used when: onBlockUpdated onExecutorAdded onTaskStart getResourceProfileId \u00b6 getResourceProfileId ( executorId : String ): Int getResourceProfileId ...FIXME getResourceProfileId is used for testing only.","title":"ExecutorMonitor"},{"location":"dynamic-allocation/ExecutorMonitor/#executormonitor","text":"ExecutorMonitor is a SparkListener and a CleanerListener .","title":"ExecutorMonitor"},{"location":"dynamic-allocation/ExecutorMonitor/#creating-instance","text":"ExecutorMonitor takes the following to be created: SparkConf ExecutorAllocationClient LiveListenerBus Clock ExecutorMonitor is created when: ExecutorAllocationManager is created","title":"Creating Instance"},{"location":"dynamic-allocation/ExecutorMonitor/#shuffleids-registry","text":"shuffleIds : Set [ Int ] ExecutorMonitor uses a mutable HashSet to track shuffle IDs...FIXME shuffleIds is initialized only when shuffleTrackingEnabled is enabled. shuffleIds is used by Tracker internal class for the following: updateTimeout , addShuffle , removeShuffle and updateActiveShuffles","title":" shuffleIds Registry"},{"location":"dynamic-allocation/ExecutorMonitor/#executors-registry","text":"executors : ConcurrentHashMap [ String , Tracker ] ExecutorMonitor uses a Java ConcurrentHashMap to track available executors. An executor is added when (via ensureExecutorIsTracked ): onBlockUpdated onExecutorAdded onTaskStart An executor is removed when onExecutorRemoved . All executors are removed when reset . executors is used when: onOtherEvent ( cleanupShuffle ) executorCount executorsKilled onUnpersistRDD onTaskEnd onJobStart onJobEnd pendingRemovalCount timedOutExecutors","title":" Executors Registry"},{"location":"dynamic-allocation/ExecutorMonitor/#fetchfromshufflesvcenabled-flag","text":"fetchFromShuffleSvcEnabled : Boolean ExecutorMonitor initializes fetchFromShuffleSvcEnabled internal flag based on the values of spark.shuffle.service.enabled and spark.shuffle.service.fetch.rdd.enabled configuration properties. fetchFromShuffleSvcEnabled is enabled ( true ) when the aforementioned configuration properties are. fetchFromShuffleSvcEnabled is used when: onBlockUpdated","title":" fetchFromShuffleSvcEnabled Flag"},{"location":"dynamic-allocation/ExecutorMonitor/#shuffletrackingenabled-flag","text":"shuffleTrackingEnabled : Boolean ExecutorMonitor initializes shuffleTrackingEnabled internal flag based on the values of spark.shuffle.service.enabled and spark.dynamicAllocation.shuffleTracking.enabled configuration properties. shuffleTrackingEnabled is enabled ( true ) when the following holds: spark.shuffle.service.enabled is disabled spark.dynamicAllocation.shuffleTracking.enabled is enabled When enabled, shuffleTrackingEnabled is used to skip execution of the following (making them noops): onJobStart onJobEnd When disabled, shuffleTrackingEnabled is used for the following: onTaskEnd shuffleCleaned shuffleIds","title":" shuffleTrackingEnabled Flag"},{"location":"dynamic-allocation/ExecutorMonitor/#sparkdynamicallocationcachedexecutoridletimeout","text":"ExecutorMonitor reads spark.dynamicAllocation.cachedExecutorIdleTimeout configuration property for Tracker to updateTimeout .","title":" spark.dynamicAllocation.cachedExecutorIdleTimeout"},{"location":"dynamic-allocation/ExecutorMonitor/#onblockupdated","text":"onBlockUpdated ( event : SparkListenerBlockUpdated ): Unit onBlockUpdated is part of the SparkListenerInterface abstraction. onBlockUpdated ...FIXME","title":" onBlockUpdated"},{"location":"dynamic-allocation/ExecutorMonitor/#onexecutoradded","text":"onExecutorAdded ( event : SparkListenerExecutorAdded ): Unit onExecutorAdded is part of the SparkListenerInterface abstraction. onExecutorAdded ...FIXME","title":" onExecutorAdded"},{"location":"dynamic-allocation/ExecutorMonitor/#onexecutorremoved","text":"onExecutorRemoved ( event : SparkListenerExecutorRemoved ): Unit onExecutorRemoved is part of the SparkListenerInterface abstraction. onExecutorRemoved ...FIXME","title":" onExecutorRemoved"},{"location":"dynamic-allocation/ExecutorMonitor/#onjobend","text":"onJobEnd ( event : SparkListenerJobEnd ): Unit onJobEnd is part of the SparkListenerInterface abstraction. onJobEnd ...FIXME","title":" onJobEnd"},{"location":"dynamic-allocation/ExecutorMonitor/#onjobstart","text":"onJobStart ( event : SparkListenerJobStart ): Unit onJobStart is part of the SparkListenerInterface abstraction. Note onJobStart does nothing and simply returns when the shuffleTrackingEnabled flag is turned off ( false ). onJobStart requests the input SparkListenerJobStart for the StageInfo s and converts...FIXME","title":" onJobStart"},{"location":"dynamic-allocation/ExecutorMonitor/#onotherevent","text":"onOtherEvent ( event : SparkListenerEvent ): Unit onOtherEvent is part of the SparkListenerInterface abstraction. onOtherEvent ...FIXME","title":" onOtherEvent"},{"location":"dynamic-allocation/ExecutorMonitor/#cleanupshuffle","text":"cleanupShuffle ( id : Int ): Unit cleanupShuffle ...FIXME cleanupShuffle is used when onOtherEvent","title":" cleanupShuffle"},{"location":"dynamic-allocation/ExecutorMonitor/#ontaskend","text":"onTaskEnd ( event : SparkListenerTaskEnd ): Unit onTaskEnd is part of the SparkListenerInterface abstraction. onTaskEnd ...FIXME","title":" onTaskEnd"},{"location":"dynamic-allocation/ExecutorMonitor/#ontaskstart","text":"onTaskStart ( event : SparkListenerTaskStart ): Unit onTaskStart is part of the SparkListenerInterface abstraction. onTaskStart ...FIXME","title":" onTaskStart"},{"location":"dynamic-allocation/ExecutorMonitor/#onunpersistrdd","text":"onUnpersistRDD ( event : SparkListenerUnpersistRDD ): Unit onUnpersistRDD is part of the SparkListenerInterface abstraction. onUnpersistRDD ...FIXME","title":" onUnpersistRDD"},{"location":"dynamic-allocation/ExecutorMonitor/#reset","text":"reset (): Unit reset ...FIXME reset is used when: FIXME","title":" reset"},{"location":"dynamic-allocation/ExecutorMonitor/#shufflecleaned","text":"shuffleCleaned ( shuffleId : Int ): Unit shuffleCleaned is part of the CleanerListener abstraction. shuffleCleaned ...FIXME","title":" shuffleCleaned"},{"location":"dynamic-allocation/ExecutorMonitor/#timedoutexecutors","text":"timedOutExecutors (): Seq [ String ] timedOutExecutors ( when : Long ): Seq [ String ] timedOutExecutors ...FIXME timedOutExecutors is used when: ExecutorAllocationManager is requested to schedule","title":" timedOutExecutors"},{"location":"dynamic-allocation/ExecutorMonitor/#executorcount","text":"executorCount : Int executorCount ...FIXME executorCount is used when: ExecutorAllocationManager is requested to addExecutors and removeExecutors ExecutorAllocationManagerSource is requested for numberAllExecutors performance metric","title":" executorCount"},{"location":"dynamic-allocation/ExecutorMonitor/#pendingremovalcount","text":"pendingRemovalCount : Int pendingRemovalCount ...FIXME pendingRemovalCount is used when: ExecutorAllocationManager is requested to removeExecutors ExecutorAllocationManagerSource is requested for numberExecutorsPendingToRemove performance metric","title":" pendingRemovalCount"},{"location":"dynamic-allocation/ExecutorMonitor/#executorskilled","text":"executorsKilled ( ids : Seq [ String ]): Unit executorsKilled ...FIXME executorsKilled is used when: ExecutorAllocationManager is requested to removeExecutors","title":" executorsKilled"},{"location":"dynamic-allocation/ExecutorMonitor/#ensureexecutoristracked","text":"ensureExecutorIsTracked ( id : String , resourceProfileId : Int ): Tracker ensureExecutorIsTracked ...FIXME ensureExecutorIsTracked is used when: onBlockUpdated onExecutorAdded onTaskStart","title":" ensureExecutorIsTracked"},{"location":"dynamic-allocation/ExecutorMonitor/#getresourceprofileid","text":"getResourceProfileId ( executorId : String ): Int getResourceProfileId ...FIXME getResourceProfileId is used for testing only.","title":" getResourceProfileId"},{"location":"dynamic-allocation/Tracker/","text":"Tracker \u00b6 Tracker is a private internal class of ExecutorMonitor . Creating Instance \u00b6 Tracker takes the following to be created: resourceProfileId Tracker is created when: ExecutorMonitor is requested to ensureExecutorIsTracked cachedBlocks Internal Registry \u00b6 cachedBlocks : Map [ Int , BitSet ] Tracker uses cachedBlocks internal registry for cached blocks (RDD IDs and partition IDs stored in an executor). cachedBlocks is used when: ExecutorMonitor is requested to onBlockUpdated , onUnpersistRDD Tracker is requested to updateTimeout removeShuffle \u00b6 removeShuffle ( id : Int ): Unit removeShuffle ...FIXME removeShuffle is used when: ExecutorMonitor is requested to cleanupShuffle updateActiveShuffles \u00b6 updateActiveShuffles ( ids : Iterable [ Int ]): Unit updateActiveShuffles ...FIXME updateActiveShuffles is used when: ExecutorMonitor is requested to onJobStart and onJobEnd updateRunningTasks \u00b6 updateRunningTasks ( delta : Int ): Unit updateRunningTasks ...FIXME updateRunningTasks is used when: ExecutorMonitor is requested to onTaskStart , onTaskEnd and onExecutorAdded updateTimeout \u00b6 updateTimeout (): Unit updateTimeout ...FIXME updateTimeout is used when: ExecutorMonitor is requested to onBlockUpdated and onUnpersistRDD Tracker is requested to updateRunningTasks , removeShuffle , updateActiveShuffles","title":"Tracker"},{"location":"dynamic-allocation/Tracker/#tracker","text":"Tracker is a private internal class of ExecutorMonitor .","title":"Tracker"},{"location":"dynamic-allocation/Tracker/#creating-instance","text":"Tracker takes the following to be created: resourceProfileId Tracker is created when: ExecutorMonitor is requested to ensureExecutorIsTracked","title":"Creating Instance"},{"location":"dynamic-allocation/Tracker/#cachedblocks-internal-registry","text":"cachedBlocks : Map [ Int , BitSet ] Tracker uses cachedBlocks internal registry for cached blocks (RDD IDs and partition IDs stored in an executor). cachedBlocks is used when: ExecutorMonitor is requested to onBlockUpdated , onUnpersistRDD Tracker is requested to updateTimeout","title":" cachedBlocks Internal Registry"},{"location":"dynamic-allocation/Tracker/#removeshuffle","text":"removeShuffle ( id : Int ): Unit removeShuffle ...FIXME removeShuffle is used when: ExecutorMonitor is requested to cleanupShuffle","title":" removeShuffle"},{"location":"dynamic-allocation/Tracker/#updateactiveshuffles","text":"updateActiveShuffles ( ids : Iterable [ Int ]): Unit updateActiveShuffles ...FIXME updateActiveShuffles is used when: ExecutorMonitor is requested to onJobStart and onJobEnd","title":" updateActiveShuffles"},{"location":"dynamic-allocation/Tracker/#updaterunningtasks","text":"updateRunningTasks ( delta : Int ): Unit updateRunningTasks ...FIXME updateRunningTasks is used when: ExecutorMonitor is requested to onTaskStart , onTaskEnd and onExecutorAdded","title":" updateRunningTasks"},{"location":"dynamic-allocation/Tracker/#updatetimeout","text":"updateTimeout (): Unit updateTimeout ...FIXME updateTimeout is used when: ExecutorMonitor is requested to onBlockUpdated and onUnpersistRDD Tracker is requested to updateRunningTasks , removeShuffle , updateActiveShuffles","title":" updateTimeout"},{"location":"dynamic-allocation/configuration-properties/","text":"Spark Configuration Properties \u00b6 spark.dynamicAllocation.cachedExecutorIdleTimeout \u00b6 How long (in seconds) to keep blocks cached Default: The largest value representable as an Int Must be >= 0 Used when: ExecutorMonitor is created RDD is requested to localCheckpoint (simply to print out a WARN message) spark.dynamicAllocation.enabled \u00b6 Default: false Used when: Utils utility is requested to isDynamicAllocationEnabled SparkSubmitArguments is requested to loadEnvironmentArguments (and validates numExecutors argument ) RDD is requested to localCheckpoint DAGScheduler is requested to checkBarrierStageWithDynamicAllocation spark.dynamicAllocation.executorAllocationRatio \u00b6 Default: 1.0 Must be between 0 (exclusive) and 1.0 (inclusive) Used when: ExecutorAllocationManager is created spark.dynamicAllocation.executorIdleTimeout \u00b6 Default: 60 spark.dynamicAllocation.initialExecutors \u00b6 Default: spark.dynamicAllocation.minExecutors spark.dynamicAllocation.maxExecutors \u00b6 Default: The largest value representable as an Int spark.dynamicAllocation.minExecutors \u00b6 Default: 0 spark.dynamicAllocation.schedulerBacklogTimeout \u00b6 (in seconds) Default: 1 spark.dynamicAllocation.shuffleTracking.enabled \u00b6 Default: false Used when: ExecutorMonitor is created spark.dynamicAllocation.shuffleTracking.timeout \u00b6 (in millis) Default: The largest value representable as an Int spark.dynamicAllocation.sustainedSchedulerBacklogTimeout \u00b6 Default: spark.dynamicAllocation.schedulerBacklogTimeout","title":"Configuration Properties"},{"location":"dynamic-allocation/configuration-properties/#spark-configuration-properties","text":"","title":"Spark Configuration Properties"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationcachedexecutoridletimeout","text":"How long (in seconds) to keep blocks cached Default: The largest value representable as an Int Must be >= 0 Used when: ExecutorMonitor is created RDD is requested to localCheckpoint (simply to print out a WARN message)","title":" spark.dynamicAllocation.cachedExecutorIdleTimeout"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationenabled","text":"Default: false Used when: Utils utility is requested to isDynamicAllocationEnabled SparkSubmitArguments is requested to loadEnvironmentArguments (and validates numExecutors argument ) RDD is requested to localCheckpoint DAGScheduler is requested to checkBarrierStageWithDynamicAllocation","title":" spark.dynamicAllocation.enabled"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationexecutorallocationratio","text":"Default: 1.0 Must be between 0 (exclusive) and 1.0 (inclusive) Used when: ExecutorAllocationManager is created","title":" spark.dynamicAllocation.executorAllocationRatio"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationexecutoridletimeout","text":"Default: 60","title":" spark.dynamicAllocation.executorIdleTimeout"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationinitialexecutors","text":"Default: spark.dynamicAllocation.minExecutors","title":" spark.dynamicAllocation.initialExecutors"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationmaxexecutors","text":"Default: The largest value representable as an Int","title":" spark.dynamicAllocation.maxExecutors"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationminexecutors","text":"Default: 0","title":" spark.dynamicAllocation.minExecutors"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationschedulerbacklogtimeout","text":"(in seconds) Default: 1","title":" spark.dynamicAllocation.schedulerBacklogTimeout"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationshuffletrackingenabled","text":"Default: false Used when: ExecutorMonitor is created","title":" spark.dynamicAllocation.shuffleTracking.enabled"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationshuffletrackingtimeout","text":"(in millis) Default: The largest value representable as an Int","title":" spark.dynamicAllocation.shuffleTracking.timeout"},{"location":"dynamic-allocation/configuration-properties/#sparkdynamicallocationsustainedschedulerbacklogtimeout","text":"Default: spark.dynamicAllocation.schedulerBacklogTimeout","title":" spark.dynamicAllocation.sustainedSchedulerBacklogTimeout"},{"location":"executor/CoarseGrainedExecutorBackend/","text":"CoarseGrainedExecutorBackend \u00b6 decommissionSelf \u00b6 decommissionSelf (): Unit decommissionSelf ...FIXME decommissionSelf is used when: CoarseGrainedExecutorBackend is requested to handle a DecommissionExecutor message Messages \u00b6 DecommissionExecutor \u00b6 DecommissionExecutor is sent out when CoarseGrainedSchedulerBackend is requested to decommissionExecutors When received, CoarseGrainedExecutorBackend decommissionSelf . Review Me \u00b6 CoarseGrainedExecutorBackend is an executor:ExecutorBackend.md[] that controls the lifecycle of a single < > and sends < > to the driver. .CoarseGrainedExecutorBackend Sending Task Status Updates to Driver's CoarseGrainedScheduler Endpoint image::CoarseGrainedExecutorBackend-statusUpdate.png[align=\"center\"] CoarseGrainedExecutorBackend is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] that < > (before accepting < >) and < >. CoarseGrainedExecutorBackend is started in a resource container (as a < >). When < >, CoarseGrainedExecutorBackend < > to communicate with the driver (with DriverEndpoint ). When < >, CoarseGrainedExecutorBackend immediately connects to the owning scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] to inform that it is ready to launch tasks. [[messages]] .CoarseGrainedExecutorBackend's RPC Messages [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Message | Description < > | < > | Forwards launch task requests from the driver to the single managed coarse-grained < >. | < > | Creates the single managed < >. Sent exclusively when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#RegisterExecutor[receives RegisterExecutor ]. < > < > < > |=== == [[LaunchTask]] Forwarding Launch Task Request to Executor (from Driver) -- LaunchTask Message Handler [source, scala] \u00b6 LaunchTask(data: SerializableBuffer) extends CoarseGrainedClusterMessage \u00b6 NOTE: CoarseGrainedExecutorBackend acts as a proxy between the driver and the managed single < > and merely re-packages LaunchTask payload (as serialized data ) to pass it along for execution. LaunchTask first decodes TaskDescription from data . You should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Got assigned task [id] LaunchTask then executor:Executor.md#launchTask[launches the task on the executor] (passing itself as the owning executor:ExecutorBackend.md[] and decoded TaskDescription ). If < > is not available, LaunchTask < > with the error code 1 and ExecutorLossReason with the following message: Received LaunchTask command but executor was null NOTE: LaunchTask is sent when CoarseGrainedSchedulerBackend is requested to launch tasks (one LaunchTask per task). == [[statusUpdate]] Sending Task Status Updates to Driver -- statusUpdate Method [source, scala] \u00b6 statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer): Unit \u00b6 NOTE: statusUpdate is part of executor:ExecutorBackend.md#statusUpdate[ExecutorBackend Contract] to send task status updates to a scheduler (on the driver). statusUpdate creates a StatusUpdate (with the input taskId , state , and data together with the < >) and sends it to the < > (if connected already). .CoarseGrainedExecutorBackend Sending Task Status Updates to Driver's CoarseGrainedScheduler Endpoint image::CoarseGrainedExecutorBackend-statusUpdate.png[align=\"center\"] When no < > is available, you should see the following WARN message in the logs: WARN Drop [msg] because has not yet connected to driver == [[driverURL]] Driver's URL The driver's URL is of the format spark://[RpcEndpoint name]@[hostname]:[port] , e.g. spark://CoarseGrainedScheduler@192.168.1.6:64859 . == [[main]] Launching CoarseGrainedExecutorBackend Standalone Application (in Resource Container) CoarseGrainedExecutorBackend is a standalone application (i.e. comes with main entry method) that parses < > and < > to communicate with the driver. [[command-line-arguments]] .CoarseGrainedExecutorBackend Command-Line Arguments [cols=\"1,^1,2\",options=\"header\",width=\"100%\"] |=== | Argument | Required? | Description | [[driver-url]] --driver-url | yes | Driver's URL. See < > | [[executor-id]] --executor-id | yes | Executor id | [[hostname]] --hostname | yes | Host name | [[cores]] --cores | yes | Number of cores (that must be greater than 0 ). | [[app-id]] --app-id | yes | Application id | [[worker-url]] --worker-url | no | Worker's URL, e.g. spark://Worker@192.168.1.6:64557 NOTE: --worker-url is only used in spark-standalone-StandaloneSchedulerBackend.md[Spark Standalone] to enforce fate-sharing with the worker. | [[user-class-path]] --user-class-path | no | User-defined class path entry which can be an URL or path to a resource (often a jar file) to be added to CLASSPATH; can be specified multiple times. |=== When executed with unrecognized command-line arguments or required arguments are missing, main shows the usage help and exits (with exit status 1 ). [source] \u00b6 $ ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend Usage: CoarseGrainedExecutorBackend [options] Options are: --driver-url --executor-id --hostname --cores --app-id --worker-url --user-class-path main is used when: (Spark Standalone) StandaloneSchedulerBackend is requested to spark-standalone:StandaloneSchedulerBackend.md#start[start] (Spark on YARN) ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#run[start] (in a YARN resource container). (Spark on Mesos) MesosCoarseGrainedSchedulerBackend is requested to spark-on-mesos:spark-mesos-MesosCoarseGrainedSchedulerBackend.md#createCommand[launch Spark executors] == [[run]] Starting CoarseGrainedExecutorBackend [source, scala] \u00b6 run( driverUrl: String, executorId: String, hostname: String, cores: Int, appId: String, workerUrl: Option[String], userClassPath: scala.Seq[URL]): Unit When executed, run executes Utils.initDaemon(log) . CAUTION: FIXME What does initDaemon do? NOTE: run spark-SparkHadoopUtil.md#runAsSparkUser[runs itself with a Hadoop UserGroupInformation ] (as a thread local variable distributed to child threads for authenticating HDFS and YARN calls). NOTE: run expects a clear hostname with no : included (for a port perhaps). [[run-driverPropsFetcher]] run uses executor:Executor.md#spark_executor_port[spark.executor.port] Spark property (or 0 if not set) for the port to rpc:index.md#create[create a RpcEnv ] called driverPropsFetcher (together with the input hostname and clientMode enabled). run rpc:index.md#setupEndpointRefByURI[resolves RpcEndpointRef for the input driverUrl ] and requests SparkAppConfig (by posting a blocking RetrieveSparkAppConfig ). IMPORTANT: This is the first moment when CoarseGrainedExecutorBackend initiates communication with the driver available at driverUrl through RpcEnv . run uses SparkAppConfig to get the driver's sparkProperties and adds SparkConf.md#spark.app.id[spark.app.id] Spark property with the value of the input appId . run rpc:index.md#shutdown[shuts driverPropsFetcher RPC Endpoint down]. run creates a SparkConf.md[SparkConf] using the Spark properties fetched from the driver, i.e. with the SparkConf.md#isExecutorStartupConf[executor-related Spark settings] if they SparkConf.md#setIfMissing[were missing] and the SparkConf.md#set[rest unconditionally]. If yarn/spark-yarn-settings.md#spark.yarn.credentials.file[spark.yarn.credentials.file] Spark property is defined in SparkConf , you should see the following INFO message in the logs: INFO Will periodically update credentials from: [spark.yarn.credentials.file] run spark-SparkHadoopUtil.md#startCredentialUpdater[requests the current SparkHadoopUtil to start start the credential updater]. NOTE: run uses spark-SparkHadoopUtil.md#get[SparkHadoopUtil.get] to access the current SparkHadoopUtil . run core:SparkEnv.md#createExecutorEnv[creates SparkEnv for executors] (with the input executorId , hostname and cores , and isLocal disabled). IMPORTANT: This is the moment when SparkEnv gets created with all the executor services. run rpc:index.md#setupEndpoint[sets up an RPC endpoint] with the name Executor and < > as the endpoint. (only in Spark Standalone) If the optional input workerUrl was defined, run sets up an RPC endpoint with the name WorkerWatcher and WorkerWatcher RPC endpoint. [NOTE] \u00b6 The optional input workerUrl is defined only when < --worker-url command-line argument>> was used to < >. --worker-url is only used in spark-standalone-StandaloneSchedulerBackend.md[Spark Standalone]. \u00b6 run 's main thread is blocked until rpc:index.md#awaitTermination[ RpcEnv terminates] and only the RPC endpoints process RPC messages. Once RpcEnv has terminated, run spark-SparkHadoopUtil.md#stopCredentialUpdater[stops the credential updater]. CAUTION: FIXME Think of the place for Utils.initDaemon , Utils.getProcessName et al. run is used when CoarseGrainedExecutorBackend standalone application is < >. == [[creating-instance]] Creating CoarseGrainedExecutorBackend Instance CoarseGrainedExecutorBackend takes the following when created: . [[rpcEnv]] rpc:index.md[RpcEnv] . driverUrl . [[executorId]] executorId . hostname . cores . userClassPath . core:SparkEnv.md[SparkEnv] NOTE: driverUrl , executorId , hostname , cores and userClassPath correspond to CoarseGrainedExecutorBackend standalone application's < >. CoarseGrainedExecutorBackend initializes the < >. NOTE: CoarseGrainedExecutorBackend is created (to act as an RPC endpoint) when < Executor RPC endpoint is registered>>. == [[onStart]] Registering with Driver -- onStart Method [source, scala] \u00b6 onStart(): Unit \u00b6 NOTE: onStart is part of rpc:RpcEndpoint.md#onStart[RpcEndpoint contract] that is executed before a RPC endpoint starts accepting messages. When executed, you should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Connecting to driver: [driverUrl] NOTE: < > is given when < >. onStart then rpc:index.md#asyncSetupEndpointRefByURI[takes the RpcEndpointRef of the driver asynchronously] and initializes the internal < > property. onStart sends a blocking scheduler:CoarseGrainedSchedulerBackend.md#RegisterExecutor[RegisterExecutor] message immediately (with < >, rpc:RpcEndpointRef.md[RpcEndpointRef] to itself, < >, < > and < >). In case of failures, onStart < > with the error code 1 and the reason (and no notification to the driver): Cannot register with driver: [driverUrl] == [[RegisteredExecutor]] Creating Single Managed Executor -- RegisteredExecutor Message Handler [source, scala] \u00b6 RegisteredExecutor extends CoarseGrainedClusterMessage with RegisterExecutorResponse When RegisteredExecutor is received, you should see the following INFO in the logs: INFO CoarseGrainedExecutorBackend: Successfully registered with driver CoarseGrainedExecutorBackend executor:Executor.md#creating-instance[creates a Executor ] (with isLocal disabled) that becomes the single managed < >. NOTE: CoarseGrainedExecutorBackend uses executorId , hostname , env , userClassPath to create the Executor that are specified when CoarseGrainedExecutorBackend < >. If creating the Executor fails with a non-fatal exception, RegisteredExecutor < > with the reason: Unable to create executor due to [message] NOTE: RegisteredExecutor is sent when CoarseGrainedSchedulerBackend RPC Endpoint receives a RegisterExecutor (that is sent right before CoarseGrainedExecutorBackend RPC Endpoint < > which happens when CoarseGrainedExecutorBackend < >). == [[RegisterExecutorFailed]] RegisterExecutorFailed [source, scala] \u00b6 RegisterExecutorFailed(message) \u00b6 When a RegisterExecutorFailed message arrives, the following ERROR is printed out to the logs: ERROR CoarseGrainedExecutorBackend: Slave registration failed: [message] CoarseGrainedExecutorBackend then exits with the exit code 1 . == [[KillTask]] Killing Tasks -- KillTask Message Handler KillTask(taskId, _, interruptThread) message kills a task (calls Executor.killTask ). If an executor has not been initialized yet (FIXME: why?), the following ERROR message is printed out to the logs and CoarseGrainedExecutorBackend exits: ERROR Received KillTask command but executor was null == [[StopExecutor]] StopExecutor Handler [source, scala] \u00b6 case object StopExecutor extends CoarseGrainedClusterMessage When StopExecutor is received, the handler turns < > internal flag on. You should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown In the end, the handler sends a < > message to itself. StopExecutor message is sent when CoarseGrainedSchedulerBackend RPC Endpoint (aka DriverEndpoint ) processes StopExecutors or RemoveExecutor messages. == [[Shutdown]] Shutdown Handler [source, scala] \u00b6 case object Shutdown extends CoarseGrainedClusterMessage Shutdown turns < > internal flag on and starts the CoarseGrainedExecutorBackend-stop-executor thread that executor:Executor.md#stop[stops the owned Executor ] (using < > reference). NOTE: Shutdown message is sent exclusively when < StopExecutor >>. == [[exitExecutor]] Terminating CoarseGrainedExecutorBackend (and Notifying Driver with RemoveExecutor) -- exitExecutor Method [source, scala] \u00b6 exitExecutor( code: Int, reason: String, throwable: Throwable = null, notifyDriver: Boolean = true): Unit When exitExecutor is executed, you should see the following ERROR message in the logs (followed by throwable if available): Executor self-exiting due to : [reason] If notifyDriver is enabled (it is by default) exitExecutor informs the < > that the executor should be removed (by sending a blocking RemoveExecutor message with < > and a ExecutorLossReason with the input reason ). You may see the following WARN message in the logs when the notification fails. Unable to notify the driver due to [message] In the end, exitExecutor terminates the CoarseGrainedExecutorBackend JVM process with the status code . NOTE: exitExecutor uses Java's https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#exit-int-[System.exit ] and initiates JVM's shutdown sequence (and executing all registered shutdown hooks). [NOTE] \u00b6 exitExecutor is used when: CoarseGrainedExecutorBackend fails to < >, < > or < > no < > has been created before < > or < > task requests * < >. \u00b6 == [[onDisconnected]] onDisconnected Callback CAUTION: FIXME == [[start]] start Method CAUTION: FIXME == [[stop]] stop Method CAUTION: FIXME == [[requestTotalExecutors]] requestTotalExecutors CAUTION: FIXME == [[extractLogUrls]] Extracting Log URLs -- extractLogUrls Method CAUTION: FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.executor.CoarseGrainedExecutorBackend logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.executor.CoarseGrainedExecutorBackend=ALL \u00b6 Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[ser]] SerializerInstance serializer:SerializerInstance.md[SerializerInstance] Initialized when < >. NOTE: CoarseGrainedExecutorBackend uses the input env to core:SparkEnv.md#closureSerializer[access closureSerializer ]. === [[driver]] Driver RpcEndpointRef rpc:RpcEndpointRef.md[RpcEndpointRef] of the driver === [[stopping]] stopping Flag Enabled when CoarseGrainedExecutorBackend gets notified to < > or < >. Default: false Used when CoarseGrainedExecutorBackend RPC Endpoint gets notified that < >. === [[executor]] Executor Single managed coarse-grained executor:Executor.md#coarse-grained-executor[Executor] managed exclusively by the CoarseGrainedExecutorBackend to forward < > and < > task requests to from the driver. Initialized after CoarseGrainedExecutorBackend < CoarseGrainedSchedulerBackend >> and stopped when CoarseGrainedExecutorBackend gets requested to < >.","title":"CoarseGrainedExecutorBackend"},{"location":"executor/CoarseGrainedExecutorBackend/#coarsegrainedexecutorbackend","text":"","title":"CoarseGrainedExecutorBackend"},{"location":"executor/CoarseGrainedExecutorBackend/#decommissionself","text":"decommissionSelf (): Unit decommissionSelf ...FIXME decommissionSelf is used when: CoarseGrainedExecutorBackend is requested to handle a DecommissionExecutor message","title":" decommissionSelf"},{"location":"executor/CoarseGrainedExecutorBackend/#messages","text":"","title":"Messages"},{"location":"executor/CoarseGrainedExecutorBackend/#decommissionexecutor","text":"DecommissionExecutor is sent out when CoarseGrainedSchedulerBackend is requested to decommissionExecutors When received, CoarseGrainedExecutorBackend decommissionSelf .","title":" DecommissionExecutor"},{"location":"executor/CoarseGrainedExecutorBackend/#review-me","text":"CoarseGrainedExecutorBackend is an executor:ExecutorBackend.md[] that controls the lifecycle of a single < > and sends < > to the driver. .CoarseGrainedExecutorBackend Sending Task Status Updates to Driver's CoarseGrainedScheduler Endpoint image::CoarseGrainedExecutorBackend-statusUpdate.png[align=\"center\"] CoarseGrainedExecutorBackend is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] that < > (before accepting < >) and < >. CoarseGrainedExecutorBackend is started in a resource container (as a < >). When < >, CoarseGrainedExecutorBackend < > to communicate with the driver (with DriverEndpoint ). When < >, CoarseGrainedExecutorBackend immediately connects to the owning scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] to inform that it is ready to launch tasks. [[messages]] .CoarseGrainedExecutorBackend's RPC Messages [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Message | Description < > | < > | Forwards launch task requests from the driver to the single managed coarse-grained < >. | < > | Creates the single managed < >. Sent exclusively when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#RegisterExecutor[receives RegisterExecutor ]. < > < > < > |=== == [[LaunchTask]] Forwarding Launch Task Request to Executor (from Driver) -- LaunchTask Message Handler","title":"Review Me"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala","text":"","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#launchtaskdata-serializablebuffer-extends-coarsegrainedclustermessage","text":"NOTE: CoarseGrainedExecutorBackend acts as a proxy between the driver and the managed single < > and merely re-packages LaunchTask payload (as serialized data ) to pass it along for execution. LaunchTask first decodes TaskDescription from data . You should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Got assigned task [id] LaunchTask then executor:Executor.md#launchTask[launches the task on the executor] (passing itself as the owning executor:ExecutorBackend.md[] and decoded TaskDescription ). If < > is not available, LaunchTask < > with the error code 1 and ExecutorLossReason with the following message: Received LaunchTask command but executor was null NOTE: LaunchTask is sent when CoarseGrainedSchedulerBackend is requested to launch tasks (one LaunchTask per task). == [[statusUpdate]] Sending Task Status Updates to Driver -- statusUpdate Method","title":"LaunchTask(data: SerializableBuffer) extends CoarseGrainedClusterMessage"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_1","text":"","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#statusupdatetaskid-long-state-taskstate-data-bytebuffer-unit","text":"NOTE: statusUpdate is part of executor:ExecutorBackend.md#statusUpdate[ExecutorBackend Contract] to send task status updates to a scheduler (on the driver). statusUpdate creates a StatusUpdate (with the input taskId , state , and data together with the < >) and sends it to the < > (if connected already). .CoarseGrainedExecutorBackend Sending Task Status Updates to Driver's CoarseGrainedScheduler Endpoint image::CoarseGrainedExecutorBackend-statusUpdate.png[align=\"center\"] When no < > is available, you should see the following WARN message in the logs: WARN Drop [msg] because has not yet connected to driver == [[driverURL]] Driver's URL The driver's URL is of the format spark://[RpcEndpoint name]@[hostname]:[port] , e.g. spark://CoarseGrainedScheduler@192.168.1.6:64859 . == [[main]] Launching CoarseGrainedExecutorBackend Standalone Application (in Resource Container) CoarseGrainedExecutorBackend is a standalone application (i.e. comes with main entry method) that parses < > and < > to communicate with the driver. [[command-line-arguments]] .CoarseGrainedExecutorBackend Command-Line Arguments [cols=\"1,^1,2\",options=\"header\",width=\"100%\"] |=== | Argument | Required? | Description | [[driver-url]] --driver-url | yes | Driver's URL. See < > | [[executor-id]] --executor-id | yes | Executor id | [[hostname]] --hostname | yes | Host name | [[cores]] --cores | yes | Number of cores (that must be greater than 0 ). | [[app-id]] --app-id | yes | Application id | [[worker-url]] --worker-url | no | Worker's URL, e.g. spark://Worker@192.168.1.6:64557 NOTE: --worker-url is only used in spark-standalone-StandaloneSchedulerBackend.md[Spark Standalone] to enforce fate-sharing with the worker. | [[user-class-path]] --user-class-path | no | User-defined class path entry which can be an URL or path to a resource (often a jar file) to be added to CLASSPATH; can be specified multiple times. |=== When executed with unrecognized command-line arguments or required arguments are missing, main shows the usage help and exits (with exit status 1 ).","title":"statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer): Unit"},{"location":"executor/CoarseGrainedExecutorBackend/#source","text":"$ ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend Usage: CoarseGrainedExecutorBackend [options] Options are: --driver-url --executor-id --hostname --cores --app-id --worker-url --user-class-path main is used when: (Spark Standalone) StandaloneSchedulerBackend is requested to spark-standalone:StandaloneSchedulerBackend.md#start[start] (Spark on YARN) ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#run[start] (in a YARN resource container). (Spark on Mesos) MesosCoarseGrainedSchedulerBackend is requested to spark-on-mesos:spark-mesos-MesosCoarseGrainedSchedulerBackend.md#createCommand[launch Spark executors] == [[run]] Starting CoarseGrainedExecutorBackend","title":"[source]"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_2","text":"run( driverUrl: String, executorId: String, hostname: String, cores: Int, appId: String, workerUrl: Option[String], userClassPath: scala.Seq[URL]): Unit When executed, run executes Utils.initDaemon(log) . CAUTION: FIXME What does initDaemon do? NOTE: run spark-SparkHadoopUtil.md#runAsSparkUser[runs itself with a Hadoop UserGroupInformation ] (as a thread local variable distributed to child threads for authenticating HDFS and YARN calls). NOTE: run expects a clear hostname with no : included (for a port perhaps). [[run-driverPropsFetcher]] run uses executor:Executor.md#spark_executor_port[spark.executor.port] Spark property (or 0 if not set) for the port to rpc:index.md#create[create a RpcEnv ] called driverPropsFetcher (together with the input hostname and clientMode enabled). run rpc:index.md#setupEndpointRefByURI[resolves RpcEndpointRef for the input driverUrl ] and requests SparkAppConfig (by posting a blocking RetrieveSparkAppConfig ). IMPORTANT: This is the first moment when CoarseGrainedExecutorBackend initiates communication with the driver available at driverUrl through RpcEnv . run uses SparkAppConfig to get the driver's sparkProperties and adds SparkConf.md#spark.app.id[spark.app.id] Spark property with the value of the input appId . run rpc:index.md#shutdown[shuts driverPropsFetcher RPC Endpoint down]. run creates a SparkConf.md[SparkConf] using the Spark properties fetched from the driver, i.e. with the SparkConf.md#isExecutorStartupConf[executor-related Spark settings] if they SparkConf.md#setIfMissing[were missing] and the SparkConf.md#set[rest unconditionally]. If yarn/spark-yarn-settings.md#spark.yarn.credentials.file[spark.yarn.credentials.file] Spark property is defined in SparkConf , you should see the following INFO message in the logs: INFO Will periodically update credentials from: [spark.yarn.credentials.file] run spark-SparkHadoopUtil.md#startCredentialUpdater[requests the current SparkHadoopUtil to start start the credential updater]. NOTE: run uses spark-SparkHadoopUtil.md#get[SparkHadoopUtil.get] to access the current SparkHadoopUtil . run core:SparkEnv.md#createExecutorEnv[creates SparkEnv for executors] (with the input executorId , hostname and cores , and isLocal disabled). IMPORTANT: This is the moment when SparkEnv gets created with all the executor services. run rpc:index.md#setupEndpoint[sets up an RPC endpoint] with the name Executor and < > as the endpoint. (only in Spark Standalone) If the optional input workerUrl was defined, run sets up an RPC endpoint with the name WorkerWatcher and WorkerWatcher RPC endpoint.","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#note","text":"The optional input workerUrl is defined only when < --worker-url command-line argument>> was used to < >.","title":"[NOTE]"},{"location":"executor/CoarseGrainedExecutorBackend/#-worker-url-is-only-used-in-spark-standalone-standaloneschedulerbackendmdspark-standalone","text":"run 's main thread is blocked until rpc:index.md#awaitTermination[ RpcEnv terminates] and only the RPC endpoints process RPC messages. Once RpcEnv has terminated, run spark-SparkHadoopUtil.md#stopCredentialUpdater[stops the credential updater]. CAUTION: FIXME Think of the place for Utils.initDaemon , Utils.getProcessName et al. run is used when CoarseGrainedExecutorBackend standalone application is < >. == [[creating-instance]] Creating CoarseGrainedExecutorBackend Instance CoarseGrainedExecutorBackend takes the following when created: . [[rpcEnv]] rpc:index.md[RpcEnv] . driverUrl . [[executorId]] executorId . hostname . cores . userClassPath . core:SparkEnv.md[SparkEnv] NOTE: driverUrl , executorId , hostname , cores and userClassPath correspond to CoarseGrainedExecutorBackend standalone application's < >. CoarseGrainedExecutorBackend initializes the < >. NOTE: CoarseGrainedExecutorBackend is created (to act as an RPC endpoint) when < Executor RPC endpoint is registered>>. == [[onStart]] Registering with Driver -- onStart Method","title":"--worker-url is only used in spark-standalone-StandaloneSchedulerBackend.md[Spark Standalone]."},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_3","text":"","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#onstart-unit","text":"NOTE: onStart is part of rpc:RpcEndpoint.md#onStart[RpcEndpoint contract] that is executed before a RPC endpoint starts accepting messages. When executed, you should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Connecting to driver: [driverUrl] NOTE: < > is given when < >. onStart then rpc:index.md#asyncSetupEndpointRefByURI[takes the RpcEndpointRef of the driver asynchronously] and initializes the internal < > property. onStart sends a blocking scheduler:CoarseGrainedSchedulerBackend.md#RegisterExecutor[RegisterExecutor] message immediately (with < >, rpc:RpcEndpointRef.md[RpcEndpointRef] to itself, < >, < > and < >). In case of failures, onStart < > with the error code 1 and the reason (and no notification to the driver): Cannot register with driver: [driverUrl] == [[RegisteredExecutor]] Creating Single Managed Executor -- RegisteredExecutor Message Handler","title":"onStart(): Unit"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_4","text":"RegisteredExecutor extends CoarseGrainedClusterMessage with RegisterExecutorResponse When RegisteredExecutor is received, you should see the following INFO in the logs: INFO CoarseGrainedExecutorBackend: Successfully registered with driver CoarseGrainedExecutorBackend executor:Executor.md#creating-instance[creates a Executor ] (with isLocal disabled) that becomes the single managed < >. NOTE: CoarseGrainedExecutorBackend uses executorId , hostname , env , userClassPath to create the Executor that are specified when CoarseGrainedExecutorBackend < >. If creating the Executor fails with a non-fatal exception, RegisteredExecutor < > with the reason: Unable to create executor due to [message] NOTE: RegisteredExecutor is sent when CoarseGrainedSchedulerBackend RPC Endpoint receives a RegisterExecutor (that is sent right before CoarseGrainedExecutorBackend RPC Endpoint < > which happens when CoarseGrainedExecutorBackend < >). == [[RegisterExecutorFailed]] RegisterExecutorFailed","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_5","text":"","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#registerexecutorfailedmessage","text":"When a RegisterExecutorFailed message arrives, the following ERROR is printed out to the logs: ERROR CoarseGrainedExecutorBackend: Slave registration failed: [message] CoarseGrainedExecutorBackend then exits with the exit code 1 . == [[KillTask]] Killing Tasks -- KillTask Message Handler KillTask(taskId, _, interruptThread) message kills a task (calls Executor.killTask ). If an executor has not been initialized yet (FIXME: why?), the following ERROR message is printed out to the logs and CoarseGrainedExecutorBackend exits: ERROR Received KillTask command but executor was null == [[StopExecutor]] StopExecutor Handler","title":"RegisterExecutorFailed(message)"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_6","text":"case object StopExecutor extends CoarseGrainedClusterMessage When StopExecutor is received, the handler turns < > internal flag on. You should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown In the end, the handler sends a < > message to itself. StopExecutor message is sent when CoarseGrainedSchedulerBackend RPC Endpoint (aka DriverEndpoint ) processes StopExecutors or RemoveExecutor messages. == [[Shutdown]] Shutdown Handler","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_7","text":"case object Shutdown extends CoarseGrainedClusterMessage Shutdown turns < > internal flag on and starts the CoarseGrainedExecutorBackend-stop-executor thread that executor:Executor.md#stop[stops the owned Executor ] (using < > reference). NOTE: Shutdown message is sent exclusively when < StopExecutor >>. == [[exitExecutor]] Terminating CoarseGrainedExecutorBackend (and Notifying Driver with RemoveExecutor) -- exitExecutor Method","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_8","text":"exitExecutor( code: Int, reason: String, throwable: Throwable = null, notifyDriver: Boolean = true): Unit When exitExecutor is executed, you should see the following ERROR message in the logs (followed by throwable if available): Executor self-exiting due to : [reason] If notifyDriver is enabled (it is by default) exitExecutor informs the < > that the executor should be removed (by sending a blocking RemoveExecutor message with < > and a ExecutorLossReason with the input reason ). You may see the following WARN message in the logs when the notification fails. Unable to notify the driver due to [message] In the end, exitExecutor terminates the CoarseGrainedExecutorBackend JVM process with the status code . NOTE: exitExecutor uses Java's https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#exit-int-[System.exit ] and initiates JVM's shutdown sequence (and executing all registered shutdown hooks).","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#note_1","text":"exitExecutor is used when: CoarseGrainedExecutorBackend fails to < >, < > or < > no < > has been created before < > or < > task requests","title":"[NOTE]"},{"location":"executor/CoarseGrainedExecutorBackend/#_1","text":"== [[onDisconnected]] onDisconnected Callback CAUTION: FIXME == [[start]] start Method CAUTION: FIXME == [[stop]] stop Method CAUTION: FIXME == [[requestTotalExecutors]] requestTotalExecutors CAUTION: FIXME == [[extractLogUrls]] Extracting Log URLs -- extractLogUrls Method CAUTION: FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.executor.CoarseGrainedExecutorBackend logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"* &lt;&gt;."},{"location":"executor/CoarseGrainedExecutorBackend/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"executor/CoarseGrainedExecutorBackend/#log4jloggerorgapachesparkexecutorcoarsegrainedexecutorbackendall","text":"Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[ser]] SerializerInstance serializer:SerializerInstance.md[SerializerInstance] Initialized when < >. NOTE: CoarseGrainedExecutorBackend uses the input env to core:SparkEnv.md#closureSerializer[access closureSerializer ]. === [[driver]] Driver RpcEndpointRef rpc:RpcEndpointRef.md[RpcEndpointRef] of the driver === [[stopping]] stopping Flag Enabled when CoarseGrainedExecutorBackend gets notified to < > or < >. Default: false Used when CoarseGrainedExecutorBackend RPC Endpoint gets notified that < >. === [[executor]] Executor Single managed coarse-grained executor:Executor.md#coarse-grained-executor[Executor] managed exclusively by the CoarseGrainedExecutorBackend to forward < > and < > task requests to from the driver. Initialized after CoarseGrainedExecutorBackend < CoarseGrainedSchedulerBackend >> and stopped when CoarseGrainedExecutorBackend gets requested to < >.","title":"log4j.logger.org.apache.spark.executor.CoarseGrainedExecutorBackend=ALL"},{"location":"executor/Executor/","text":"Executor \u00b6 Executor is a process that is used for executing scheduler:Task.md[tasks]. Executor typically runs for the entire lifetime of a Spark application which is called static allocation of executors (but you could also opt in for dynamic allocation ). Executors are managed by executor:ExecutorBackend.md[executor backends]. Executors < > to the < > on the driver. Executors provide in-memory storage for RDDs that are cached in Spark applications (via storage:BlockManager.md[]). When started, an executor first registers itself with the driver that establishes a communication channel directly to the driver to accept tasks for execution. ../images/executor/executor-taskrunner-executorbackend.png) Executor offers are described by executor id and the host on which an executor runs (see < > in this document). Executors can run multiple tasks over its lifetime, both in parallel and sequentially. They track executor:TaskRunner.md[running tasks] (by their task ids in < > internal registry). Consult < > section. Executors use a < > for < >. Executors send < > (and heartbeats) using the < >. It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster. Executors are described by their id , hostname , environment (as SparkEnv ), and classpath (and, less importantly, and more for internal optimization, whether they run in spark-local:index.md[local] or spark-cluster.md[cluster] mode). Creating Instance \u00b6 Executor takes the following to be created: Executor ID Host name SparkEnv User-defined jars (default: empty ) isLocal flag (default: false ) Java's UncaughtExceptionHandler (default: SparkUncaughtExceptionHandler ) Resources ( Map[String, ResourceInformation] ) Executor is created when: CoarseGrainedExecutorBackend is requested to handle a RegisteredExecutor message (after having registered with the driver) LocalEndpoint is created When Created \u00b6 When created, Executor prints out the following INFO messages to the logs: Starting executor ID [executorId] on host [executorHostname] (only for non-local modes) Executor sets SparkUncaughtExceptionHandler as the default handler invoked when a thread abruptly terminates due to an uncaught exception. (only for non-local modes) Executor requests the BlockManager to initialize (with the Spark application id of the SparkConf ). (only for non-local modes) Executor requests the MetricsSystem to register the ExecutorSource and shuffleMetricsSource of the BlockManager . Executor uses SparkEnv to access the MetricsSystem and BlockManager . Executor creates a task class loader (optionally with REPL support ) and requests the system Serializer to use as the default classloader (for deserializing tasks). Executor starts sending heartbeats with the metrics of active tasks . Fetching File and Jar Dependencies \u00b6 updateDependencies ( newFiles : Map [ String , Long ], newJars : Map [ String , Long ]): Unit updateDependencies fetches missing or outdated extra files (in the given newFiles ). For every name-timestamp pair that...FIXME..., updateDependencies prints out the following INFO message to the logs: Fetching [name] with timestamp [timestamp] updateDependencies fetches missing or outdated extra jars (in the given newJars ). For every name-timestamp pair that...FIXME..., updateDependencies prints out the following INFO message to the logs: Fetching [name] with timestamp [timestamp] updateDependencies fetches the file to the SparkFiles root directory . updateDependencies ...FIXME updateDependencies is used when: TaskRunner is requested to start (and run a task) spark.driver.maxResultSize \u00b6 Executor uses the spark.driver.maxResultSize for TaskRunner when requested to run a task (and decide on a serialized task result ). Maximum Size of Direct Results \u00b6 Executor uses the minimum of spark.task.maxDirectResultSize and spark.rpc.message.maxSize when TaskRunner is requested to run a task (and decide on the type of a serialized task result ). Logging \u00b6 Enable ALL logging level for org.apache.spark.executor.Executor logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.executor.Executor=ALL Refer to Logging . Review Me \u00b6 == [[isLocal]] isLocal Flag Executor is given a isLocal flag when created. This is how the executor knows whether it runs in local or cluster mode. It is disabled by default. The flag is turned on for spark-local:index.md[Spark local] (via spark-local:spark-LocalEndpoint.md[LocalEndpoint]). == [[userClassPath]] User-Defined Jars Executor is given user-defined jars when created. There are no jars defined by default. The jars are specified using configuration-properties.md#spark.executor.extraClassPath[spark.executor.extraClassPath] configuration property (via executor:CoarseGrainedExecutorBackend.md#main[--user-class-path] command-line option of CoarseGrainedExecutorBackend). Running Tasks Registry \u00b6 runningTasks : Map [ Long , TaskRunner ] Executor tracks TaskRunners by task IDs. HeartbeatReceiver RPC Endpoint Reference \u00b6 RPC endpoint reference to HeartbeatReceiver on the driver . Set when Executor < >. Used exclusively when Executor < > (that happens every < > interval). == [[launchTask]] Launching Task [source, scala] \u00b6 launchTask( context: ExecutorBackend, taskDescription: TaskDescription): Unit launchTask simply creates a executor:TaskRunner.md[] (with the given executor:ExecutorBackend.md[] and the TaskDescription ) and adds it to the < > internal registry. In the end, launchTask requests the < > to execute the TaskRunner (sometime in the future). .Launching tasks on executor using TaskRunners image::executor-taskrunner-executorbackend.png[align=\"center\"] launchTask is used when: CoarseGrainedExecutorBackend is requested to executor:CoarseGrainedExecutorBackend.md#LaunchTask[handle a LaunchTask message] LocalEndpoint RPC endpoint (of spark-local:spark-LocalSchedulerBackend.md#[LocalSchedulerBackend]) is requested to spark-local:spark-LocalEndpoint.md#reviveOffers[reviveOffers] MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#launchTask[launchTask] == [[heartbeater]] Heartbeat Sender Thread heartbeater is a daemon {java-javadoc-url}/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor] with a single thread. The name of the thread pool is driver-heartbeater . == [[coarse-grained-executor]] Coarse-Grained Executors Coarse-grained executors are executors that use executor:CoarseGrainedExecutorBackend.md[] for task scheduling. == [[resource-offers]] Resource Offers Read scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers] in TaskSchedulerImpl and scheduler:TaskSetManager.md#resourceOffers[resourceOffer] in TaskSetManager. == [[threadPool]] Executor task launch worker Thread Pool Executor uses threadPool daemon cached thread pool with the name Executor task launch worker-[ID] (with ID being the task id) for < >. threadPool is created when < > and shut down when < >. == [[memory]] Executor Memory You can control the amount of memory per executor using configuration-properties.md#spark.executor.memory[spark.executor.memory] configuration property. It sets the available memory equally for all executors per application. The amount of memory per executor is looked up when SparkContext.md#creating-instance[SparkContext is created]. You can change the assigned memory per executor per node in spark-standalone:index.md[standalone cluster] using SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable. You can find the value displayed as Memory per Node in spark-standalone:Master.md[web UI for standalone Master] (as depicted in the figure below). .Memory per Node in Spark Standalone's web UI image::spark-standalone-webui-memory-per-node.png[align=\"center\"] The above figure shows the result of running tools:spark-shell.md[Spark shell] with the amount of memory per executor defined explicitly (on command line), i.e. ./bin/spark-shell --master spark://localhost:7077 -c spark.executor.memory=2g Metrics \u00b6 Every executor registers its own executor:ExecutorSource.md[] to report metrics . == [[stop]] Stopping Executor [source, scala] \u00b6 stop(): Unit \u00b6 stop requests core:SparkEnv.md#metricsSystem[MetricsSystem] for a report . stop shuts < > down (and waits at most 10 seconds). stop shuts < > down. (only when < >) stop core:SparkEnv.md#stop[requests SparkEnv to stop]. stop is used when executor:CoarseGrainedExecutorBackend.md#Shutdown[CoarseGrainedExecutorBackend] and spark-local:spark-LocalEndpoint.md#StopExecutor[LocalEndpoint] are requested to stop their managed executors. == [[computeTotalGcTime]] computeTotalGcTime Method [source, scala] \u00b6 computeTotalGcTime(): Long \u00b6 computeTotalGcTime...FIXME computeTotalGcTime is used when: TaskRunner is requested to executor:TaskRunner.md#collectAccumulatorsAndResetStatusOnFailure[collectAccumulatorsAndResetStatusOnFailure] and executor:TaskRunner.md#run[run] Executor is requested to < > == [[createClassLoader]] createClassLoader Method [source, scala] \u00b6 createClassLoader(): MutableURLClassLoader \u00b6 createClassLoader...FIXME createClassLoader is used when...FIXME == [[addReplClassLoaderIfNeeded]] addReplClassLoaderIfNeeded Method [source, scala] \u00b6 addReplClassLoaderIfNeeded( parent: ClassLoader): ClassLoader addReplClassLoaderIfNeeded...FIXME addReplClassLoaderIfNeeded is used when...FIXME == [[reportHeartBeat]] Heartbeating With Partial Metrics For Active Tasks To Driver [source, scala] \u00b6 reportHeartBeat(): Unit \u00b6 reportHeartBeat collects executor:TaskRunner.md[TaskRunners] for < > (aka active tasks ) with their executor:TaskRunner.md#task[tasks] deserialized (i.e. either ready for execution or already started). executor:TaskRunner.md[] has TaskRunner.md#task[task] deserialized when it executor:TaskRunner.md#run[runs the task]. For every running task, reportHeartBeat takes its scheduler:Task.md#metrics[TaskMetrics] and: Requests executor:TaskMetrics.md#mergeShuffleReadMetrics[ShuffleRead metrics to be merged] executor:TaskMetrics.md#setJvmGCTime[Sets jvmGCTime metrics] reportHeartBeat then records the latest values of executor:TaskMetrics.md#accumulators[internal and external accumulators] for every task. NOTE: Internal accumulators are a task's metrics while external accumulators are a Spark application's accumulators that a user has created. reportHeartBeat sends a blocking Heartbeat message to < HeartbeatReceiver endpoint>> (running on the driver). reportHeartBeat uses the value of configuration-properties.md#spark.executor.heartbeatInterval[spark.executor.heartbeatInterval] configuration property for the RPC timeout. NOTE: A Heartbeat message contains the executor identifier, the accumulator updates, and the identifier of the storage:BlockManager.md[]. If the response (from < HeartbeatReceiver endpoint>>) is to re-register the BlockManager , you should see the following INFO message in the logs and reportHeartBeat requests the BlockManager to storage:BlockManager.md#reregister[re-register] (which will register the blocks the BlockManager manages with the driver). [source,plaintext] \u00b6 Told to re-register on heartbeat \u00b6 HeartbeatResponse requests the BlockManager to re-register when either scheduler:TaskScheduler.md#executorHeartbeatReceived[TaskScheduler] or HeartbeatReceiver know nothing about the executor. When posting the Heartbeat was successful, reportHeartBeat resets < > internal counter. In case of a non-fatal exception, you should see the following WARN message in the logs (followed by the stack trace). Issue communicating with driver in heartbeater Every failure reportHeartBeat increments < > up to configuration-properties.md#spark.executor.heartbeat.maxFailures[spark.executor.heartbeat.maxFailures] configuration property. When the heartbeat failures reaches the maximum, you should see the following ERROR message in the logs and the executor terminates with the error code: 56 . Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times reportHeartBeat is used when Executor is requested to < > (that happens every configuration-properties.md#spark.executor.heartbeatInterval[spark.executor.heartbeatInterval]). == [[startDriverHeartbeater]][[heartbeats-and-active-task-metrics]] Sending Heartbeats and Active Tasks Metrics Executors keep sending < > to the driver every < > (defaults to 10s with some random initial delay so the heartbeats from different executors do not pile up on the driver). .Executors use HeartbeatReceiver endpoint to report task metrics image::executor-heartbeatReceiver-endpoint.png[align=\"center\"] An executor sends heartbeats using the < >. .HeartbeatReceiver's Heartbeat Message Handler image::HeartbeatReceiver-Heartbeat.png[align=\"center\"] For each scheduler:Task.md[task] in executor:TaskRunner.md[] (in < > internal registry), the task's metrics are computed (i.e. mergeShuffleReadMetrics and setJvmGCTime ) that become part of the heartbeat (with accumulators). NOTE: Executors track the executor:TaskRunner.md[] that run scheduler:Task.md[tasks]. A executor:TaskRunner.md#run[task might not be assigned to a TaskRunner yet] when the executor sends a heartbeat. A blocking Heartbeat message that holds the executor id, all accumulator updates (per task id), and storage:BlockManagerId.md[] is sent to HeartbeatReceiver RPC endpoint (with < > timeout). If the response requests to reregister BlockManager , you should see the following INFO message in the logs: Told to re-register on heartbeat BlockManager is requested to storage:BlockManager.md#reregister[reregister]. The internal < > counter is reset (i.e. becomes 0 ). If there are any issues with communicating with the driver, you should see the following WARN message in the logs: [source,plaintext] \u00b6 Issue communicating with driver in heartbeater \u00b6 The internal < > is incremented and checked to be less than the < > (i.e. spark.executor.heartbeat.maxFailures Spark property). If the number is greater, the following ERROR is printed out to the logs: Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times The executor exits (using System.exit and exit code 56). == [[internal-properties]] Internal Properties === [[executorSource]] ExecutorSource executor:ExecutorSource.md[] === [[heartbeatFailures]] heartbeatFailures","title":"Executor"},{"location":"executor/Executor/#executor","text":"Executor is a process that is used for executing scheduler:Task.md[tasks]. Executor typically runs for the entire lifetime of a Spark application which is called static allocation of executors (but you could also opt in for dynamic allocation ). Executors are managed by executor:ExecutorBackend.md[executor backends]. Executors < > to the < > on the driver. Executors provide in-memory storage for RDDs that are cached in Spark applications (via storage:BlockManager.md[]). When started, an executor first registers itself with the driver that establishes a communication channel directly to the driver to accept tasks for execution. ../images/executor/executor-taskrunner-executorbackend.png) Executor offers are described by executor id and the host on which an executor runs (see < > in this document). Executors can run multiple tasks over its lifetime, both in parallel and sequentially. They track executor:TaskRunner.md[running tasks] (by their task ids in < > internal registry). Consult < > section. Executors use a < > for < >. Executors send < > (and heartbeats) using the < >. It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster. Executors are described by their id , hostname , environment (as SparkEnv ), and classpath (and, less importantly, and more for internal optimization, whether they run in spark-local:index.md[local] or spark-cluster.md[cluster] mode).","title":"Executor"},{"location":"executor/Executor/#creating-instance","text":"Executor takes the following to be created: Executor ID Host name SparkEnv User-defined jars (default: empty ) isLocal flag (default: false ) Java's UncaughtExceptionHandler (default: SparkUncaughtExceptionHandler ) Resources ( Map[String, ResourceInformation] ) Executor is created when: CoarseGrainedExecutorBackend is requested to handle a RegisteredExecutor message (after having registered with the driver) LocalEndpoint is created","title":"Creating Instance"},{"location":"executor/Executor/#when-created","text":"When created, Executor prints out the following INFO messages to the logs: Starting executor ID [executorId] on host [executorHostname] (only for non-local modes) Executor sets SparkUncaughtExceptionHandler as the default handler invoked when a thread abruptly terminates due to an uncaught exception. (only for non-local modes) Executor requests the BlockManager to initialize (with the Spark application id of the SparkConf ). (only for non-local modes) Executor requests the MetricsSystem to register the ExecutorSource and shuffleMetricsSource of the BlockManager . Executor uses SparkEnv to access the MetricsSystem and BlockManager . Executor creates a task class loader (optionally with REPL support ) and requests the system Serializer to use as the default classloader (for deserializing tasks). Executor starts sending heartbeats with the metrics of active tasks .","title":"When Created"},{"location":"executor/Executor/#fetching-file-and-jar-dependencies","text":"updateDependencies ( newFiles : Map [ String , Long ], newJars : Map [ String , Long ]): Unit updateDependencies fetches missing or outdated extra files (in the given newFiles ). For every name-timestamp pair that...FIXME..., updateDependencies prints out the following INFO message to the logs: Fetching [name] with timestamp [timestamp] updateDependencies fetches missing or outdated extra jars (in the given newJars ). For every name-timestamp pair that...FIXME..., updateDependencies prints out the following INFO message to the logs: Fetching [name] with timestamp [timestamp] updateDependencies fetches the file to the SparkFiles root directory . updateDependencies ...FIXME updateDependencies is used when: TaskRunner is requested to start (and run a task)","title":" Fetching File and Jar Dependencies"},{"location":"executor/Executor/#sparkdrivermaxresultsize","text":"Executor uses the spark.driver.maxResultSize for TaskRunner when requested to run a task (and decide on a serialized task result ).","title":" spark.driver.maxResultSize"},{"location":"executor/Executor/#maximum-size-of-direct-results","text":"Executor uses the minimum of spark.task.maxDirectResultSize and spark.rpc.message.maxSize when TaskRunner is requested to run a task (and decide on the type of a serialized task result ).","title":" Maximum Size of Direct Results"},{"location":"executor/Executor/#logging","text":"Enable ALL logging level for org.apache.spark.executor.Executor logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.executor.Executor=ALL Refer to Logging .","title":"Logging"},{"location":"executor/Executor/#review-me","text":"== [[isLocal]] isLocal Flag Executor is given a isLocal flag when created. This is how the executor knows whether it runs in local or cluster mode. It is disabled by default. The flag is turned on for spark-local:index.md[Spark local] (via spark-local:spark-LocalEndpoint.md[LocalEndpoint]). == [[userClassPath]] User-Defined Jars Executor is given user-defined jars when created. There are no jars defined by default. The jars are specified using configuration-properties.md#spark.executor.extraClassPath[spark.executor.extraClassPath] configuration property (via executor:CoarseGrainedExecutorBackend.md#main[--user-class-path] command-line option of CoarseGrainedExecutorBackend).","title":"Review Me"},{"location":"executor/Executor/#running-tasks-registry","text":"runningTasks : Map [ Long , TaskRunner ] Executor tracks TaskRunners by task IDs.","title":" Running Tasks Registry"},{"location":"executor/Executor/#heartbeatreceiver-rpc-endpoint-reference","text":"RPC endpoint reference to HeartbeatReceiver on the driver . Set when Executor < >. Used exclusively when Executor < > (that happens every < > interval). == [[launchTask]] Launching Task","title":" HeartbeatReceiver RPC Endpoint Reference"},{"location":"executor/Executor/#source-scala","text":"launchTask( context: ExecutorBackend, taskDescription: TaskDescription): Unit launchTask simply creates a executor:TaskRunner.md[] (with the given executor:ExecutorBackend.md[] and the TaskDescription ) and adds it to the < > internal registry. In the end, launchTask requests the < > to execute the TaskRunner (sometime in the future). .Launching tasks on executor using TaskRunners image::executor-taskrunner-executorbackend.png[align=\"center\"] launchTask is used when: CoarseGrainedExecutorBackend is requested to executor:CoarseGrainedExecutorBackend.md#LaunchTask[handle a LaunchTask message] LocalEndpoint RPC endpoint (of spark-local:spark-LocalSchedulerBackend.md#[LocalSchedulerBackend]) is requested to spark-local:spark-LocalEndpoint.md#reviveOffers[reviveOffers] MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#launchTask[launchTask] == [[heartbeater]] Heartbeat Sender Thread heartbeater is a daemon {java-javadoc-url}/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor] with a single thread. The name of the thread pool is driver-heartbeater . == [[coarse-grained-executor]] Coarse-Grained Executors Coarse-grained executors are executors that use executor:CoarseGrainedExecutorBackend.md[] for task scheduling. == [[resource-offers]] Resource Offers Read scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers] in TaskSchedulerImpl and scheduler:TaskSetManager.md#resourceOffers[resourceOffer] in TaskSetManager. == [[threadPool]] Executor task launch worker Thread Pool Executor uses threadPool daemon cached thread pool with the name Executor task launch worker-[ID] (with ID being the task id) for < >. threadPool is created when < > and shut down when < >. == [[memory]] Executor Memory You can control the amount of memory per executor using configuration-properties.md#spark.executor.memory[spark.executor.memory] configuration property. It sets the available memory equally for all executors per application. The amount of memory per executor is looked up when SparkContext.md#creating-instance[SparkContext is created]. You can change the assigned memory per executor per node in spark-standalone:index.md[standalone cluster] using SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable. You can find the value displayed as Memory per Node in spark-standalone:Master.md[web UI for standalone Master] (as depicted in the figure below). .Memory per Node in Spark Standalone's web UI image::spark-standalone-webui-memory-per-node.png[align=\"center\"] The above figure shows the result of running tools:spark-shell.md[Spark shell] with the amount of memory per executor defined explicitly (on command line), i.e. ./bin/spark-shell --master spark://localhost:7077 -c spark.executor.memory=2g","title":"[source, scala]"},{"location":"executor/Executor/#metrics","text":"Every executor registers its own executor:ExecutorSource.md[] to report metrics . == [[stop]] Stopping Executor","title":"Metrics"},{"location":"executor/Executor/#source-scala_1","text":"","title":"[source, scala]"},{"location":"executor/Executor/#stop-unit","text":"stop requests core:SparkEnv.md#metricsSystem[MetricsSystem] for a report . stop shuts < > down (and waits at most 10 seconds). stop shuts < > down. (only when < >) stop core:SparkEnv.md#stop[requests SparkEnv to stop]. stop is used when executor:CoarseGrainedExecutorBackend.md#Shutdown[CoarseGrainedExecutorBackend] and spark-local:spark-LocalEndpoint.md#StopExecutor[LocalEndpoint] are requested to stop their managed executors. == [[computeTotalGcTime]] computeTotalGcTime Method","title":"stop(): Unit"},{"location":"executor/Executor/#source-scala_2","text":"","title":"[source, scala]"},{"location":"executor/Executor/#computetotalgctime-long","text":"computeTotalGcTime...FIXME computeTotalGcTime is used when: TaskRunner is requested to executor:TaskRunner.md#collectAccumulatorsAndResetStatusOnFailure[collectAccumulatorsAndResetStatusOnFailure] and executor:TaskRunner.md#run[run] Executor is requested to < > == [[createClassLoader]] createClassLoader Method","title":"computeTotalGcTime(): Long"},{"location":"executor/Executor/#source-scala_3","text":"","title":"[source, scala]"},{"location":"executor/Executor/#createclassloader-mutableurlclassloader","text":"createClassLoader...FIXME createClassLoader is used when...FIXME == [[addReplClassLoaderIfNeeded]] addReplClassLoaderIfNeeded Method","title":"createClassLoader(): MutableURLClassLoader"},{"location":"executor/Executor/#source-scala_4","text":"addReplClassLoaderIfNeeded( parent: ClassLoader): ClassLoader addReplClassLoaderIfNeeded...FIXME addReplClassLoaderIfNeeded is used when...FIXME == [[reportHeartBeat]] Heartbeating With Partial Metrics For Active Tasks To Driver","title":"[source, scala]"},{"location":"executor/Executor/#source-scala_5","text":"","title":"[source, scala]"},{"location":"executor/Executor/#reportheartbeat-unit","text":"reportHeartBeat collects executor:TaskRunner.md[TaskRunners] for < > (aka active tasks ) with their executor:TaskRunner.md#task[tasks] deserialized (i.e. either ready for execution or already started). executor:TaskRunner.md[] has TaskRunner.md#task[task] deserialized when it executor:TaskRunner.md#run[runs the task]. For every running task, reportHeartBeat takes its scheduler:Task.md#metrics[TaskMetrics] and: Requests executor:TaskMetrics.md#mergeShuffleReadMetrics[ShuffleRead metrics to be merged] executor:TaskMetrics.md#setJvmGCTime[Sets jvmGCTime metrics] reportHeartBeat then records the latest values of executor:TaskMetrics.md#accumulators[internal and external accumulators] for every task. NOTE: Internal accumulators are a task's metrics while external accumulators are a Spark application's accumulators that a user has created. reportHeartBeat sends a blocking Heartbeat message to < HeartbeatReceiver endpoint>> (running on the driver). reportHeartBeat uses the value of configuration-properties.md#spark.executor.heartbeatInterval[spark.executor.heartbeatInterval] configuration property for the RPC timeout. NOTE: A Heartbeat message contains the executor identifier, the accumulator updates, and the identifier of the storage:BlockManager.md[]. If the response (from < HeartbeatReceiver endpoint>>) is to re-register the BlockManager , you should see the following INFO message in the logs and reportHeartBeat requests the BlockManager to storage:BlockManager.md#reregister[re-register] (which will register the blocks the BlockManager manages with the driver).","title":"reportHeartBeat(): Unit"},{"location":"executor/Executor/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"executor/Executor/#told-to-re-register-on-heartbeat","text":"HeartbeatResponse requests the BlockManager to re-register when either scheduler:TaskScheduler.md#executorHeartbeatReceived[TaskScheduler] or HeartbeatReceiver know nothing about the executor. When posting the Heartbeat was successful, reportHeartBeat resets < > internal counter. In case of a non-fatal exception, you should see the following WARN message in the logs (followed by the stack trace). Issue communicating with driver in heartbeater Every failure reportHeartBeat increments < > up to configuration-properties.md#spark.executor.heartbeat.maxFailures[spark.executor.heartbeat.maxFailures] configuration property. When the heartbeat failures reaches the maximum, you should see the following ERROR message in the logs and the executor terminates with the error code: 56 . Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times reportHeartBeat is used when Executor is requested to < > (that happens every configuration-properties.md#spark.executor.heartbeatInterval[spark.executor.heartbeatInterval]). == [[startDriverHeartbeater]][[heartbeats-and-active-task-metrics]] Sending Heartbeats and Active Tasks Metrics Executors keep sending < > to the driver every < > (defaults to 10s with some random initial delay so the heartbeats from different executors do not pile up on the driver). .Executors use HeartbeatReceiver endpoint to report task metrics image::executor-heartbeatReceiver-endpoint.png[align=\"center\"] An executor sends heartbeats using the < >. .HeartbeatReceiver's Heartbeat Message Handler image::HeartbeatReceiver-Heartbeat.png[align=\"center\"] For each scheduler:Task.md[task] in executor:TaskRunner.md[] (in < > internal registry), the task's metrics are computed (i.e. mergeShuffleReadMetrics and setJvmGCTime ) that become part of the heartbeat (with accumulators). NOTE: Executors track the executor:TaskRunner.md[] that run scheduler:Task.md[tasks]. A executor:TaskRunner.md#run[task might not be assigned to a TaskRunner yet] when the executor sends a heartbeat. A blocking Heartbeat message that holds the executor id, all accumulator updates (per task id), and storage:BlockManagerId.md[] is sent to HeartbeatReceiver RPC endpoint (with < > timeout). If the response requests to reregister BlockManager , you should see the following INFO message in the logs: Told to re-register on heartbeat BlockManager is requested to storage:BlockManager.md#reregister[reregister]. The internal < > counter is reset (i.e. becomes 0 ). If there are any issues with communicating with the driver, you should see the following WARN message in the logs:","title":"Told to re-register on heartbeat"},{"location":"executor/Executor/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"executor/Executor/#issue-communicating-with-driver-in-heartbeater","text":"The internal < > is incremented and checked to be less than the < > (i.e. spark.executor.heartbeat.maxFailures Spark property). If the number is greater, the following ERROR is printed out to the logs: Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times The executor exits (using System.exit and exit code 56). == [[internal-properties]] Internal Properties === [[executorSource]] ExecutorSource executor:ExecutorSource.md[] === [[heartbeatFailures]] heartbeatFailures","title":"Issue communicating with driver in heartbeater"},{"location":"executor/ExecutorBackend/","text":"= ExecutorBackend ExecutorBackend is a < > that executor:TaskRunner.md[TaskRunners] use to < > to a scheduler. .ExecutorBackend receives notifications from TaskRunners image::ExecutorBackend.png[align=\"center\"] NOTE: TaskRunner manages a single individual scheduler:Task.md[task] and is managed by an executor:Executor.md#launchTask[ Executor to launch a task]. It is effectively a bridge between the driver and an executor, i.e. there are two endpoints running. There are three concrete executor backends: executor:CoarseGrainedExecutorBackend.md[] spark-local:spark-LocalSchedulerBackend.md[] (for spark-local:index.md[Spark local]) spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md[] == [[contract]] ExecutorBackend Contract === [[statusUpdate]] statusUpdate Method [source, scala] \u00b6 statusUpdate( taskId: Long, state: TaskState, data: ByteBuffer): Unit Used when TaskRunner is requested to executor:TaskRunner.md#run[run a task] (to send task status updates).","title":"ExecutorBackend"},{"location":"executor/ExecutorBackend/#source-scala","text":"statusUpdate( taskId: Long, state: TaskState, data: ByteBuffer): Unit Used when TaskRunner is requested to executor:TaskRunner.md#run[run a task] (to send task status updates).","title":"[source, scala]"},{"location":"executor/ExecutorLogUrlHandler/","text":"ExecutorLogUrlHandler \u00b6 Creating Instance \u00b6 ExecutorLogUrlHandler takes the following to be created: Optional Log URL Pattern ExecutorLogUrlHandler is created for the following: DriverEndpoint HistoryAppStatusStore Applying Pattern \u00b6 applyPattern ( logUrls : Map [ String , String ], attributes : Map [ String , String ]): Map [ String , String ] applyPattern doApplyPattern for logUrlPattern defined or simply returns the given logUrls back. applyPattern is used when: DriverEndpoint is requested to handle a RegisterExecutor message (and creates a ExecutorData ) HistoryAppStatusStore is requested to replaceLogUrls doApplyPattern \u00b6 doApplyPattern ( logUrls : Map [ String , String ], attributes : Map [ String , String ], urlPattern : String ): Map [ String , String ] doApplyPattern ...FIXME","title":"ExecutorLogUrlHandler"},{"location":"executor/ExecutorLogUrlHandler/#executorlogurlhandler","text":"","title":"ExecutorLogUrlHandler"},{"location":"executor/ExecutorLogUrlHandler/#creating-instance","text":"ExecutorLogUrlHandler takes the following to be created: Optional Log URL Pattern ExecutorLogUrlHandler is created for the following: DriverEndpoint HistoryAppStatusStore","title":"Creating Instance"},{"location":"executor/ExecutorLogUrlHandler/#applying-pattern","text":"applyPattern ( logUrls : Map [ String , String ], attributes : Map [ String , String ]): Map [ String , String ] applyPattern doApplyPattern for logUrlPattern defined or simply returns the given logUrls back. applyPattern is used when: DriverEndpoint is requested to handle a RegisterExecutor message (and creates a ExecutorData ) HistoryAppStatusStore is requested to replaceLogUrls","title":" Applying Pattern"},{"location":"executor/ExecutorLogUrlHandler/#doapplypattern","text":"doApplyPattern ( logUrls : Map [ String , String ], attributes : Map [ String , String ], urlPattern : String ): Map [ String , String ] doApplyPattern ...FIXME","title":" doApplyPattern"},{"location":"executor/ExecutorMetricsPoller/","text":"ExecutorMetricsPoller \u00b6 ExecutorMetricsPoller is...FIXME","title":"ExecutorMetricsPoller"},{"location":"executor/ExecutorMetricsPoller/#executormetricspoller","text":"ExecutorMetricsPoller is...FIXME","title":"ExecutorMetricsPoller"},{"location":"executor/ExecutorSource/","text":"ExecutorSource \u00b6 ExecutorSource is a Source of Executor s. Creating Instance \u00b6 ExecutorSource takes the following to be created: ThreadPoolExecutor Executor ID ( unused ) File System Schemes (to report based on spark.executor.metrics.fileSystemSchemes ) ExecutorSource is created when: Executor is created Name \u00b6 ExecutorSource is known under the name executor . Metrics \u00b6 metricRegistry : MetricRegistry metricRegistry is part of the Source abstraction. Name Description threadpool.activeTasks Approximate number of threads that are actively executing tasks (based on ThreadPoolExecutor.getActiveCount ) others","title":"ExecutorSource"},{"location":"executor/ExecutorSource/#executorsource","text":"ExecutorSource is a Source of Executor s.","title":"ExecutorSource"},{"location":"executor/ExecutorSource/#creating-instance","text":"ExecutorSource takes the following to be created: ThreadPoolExecutor Executor ID ( unused ) File System Schemes (to report based on spark.executor.metrics.fileSystemSchemes ) ExecutorSource is created when: Executor is created","title":"Creating Instance"},{"location":"executor/ExecutorSource/#name","text":"ExecutorSource is known under the name executor .","title":" Name"},{"location":"executor/ExecutorSource/#metrics","text":"metricRegistry : MetricRegistry metricRegistry is part of the Source abstraction. Name Description threadpool.activeTasks Approximate number of threads that are actively executing tasks (based on ThreadPoolExecutor.getActiveCount ) others","title":" Metrics"},{"location":"executor/ShuffleReadMetrics/","text":"ShuffleReadMetrics \u00b6 ShuffleReadMetrics is a collection of metrics (accumulators) on reading shuffle data. TaskMetrics \u00b6 ShuffleReadMetrics is available using TaskMetrics.shuffleReadMetrics . Serializable \u00b6 ShuffleReadMetrics is a Serializable ( Java ).","title":"ShuffleReadMetrics"},{"location":"executor/ShuffleReadMetrics/#shufflereadmetrics","text":"ShuffleReadMetrics is a collection of metrics (accumulators) on reading shuffle data.","title":"ShuffleReadMetrics"},{"location":"executor/ShuffleReadMetrics/#taskmetrics","text":"ShuffleReadMetrics is available using TaskMetrics.shuffleReadMetrics .","title":" TaskMetrics"},{"location":"executor/ShuffleReadMetrics/#serializable","text":"ShuffleReadMetrics is a Serializable ( Java ).","title":" Serializable"},{"location":"executor/ShuffleWriteMetrics/","text":"ShuffleWriteMetrics \u00b6 ShuffleWriteMetrics is a ShuffleWriteMetricsReporter of metrics ( accumulators ) related to writing shuffle data (in shuffle map tasks): Shuffle Bytes Written Shuffle Write Time Shuffle Records Written Creating Instance \u00b6 ShuffleWriteMetrics takes no input arguments to be created. ShuffleWriteMetrics is created when: TaskMetrics is created ShuffleExternalSorter is requested to writeSortedFile MapIterator (of BytesToBytesMap ) is requested to spill ExternalAppendOnlyMap is created ExternalSorter is requested to spillMemoryIteratorToDisk UnsafeExternalSorter is requested to spill SpillableIterator (of UnsafeExternalSorter ) is requested to spill TaskMetrics \u00b6 ShuffleWriteMetrics is available using TaskMetrics.shuffleWriteMetrics . Serializable \u00b6 ShuffleWriteMetrics is a Serializable ( Java ).","title":"ShuffleWriteMetrics"},{"location":"executor/ShuffleWriteMetrics/#shufflewritemetrics","text":"ShuffleWriteMetrics is a ShuffleWriteMetricsReporter of metrics ( accumulators ) related to writing shuffle data (in shuffle map tasks): Shuffle Bytes Written Shuffle Write Time Shuffle Records Written","title":"ShuffleWriteMetrics"},{"location":"executor/ShuffleWriteMetrics/#creating-instance","text":"ShuffleWriteMetrics takes no input arguments to be created. ShuffleWriteMetrics is created when: TaskMetrics is created ShuffleExternalSorter is requested to writeSortedFile MapIterator (of BytesToBytesMap ) is requested to spill ExternalAppendOnlyMap is created ExternalSorter is requested to spillMemoryIteratorToDisk UnsafeExternalSorter is requested to spill SpillableIterator (of UnsafeExternalSorter ) is requested to spill","title":"Creating Instance"},{"location":"executor/ShuffleWriteMetrics/#taskmetrics","text":"ShuffleWriteMetrics is available using TaskMetrics.shuffleWriteMetrics .","title":" TaskMetrics"},{"location":"executor/ShuffleWriteMetrics/#serializable","text":"ShuffleWriteMetrics is a Serializable ( Java ).","title":" Serializable"},{"location":"executor/TaskMetrics/","text":"TaskMetrics \u00b6 TaskMetrics is a collection of metrics ( accumulators ) tracked during execution of a task . Creating Instance \u00b6 TaskMetrics takes no input arguments to be created. TaskMetrics is created when: Stage is requested to makeNewStageAttempt Metrics \u00b6 ShuffleWriteMetrics \u00b6 ShuffleWriteMetrics shuffle.write.bytesWritten shuffle.write.recordsWritten shuffle.write.writeTime ShuffleWriteMetrics is exposed using Dropwizard metrics system using ExecutorSource (when TaskRunner is about to finish running ): shuffleBytesWritten shuffleRecordsWritten shuffleWriteTime ShuffleWriteMetrics can be monitored using: StatsReportListener (when a stage completes ) shuffle bytes written JsonProtocol (when requested to taskMetricsToJson ) Shuffle Bytes Written Shuffle Write Time Shuffle Records Written shuffleWriteMetrics is used when: ShuffleWriteProcessor is requested for a ShuffleWriteMetricsReporter SortShuffleWriter is created AppStatusListener is requested to handle a SparkListenerTaskEnd LiveTask is requested to updateMetrics ExternalSorter is requested to writePartitionedFile (to create a DiskBlockObjectWriter ), writePartitionedMapOutput ShuffleExchangeExec ( Spark SQL ) is requested for a ShuffleWriteProcessor (to create a ShuffleDependency ) Memory Bytes Spilled \u00b6 memoryBytesSpilled metric memoryBytesSpilled is exposed using Dropwizard metrics system using ExecutorSource as memoryBytesSpilled . incMemoryBytesSpilled is used when: Aggregator is requested to updateMetrics CoGroupedRDD is requested to compute BlockStoreShuffleReader is requested to read ShuffleExternalSorter is requested to spill ExternalSorter is requested to writePartitionedFile and writePartitionedMapOutput UnsafeExternalSorter is requested to createWithExistingInMemorySorter and spill SpillableIterator (of UnsafeExternalSorter ) is requested to spill TaskContext \u00b6 TaskMetrics is available using TaskContext.taskMetrics . TaskContext . get . taskMetrics Serializable \u00b6 TaskMetrics is a Serializable ( Java ). Task \u00b6 TaskMetrics is part of Task . task . metrics SparkListener \u00b6 TaskMetrics is available using SparkListener and intercepting SparkListenerTaskEnd events. StatsReportListener \u00b6 StatsReportListener can be used for summary statistics at runtime (after a stage completes). Spark History Server \u00b6 Spark History Server uses EventLoggingListener to intercept post-execution statistics (incl. TaskMetrics ).","title":"TaskMetrics"},{"location":"executor/TaskMetrics/#taskmetrics","text":"TaskMetrics is a collection of metrics ( accumulators ) tracked during execution of a task .","title":"TaskMetrics"},{"location":"executor/TaskMetrics/#creating-instance","text":"TaskMetrics takes no input arguments to be created. TaskMetrics is created when: Stage is requested to makeNewStageAttempt","title":"Creating Instance"},{"location":"executor/TaskMetrics/#metrics","text":"","title":"Metrics"},{"location":"executor/TaskMetrics/#shufflewritemetrics","text":"ShuffleWriteMetrics shuffle.write.bytesWritten shuffle.write.recordsWritten shuffle.write.writeTime ShuffleWriteMetrics is exposed using Dropwizard metrics system using ExecutorSource (when TaskRunner is about to finish running ): shuffleBytesWritten shuffleRecordsWritten shuffleWriteTime ShuffleWriteMetrics can be monitored using: StatsReportListener (when a stage completes ) shuffle bytes written JsonProtocol (when requested to taskMetricsToJson ) Shuffle Bytes Written Shuffle Write Time Shuffle Records Written shuffleWriteMetrics is used when: ShuffleWriteProcessor is requested for a ShuffleWriteMetricsReporter SortShuffleWriter is created AppStatusListener is requested to handle a SparkListenerTaskEnd LiveTask is requested to updateMetrics ExternalSorter is requested to writePartitionedFile (to create a DiskBlockObjectWriter ), writePartitionedMapOutput ShuffleExchangeExec ( Spark SQL ) is requested for a ShuffleWriteProcessor (to create a ShuffleDependency )","title":" ShuffleWriteMetrics"},{"location":"executor/TaskMetrics/#memory-bytes-spilled","text":"memoryBytesSpilled metric memoryBytesSpilled is exposed using Dropwizard metrics system using ExecutorSource as memoryBytesSpilled . incMemoryBytesSpilled is used when: Aggregator is requested to updateMetrics CoGroupedRDD is requested to compute BlockStoreShuffleReader is requested to read ShuffleExternalSorter is requested to spill ExternalSorter is requested to writePartitionedFile and writePartitionedMapOutput UnsafeExternalSorter is requested to createWithExistingInMemorySorter and spill SpillableIterator (of UnsafeExternalSorter ) is requested to spill","title":" Memory Bytes Spilled"},{"location":"executor/TaskMetrics/#taskcontext","text":"TaskMetrics is available using TaskContext.taskMetrics . TaskContext . get . taskMetrics","title":" TaskContext"},{"location":"executor/TaskMetrics/#serializable","text":"TaskMetrics is a Serializable ( Java ).","title":" Serializable"},{"location":"executor/TaskMetrics/#task","text":"TaskMetrics is part of Task . task . metrics","title":" Task"},{"location":"executor/TaskMetrics/#sparklistener","text":"TaskMetrics is available using SparkListener and intercepting SparkListenerTaskEnd events.","title":" SparkListener"},{"location":"executor/TaskMetrics/#statsreportlistener","text":"StatsReportListener can be used for summary statistics at runtime (after a stage completes).","title":" StatsReportListener"},{"location":"executor/TaskMetrics/#spark-history-server","text":"Spark History Server uses EventLoggingListener to intercept post-execution statistics (incl. TaskMetrics ).","title":"Spark History Server"},{"location":"executor/TaskRunner/","text":"TaskRunner \u00b6 TaskRunner is a thread of execution to run a task . Internal Class TaskRunner is an internal class of Executor with full access to internal registries. TaskRunner is a java.lang.Runnable so once a TaskRunner has completed execution it must not be restarted. Creating Instance \u00b6 TaskRunner takes the following to be created: ExecutorBackend (that manages the parent Executor ) TaskDescription TaskRunner is created when: Executor is requested to launch a task Demo \u00b6 ./bin/spark-shell --conf spark.driver.maxResultSize=1m scala> println(sc.version) 3.0.1 val maxResultSize = sc.getConf.get(\"spark.driver.maxResultSize\") assert(maxResultSize == \"1m\") val rddOver1m = sc.range(0, 1024 * 1024 + 10, 1) scala> rddOver1m.collect ERROR TaskSetManager: Total size of serialized results of 2 tasks (1030.8 KiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) ERROR TaskSetManager: Total size of serialized results of 3 tasks (1546.2 KiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) ERROR TaskSetManager: Total size of serialized results of 4 tasks (2.0 MiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7, 192.168.68.105, executor driver): TaskKilled (Tasks result size has exceeded maxResultSize) WARN TaskSetManager: Lost task 5.0 in stage 0.0 (TID 5, 192.168.68.105, executor driver): TaskKilled (Tasks result size has exceeded maxResultSize) WARN TaskSetManager: Lost task 12.0 in stage 0.0 (TID 12, 192.168.68.105, executor driver): TaskKilled (Tasks result size has exceeded maxResultSize) ERROR TaskSetManager: Total size of serialized results of 5 tasks (2.5 MiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) WARN TaskSetManager: Lost task 8.0 in stage 0.0 (TID 8, 192.168.68.105, executor driver): TaskKilled (Tasks result size has exceeded maxResultSize) ... org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 2 tasks (1030.8 KiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059) ... Thread Name \u00b6 TaskRunner uses the following thread name (with the taskId of the TaskDescription ): Executor task launch worker for task [taskId] Running Task \u00b6 run (): Unit run is part of the java.lang.Runnable abstraction. Initialization \u00b6 run initializes the threadId internal registry as the current thread identifier (using Thread.getId ). run sets the name of the current thread of execution as the threadName . run creates a TaskMemoryManager (for the current MemoryManager and taskId ). run uses SparkEnv to access the current MemoryManager . run starts tracking the time to deserialize a task and sets the current thread's context classloader. run creates a closure Serializer . run uses SparkEnv to access the closure Serializer . run prints out the following INFO message to the logs (with the taskName and taskId ): Running [taskName] (TID [taskId]) run notifies the ExecutorBackend that the status of the task has changed to RUNNING (for the taskId ). run computes the total amount of time this JVM process has spent in garbage collection . run uses the addedFiles and addedJars (of the given TaskDescription ) to update dependencies . run takes the serializedTask of the given TaskDescription and requests the closure Serializer to deserialize the task . run sets the task internal reference to hold the deserialized task. For non-local environments, run prints out the following DEBUG message to the logs before requesting the MapOutputTrackerWorker to update the epoch (using the epoch of the Task to be executed). run uses SparkEnv to access the MapOutputTrackerWorker . Task [taskId]'s epoch is [epoch] run requests the metricsPoller ...FIXME run records the current time as the task's start time ( taskStartTimeNs ). run requests the Task to run (with taskAttemptId as taskId , attemptNumber from TaskDescription , and metricsSystem as the current MetricsSystem ). Note run uses SparkEnv to access the MetricsSystem . Note The task runs inside a \"monitored\" block ( try-finally block) to detect any memory and lock leaks after the task's run finishes regardless of the final outcome - the computed value or an exception thrown. run creates a Serializer and requests it to serialize the task result ( valueBytes ). Note run uses SparkEnv to access the Serializer . run updates the metrics of the Task executed. run updates the metric counters in the ExecutorSource . run requests the Task executed for accumulator updates and the ExecutorMetricsPoller for metric peaks . Serialized Task Result \u00b6 run creates a DirectTaskResult (with the serialized task result, the accumulator updates and the metric peaks) and requests the closure Serializer to serialize it . Note The serialized DirectTaskResult is a java.nio.ByteBuffer . run selects between the DirectTaskResult and an IndirectTaskResult based on the size of the serialized task result ( limit of this serializedDirectResult byte buffer): With the size above spark.driver.maxResultSize , run prints out the following WARN message to the logs and serializes an IndirectTaskResult with a TaskResultBlockId . Finished [taskName] (TID [taskId]). Result is larger than maxResultSize ([resultSize] > [maxResultSize]), dropping it. With the size above maxDirectResultSize , run creates an TaskResultBlockId and requests the BlockManager to store the task result locally (with MEMORY_AND_DISK_SER ). run prints out the following INFO message to the logs and serializes an IndirectTaskResult with a TaskResultBlockId . Finished [taskName] (TID [taskId]). [resultSize] bytes result sent via BlockManager) run prints out the following INFO message to the logs and uses the DirectTaskResult created earlier. Finished [taskName] (TID [taskId]). [resultSize] bytes result sent to driver Note serializedResult is either a IndirectTaskResult (possibly with the block stored in BlockManager ) or a DirectTaskResult . Incrementing succeededTasks Counter \u00b6 run requests the ExecutorSource to increment succeededTasks counter. Marking Task Finished \u00b6 run setTaskFinishedAndClearInterruptStatus . Notifying ExecutorBackend that Task Finished \u00b6 run notifies the ExecutorBackend that the status of the taskId has changed to FINISHED . Note ExecutorBackend is given when the TaskRunner is created. Wrapping Up \u00b6 In the end, regardless of the task's execution status (successful or failed), run removes the taskId from runningTasks registry. In case a onTaskStart notification was sent out, run requests the ExecutorMetricsPoller to onTaskCompletion . Exceptions \u00b6 run handles certain exceptions. Exception Type TaskState Serialized ByteBuffer FetchFailedException FAILED TaskFailedReason TaskKilledException KILLED TaskKilled InterruptedException KILLED TaskKilled CommitDeniedException FAILED TaskFailedReason Throwable FAILED ExceptionFailure FetchFailedException \u00b6 When shuffle:FetchFailedException.md[FetchFailedException] is reported while running a task, run < >. run shuffle:FetchFailedException.md#toTaskFailedReason[requests FetchFailedException for the TaskFailedReason ], serializes it and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and a serialized reason). NOTE: ExecutorBackend was specified when < >. NOTE: run uses a closure serializer:Serializer.md[Serializer] to serialize the failure reason. The Serializer was created before run ran the task. TaskKilledException \u00b6 When TaskKilledException is reported while running a task, you should see the following INFO message in the logs: Executor killed [taskName] (TID [taskId]), reason: [reason] run then < > and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has been killed] (with < >, TaskState.KILLED , and a serialized TaskKilled object). InterruptedException (with Task Killed) \u00b6 When InterruptedException is reported while running a task, and the task has been killed, you should see the following INFO message in the logs: Executor interrupted and killed [taskName] (TID [taskId]), reason: [killReason] run then < > and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has been killed] (with < >, TaskState.KILLED , and a serialized TaskKilled object). NOTE: The difference between this InterruptedException and < > is the INFO message in the logs. CommitDeniedException \u00b6 When CommitDeniedException is reported while running a task, run < > and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and a serialized TaskKilled object). NOTE: The difference between this CommitDeniedException and < > is just the reason being sent to ExecutorBackend . Throwable \u00b6 When run catches a Throwable , you should see the following ERROR message in the logs (followed by the exception). Exception in [taskName] (TID [taskId]) run then records the following task metrics (only when < > is available): TaskMetrics.md#setExecutorRunTime[executorRunTime] TaskMetrics.md#setJvmGCTime[jvmGCTime] run then scheduler:Task.md#collectAccumulatorUpdates[collects the latest values of internal and external accumulators] (with taskFailed flag enabled to inform that the collection is for a failed task). Otherwise, when < > is not available, the accumulator collection is empty. run converts the task accumulators to collection of AccumulableInfo , creates a ExceptionFailure (with the accumulators), and serializer:Serializer.md#serialize[serializes them]. NOTE: run uses a closure serializer:Serializer.md[Serializer] to serialize the ExceptionFailure . CAUTION: FIXME Why does run create new ExceptionFailure(t, accUpdates).withAccums(accums) , i.e. accumulators occur twice in the object. run < > and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and the serialized ExceptionFailure ). run may also trigger SparkUncaughtExceptionHandler.uncaughtException(t) if this is a fatal error. NOTE: The difference between this most Throwable case and other FAILED cases (i.e. < > and < >) is just the serialized ExceptionFailure vs a reason being sent to ExecutorBackend , respectively. collectAccumulatorsAndResetStatusOnFailure \u00b6 collectAccumulatorsAndResetStatusOnFailure ( taskStartTimeNs : Long ) collectAccumulatorsAndResetStatusOnFailure ...FIXME Killing Task \u00b6 kill ( interruptThread : Boolean , reason : String ): Unit kill marks the TaskRunner as < > and scheduler:Task.md#kill[kills the task] (if available and not < > already). NOTE: kill passes the input interruptThread on to the task itself while killing it. When executed, you should see the following INFO message in the logs: Executor is trying to kill [taskName] (TID [taskId]), reason: [reason] NOTE: < > flag is checked periodically in < > to stop executing the task. Once killed, the task will eventually stop. Logging \u00b6 Enable ALL logging level for org.apache.spark.executor.Executor logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.executor.Executor=ALL Refer to Logging . Internal Properties \u00b6 finished Flag \u00b6 finished flag says whether the < > has finished ( true ) or not ( false ) Default: false Enabled ( true ) after TaskRunner has been requested to < > Used when TaskRunner is requested to < > reasonIfKilled \u00b6 Reason to < > (and avoid < >) Default: (empty) ( None ) startGCTime Timestamp \u00b6 Timestamp (which is really the Executor.md#computeTotalGcTime[total amount of time this Executor JVM process has already spent in garbage collection]) that is used to mark the GC \"zero\" time (when < >) and then compute the JVM GC time metric when: TaskRunner is requested to < > and < > Executor is requested to Executor.md#reportHeartBeat[reportHeartBeat] Task \u00b6 Deserialized scheduler:Task.md[task] to execute Used when: TaskRunner is requested to < >, < >, < >, < > Executor is requested to Executor.md#reportHeartBeat[reportHeartBeat] Task Name \u00b6 The name of the task (of the TaskDescription ) that is used exclusively for < > purposes when TaskRunner is requested to < > and < > the task Thread Id \u00b6 Current thread ID Default: -1 Set immediately when TaskRunner is requested to < > and used exclusively when TaskReaper is requested for the thread info of the current thread (aka thread dump )","title":"TaskRunner"},{"location":"executor/TaskRunner/#taskrunner","text":"TaskRunner is a thread of execution to run a task . Internal Class TaskRunner is an internal class of Executor with full access to internal registries. TaskRunner is a java.lang.Runnable so once a TaskRunner has completed execution it must not be restarted.","title":"TaskRunner"},{"location":"executor/TaskRunner/#creating-instance","text":"TaskRunner takes the following to be created: ExecutorBackend (that manages the parent Executor ) TaskDescription TaskRunner is created when: Executor is requested to launch a task","title":"Creating Instance"},{"location":"executor/TaskRunner/#demo","text":"./bin/spark-shell --conf spark.driver.maxResultSize=1m scala> println(sc.version) 3.0.1 val maxResultSize = sc.getConf.get(\"spark.driver.maxResultSize\") assert(maxResultSize == \"1m\") val rddOver1m = sc.range(0, 1024 * 1024 + 10, 1) scala> rddOver1m.collect ERROR TaskSetManager: Total size of serialized results of 2 tasks (1030.8 KiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) ERROR TaskSetManager: Total size of serialized results of 3 tasks (1546.2 KiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) ERROR TaskSetManager: Total size of serialized results of 4 tasks (2.0 MiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7, 192.168.68.105, executor driver): TaskKilled (Tasks result size has exceeded maxResultSize) WARN TaskSetManager: Lost task 5.0 in stage 0.0 (TID 5, 192.168.68.105, executor driver): TaskKilled (Tasks result size has exceeded maxResultSize) WARN TaskSetManager: Lost task 12.0 in stage 0.0 (TID 12, 192.168.68.105, executor driver): TaskKilled (Tasks result size has exceeded maxResultSize) ERROR TaskSetManager: Total size of serialized results of 5 tasks (2.5 MiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) WARN TaskSetManager: Lost task 8.0 in stage 0.0 (TID 8, 192.168.68.105, executor driver): TaskKilled (Tasks result size has exceeded maxResultSize) ... org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 2 tasks (1030.8 KiB) is bigger than spark.driver.maxResultSize (1024.0 KiB) at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059) ...","title":"Demo"},{"location":"executor/TaskRunner/#thread-name","text":"TaskRunner uses the following thread name (with the taskId of the TaskDescription ): Executor task launch worker for task [taskId]","title":" Thread Name"},{"location":"executor/TaskRunner/#running-task","text":"run (): Unit run is part of the java.lang.Runnable abstraction.","title":" Running Task"},{"location":"executor/TaskRunner/#initialization","text":"run initializes the threadId internal registry as the current thread identifier (using Thread.getId ). run sets the name of the current thread of execution as the threadName . run creates a TaskMemoryManager (for the current MemoryManager and taskId ). run uses SparkEnv to access the current MemoryManager . run starts tracking the time to deserialize a task and sets the current thread's context classloader. run creates a closure Serializer . run uses SparkEnv to access the closure Serializer . run prints out the following INFO message to the logs (with the taskName and taskId ): Running [taskName] (TID [taskId]) run notifies the ExecutorBackend that the status of the task has changed to RUNNING (for the taskId ). run computes the total amount of time this JVM process has spent in garbage collection . run uses the addedFiles and addedJars (of the given TaskDescription ) to update dependencies . run takes the serializedTask of the given TaskDescription and requests the closure Serializer to deserialize the task . run sets the task internal reference to hold the deserialized task. For non-local environments, run prints out the following DEBUG message to the logs before requesting the MapOutputTrackerWorker to update the epoch (using the epoch of the Task to be executed). run uses SparkEnv to access the MapOutputTrackerWorker . Task [taskId]'s epoch is [epoch] run requests the metricsPoller ...FIXME run records the current time as the task's start time ( taskStartTimeNs ). run requests the Task to run (with taskAttemptId as taskId , attemptNumber from TaskDescription , and metricsSystem as the current MetricsSystem ). Note run uses SparkEnv to access the MetricsSystem . Note The task runs inside a \"monitored\" block ( try-finally block) to detect any memory and lock leaks after the task's run finishes regardless of the final outcome - the computed value or an exception thrown. run creates a Serializer and requests it to serialize the task result ( valueBytes ). Note run uses SparkEnv to access the Serializer . run updates the metrics of the Task executed. run updates the metric counters in the ExecutorSource . run requests the Task executed for accumulator updates and the ExecutorMetricsPoller for metric peaks .","title":" Initialization"},{"location":"executor/TaskRunner/#serialized-task-result","text":"run creates a DirectTaskResult (with the serialized task result, the accumulator updates and the metric peaks) and requests the closure Serializer to serialize it . Note The serialized DirectTaskResult is a java.nio.ByteBuffer . run selects between the DirectTaskResult and an IndirectTaskResult based on the size of the serialized task result ( limit of this serializedDirectResult byte buffer): With the size above spark.driver.maxResultSize , run prints out the following WARN message to the logs and serializes an IndirectTaskResult with a TaskResultBlockId . Finished [taskName] (TID [taskId]). Result is larger than maxResultSize ([resultSize] > [maxResultSize]), dropping it. With the size above maxDirectResultSize , run creates an TaskResultBlockId and requests the BlockManager to store the task result locally (with MEMORY_AND_DISK_SER ). run prints out the following INFO message to the logs and serializes an IndirectTaskResult with a TaskResultBlockId . Finished [taskName] (TID [taskId]). [resultSize] bytes result sent via BlockManager) run prints out the following INFO message to the logs and uses the DirectTaskResult created earlier. Finished [taskName] (TID [taskId]). [resultSize] bytes result sent to driver Note serializedResult is either a IndirectTaskResult (possibly with the block stored in BlockManager ) or a DirectTaskResult .","title":" Serialized Task Result"},{"location":"executor/TaskRunner/#incrementing-succeededtasks-counter","text":"run requests the ExecutorSource to increment succeededTasks counter.","title":" Incrementing succeededTasks Counter"},{"location":"executor/TaskRunner/#marking-task-finished","text":"run setTaskFinishedAndClearInterruptStatus .","title":" Marking Task Finished"},{"location":"executor/TaskRunner/#notifying-executorbackend-that-task-finished","text":"run notifies the ExecutorBackend that the status of the taskId has changed to FINISHED . Note ExecutorBackend is given when the TaskRunner is created.","title":" Notifying ExecutorBackend that Task Finished"},{"location":"executor/TaskRunner/#wrapping-up","text":"In the end, regardless of the task's execution status (successful or failed), run removes the taskId from runningTasks registry. In case a onTaskStart notification was sent out, run requests the ExecutorMetricsPoller to onTaskCompletion .","title":" Wrapping Up"},{"location":"executor/TaskRunner/#exceptions","text":"run handles certain exceptions. Exception Type TaskState Serialized ByteBuffer FetchFailedException FAILED TaskFailedReason TaskKilledException KILLED TaskKilled InterruptedException KILLED TaskKilled CommitDeniedException FAILED TaskFailedReason Throwable FAILED ExceptionFailure","title":" Exceptions"},{"location":"executor/TaskRunner/#fetchfailedexception","text":"When shuffle:FetchFailedException.md[FetchFailedException] is reported while running a task, run < >. run shuffle:FetchFailedException.md#toTaskFailedReason[requests FetchFailedException for the TaskFailedReason ], serializes it and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and a serialized reason). NOTE: ExecutorBackend was specified when < >. NOTE: run uses a closure serializer:Serializer.md[Serializer] to serialize the failure reason. The Serializer was created before run ran the task.","title":" FetchFailedException"},{"location":"executor/TaskRunner/#taskkilledexception","text":"When TaskKilledException is reported while running a task, you should see the following INFO message in the logs: Executor killed [taskName] (TID [taskId]), reason: [reason] run then < > and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has been killed] (with < >, TaskState.KILLED , and a serialized TaskKilled object).","title":" TaskKilledException"},{"location":"executor/TaskRunner/#interruptedexception-with-task-killed","text":"When InterruptedException is reported while running a task, and the task has been killed, you should see the following INFO message in the logs: Executor interrupted and killed [taskName] (TID [taskId]), reason: [killReason] run then < > and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has been killed] (with < >, TaskState.KILLED , and a serialized TaskKilled object). NOTE: The difference between this InterruptedException and < > is the INFO message in the logs.","title":" InterruptedException (with Task Killed)"},{"location":"executor/TaskRunner/#commitdeniedexception","text":"When CommitDeniedException is reported while running a task, run < > and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and a serialized TaskKilled object). NOTE: The difference between this CommitDeniedException and < > is just the reason being sent to ExecutorBackend .","title":" CommitDeniedException"},{"location":"executor/TaskRunner/#throwable","text":"When run catches a Throwable , you should see the following ERROR message in the logs (followed by the exception). Exception in [taskName] (TID [taskId]) run then records the following task metrics (only when < > is available): TaskMetrics.md#setExecutorRunTime[executorRunTime] TaskMetrics.md#setJvmGCTime[jvmGCTime] run then scheduler:Task.md#collectAccumulatorUpdates[collects the latest values of internal and external accumulators] (with taskFailed flag enabled to inform that the collection is for a failed task). Otherwise, when < > is not available, the accumulator collection is empty. run converts the task accumulators to collection of AccumulableInfo , creates a ExceptionFailure (with the accumulators), and serializer:Serializer.md#serialize[serializes them]. NOTE: run uses a closure serializer:Serializer.md[Serializer] to serialize the ExceptionFailure . CAUTION: FIXME Why does run create new ExceptionFailure(t, accUpdates).withAccums(accums) , i.e. accumulators occur twice in the object. run < > and ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and the serialized ExceptionFailure ). run may also trigger SparkUncaughtExceptionHandler.uncaughtException(t) if this is a fatal error. NOTE: The difference between this most Throwable case and other FAILED cases (i.e. < > and < >) is just the serialized ExceptionFailure vs a reason being sent to ExecutorBackend , respectively.","title":" Throwable"},{"location":"executor/TaskRunner/#collectaccumulatorsandresetstatusonfailure","text":"collectAccumulatorsAndResetStatusOnFailure ( taskStartTimeNs : Long ) collectAccumulatorsAndResetStatusOnFailure ...FIXME","title":" collectAccumulatorsAndResetStatusOnFailure"},{"location":"executor/TaskRunner/#killing-task","text":"kill ( interruptThread : Boolean , reason : String ): Unit kill marks the TaskRunner as < > and scheduler:Task.md#kill[kills the task] (if available and not < > already). NOTE: kill passes the input interruptThread on to the task itself while killing it. When executed, you should see the following INFO message in the logs: Executor is trying to kill [taskName] (TID [taskId]), reason: [reason] NOTE: < > flag is checked periodically in < > to stop executing the task. Once killed, the task will eventually stop.","title":" Killing Task"},{"location":"executor/TaskRunner/#logging","text":"Enable ALL logging level for org.apache.spark.executor.Executor logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.executor.Executor=ALL Refer to Logging .","title":"Logging"},{"location":"executor/TaskRunner/#internal-properties","text":"","title":"Internal Properties"},{"location":"executor/TaskRunner/#finished-flag","text":"finished flag says whether the < > has finished ( true ) or not ( false ) Default: false Enabled ( true ) after TaskRunner has been requested to < > Used when TaskRunner is requested to < >","title":" finished Flag"},{"location":"executor/TaskRunner/#reasonifkilled","text":"Reason to < > (and avoid < >) Default: (empty) ( None )","title":" reasonIfKilled"},{"location":"executor/TaskRunner/#startgctime-timestamp","text":"Timestamp (which is really the Executor.md#computeTotalGcTime[total amount of time this Executor JVM process has already spent in garbage collection]) that is used to mark the GC \"zero\" time (when < >) and then compute the JVM GC time metric when: TaskRunner is requested to < > and < > Executor is requested to Executor.md#reportHeartBeat[reportHeartBeat]","title":" startGCTime Timestamp"},{"location":"executor/TaskRunner/#task","text":"Deserialized scheduler:Task.md[task] to execute Used when: TaskRunner is requested to < >, < >, < >, < > Executor is requested to Executor.md#reportHeartBeat[reportHeartBeat]","title":" Task"},{"location":"executor/TaskRunner/#task-name","text":"The name of the task (of the TaskDescription ) that is used exclusively for < > purposes when TaskRunner is requested to < > and < > the task","title":" Task Name"},{"location":"executor/TaskRunner/#thread-id","text":"Current thread ID Default: -1 Set immediately when TaskRunner is requested to < > and used exclusively when TaskReaper is requested for the thread info of the current thread (aka thread dump )","title":" Thread Id"},{"location":"exercises/spark-examples-wordcount-spark-shell/","text":"== WordCount using Spark shell It is like any introductory big data example should somehow demonstrate how to count words in distributed fashion. In the following example you're going to count the words in README.md file that sits in your Spark distribution and save the result under README.count directory. You're going to use spark-shell.md[the Spark shell] for the example. Execute spark-shell . [source,scala] \u00b6 val lines = sc.textFile(\"README.md\") // <1> val words = lines.flatMap(_.split(\"\\s+\")) // <2> val wc = words.map(w => (w, 1)).reduceByKey(_ + _) // <3> wc.saveAsTextFile(\"README.count\") // <4> \u00b6 <1> Read the text file - refer to spark-io.md[Using Input and Output (I/O)]. <2> Split each line into words and flatten the result. <3> Map each word into a pair and count them by word (key). <4> Save the result into text files - one per partition. After you have executed the example, see the contents of the README.count directory: $ ls -lt README.count total 16 -rw-r--r-- 1 jacek staff 0 9 pa\u017a 13:36 _SUCCESS -rw-r--r-- 1 jacek staff 1963 9 pa\u017a 13:36 part-00000 -rw-r--r-- 1 jacek staff 1663 9 pa\u017a 13:36 part-00001 The files part-0000x contain the pairs of word and the count. $ cat README.count/part-00000 (package,1) (this,1) (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1) (Because,1) (Python,2) (cluster.,1) (its,1) ([run,1) ... === Further (self-)development Please read the questions and give answers first before looking at the link given. Why are there two files under the directory? How could you have only one? How to filter out words by name? How to count words? Please refer to the chapter spark-rdd-partitions.md[Partitions] to find some of the answers.","title":"WordCount using Spark shell"},{"location":"exercises/spark-examples-wordcount-spark-shell/#sourcescala","text":"val lines = sc.textFile(\"README.md\") // <1> val words = lines.flatMap(_.split(\"\\s+\")) // <2> val wc = words.map(w => (w, 1)).reduceByKey(_ + _) // <3>","title":"[source,scala]"},{"location":"exercises/spark-examples-wordcount-spark-shell/#wcsaveastextfilereadmecount-4","text":"<1> Read the text file - refer to spark-io.md[Using Input and Output (I/O)]. <2> Split each line into words and flatten the result. <3> Map each word into a pair and count them by word (key). <4> Save the result into text files - one per partition. After you have executed the example, see the contents of the README.count directory: $ ls -lt README.count total 16 -rw-r--r-- 1 jacek staff 0 9 pa\u017a 13:36 _SUCCESS -rw-r--r-- 1 jacek staff 1963 9 pa\u017a 13:36 part-00000 -rw-r--r-- 1 jacek staff 1663 9 pa\u017a 13:36 part-00001 The files part-0000x contain the pairs of word and the count. $ cat README.count/part-00000 (package,1) (this,1) (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1) (Because,1) (Python,2) (cluster.,1) (its,1) ([run,1) ... === Further (self-)development Please read the questions and give answers first before looking at the link given. Why are there two files under the directory? How could you have only one? How to filter out words by name? How to count words? Please refer to the chapter spark-rdd-partitions.md[Partitions] to find some of the answers.","title":"wc.saveAsTextFile(\"README.count\")                  // &lt;4&gt;"},{"location":"exercises/spark-exercise-custom-scheduler-listener/","text":"== Exercise: Developing Custom SparkListener to monitor DAGScheduler in Scala The example shows how to develop a custom Spark Listener. You should read SparkListener.md[] first to understand the motivation for the example. === Requirements https://www.jetbrains.com/idea/[IntelliJ IDEA] (or eventually http://www.scala-sbt.org/[sbt ] alone if you're adventurous). Access to Internet to download Apache Spark's dependencies. === Setting up Scala project using IntelliJ IDEA Create a new project custom-spark-listener . Add the following line to build.sbt (the main configuration file for the sbt project) that adds the dependency on Apache Spark. libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.0.1\" build.sbt should look as follows: [source, scala] \u00b6 name := \"custom-spark-listener\" organization := \"pl.jaceklaskowski.spark\" version := \"1.0\" scalaVersion := \"2.11.8\" libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.0.1\" \u00b6 === Custom Listener - pl.jaceklaskowski.spark.CustomSparkListener Create a Scala class -- CustomSparkListener -- for your custom SparkListener . It should be under src/main/scala directory (create one if it does not exist). The aim of the class is to intercept scheduler events about jobs being started and tasks completed. [source,scala] \u00b6 package pl.jaceklaskowski.spark import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListener, SparkListenerJobStart} class CustomSparkListener extends SparkListener { override def onJobStart(jobStart: SparkListenerJobStart) { println(s\"Job started with ${jobStart.stageInfos.size} stages: $jobStart\") } override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = { println(s\"Stage ${stageCompleted.stageInfo.stageId} completed with ${stageCompleted.stageInfo.numTasks} tasks.\") } } === Creating deployable package Package the custom Spark listener. Execute sbt package command in the custom-spark-listener project's main directory. $ sbt package [info] Loading global plugins from /Users/jacek/.sbt/0.13/plugins [info] Loading project definition from /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/project [info] Updating {file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/project/}custom-spark-listener-build... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Set current project to custom-spark-listener (in build file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/) [info] Updating {file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/}custom-spark-listener... [info] Resolving jline#jline;2.12.1 ... [info] Done updating. [info] Compiling 1 Scala source to /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/target/scala-2.11/classes... [info] Packaging /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/target/scala-2.11/custom-spark-listener_2.11-1.0.jar ... [info] Done packaging. [success] Total time: 8 s, completed Oct 27, 2016 11:23:50 AM You should find the result jar file with the custom scheduler listener ready under target/scala-2.11 directory, e.g. target/scala-2.11/custom-spark-listener_2.11-1.0.jar . === Activating Custom Listener in Spark shell Start ../spark-shell.md[spark-shell] with additional configurations for the extra custom listener and the jar that includes the class. $ spark-shell \\ --conf spark.logConf=true \\ --conf spark.extraListeners=pl.jaceklaskowski.spark.CustomSparkListener \\ --driver-class-path target/scala-2.11/custom-spark-listener_2.11-1.0.jar Create a ../spark-sql-Dataset.md#implicits[Dataset] and execute an action like show to start a job as follows: scala> spark.read.text(\"README.md\").count [CustomSparkListener] Job started with 2 stages: SparkListenerJobStart(1,1473946006715,WrappedArray(org.apache.spark.scheduler.StageInfo@71515592, org.apache.spark.scheduler.StageInfo@6852819d),{spark.rdd.scope.noOverride=true, spark.rdd.scope={\"id\":\"14\",\"name\":\"collect\"}, spark.sql.execution.id=2}) [CustomSparkListener] Stage 1 completed with 1 tasks. [CustomSparkListener] Stage 2 completed with 1 tasks. res0: Long = 7 The lines with [CustomSparkListener] came from your custom Spark listener. Congratulations! The exercise's over. === BONUS Activating Custom Listener in Spark Application TIP: Read SparkContext.md#addSparkListener[Registering SparkListener]. === Questions What are the pros and cons of using the command line version vs inside a Spark application?","title":"Developing Custom SparkListener to monitor DAGScheduler in Scala"},{"location":"exercises/spark-exercise-custom-scheduler-listener/#source-scala","text":"name := \"custom-spark-listener\" organization := \"pl.jaceklaskowski.spark\" version := \"1.0\" scalaVersion := \"2.11.8\"","title":"[source, scala]"},{"location":"exercises/spark-exercise-custom-scheduler-listener/#librarydependencies-orgapachespark-spark-core-201","text":"=== Custom Listener - pl.jaceklaskowski.spark.CustomSparkListener Create a Scala class -- CustomSparkListener -- for your custom SparkListener . It should be under src/main/scala directory (create one if it does not exist). The aim of the class is to intercept scheduler events about jobs being started and tasks completed.","title":"libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.0.1\""},{"location":"exercises/spark-exercise-custom-scheduler-listener/#sourcescala","text":"package pl.jaceklaskowski.spark import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListener, SparkListenerJobStart} class CustomSparkListener extends SparkListener { override def onJobStart(jobStart: SparkListenerJobStart) { println(s\"Job started with ${jobStart.stageInfos.size} stages: $jobStart\") } override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = { println(s\"Stage ${stageCompleted.stageInfo.stageId} completed with ${stageCompleted.stageInfo.numTasks} tasks.\") } } === Creating deployable package Package the custom Spark listener. Execute sbt package command in the custom-spark-listener project's main directory. $ sbt package [info] Loading global plugins from /Users/jacek/.sbt/0.13/plugins [info] Loading project definition from /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/project [info] Updating {file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/project/}custom-spark-listener-build... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Set current project to custom-spark-listener (in build file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/) [info] Updating {file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/}custom-spark-listener... [info] Resolving jline#jline;2.12.1 ... [info] Done updating. [info] Compiling 1 Scala source to /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/target/scala-2.11/classes... [info] Packaging /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/target/scala-2.11/custom-spark-listener_2.11-1.0.jar ... [info] Done packaging. [success] Total time: 8 s, completed Oct 27, 2016 11:23:50 AM You should find the result jar file with the custom scheduler listener ready under target/scala-2.11 directory, e.g. target/scala-2.11/custom-spark-listener_2.11-1.0.jar . === Activating Custom Listener in Spark shell Start ../spark-shell.md[spark-shell] with additional configurations for the extra custom listener and the jar that includes the class. $ spark-shell \\ --conf spark.logConf=true \\ --conf spark.extraListeners=pl.jaceklaskowski.spark.CustomSparkListener \\ --driver-class-path target/scala-2.11/custom-spark-listener_2.11-1.0.jar Create a ../spark-sql-Dataset.md#implicits[Dataset] and execute an action like show to start a job as follows: scala> spark.read.text(\"README.md\").count [CustomSparkListener] Job started with 2 stages: SparkListenerJobStart(1,1473946006715,WrappedArray(org.apache.spark.scheduler.StageInfo@71515592, org.apache.spark.scheduler.StageInfo@6852819d),{spark.rdd.scope.noOverride=true, spark.rdd.scope={\"id\":\"14\",\"name\":\"collect\"}, spark.sql.execution.id=2}) [CustomSparkListener] Stage 1 completed with 1 tasks. [CustomSparkListener] Stage 2 completed with 1 tasks. res0: Long = 7 The lines with [CustomSparkListener] came from your custom Spark listener. Congratulations! The exercise's over. === BONUS Activating Custom Listener in Spark Application TIP: Read SparkContext.md#addSparkListener[Registering SparkListener]. === Questions What are the pros and cons of using the command line version vs inside a Spark application?","title":"[source,scala]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/","text":"== Working with Datasets from JDBC Data Sources (and PostgreSQL) Start spark-shell with the JDBC driver for the database you want to use. In our case, it is PostgreSQL JDBC Driver. NOTE: Download the jar for PostgreSQL JDBC Driver 42.1.1 directly from the http://central.maven.org/maven2/org/postgresql/postgresql/42.1.1/postgresql-42.1.1.jar[Maven repository]. [TIP] \u00b6 Execute the command to have the jar downloaded into ~/.ivy2/jars directory by spark-shell itself: ./bin/spark-shell --packages org.postgresql:postgresql:42.1.1 The entire path to the driver file is then like /Users/jacek/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar . You should see the following while spark-shell downloads the driver. Ivy Default Cache set to: /Users/jacek/.ivy2/cache The jars for the packages stored in: /Users/jacek/.ivy2/jars :: loading settings :: url = jar:file:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml org.postgresql#postgresql added as a dependency :: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0 confs: [default] found org.postgresql#postgresql;42.1.1 in central downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.1.1/postgresql-42.1.1.jar ... [SUCCESSFUL ] org.postgresql#postgresql;42.1.1!postgresql.jar(bundle) (205ms) :: resolution report :: resolve 1887ms :: artifacts dl 207ms :: modules in use: org.postgresql#postgresql;42.1.1 from central in [default] --------------------------------------------------------------------- | | modules || artifacts | | conf | number| search|dwnlded|evicted|| number|dwnlded| --------------------------------------------------------------------- | default | 1 | 1 | 1 | 0 || 1 | 1 | --------------------------------------------------------------------- :: retrieving :: org.apache.spark#spark-submit-parent confs: [default] 1 artifacts copied, 0 already retrieved (695kB/8ms) \u00b6 Start ./bin/spark-shell with spark-submit.md#driver-class-path[--driver-class-path] command line option and the driver jar. SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell --driver-class-path /Users/jacek/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar It will give you the proper setup for accessing PostgreSQL using the JDBC driver. Execute the following to access projects table in sparkdb . [source, scala] \u00b6 // that gives an one-partition Dataset val opts = Map( \"url\" -> \"jdbc:postgresql:sparkdb\", \"dbtable\" -> \"projects\") val df = spark. read. format(\"jdbc\"). options(opts). load NOTE: Use user and password options to specify the credentials if needed. [source, scala] \u00b6 // Note the number of partition (aka numPartitions) scala> df.explain == Physical Plan == *Scan JDBCRelation(projects) [numPartitions=1] [id#0,name#1,website#2] ReadSchema: struct scala> df.show(truncate = false) +---+------------+-----------------------+ |id |name |website | +---+------------+-----------------------+ |1 |Apache Spark| http://spark.apache.org | |2 |Apache Hive | http://hive.apache.org | |3 |Apache Kafka| http://kafka.apache.org | |4 |Apache Flink| http://flink.apache.org | +---+------------+-----------------------+ // use jdbc method with predicates to define partitions import java.util.Properties val df4parts = spark. read. jdbc( url = \"jdbc:postgresql:sparkdb\", table = \"projects\", predicates = Array(\"id=1\", \"id=2\", \"id=3\", \"id=4\"), connectionProperties = new Properties()) // Note the number of partitions (aka numPartitions) scala> df4parts.explain == Physical Plan == *Scan JDBCRelation(projects) [numPartitions=4] [id#16,name#17,website#18] ReadSchema: struct scala> df4parts.show(truncate = false) +---+------------+-----------------------+ |id |name |website | +---+------------+-----------------------+ |1 |Apache Spark| http://spark.apache.org | |2 |Apache Hive | http://hive.apache.org | |3 |Apache Kafka| http://kafka.apache.org | |4 |Apache Flink| http://flink.apache.org | +---+------------+-----------------------+ === Troubleshooting If things can go wrong, they sooner or later go wrong. Here is a list of possible issues and their solutions. ==== java.sql.SQLException: No suitable driver Ensure that the JDBC driver sits on the CLASSPATH. Use spark-submit.md#driver-class-path[--driver-class-path] as described above ( --packages or --jars do not work). scala> val df = spark. | read. | format(\"jdbc\"). | options(opts). | load java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:83) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:158) ... 52 elided === PostgreSQL Setup NOTE: I'm on Mac OS X so YMMV (aka Your Mileage May Vary ). Use the sections to have a properly configured PostgreSQL database. < > < > < > < > < > < > < > ==== [[installation]] Installation Install PostgreSQL as described in...TK CAUTION: This page serves as a cheatsheet for the author so he does not have to search Internet to find the installation steps. $ initdb /usr/local/var/postgres -E utf8 The files belonging to this database system will be owned by user \"jacek\". This user must also own the server process. The database cluster will be initialized with locale \"pl_pl.utf-8\". initdb: could not find suitable text search configuration for locale \"pl_pl.utf-8\" The default text search configuration will be set to \"simple\". Data page checksums are disabled. creating directory /usr/local/var/postgres ... ok creating subdirectories ... ok selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting dynamic shared memory implementation ... posix creating configuration files ... ok creating template1 database in /usr/local/var/postgres/base/1 ... ok initializing pg_authid ... ok initializing dependencies ... ok creating system views ... ok loading system objects' descriptions ... ok creating collations ... ok creating conversions ... ok creating dictionaries ... ok setting privileges on built-in objects ... ok creating information schema ... ok loading PL/pgSQL server-side language ... ok vacuuming database template1 ... ok copying template1 to template0 ... ok copying template1 to postgres ... ok syncing data to disk ... ok WARNING: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /usr/local/var/postgres -l logfile start ==== [[starting-database-server]] Starting Database Server NOTE: Consult http://www.postgresql.org/docs/current/static/server-start.html[17.3 . Starting the Database Server] in the official documentation. [TIP] \u00b6 Enable all logs in PostgreSQL to see query statements. log_statement = 'all' Add log_statement = 'all' to /usr/local/var/postgres/postgresql.conf on Mac OS X with PostgreSQL installed using brew . \u00b6 Start the database server using pg_ctl . $ pg_ctl -D /usr/local/var/postgres -l logfile start server starting Alternatively, you can run the database server using postgres . $ postgres -D /usr/local/var/postgres ==== [[creating-database]] Create Database $ createdb sparkdb TIP: Consult http://www.postgresql.org/docs/current/static/app-createdb.html[createdb ] in the official documentation. ==== Accessing Database Use psql sparkdb to access the database. $ psql sparkdb psql (9.6.2) Type \"help\" for help. sparkdb=# Execute SELECT version() to know the version of the database server you have connected to. sparkdb=# SELECT version(); version -------------------------------------------------------------------------------------------------------------- PostgreSQL 9.6.2 on x86_64-apple-darwin14.5.0, compiled by Apple LLVM version 7.0.2 (clang-700.1.81), 64-bit (1 row) Use \\h for help and \\q to leave a session. ==== Creating Table Create a table using CREATE TABLE command. CREATE TABLE projects ( id SERIAL PRIMARY KEY, name text, website text ); Insert rows to initialize the table with data. INSERT INTO projects (name, website) VALUES ('Apache Spark', 'http://spark.apache.org'); INSERT INTO projects (name, website) VALUES ('Apache Hive', 'http://hive.apache.org'); INSERT INTO projects VALUES (DEFAULT, 'Apache Kafka', 'http://kafka.apache.org'); INSERT INTO projects VALUES (DEFAULT, 'Apache Flink', 'http://flink.apache.org'); Execute select * from projects; to ensure that you have the following records in projects table: sparkdb=# select * from projects; id | name | website ----+--------------+------------------------- 1 | Apache Spark | http://spark.apache.org 2 | Apache Hive | http://hive.apache.org 3 | Apache Kafka | http://kafka.apache.org 4 | Apache Flink | http://flink.apache.org (4 rows) ==== Dropping Database $ dropdb sparkdb TIP: Consult http://www.postgresql.org/docs/current/static/app-dropdb.html[dropdb ] in the official documentation. ==== Stopping Database Server pg_ctl -D /usr/local/var/postgres stop","title":"Working with Datasets from JDBC Data Sources (and PostgreSQL)"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#tip","text":"Execute the command to have the jar downloaded into ~/.ivy2/jars directory by spark-shell itself: ./bin/spark-shell --packages org.postgresql:postgresql:42.1.1 The entire path to the driver file is then like /Users/jacek/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar . You should see the following while spark-shell downloads the driver.","title":"[TIP]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#ivy-default-cache-set-to-usersjacekivy2cache-the-jars-for-the-packages-stored-in-usersjacekivy2jars-loading-settings-url-jarfileusersjacekdevosssparkassemblytargetscala-211jarsivy-240jarorgapacheivycoresettingsivysettingsxml-orgpostgresqlpostgresql-added-as-a-dependency-resolving-dependencies-orgapachesparkspark-submit-parent10-confs-default-found-orgpostgresqlpostgresql4211-in-central-downloading-httpsrepo1mavenorgmaven2orgpostgresqlpostgresql4211postgresql-4211jar-successful-orgpostgresqlpostgresql4211postgresqljarbundle-205ms-resolution-report-resolve-1887ms-artifacts-dl-207ms-modules-in-use-orgpostgresqlpostgresql4211-from-central-in-default-modules-artifacts-conf-number-searchdwnldedevicted-numberdwnlded-default-1-1-1-0-1-1-retrieving-orgapachesparkspark-submit-parent-confs-default-1-artifacts-copied-0-already-retrieved-695kb8ms","text":"Start ./bin/spark-shell with spark-submit.md#driver-class-path[--driver-class-path] command line option and the driver jar. SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell --driver-class-path /Users/jacek/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar It will give you the proper setup for accessing PostgreSQL using the JDBC driver. Execute the following to access projects table in sparkdb .","title":"Ivy Default Cache set to: /Users/jacek/.ivy2/cache\nThe jars for the packages stored in: /Users/jacek/.ivy2/jars\n:: loading settings :: url = jar:file:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\norg.postgresql#postgresql added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n    confs: [default]\n    found org.postgresql#postgresql;42.1.1 in central\ndownloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.1.1/postgresql-42.1.1.jar ...\n    [SUCCESSFUL ] org.postgresql#postgresql;42.1.1!postgresql.jar(bundle) (205ms)\n:: resolution report :: resolve 1887ms :: artifacts dl 207ms\n    :: modules in use:\n    org.postgresql#postgresql;42.1.1 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n    ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n    confs: [default]\n    1 artifacts copied, 0 already retrieved (695kB/8ms)\n"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#source-scala","text":"// that gives an one-partition Dataset val opts = Map( \"url\" -> \"jdbc:postgresql:sparkdb\", \"dbtable\" -> \"projects\") val df = spark. read. format(\"jdbc\"). options(opts). load NOTE: Use user and password options to specify the credentials if needed.","title":"[source, scala]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#source-scala_1","text":"// Note the number of partition (aka numPartitions) scala> df.explain == Physical Plan == *Scan JDBCRelation(projects) [numPartitions=1] [id#0,name#1,website#2] ReadSchema: struct scala> df.show(truncate = false) +---+------------+-----------------------+ |id |name |website | +---+------------+-----------------------+ |1 |Apache Spark| http://spark.apache.org | |2 |Apache Hive | http://hive.apache.org | |3 |Apache Kafka| http://kafka.apache.org | |4 |Apache Flink| http://flink.apache.org | +---+------------+-----------------------+ // use jdbc method with predicates to define partitions import java.util.Properties val df4parts = spark. read. jdbc( url = \"jdbc:postgresql:sparkdb\", table = \"projects\", predicates = Array(\"id=1\", \"id=2\", \"id=3\", \"id=4\"), connectionProperties = new Properties()) // Note the number of partitions (aka numPartitions) scala> df4parts.explain == Physical Plan == *Scan JDBCRelation(projects) [numPartitions=4] [id#16,name#17,website#18] ReadSchema: struct scala> df4parts.show(truncate = false) +---+------------+-----------------------+ |id |name |website | +---+------------+-----------------------+ |1 |Apache Spark| http://spark.apache.org | |2 |Apache Hive | http://hive.apache.org | |3 |Apache Kafka| http://kafka.apache.org | |4 |Apache Flink| http://flink.apache.org | +---+------------+-----------------------+ === Troubleshooting If things can go wrong, they sooner or later go wrong. Here is a list of possible issues and their solutions. ==== java.sql.SQLException: No suitable driver Ensure that the JDBC driver sits on the CLASSPATH. Use spark-submit.md#driver-class-path[--driver-class-path] as described above ( --packages or --jars do not work). scala> val df = spark. | read. | format(\"jdbc\"). | options(opts). | load java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:83) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:158) ... 52 elided === PostgreSQL Setup NOTE: I'm on Mac OS X so YMMV (aka Your Mileage May Vary ). Use the sections to have a properly configured PostgreSQL database. < > < > < > < > < > < > < > ==== [[installation]] Installation Install PostgreSQL as described in...TK CAUTION: This page serves as a cheatsheet for the author so he does not have to search Internet to find the installation steps. $ initdb /usr/local/var/postgres -E utf8 The files belonging to this database system will be owned by user \"jacek\". This user must also own the server process. The database cluster will be initialized with locale \"pl_pl.utf-8\". initdb: could not find suitable text search configuration for locale \"pl_pl.utf-8\" The default text search configuration will be set to \"simple\". Data page checksums are disabled. creating directory /usr/local/var/postgres ... ok creating subdirectories ... ok selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting dynamic shared memory implementation ... posix creating configuration files ... ok creating template1 database in /usr/local/var/postgres/base/1 ... ok initializing pg_authid ... ok initializing dependencies ... ok creating system views ... ok loading system objects' descriptions ... ok creating collations ... ok creating conversions ... ok creating dictionaries ... ok setting privileges on built-in objects ... ok creating information schema ... ok loading PL/pgSQL server-side language ... ok vacuuming database template1 ... ok copying template1 to template0 ... ok copying template1 to postgres ... ok syncing data to disk ... ok WARNING: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /usr/local/var/postgres -l logfile start ==== [[starting-database-server]] Starting Database Server NOTE: Consult http://www.postgresql.org/docs/current/static/server-start.html[17.3 . Starting the Database Server] in the official documentation.","title":"[source, scala]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#tip_1","text":"Enable all logs in PostgreSQL to see query statements. log_statement = 'all'","title":"[TIP]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#add-log_statement-all-to-usrlocalvarpostgrespostgresqlconf-on-mac-os-x-with-postgresql-installed-using-brew","text":"Start the database server using pg_ctl . $ pg_ctl -D /usr/local/var/postgres -l logfile start server starting Alternatively, you can run the database server using postgres . $ postgres -D /usr/local/var/postgres ==== [[creating-database]] Create Database $ createdb sparkdb TIP: Consult http://www.postgresql.org/docs/current/static/app-createdb.html[createdb ] in the official documentation. ==== Accessing Database Use psql sparkdb to access the database. $ psql sparkdb psql (9.6.2) Type \"help\" for help. sparkdb=# Execute SELECT version() to know the version of the database server you have connected to. sparkdb=# SELECT version(); version -------------------------------------------------------------------------------------------------------------- PostgreSQL 9.6.2 on x86_64-apple-darwin14.5.0, compiled by Apple LLVM version 7.0.2 (clang-700.1.81), 64-bit (1 row) Use \\h for help and \\q to leave a session. ==== Creating Table Create a table using CREATE TABLE command. CREATE TABLE projects ( id SERIAL PRIMARY KEY, name text, website text ); Insert rows to initialize the table with data. INSERT INTO projects (name, website) VALUES ('Apache Spark', 'http://spark.apache.org'); INSERT INTO projects (name, website) VALUES ('Apache Hive', 'http://hive.apache.org'); INSERT INTO projects VALUES (DEFAULT, 'Apache Kafka', 'http://kafka.apache.org'); INSERT INTO projects VALUES (DEFAULT, 'Apache Flink', 'http://flink.apache.org'); Execute select * from projects; to ensure that you have the following records in projects table: sparkdb=# select * from projects; id | name | website ----+--------------+------------------------- 1 | Apache Spark | http://spark.apache.org 2 | Apache Hive | http://hive.apache.org 3 | Apache Kafka | http://kafka.apache.org 4 | Apache Flink | http://flink.apache.org (4 rows) ==== Dropping Database $ dropdb sparkdb TIP: Consult http://www.postgresql.org/docs/current/static/app-dropdb.html[dropdb ] in the official documentation. ==== Stopping Database Server pg_ctl -D /usr/local/var/postgres stop","title":"Add log_statement = 'all' to /usr/local/var/postgres/postgresql.conf on Mac OS X with PostgreSQL installed using brew."},{"location":"exercises/spark-exercise-failing-stage/","text":"== Exercise: Causing Stage to Fail The example shows how Spark re-executes a stage in case of stage failure. === Recipe Start a Spark cluster, e.g. 1-node Hadoop YARN. start-yarn.sh // 2-stage job -- it _appears_ that a stage can be failed only when there is a shuffle sc.parallelize(0 to 3e3.toInt, 2).map(n => (n % 2, n)).groupByKey.count Use 2 executors at least so you can kill one and keep the application up and running (on one executor). YARN_CONF_DIR=hadoop-conf ./bin/spark-shell --master yarn \\ -c spark.shuffle.service.enabled=true \\ --num-executors 2","title":"Causing Stage to Fail"},{"location":"exercises/spark-exercise-pairrddfunctions-oneliners/","text":"== Exercise: One-liners using PairRDDFunctions This is a set of one-liners to give you a entry point into using rdd:PairRDDFunctions.md[PairRDDFunctions]. === Exercise How would you go about solving a requirement to pair elements of the same key and creating a new RDD out of the matched values? [source, scala] \u00b6 val users = Seq((1, \"user1\"), (1, \"user2\"), (2, \"user1\"), (2, \"user3\"), (3,\"user2\"), (3,\"user4\"), (3,\"user1\")) // Input RDD val us = sc.parallelize(users) // ...your code here // Desired output Seq(\"user1\",\"user2\"),(\"user1\",\"user3\"),(\"user1\",\"user4\"),(\"user2\",\"user4\"))","title":"One-liners using PairRDDFunctions"},{"location":"exercises/spark-exercise-pairrddfunctions-oneliners/#source-scala","text":"val users = Seq((1, \"user1\"), (1, \"user2\"), (2, \"user1\"), (2, \"user3\"), (3,\"user2\"), (3,\"user4\"), (3,\"user1\")) // Input RDD val us = sc.parallelize(users) // ...your code here // Desired output Seq(\"user1\",\"user2\"),(\"user1\",\"user3\"),(\"user1\",\"user4\"),(\"user2\",\"user4\"))","title":"[source, scala]"},{"location":"exercises/spark-exercise-standalone-master-ha/","text":"== Spark Standalone - Using ZooKeeper for High-Availability of Master TIP: Read ../spark-standalone-Master.md#recovery-mode[Recovery Mode] to know the theory. You're going to start two standalone Masters. You'll need 4 terminals (adjust addresses as needed): Start ZooKeeper. Create a configuration file ha.conf with the content as follows: spark.deploy.recoveryMode=ZOOKEEPER spark.deploy.zookeeper.url=<zookeeper_host>:2181 spark.deploy.zookeeper.dir=/spark Start the first standalone Master. ./sbin/start-master.sh -h localhost -p 7077 --webui-port 8080 --properties-file ha.conf Start the second standalone Master. NOTE: It is not possible to start another instance of standalone Master on the same machine using ./sbin/start-master.sh . The reason is that the script assumes one instance per machine only. We're going to change the script to make it possible. $ cp ./sbin/start-master{,-2}.sh $ grep \"CLASS 1\" ./sbin/start-master-2.sh \"$\\{SPARK_HOME}/sbin\"/spark-daemon.sh start $CLASS 1 \\ $ sed -i -e 's/CLASS 1/CLASS 2/' sbin/start-master-2.sh $ grep \"CLASS 1\" ./sbin/start-master-2.sh $ grep \"CLASS 2\" ./sbin/start-master-2.sh \"$\\{SPARK_HOME}/sbin\"/spark-daemon.sh start $CLASS 2 \\ $ ./sbin/start-master-2.sh -h localhost -p 17077 --webui-port 18080 --properties-file ha.conf You can check how many instances you're currently running using jps command as follows: $ jps -lm 5024 sun.tools.jps.Jps -lm 4994 org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080 -h localhost -p 17077 --webui-port 18080 --properties-file ha.conf 4808 org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080 -h localhost -p 7077 --webui-port 8080 --properties-file ha.conf 4778 org.apache.zookeeper.server.quorum.QuorumPeerMain config/zookeeper.properties Start a standalone Worker. ./sbin/start-slave.sh spark://localhost:7077,localhost:17077 Start Spark shell. ./bin/spark-shell --master spark://localhost:7077,localhost:17077 Wait till the Spark shell connects to an active standalone Master. Find out which standalone Master is active (there can only be one). Kill it. Observe how the other standalone Master takes over and lets the Spark shell register with itself. Check out the master's UI. Optionally, kill the worker, make sure it goes away instantly in the active master's logs.","title":"Spark Standalone - Using ZooKeeper for High-Availability of Master"},{"location":"exercises/spark-exercise-take-multiple-jobs/","text":"== Exercise: Learning Jobs and Partitions Using take Action The exercise aims for introducing take action and using spark-shell and web UI. It should introduce you to the concepts of partitions and jobs. The following snippet creates an RDD of 16 elements with 16 partitions. scala> val r1 = sc.parallelize(0 to 15, 16) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at <console>:18 scala> r1.partitions.size res63: Int = 16 scala> r1.foreachPartition(it => println(\">>> partition size: \" + it.size)) ... >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 ... // the machine has 8 cores ... // so first 8 tasks get executed immediately ... // with the others after a core is free to take on new tasks. >>> partition size: 1 ... >>> partition size: 1 ... >>> partition size: 1 ... >>> partition size: 1 >>> partition size: 1 ... >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 All 16 partitions have one element. When you execute r1.take(1) only one job gets run since it is enough to compute one task on one partition. CAUTION: FIXME Snapshot from web UI - note the number of tasks However, when you execute r1.take(2) two jobs get run as the implementation assumes one job with one partition, and if the elements didn't total to the number of elements requested in take , quadruple the partitions to work on in the following jobs. CAUTION: FIXME Snapshot from web UI - note the number of tasks Can you guess how many jobs are run for r1.take(15) ? How many tasks per job? CAUTION: FIXME Snapshot from web UI - note the number of tasks Answer: 3.","title":"Learning Jobs and Partitions Using take Action"},{"location":"exercises/spark-first-app/","text":"== Your first Spark application (using Scala and sbt) This page gives you the exact steps to develop and run a complete Spark application using http://www.scala-lang.org/[Scala ] programming language and http://www.scala-sbt.org/[sbt ] as the build tool. [TIP] Refer to Quick Start's http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/quick-start.html#self-contained-applications[Self-Contained Applications] in the official documentation. The sample application called SparkMe App is...FIXME === Overview You're going to use http://www.scala-sbt.org/[sbt ] as the project build tool. It uses build.sbt for the project's description as well as the dependencies, i.e. the version of Apache Spark and others. The application's main code is under src/main/scala directory, in SparkMeApp.scala file. With the files in a directory, executing sbt package results in a package that can be deployed onto a Spark cluster using spark-submit . In this example, you're going to use Spark's local/spark-local.md[local mode]. === Project's build - build.sbt Any Scala project managed by sbt uses build.sbt as the central place for configuration, including project dependencies denoted as libraryDependencies . build.sbt name := \"SparkMe Project\" version := \"1.0\" organization := \"pl.japila\" scalaVersion := \"2.11.7\" libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.6.0-SNAPSHOT\" // <1> resolvers += Resolver.mavenLocal <1> Use the development version of Spark 1.6.0-SNAPSHOT === SparkMe Application The application uses a single command-line parameter (as args(0) ) that is the file to process. The file is read and the number of lines printed out. package pl.japila.spark import org.apache.spark.{SparkContext, SparkConf} object SparkMeApp { def main(args: Array[String]) { val conf = new SparkConf().setAppName(\"SparkMe Application\") val sc = new SparkContext(conf) val fileName = args(0) val lines = sc.textFile(fileName).cache val c = lines.count println(s\"There are $c lines in $fileName\") } } === sbt version - project/build.properties sbt (launcher) uses project/build.properties file to set (the real) sbt up sbt.version=0.13.9 TIP: With the file the build is more predictable as the version of sbt doesn't depend on the sbt launcher. === Packaging Application Execute sbt package to package the application. \u279c sparkme-app sbt package [info] Loading global plugins from /Users/jacek/.sbt/0.13/plugins [info] Loading project definition from /Users/jacek/dev/sandbox/sparkme-app/project [info] Set current project to SparkMe Project (in build file:/Users/jacek/dev/sandbox/sparkme-app/) [info] Compiling 1 Scala source to /Users/jacek/dev/sandbox/sparkme-app/target/scala-2.11/classes... [info] Packaging /Users/jacek/dev/sandbox/sparkme-app/target/scala-2.11/sparkme-project_2.11-1.0.jar ... [info] Done packaging. [success] Total time: 3 s, completed Sep 23, 2015 12:47:52 AM The application uses only classes that comes with Spark so package is enough. In target/scala-2.11/sparkme-project_2.11-1.0.jar there is the final application ready for deployment. === Submitting Application to Spark (local) NOTE: The application is going to be deployed to local[*] . Change it to whatever cluster you have available (refer to spark-cluster.md[Running Spark in cluster]). spark-submit the SparkMe application and specify the file to process (as it is the only and required input parameter to the application), e.g. build.sbt of the project. NOTE: build.sbt is sbt's build definition and is only used as an input file for demonstration purposes. Any file is going to work fine. \u279c sparkme-app ~/dev/oss/spark/bin/spark-submit --master \"local[*]\" --class pl.japila.spark.SparkMeApp target/scala-2.11/sparkme-project_2.11-1.0.jar build.sbt Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties To adjust logging level use sc.setLogLevel(\"INFO\") 15/09/23 01:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 15/09/23 01:06:04 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set. There are 8 lines in build.sbt NOTE: Disregard the two above WARN log messages. You're done. Sincere congratulations!","title":"Your first complete Spark application (using Scala and sbt)"},{"location":"exercises/spark-hello-world-using-spark-shell/","text":"== Exercise: Spark's Hello World using Spark shell and Scala Run Spark shell and count the number of words in a file using MapReduce pattern. Use sc.textFile to read the file into memory Use RDD.flatMap for a mapper step Use reduceByKey for a reducer step","title":"Spark's Hello World using Spark shell and Scala"},{"location":"exercises/spark-sql-hive-orc-example/","text":"== Using Spark SQL to update data in Hive using ORC files The example has showed up on Spark's users mailing list. [CAUTION] \u00b6 FIXME Offer a complete working solution in Scala FIXME Load ORC files into dataframe ** val df = hiveContext.read.format(\"orc\").load(to/path) ==== Solution was to use Hive in ORC format with partitions: A table in Hive stored as an ORC file (using partitioning) Using SQLContext.sql to insert data into the table Using SQLContext.sql to periodically run ALTER TABLE...CONCATENATE to merge your many small files into larger files optimized for your HDFS block size ** Since the CONCATENATE command operates on files in place it is transparent to any downstream processing Hive solution is just to concatenate the files ** it does not alter or change records. ** it's possible to update data in Hive using ORC format ** With transactional tables in Hive together with insert, update, delete, it does the \"concatenate \" for you automatically in regularly intervals. Currently this works only with tables in orc.format (stored as orc) ** Alternatively, use Hbase with Phoenix as the SQL layer on top ** Hive was originally not designed for updates, because it was.purely warehouse focused, the most recent one can do updates, deletes etc in a transactional way. Criteria: spark-streaming/spark-streaming.md[Spark Streaming] jobs are receiving a lot of small events (avg 10kb) Events are stored to HDFS, e.g. for Pig jobs There are a lot of small files in HDFS (several millions)","title":"Using Spark SQL to update data in Hive using ORC files"},{"location":"exercises/spark-sql-hive-orc-example/#caution","text":"FIXME Offer a complete working solution in Scala FIXME Load ORC files into dataframe ** val df = hiveContext.read.format(\"orc\").load(to/path) ==== Solution was to use Hive in ORC format with partitions: A table in Hive stored as an ORC file (using partitioning) Using SQLContext.sql to insert data into the table Using SQLContext.sql to periodically run ALTER TABLE...CONCATENATE to merge your many small files into larger files optimized for your HDFS block size ** Since the CONCATENATE command operates on files in place it is transparent to any downstream processing Hive solution is just to concatenate the files ** it does not alter or change records. ** it's possible to update data in Hive using ORC format ** With transactional tables in Hive together with insert, update, delete, it does the \"concatenate \" for you automatically in regularly intervals. Currently this works only with tables in orc.format (stored as orc) ** Alternatively, use Hbase with Phoenix as the SQL layer on top ** Hive was originally not designed for updates, because it was.purely warehouse focused, the most recent one can do updates, deletes etc in a transactional way. Criteria: spark-streaming/spark-streaming.md[Spark Streaming] jobs are receiving a lot of small events (avg 10kb) Events are stored to HDFS, e.g. for Pig jobs There are a lot of small files in HDFS (several millions)","title":"[CAUTION]"},{"location":"external-shuffle-service/","text":"External Shuffle Service \u00b6 External Shuffle Service is a Spark service to serve RDD and shuffle blocks outside and for Executor s. ExternalShuffleService can be started as a command-line application or automatically as part of a worker node in a Spark cluster (e.g. Standalone Worker ). External Shuffle Service is enabled in a Spark application using spark.shuffle.service.enabled configuration property.","title":"External Shuffle Service"},{"location":"external-shuffle-service/#external-shuffle-service","text":"External Shuffle Service is a Spark service to serve RDD and shuffle blocks outside and for Executor s. ExternalShuffleService can be started as a command-line application or automatically as part of a worker node in a Spark cluster (e.g. Standalone Worker ). External Shuffle Service is enabled in a Spark application using spark.shuffle.service.enabled configuration property.","title":"External Shuffle Service"},{"location":"external-shuffle-service/ExecutorShuffleInfo/","text":"ExecutorShuffleInfo \u00b6 ExecutorShuffleInfo is...FIXME","title":"ExecutorShuffleInfo"},{"location":"external-shuffle-service/ExecutorShuffleInfo/#executorshuffleinfo","text":"ExecutorShuffleInfo is...FIXME","title":"ExecutorShuffleInfo"},{"location":"external-shuffle-service/ExternalBlockHandler/","text":"ExternalBlockHandler \u00b6 ExternalBlockHandler is an RpcHandler . Creating Instance \u00b6 ExternalBlockHandler takes the following to be created: TransportConf Registered Executors File ExternalBlockHandler creates the following: ShuffleMetrics OneForOneStreamManager ExternalShuffleBlockResolver ExternalBlockHandler is created when: ExternalShuffleService is requested for an ExternalBlockHandler YarnShuffleService is requested to serviceInit OneForOneStreamManager \u00b6 ExternalBlockHandler can be given or creates an OneForOneStreamManager to be created . ExternalShuffleBlockResolver \u00b6 ExternalBlockHandler can be given or creates an ExternalShuffleBlockResolver to be created . ExternalShuffleBlockResolver is used for the following: registerExecutor when ExternalBlockHandler is requested to handle a RegisterExecutor message removeBlocks when ExternalBlockHandler is requested to handle a RemoveBlocks message getLocalDirs when ExternalBlockHandler is requested to handle a GetLocalDirsForExecutors message applicationRemoved when ExternalBlockHandler is requested to applicationRemoved executorRemoved when ExternalBlockHandler is requested to executorRemoved registerExecutor when ExternalBlockHandler is requested to reregisterExecutor ExternalShuffleBlockResolver is used for the following: getBlockData and getRddBlockData for ManagedBufferIterator getBlockData and getContinuousBlocksData for ShuffleManagedBufferIterator ExternalShuffleBlockResolver is closed when is ExternalBlockHandler . Registered Executors File \u00b6 ExternalBlockHandler can be given a Java's File (or null ) to be created . This file is simply to create an ExternalShuffleBlockResolver . Messages \u00b6 FetchShuffleBlocks \u00b6 Request to read a set of blocks \"Posted\" (created) when: OneForOneBlockFetcher is requested to createFetchShuffleBlocksMsg When received, ExternalBlockHandler requests the OneForOneStreamManager to registerStream (with a ShuffleManagedBufferIterator ). ExternalBlockHandler prints out the following TRACE message to the logs: Registered streamId [streamId] with [numBlockIds] buffers for client [clientId] from host [remoteAddress] In the end, ExternalBlockHandler responds with a StreamHandle (of streamId and numBlockIds ). GetLocalDirsForExecutors \u00b6 OpenBlocks \u00b6 Note For backward compatibility and like FetchShuffleBlocks . RegisterExecutor \u00b6 RemoveBlocks \u00b6 ShuffleMetrics \u00b6 Executor Removed Notification \u00b6 void executorRemoved ( String executorId , String appId ) executorRemoved requests the ExternalShuffleBlockResolver to executorRemoved . executorRemoved is used when: ExternalShuffleService is requested to executorRemoved Application Finished Notification \u00b6 void applicationRemoved ( String appId , boolean cleanupLocalDirs ) applicationRemoved requests the ExternalShuffleBlockResolver to applicationRemoved . applicationRemoved is used when: ExternalShuffleService is requested to applicationRemoved YarnShuffleService (Spark on YARN) is requested to stopApplication Logging \u00b6 Enable ALL logging level for org.apache.spark.network.shuffle.ExternalBlockHandler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.shuffle.ExternalBlockHandler=ALL Refer to Logging .","title":"ExternalBlockHandler"},{"location":"external-shuffle-service/ExternalBlockHandler/#externalblockhandler","text":"ExternalBlockHandler is an RpcHandler .","title":"ExternalBlockHandler"},{"location":"external-shuffle-service/ExternalBlockHandler/#creating-instance","text":"ExternalBlockHandler takes the following to be created: TransportConf Registered Executors File ExternalBlockHandler creates the following: ShuffleMetrics OneForOneStreamManager ExternalShuffleBlockResolver ExternalBlockHandler is created when: ExternalShuffleService is requested for an ExternalBlockHandler YarnShuffleService is requested to serviceInit","title":"Creating Instance"},{"location":"external-shuffle-service/ExternalBlockHandler/#oneforonestreammanager","text":"ExternalBlockHandler can be given or creates an OneForOneStreamManager to be created .","title":" OneForOneStreamManager"},{"location":"external-shuffle-service/ExternalBlockHandler/#externalshuffleblockresolver","text":"ExternalBlockHandler can be given or creates an ExternalShuffleBlockResolver to be created . ExternalShuffleBlockResolver is used for the following: registerExecutor when ExternalBlockHandler is requested to handle a RegisterExecutor message removeBlocks when ExternalBlockHandler is requested to handle a RemoveBlocks message getLocalDirs when ExternalBlockHandler is requested to handle a GetLocalDirsForExecutors message applicationRemoved when ExternalBlockHandler is requested to applicationRemoved executorRemoved when ExternalBlockHandler is requested to executorRemoved registerExecutor when ExternalBlockHandler is requested to reregisterExecutor ExternalShuffleBlockResolver is used for the following: getBlockData and getRddBlockData for ManagedBufferIterator getBlockData and getContinuousBlocksData for ShuffleManagedBufferIterator ExternalShuffleBlockResolver is closed when is ExternalBlockHandler .","title":" ExternalShuffleBlockResolver"},{"location":"external-shuffle-service/ExternalBlockHandler/#registered-executors-file","text":"ExternalBlockHandler can be given a Java's File (or null ) to be created . This file is simply to create an ExternalShuffleBlockResolver .","title":" Registered Executors File"},{"location":"external-shuffle-service/ExternalBlockHandler/#messages","text":"","title":" Messages"},{"location":"external-shuffle-service/ExternalBlockHandler/#fetchshuffleblocks","text":"Request to read a set of blocks \"Posted\" (created) when: OneForOneBlockFetcher is requested to createFetchShuffleBlocksMsg When received, ExternalBlockHandler requests the OneForOneStreamManager to registerStream (with a ShuffleManagedBufferIterator ). ExternalBlockHandler prints out the following TRACE message to the logs: Registered streamId [streamId] with [numBlockIds] buffers for client [clientId] from host [remoteAddress] In the end, ExternalBlockHandler responds with a StreamHandle (of streamId and numBlockIds ).","title":" FetchShuffleBlocks"},{"location":"external-shuffle-service/ExternalBlockHandler/#getlocaldirsforexecutors","text":"","title":" GetLocalDirsForExecutors"},{"location":"external-shuffle-service/ExternalBlockHandler/#openblocks","text":"Note For backward compatibility and like FetchShuffleBlocks .","title":" OpenBlocks"},{"location":"external-shuffle-service/ExternalBlockHandler/#registerexecutor","text":"","title":" RegisterExecutor"},{"location":"external-shuffle-service/ExternalBlockHandler/#removeblocks","text":"","title":" RemoveBlocks"},{"location":"external-shuffle-service/ExternalBlockHandler/#shufflemetrics","text":"","title":" ShuffleMetrics"},{"location":"external-shuffle-service/ExternalBlockHandler/#executor-removed-notification","text":"void executorRemoved ( String executorId , String appId ) executorRemoved requests the ExternalShuffleBlockResolver to executorRemoved . executorRemoved is used when: ExternalShuffleService is requested to executorRemoved","title":" Executor Removed Notification"},{"location":"external-shuffle-service/ExternalBlockHandler/#application-finished-notification","text":"void applicationRemoved ( String appId , boolean cleanupLocalDirs ) applicationRemoved requests the ExternalShuffleBlockResolver to applicationRemoved . applicationRemoved is used when: ExternalShuffleService is requested to applicationRemoved YarnShuffleService (Spark on YARN) is requested to stopApplication","title":" Application Finished Notification"},{"location":"external-shuffle-service/ExternalBlockHandler/#logging","text":"Enable ALL logging level for org.apache.spark.network.shuffle.ExternalBlockHandler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.shuffle.ExternalBlockHandler=ALL Refer to Logging .","title":"Logging"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/","text":"ExternalShuffleBlockResolver \u00b6 ExternalShuffleBlockResolver manages converting shuffle BlockId s into physical segments of local files (from a process outside of Executor s). Creating Instance \u00b6 ExternalShuffleBlockResolver takes the following to be created: TransportConf registeredExecutor File (Java's File ) Directory Cleaner ExternalShuffleBlockResolver is created when: ExternalBlockHandler is created Executors \u00b6 ExternalShuffleBlockResolver uses a mapping of ExecutorShuffleInfo s by AppExecId . ExternalShuffleBlockResolver can (re)load this mapping from a registeredExecutor file or simply start from scratch. A new mapping is added when registering an executor . Directory Cleaner Executor \u00b6 ExternalShuffleBlockResolver can be given a Java Executor or use a single worker thread executor (with spark-shuffle-directory-cleaner thread prefix). The Executor is used to schedule a thread to clean up executor's local directories and non-shuffle and non-RDD files in executor's local directories . spark.shuffle.service.fetch.rdd.enabled \u00b6 ExternalShuffleBlockResolver uses spark.shuffle.service.fetch.rdd.enabled configuration property to control whether or not to remove cached RDD files (alongside shuffle output files). Registering Executor \u00b6 void registerExecutor ( String appId , String execId , ExecutorShuffleInfo executorInfo ) registerExecutor ...FIXME registerExecutor is used when: ExternalBlockHandler is requested to handle a RegisterExecutor message and reregisterExecutor Cleaning Up Local Directories for Removed Executor \u00b6 void executorRemoved ( String executorId , String appId ) executorRemoved prints out the following INFO message to the logs: Clean up non-shuffle and non-RDD files associated with the finished executor [executorId] executorRemoved looks up the executor in the executors internal registry. When found, executorRemoved prints out the following INFO message to the logs and requests the Directory Cleaner Executor to execute asynchronous deletion of the executor's local directories (on a separate thread). Cleaning up non-shuffle and non-RDD files in executor [AppExecId]'s [localDirs] local dirs When not found, executorRemoved prints out the following INFO message to the logs: Executor is not registered (appId=[appId], execId=[executorId]) executorRemoved is used when: ExternalBlockHandler is requested to executorRemoved deleteNonShuffleServiceServedFiles \u00b6 void deleteNonShuffleServiceServedFiles ( String [] dirs ) deleteNonShuffleServiceServedFiles creates a Java FilenameFilter for files that meet all of the following: A file name does not end with .index or .data With rddFetchEnabled is enabled, a file name does not start with rdd_ prefix deleteNonShuffleServiceServedFiles deletes files and directories (based on the FilenameFilter ) in every directory (in the input dirs ). deleteNonShuffleServiceServedFiles prints out the following DEBUG message to the logs: Successfully cleaned up files not served by shuffle service in directory: [localDir] In case of any exceptions, deleteNonShuffleServiceServedFiles prints out the following ERROR message to the logs: Failed to delete files not served by shuffle service in directory: [localDir] Application Removed Notification \u00b6 void applicationRemoved ( String appId , boolean cleanupLocalDirs ) applicationRemoved ...FIXME applicationRemoved is used when: ExternalBlockHandler is requested to applicationRemoved deleteExecutorDirs \u00b6 void deleteExecutorDirs ( String [] dirs ) deleteExecutorDirs ...FIXME Fetching Block Data \u00b6 ManagedBuffer getBlockData ( String appId , String execId , int shuffleId , long mapId , int reduceId ) getBlockData ...FIXME getBlockData is used when: ManagedBufferIterator is created ShuffleManagedBufferIterator is requested for next ManagedBuffer Logging \u00b6 Enable ALL logging level for org.apache.spark.network.shuffle.ExternalShuffleBlockResolver logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.shuffle.ExternalShuffleBlockResolver=ALL Refer to Logging .","title":"ExternalShuffleBlockResolver"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#externalshuffleblockresolver","text":"ExternalShuffleBlockResolver manages converting shuffle BlockId s into physical segments of local files (from a process outside of Executor s).","title":"ExternalShuffleBlockResolver"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#creating-instance","text":"ExternalShuffleBlockResolver takes the following to be created: TransportConf registeredExecutor File (Java's File ) Directory Cleaner ExternalShuffleBlockResolver is created when: ExternalBlockHandler is created","title":"Creating Instance"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#executors","text":"ExternalShuffleBlockResolver uses a mapping of ExecutorShuffleInfo s by AppExecId . ExternalShuffleBlockResolver can (re)load this mapping from a registeredExecutor file or simply start from scratch. A new mapping is added when registering an executor .","title":" Executors"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#directory-cleaner-executor","text":"ExternalShuffleBlockResolver can be given a Java Executor or use a single worker thread executor (with spark-shuffle-directory-cleaner thread prefix). The Executor is used to schedule a thread to clean up executor's local directories and non-shuffle and non-RDD files in executor's local directories .","title":" Directory Cleaner Executor"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#sparkshuffleservicefetchrddenabled","text":"ExternalShuffleBlockResolver uses spark.shuffle.service.fetch.rdd.enabled configuration property to control whether or not to remove cached RDD files (alongside shuffle output files).","title":" spark.shuffle.service.fetch.rdd.enabled"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#registering-executor","text":"void registerExecutor ( String appId , String execId , ExecutorShuffleInfo executorInfo ) registerExecutor ...FIXME registerExecutor is used when: ExternalBlockHandler is requested to handle a RegisterExecutor message and reregisterExecutor","title":" Registering Executor"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#cleaning-up-local-directories-for-removed-executor","text":"void executorRemoved ( String executorId , String appId ) executorRemoved prints out the following INFO message to the logs: Clean up non-shuffle and non-RDD files associated with the finished executor [executorId] executorRemoved looks up the executor in the executors internal registry. When found, executorRemoved prints out the following INFO message to the logs and requests the Directory Cleaner Executor to execute asynchronous deletion of the executor's local directories (on a separate thread). Cleaning up non-shuffle and non-RDD files in executor [AppExecId]'s [localDirs] local dirs When not found, executorRemoved prints out the following INFO message to the logs: Executor is not registered (appId=[appId], execId=[executorId]) executorRemoved is used when: ExternalBlockHandler is requested to executorRemoved","title":" Cleaning Up Local Directories for Removed Executor"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#deletenonshuffleserviceservedfiles","text":"void deleteNonShuffleServiceServedFiles ( String [] dirs ) deleteNonShuffleServiceServedFiles creates a Java FilenameFilter for files that meet all of the following: A file name does not end with .index or .data With rddFetchEnabled is enabled, a file name does not start with rdd_ prefix deleteNonShuffleServiceServedFiles deletes files and directories (based on the FilenameFilter ) in every directory (in the input dirs ). deleteNonShuffleServiceServedFiles prints out the following DEBUG message to the logs: Successfully cleaned up files not served by shuffle service in directory: [localDir] In case of any exceptions, deleteNonShuffleServiceServedFiles prints out the following ERROR message to the logs: Failed to delete files not served by shuffle service in directory: [localDir]","title":" deleteNonShuffleServiceServedFiles"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#application-removed-notification","text":"void applicationRemoved ( String appId , boolean cleanupLocalDirs ) applicationRemoved ...FIXME applicationRemoved is used when: ExternalBlockHandler is requested to applicationRemoved","title":" Application Removed Notification"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#deleteexecutordirs","text":"void deleteExecutorDirs ( String [] dirs ) deleteExecutorDirs ...FIXME","title":" deleteExecutorDirs"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#fetching-block-data","text":"ManagedBuffer getBlockData ( String appId , String execId , int shuffleId , long mapId , int reduceId ) getBlockData ...FIXME getBlockData is used when: ManagedBufferIterator is created ShuffleManagedBufferIterator is requested for next ManagedBuffer","title":" Fetching Block Data"},{"location":"external-shuffle-service/ExternalShuffleBlockResolver/#logging","text":"Enable ALL logging level for org.apache.spark.network.shuffle.ExternalShuffleBlockResolver logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.shuffle.ExternalShuffleBlockResolver=ALL Refer to Logging .","title":"Logging"},{"location":"external-shuffle-service/ExternalShuffleService/","text":"ExternalShuffleService \u00b6 ExternalShuffleService is a Spark service that can serve RDD and shuffle blocks. ExternalShuffleService manages shuffle output files so they are available to executors. As the shuffle output files are managed externally to the executors it offers an uninterrupted access to the shuffle output files regardless of executors being killed or down (esp. with Dynamic Allocation of Executors ). ExternalShuffleService can be launched from command line . ExternalShuffleService is enabled on the driver and executors using spark.shuffle.service.enabled configuration property. Note Spark on YARN uses a custom external shuffle service ( YarnShuffleService ). Launching ExternalShuffleService \u00b6 ExternalShuffleService can be launched as a standalone application using spark-class . spark-class org.apache.spark.deploy.ExternalShuffleService main Entry Point \u00b6 main ( args : Array [ String ]): Unit main is the entry point of ExternalShuffleService standalone application. main prints out the following INFO message to the logs: Started daemon with process name: [name] main registers signal handlers for TERM , HUP , INT signals. main loads the default Spark properties . main creates a SecurityManager . main turns spark.shuffle.service.enabled to true explicitly (since this service is started from the command line for a reason). main creates an ExternalShuffleService and starts it . main prints out the following DEBUG message to the logs: Adding shutdown hook main registers a shutdown hook. When triggered, the shutdown hook prints the following INFO message to the logs and requests the ExternalShuffleService to stop . Shutting down shuffle service. Creating Instance \u00b6 ExternalShuffleService takes the following to be created: SparkConf SecurityManager ExternalShuffleService is created when: ExternalShuffleService standalone application is started Worker (Spark Standalone) is created (and initializes an ExternalShuffleService ) TransportServer \u00b6 server : TransportServer ExternalShuffleService uses an internal reference to a TransportServer that is created when ExternalShuffleService is started . ExternalShuffleService uses an ExternalBlockHandler to handle RPC messages (and serve RDD blocks and shuffle blocks). TransportServer is closed when ExternalShuffleService is requested to stop . TransportServer is used for metrics. Port \u00b6 ExternalShuffleService uses spark.shuffle.service.port configuration property for the port to listen to when started . spark.shuffle.service.enabled \u00b6 ExternalShuffleService uses spark.shuffle.service.enabled configuration property to control whether or not is enabled (and should be started when requested). ExternalBlockHandler \u00b6 blockHandler : ExternalBlockHandler ExternalShuffleService creates an ExternalBlockHandler when created . With spark.shuffle.service.db.enabled and spark.shuffle.service.enabled configuration properties enabled, the ExternalBlockHandler is given a local directory with a registeredExecutors.ldb file . blockHandler is used to create a TransportContext that creates the TransportServer . blockHandler is used when: applicationRemoved executorRemoved findRegisteredExecutorsDBFile \u00b6 findRegisteredExecutorsDBFile ( dbName : String ): File findRegisteredExecutorsDBFile returns one of the local directories (defined using spark.local.dir configuration property) with the input dbName file or null when no directories defined. findRegisteredExecutorsDBFile searches the local directories (defined using spark.local.dir configuration property) for the input dbName file. Unless found, findRegisteredExecutorsDBFile takes the first local directory. With no local directories defined in spark.local.dir configuration property, findRegisteredExecutorsDBFile prints out the following WARN message to the logs and returns null . 'spark.local.dir' should be set first when we use db in ExternalShuffleService. Note that this only affects standalone mode. Starting ExternalShuffleService \u00b6 start (): Unit start prints out the following INFO message to the logs: Starting shuffle service on port [port] (auth enabled = [authEnabled]) start creates a AuthServerBootstrap with authentication enabled (using SecurityManager ). start creates a TransportContext (with the ExternalBlockHandler ) and requests it to create a server (on the port ). start ...FIXME start is used when: ExternalShuffleService is requested to startIfEnabled and is launched (as a command-line application) startIfEnabled \u00b6 startIfEnabled (): Unit startIfEnabled starts the external shuffle service if enabled . startIfEnabled is used when: Worker (Spark Standalone) is requested to startExternalShuffleService Executor Removed Notification \u00b6 executorRemoved ( executorId : String , appId : String ): Unit executorRemoved requests the ExternalBlockHandler to executorRemoved . executorRemoved is used when: Worker (Spark Standalone) is requested to handleExecutorStateChanged Application Finished Notification \u00b6 applicationRemoved ( appId : String ): Unit applicationRemoved requests the ExternalBlockHandler to applicationRemoved (with cleanupLocalDirs flag enabled). applicationRemoved is used when: Worker (Spark Standalone) is requested to handle WorkDirCleanup message and maybeCleanupApplication Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.ExternalShuffleService logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.ExternalShuffleService=ALL Refer to Logging .","title":"ExternalShuffleService"},{"location":"external-shuffle-service/ExternalShuffleService/#externalshuffleservice","text":"ExternalShuffleService is a Spark service that can serve RDD and shuffle blocks. ExternalShuffleService manages shuffle output files so they are available to executors. As the shuffle output files are managed externally to the executors it offers an uninterrupted access to the shuffle output files regardless of executors being killed or down (esp. with Dynamic Allocation of Executors ). ExternalShuffleService can be launched from command line . ExternalShuffleService is enabled on the driver and executors using spark.shuffle.service.enabled configuration property. Note Spark on YARN uses a custom external shuffle service ( YarnShuffleService ).","title":"ExternalShuffleService"},{"location":"external-shuffle-service/ExternalShuffleService/#launching-externalshuffleservice","text":"ExternalShuffleService can be launched as a standalone application using spark-class . spark-class org.apache.spark.deploy.ExternalShuffleService","title":" Launching ExternalShuffleService"},{"location":"external-shuffle-service/ExternalShuffleService/#main-entry-point","text":"main ( args : Array [ String ]): Unit main is the entry point of ExternalShuffleService standalone application. main prints out the following INFO message to the logs: Started daemon with process name: [name] main registers signal handlers for TERM , HUP , INT signals. main loads the default Spark properties . main creates a SecurityManager . main turns spark.shuffle.service.enabled to true explicitly (since this service is started from the command line for a reason). main creates an ExternalShuffleService and starts it . main prints out the following DEBUG message to the logs: Adding shutdown hook main registers a shutdown hook. When triggered, the shutdown hook prints the following INFO message to the logs and requests the ExternalShuffleService to stop . Shutting down shuffle service.","title":" main Entry Point"},{"location":"external-shuffle-service/ExternalShuffleService/#creating-instance","text":"ExternalShuffleService takes the following to be created: SparkConf SecurityManager ExternalShuffleService is created when: ExternalShuffleService standalone application is started Worker (Spark Standalone) is created (and initializes an ExternalShuffleService )","title":"Creating Instance"},{"location":"external-shuffle-service/ExternalShuffleService/#transportserver","text":"server : TransportServer ExternalShuffleService uses an internal reference to a TransportServer that is created when ExternalShuffleService is started . ExternalShuffleService uses an ExternalBlockHandler to handle RPC messages (and serve RDD blocks and shuffle blocks). TransportServer is closed when ExternalShuffleService is requested to stop . TransportServer is used for metrics.","title":" TransportServer"},{"location":"external-shuffle-service/ExternalShuffleService/#port","text":"ExternalShuffleService uses spark.shuffle.service.port configuration property for the port to listen to when started .","title":" Port"},{"location":"external-shuffle-service/ExternalShuffleService/#sparkshuffleserviceenabled","text":"ExternalShuffleService uses spark.shuffle.service.enabled configuration property to control whether or not is enabled (and should be started when requested).","title":" spark.shuffle.service.enabled"},{"location":"external-shuffle-service/ExternalShuffleService/#externalblockhandler","text":"blockHandler : ExternalBlockHandler ExternalShuffleService creates an ExternalBlockHandler when created . With spark.shuffle.service.db.enabled and spark.shuffle.service.enabled configuration properties enabled, the ExternalBlockHandler is given a local directory with a registeredExecutors.ldb file . blockHandler is used to create a TransportContext that creates the TransportServer . blockHandler is used when: applicationRemoved executorRemoved","title":" ExternalBlockHandler"},{"location":"external-shuffle-service/ExternalShuffleService/#findregisteredexecutorsdbfile","text":"findRegisteredExecutorsDBFile ( dbName : String ): File findRegisteredExecutorsDBFile returns one of the local directories (defined using spark.local.dir configuration property) with the input dbName file or null when no directories defined. findRegisteredExecutorsDBFile searches the local directories (defined using spark.local.dir configuration property) for the input dbName file. Unless found, findRegisteredExecutorsDBFile takes the first local directory. With no local directories defined in spark.local.dir configuration property, findRegisteredExecutorsDBFile prints out the following WARN message to the logs and returns null . 'spark.local.dir' should be set first when we use db in ExternalShuffleService. Note that this only affects standalone mode.","title":" findRegisteredExecutorsDBFile"},{"location":"external-shuffle-service/ExternalShuffleService/#starting-externalshuffleservice","text":"start (): Unit start prints out the following INFO message to the logs: Starting shuffle service on port [port] (auth enabled = [authEnabled]) start creates a AuthServerBootstrap with authentication enabled (using SecurityManager ). start creates a TransportContext (with the ExternalBlockHandler ) and requests it to create a server (on the port ). start ...FIXME start is used when: ExternalShuffleService is requested to startIfEnabled and is launched (as a command-line application)","title":" Starting ExternalShuffleService"},{"location":"external-shuffle-service/ExternalShuffleService/#startifenabled","text":"startIfEnabled (): Unit startIfEnabled starts the external shuffle service if enabled . startIfEnabled is used when: Worker (Spark Standalone) is requested to startExternalShuffleService","title":" startIfEnabled"},{"location":"external-shuffle-service/ExternalShuffleService/#executor-removed-notification","text":"executorRemoved ( executorId : String , appId : String ): Unit executorRemoved requests the ExternalBlockHandler to executorRemoved . executorRemoved is used when: Worker (Spark Standalone) is requested to handleExecutorStateChanged","title":" Executor Removed Notification"},{"location":"external-shuffle-service/ExternalShuffleService/#application-finished-notification","text":"applicationRemoved ( appId : String ): Unit applicationRemoved requests the ExternalBlockHandler to applicationRemoved (with cleanupLocalDirs flag enabled). applicationRemoved is used when: Worker (Spark Standalone) is requested to handle WorkDirCleanup message and maybeCleanupApplication","title":" Application Finished Notification"},{"location":"external-shuffle-service/ExternalShuffleService/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.ExternalShuffleService logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.ExternalShuffleService=ALL Refer to Logging .","title":"Logging"},{"location":"external-shuffle-service/configuration-properties/","text":"Spark Configuration Properties of External Shuffle Service \u00b6 The following are configuration properties of External Shuffle Service . spark.shuffle.service.db.enabled \u00b6 Whether to use db in ExternalShuffleService . Note that this only affects standalone mode. Default: true Used when: ExternalShuffleService is requested for an ExternalBlockHandler Worker (Spark Standalone) is requested to handle a WorkDirCleanup message spark.shuffle.service.enabled \u00b6 Controls whether to use the External Shuffle Service Default: false Note LocalSparkCluster turns this property off explicitly when started. Used when: BlacklistTracker is requested to updateBlacklistForFetchFailure ExecutorMonitor is created ExecutorAllocationManager is requested to validateSettings SparkEnv utility is requested to create a \"base\" SparkEnv ExternalShuffleService is created and started Worker (Spark Standalone) is requested to handle a WorkDirCleanup message or started ExecutorRunnable (Spark on YARN) is requested to startContainer spark.shuffle.service.fetch.rdd.enabled \u00b6 Enables ExternalShuffleService for fetching disk persisted RDD blocks. When enabled with Dynamic Resource Allocation executors having only disk persisted blocks are considered idle after spark.dynamicAllocation.executorIdleTimeout and will be released accordingly. Default: false Used when: ExternalShuffleBlockResolver is created SparkEnv utility is requested to create a \"base\" SparkEnv ExecutorMonitor is created spark.shuffle.service.port \u00b6 Port of the external shuffle service Default: 7337 Used when: ExternalShuffleService is created StorageUtils utility is requested for the port of an external shuffle service","title":"Configuration Properties"},{"location":"external-shuffle-service/configuration-properties/#spark-configuration-properties-of-external-shuffle-service","text":"The following are configuration properties of External Shuffle Service .","title":"Spark Configuration Properties of External Shuffle Service"},{"location":"external-shuffle-service/configuration-properties/#sparkshuffleservicedbenabled","text":"Whether to use db in ExternalShuffleService . Note that this only affects standalone mode. Default: true Used when: ExternalShuffleService is requested for an ExternalBlockHandler Worker (Spark Standalone) is requested to handle a WorkDirCleanup message","title":" spark.shuffle.service.db.enabled"},{"location":"external-shuffle-service/configuration-properties/#sparkshuffleserviceenabled","text":"Controls whether to use the External Shuffle Service Default: false Note LocalSparkCluster turns this property off explicitly when started. Used when: BlacklistTracker is requested to updateBlacklistForFetchFailure ExecutorMonitor is created ExecutorAllocationManager is requested to validateSettings SparkEnv utility is requested to create a \"base\" SparkEnv ExternalShuffleService is created and started Worker (Spark Standalone) is requested to handle a WorkDirCleanup message or started ExecutorRunnable (Spark on YARN) is requested to startContainer","title":" spark.shuffle.service.enabled"},{"location":"external-shuffle-service/configuration-properties/#sparkshuffleservicefetchrddenabled","text":"Enables ExternalShuffleService for fetching disk persisted RDD blocks. When enabled with Dynamic Resource Allocation executors having only disk persisted blocks are considered idle after spark.dynamicAllocation.executorIdleTimeout and will be released accordingly. Default: false Used when: ExternalShuffleBlockResolver is created SparkEnv utility is requested to create a \"base\" SparkEnv ExecutorMonitor is created","title":" spark.shuffle.service.fetch.rdd.enabled"},{"location":"external-shuffle-service/configuration-properties/#sparkshuffleserviceport","text":"Port of the external shuffle service Default: 7337 Used when: ExternalShuffleService is created StorageUtils utility is requested for the port of an external shuffle service","title":" spark.shuffle.service.port"},{"location":"history-server/","text":"Spark History Server \u00b6 Spark History Server is the web UI of Spark applications with event log collection enabled (based on spark.eventLog.enabled configuration property). Spark History Server is an extension of Spark's web UI . Spark History Server can be started using start-history-server.sh and stopped using stop-history-server.sh shell scripts. Spark History Server supports custom configuration properties that can be defined using --properties-file [propertiesFile] command-line option. The properties file can have any valid spark. -prefixed Spark property. $ ./sbin/start-history-server.sh --properties-file history.properties If not specified explicitly, Spark History Server uses the default configuration file, i.e. spark-defaults.conf . Spark History Server can replay events from event log files recorded by EventLoggingListener . start-history-server.sh Shell Script \u00b6 $SPARK_HOME/sbin/start-history-server.sh shell script (where SPARK_HOME is the directory of your Spark installation) is used to start a Spark History Server instance. $ ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to .../spark/logs/spark-jacek-org.apache.spark.deploy.history.HistoryServer-1-japila.out Internally, start-history-server.sh script starts org.apache.spark.deploy.history.HistoryServer standalone application (using spark-daemon.sh shell script). $ ./bin/spark-class org.apache.spark.deploy.history.HistoryServer Tip Using the more explicit approach with spark-class to start Spark History Server could be easier to trace execution by seeing the logs printed out to the standard output and hence terminal directly. When started, start-history-server.sh prints out the following INFO message to the logs: Started daemon with process name: [processName] start-history-server.sh registers signal handlers (using SignalUtils ) for TERM , HUP , INT to log their execution: RECEIVED SIGNAL [signal] start-history-server.sh inits security if enabled (based on spark.history.kerberos.enabled configuration property). start-history-server.sh creates a SecurityManager . start-history-server.sh creates a ApplicationHistoryProvider (based on spark.history.provider configuration property). In the end, start-history-server.sh creates a HistoryServer and requests it to bind to the port (based on spark.history.ui.port configuration property). Note The host's IP can be specified using SPARK_LOCAL_IP environment variable (defaults to 0.0.0.0 ). start-history-server.sh prints out the following INFO message to the logs: Bound HistoryServer to [host], and started at [webUrl] start-history-server.sh registers a shutdown hook to call stop on the HistoryServer instance. stop-history-server.sh Shell Script \u00b6 $SPARK_HOME/sbin/stop-history-server.sh shell script (where SPARK_HOME is the directory of your Spark installation) is used to stop a running instance of Spark History Server. $ ./sbin/stop-history-server.sh stopping org.apache.spark.deploy.history.HistoryServer","title":"Spark History Server"},{"location":"history-server/#spark-history-server","text":"Spark History Server is the web UI of Spark applications with event log collection enabled (based on spark.eventLog.enabled configuration property). Spark History Server is an extension of Spark's web UI . Spark History Server can be started using start-history-server.sh and stopped using stop-history-server.sh shell scripts. Spark History Server supports custom configuration properties that can be defined using --properties-file [propertiesFile] command-line option. The properties file can have any valid spark. -prefixed Spark property. $ ./sbin/start-history-server.sh --properties-file history.properties If not specified explicitly, Spark History Server uses the default configuration file, i.e. spark-defaults.conf . Spark History Server can replay events from event log files recorded by EventLoggingListener .","title":"Spark History Server"},{"location":"history-server/#start-history-serversh-shell-script","text":"$SPARK_HOME/sbin/start-history-server.sh shell script (where SPARK_HOME is the directory of your Spark installation) is used to start a Spark History Server instance. $ ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to .../spark/logs/spark-jacek-org.apache.spark.deploy.history.HistoryServer-1-japila.out Internally, start-history-server.sh script starts org.apache.spark.deploy.history.HistoryServer standalone application (using spark-daemon.sh shell script). $ ./bin/spark-class org.apache.spark.deploy.history.HistoryServer Tip Using the more explicit approach with spark-class to start Spark History Server could be easier to trace execution by seeing the logs printed out to the standard output and hence terminal directly. When started, start-history-server.sh prints out the following INFO message to the logs: Started daemon with process name: [processName] start-history-server.sh registers signal handlers (using SignalUtils ) for TERM , HUP , INT to log their execution: RECEIVED SIGNAL [signal] start-history-server.sh inits security if enabled (based on spark.history.kerberos.enabled configuration property). start-history-server.sh creates a SecurityManager . start-history-server.sh creates a ApplicationHistoryProvider (based on spark.history.provider configuration property). In the end, start-history-server.sh creates a HistoryServer and requests it to bind to the port (based on spark.history.ui.port configuration property). Note The host's IP can be specified using SPARK_LOCAL_IP environment variable (defaults to 0.0.0.0 ). start-history-server.sh prints out the following INFO message to the logs: Bound HistoryServer to [host], and started at [webUrl] start-history-server.sh registers a shutdown hook to call stop on the HistoryServer instance.","title":" start-history-server.sh Shell Script"},{"location":"history-server/#stop-history-serversh-shell-script","text":"$SPARK_HOME/sbin/stop-history-server.sh shell script (where SPARK_HOME is the directory of your Spark installation) is used to stop a running instance of Spark History Server. $ ./sbin/stop-history-server.sh stopping org.apache.spark.deploy.history.HistoryServer","title":" stop-history-server.sh Shell Script"},{"location":"history-server/ApplicationCache/","text":"== [[ApplicationCache]] ApplicationCache ApplicationCache is...FIXME ApplicationCache is < > exclusively when HistoryServer is HistoryServer.md#appCache[created]. ApplicationCache uses https://github.com/google/guava/wiki/Release14[Google Guava 14.0.1] library for the internal < >. [[internal-registries]] .ApplicationCache's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appLoader | [[appLoader]] Google Guava's https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/CacheLoader.html[CacheLoader ] with a custom ++ https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/CacheLoader.html#load(K)++[load ] which is simply < >. Used when...FIXME | removalListener | [[removalListener]] | appCache a| [[appCache]] Google Guava's https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/LoadingCache.html[LoadingCache ] of CacheKey keys and CacheEntry entries Used when ApplicationCache is requested for the following: < > given appId and attemptId IDs FIXME (other uses) | metrics | [[metrics]] |=== === [[creating-instance]] Creating ApplicationCache Instance ApplicationCache takes the following when created: [[operations]] ApplicationCacheOperations.md[ApplicationCacheOperations] [[retainedApplications]] retainedApplications [[clock]] Clock ApplicationCache initializes the < >. === [[loadApplicationEntry]] loadApplicationEntry Internal Method [source, scala] \u00b6 loadApplicationEntry(appId: String, attemptId: Option[String]): CacheEntry \u00b6 loadApplicationEntry ...FIXME NOTE: loadApplicationEntry is used exclusively when ApplicationCache is requested to < >. === [[load]] Loading Cached Spark Application UI -- load Method [source, scala] \u00b6 load(key: CacheKey): CacheEntry \u00b6 NOTE: load is part of Google Guava's https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/CacheLoader.html[CacheLoader ] to retrieve a CacheEntry , based on a CacheKey , for < >. load simply relays to < > with the appId and attemptId of the input CacheKey . === [[get]] Requesting Cached UI of Spark Application (CacheEntry) -- get Method [source, scala] \u00b6 get(appId: String, attemptId: Option[String] = None): CacheEntry \u00b6 get ...FIXME NOTE: get is used exclusively when ApplicationCache is requested to < >. === [[withSparkUI]] Executing Closure While Holding Application's UI Read Lock -- withSparkUI Method [source, scala] \u00b6 withSparkUI T (fn: SparkUI => T): T \u00b6 withSparkUI ...FIXME NOTE: withSparkUI is used when HistoryServer is requested to HistoryServer.md#withSparkUI[withSparkUI] and HistoryServer.md#loadAppUi[loadAppUi].","title":"ApplicationCache"},{"location":"history-server/ApplicationCache/#source-scala","text":"","title":"[source, scala]"},{"location":"history-server/ApplicationCache/#loadapplicationentryappid-string-attemptid-optionstring-cacheentry","text":"loadApplicationEntry ...FIXME NOTE: loadApplicationEntry is used exclusively when ApplicationCache is requested to < >. === [[load]] Loading Cached Spark Application UI -- load Method","title":"loadApplicationEntry(appId: String, attemptId: Option[String]): CacheEntry"},{"location":"history-server/ApplicationCache/#source-scala_1","text":"","title":"[source, scala]"},{"location":"history-server/ApplicationCache/#loadkey-cachekey-cacheentry","text":"NOTE: load is part of Google Guava's https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/CacheLoader.html[CacheLoader ] to retrieve a CacheEntry , based on a CacheKey , for < >. load simply relays to < > with the appId and attemptId of the input CacheKey . === [[get]] Requesting Cached UI of Spark Application (CacheEntry) -- get Method","title":"load(key: CacheKey): CacheEntry"},{"location":"history-server/ApplicationCache/#source-scala_2","text":"","title":"[source, scala]"},{"location":"history-server/ApplicationCache/#getappid-string-attemptid-optionstring-none-cacheentry","text":"get ...FIXME NOTE: get is used exclusively when ApplicationCache is requested to < >. === [[withSparkUI]] Executing Closure While Holding Application's UI Read Lock -- withSparkUI Method","title":"get(appId: String, attemptId: Option[String] = None): CacheEntry"},{"location":"history-server/ApplicationCache/#source-scala_3","text":"","title":"[source, scala]"},{"location":"history-server/ApplicationCache/#withsparkuitfn-sparkui-t-t","text":"withSparkUI ...FIXME NOTE: withSparkUI is used when HistoryServer is requested to HistoryServer.md#withSparkUI[withSparkUI] and HistoryServer.md#loadAppUi[loadAppUi].","title":"withSparkUIT(fn: SparkUI =&gt; T): T"},{"location":"history-server/ApplicationCacheOperations/","text":"== [[ApplicationCacheOperations]] ApplicationCacheOperations ApplicationCacheOperations is the < > of...FIXME [[contract]] [source, scala] package org.apache.spark.deploy.history trait ApplicationCacheOperations { // only required methods that have no implementation // the others follow def getAppUI(appId: String, attemptId: Option[String]): Option[LoadedAppUI] def attachSparkUI( appId: String, attemptId: Option[String], ui: SparkUI, completed: Boolean): Unit def detachSparkUI(appId: String, attemptId: Option[String], ui: SparkUI): Unit } NOTE: ApplicationCacheOperations is a private[history] contract. .(Subset of) ApplicationCacheOperations Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | getAppUI | [[getAppUI]] spark-webui-SparkUI.md[SparkUI] (the UI of a Spark application) Used exclusively when ApplicationCache is requested for ApplicationCache.md#loadApplicationEntry[loadApplicationEntry] | attachSparkUI | [[attachSparkUI]] | detachSparkUI | [[detachSparkUI]] |=== [[implementations]] NOTE: HistoryServer.md[HistoryServer] is the one and only known implementation of < > in Apache Spark.","title":"ApplicationCacheOperations"},{"location":"history-server/ApplicationHistoryProvider/","text":"ApplicationHistoryProvider \u00b6 ApplicationHistoryProvider is an abstraction of history providers . Contract \u00b6 getApplicationInfo \u00b6 getApplicationInfo ( appId : String ): Option [ ApplicationInfo ] Used when...FIXME getAppUI \u00b6 getAppUI ( appId : String , attemptId : Option [ String ]): Option [ LoadedAppUI ] SparkUI for a given application (by appId ) Used when HistoryServer is requested for the UI of a Spark application getListing \u00b6 getListing (): Iterator [ ApplicationInfo ] Used when...FIXME onUIDetached \u00b6 onUIDetached ( appId : String , attemptId : Option [ String ], ui : SparkUI ): Unit Used when...FIXME writeEventLogs \u00b6 writeEventLogs ( appId : String , attemptId : Option [ String ], zipStream : ZipOutputStream ): Unit Writes events to a stream Used when...FIXME Implementations \u00b6 FsHistoryProvider","title":"ApplicationHistoryProvider"},{"location":"history-server/ApplicationHistoryProvider/#applicationhistoryprovider","text":"ApplicationHistoryProvider is an abstraction of history providers .","title":"ApplicationHistoryProvider"},{"location":"history-server/ApplicationHistoryProvider/#contract","text":"","title":"Contract"},{"location":"history-server/ApplicationHistoryProvider/#getapplicationinfo","text":"getApplicationInfo ( appId : String ): Option [ ApplicationInfo ] Used when...FIXME","title":" getApplicationInfo"},{"location":"history-server/ApplicationHistoryProvider/#getappui","text":"getAppUI ( appId : String , attemptId : Option [ String ]): Option [ LoadedAppUI ] SparkUI for a given application (by appId ) Used when HistoryServer is requested for the UI of a Spark application","title":" getAppUI"},{"location":"history-server/ApplicationHistoryProvider/#getlisting","text":"getListing (): Iterator [ ApplicationInfo ] Used when...FIXME","title":" getListing"},{"location":"history-server/ApplicationHistoryProvider/#onuidetached","text":"onUIDetached ( appId : String , attemptId : Option [ String ], ui : SparkUI ): Unit Used when...FIXME","title":" onUIDetached"},{"location":"history-server/ApplicationHistoryProvider/#writeeventlogs","text":"writeEventLogs ( appId : String , attemptId : Option [ String ], zipStream : ZipOutputStream ): Unit Writes events to a stream Used when...FIXME","title":" writeEventLogs"},{"location":"history-server/ApplicationHistoryProvider/#implementations","text":"FsHistoryProvider","title":"Implementations"},{"location":"history-server/EventLogFileWriter/","text":"EventLogFileWriter \u00b6 EventLogFileWriter is...FIXME","title":"EventLogFileWriter"},{"location":"history-server/EventLogFileWriter/#eventlogfilewriter","text":"EventLogFileWriter is...FIXME","title":"EventLogFileWriter"},{"location":"history-server/EventLoggingListener/","text":"EventLoggingListener \u00b6 EventLoggingListener is a SparkListener that writes out JSON-encoded events of a Spark application with event logging enabled (based on spark.eventLog.enabled configuration property). EventLoggingListener supports custom configuration properties . EventLoggingListener writes out log files to a directory (based on spark.eventLog.dir configuration property). Creating Instance \u00b6 EventLoggingListener takes the following to be created: Application ID Application Attempt ID Log Directory SparkConf Hadoop Configuration EventLoggingListener is created when SparkContext is created (with spark.eventLog.enabled enabled). EventLogFileWriter \u00b6 logWriter : EventLogFileWriter EventLoggingListener creates a EventLogFileWriter when created . Note All arguments to create an EventLoggingListener are passed to the EventLogFileWriter . The EventLogFileWriter is started when EventLoggingListener is started . The EventLogFileWriter is stopped when EventLoggingListener is stopped . The EventLogFileWriter is requested to writeEvent when EventLoggingListener is requested to start and log an event . Starting EventLoggingListener \u00b6 start (): Unit start requests the EventLogFileWriter to start and initEventLog . initEventLog \u00b6 initEventLog (): Unit initEventLog ...FIXME Logging Event \u00b6 logEvent ( event : SparkListenerEvent , flushLogger : Boolean = false ): Unit logEvent persists the given SparkListenerEvent in JSON format. logEvent converts the event to JSON format and requests the EventLogFileWriter to write it out . Stopping EventLoggingListener \u00b6 stop (): Unit stop requests the EventLogFileWriter to stop . stop is used when SparkContext is requested to stop . inprogress File Extension \u00b6 EventLoggingListener uses .inprogress file extension for in-flight event log files of active Spark applications. Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.EventLoggingListener logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.EventLoggingListener=ALL Refer to Logging .","title":"EventLoggingListener"},{"location":"history-server/EventLoggingListener/#eventlogginglistener","text":"EventLoggingListener is a SparkListener that writes out JSON-encoded events of a Spark application with event logging enabled (based on spark.eventLog.enabled configuration property). EventLoggingListener supports custom configuration properties . EventLoggingListener writes out log files to a directory (based on spark.eventLog.dir configuration property).","title":"EventLoggingListener"},{"location":"history-server/EventLoggingListener/#creating-instance","text":"EventLoggingListener takes the following to be created: Application ID Application Attempt ID Log Directory SparkConf Hadoop Configuration EventLoggingListener is created when SparkContext is created (with spark.eventLog.enabled enabled).","title":"Creating Instance"},{"location":"history-server/EventLoggingListener/#eventlogfilewriter","text":"logWriter : EventLogFileWriter EventLoggingListener creates a EventLogFileWriter when created . Note All arguments to create an EventLoggingListener are passed to the EventLogFileWriter . The EventLogFileWriter is started when EventLoggingListener is started . The EventLogFileWriter is stopped when EventLoggingListener is stopped . The EventLogFileWriter is requested to writeEvent when EventLoggingListener is requested to start and log an event .","title":" EventLogFileWriter"},{"location":"history-server/EventLoggingListener/#starting-eventlogginglistener","text":"start (): Unit start requests the EventLogFileWriter to start and initEventLog .","title":" Starting EventLoggingListener"},{"location":"history-server/EventLoggingListener/#initeventlog","text":"initEventLog (): Unit initEventLog ...FIXME","title":" initEventLog"},{"location":"history-server/EventLoggingListener/#logging-event","text":"logEvent ( event : SparkListenerEvent , flushLogger : Boolean = false ): Unit logEvent persists the given SparkListenerEvent in JSON format. logEvent converts the event to JSON format and requests the EventLogFileWriter to write it out .","title":" Logging Event"},{"location":"history-server/EventLoggingListener/#stopping-eventlogginglistener","text":"stop (): Unit stop requests the EventLogFileWriter to stop . stop is used when SparkContext is requested to stop .","title":" Stopping EventLoggingListener"},{"location":"history-server/EventLoggingListener/#inprogress-file-extension","text":"EventLoggingListener uses .inprogress file extension for in-flight event log files of active Spark applications.","title":" inprogress File Extension"},{"location":"history-server/EventLoggingListener/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.EventLoggingListener logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.EventLoggingListener=ALL Refer to Logging .","title":"Logging"},{"location":"history-server/FsHistoryProvider/","text":"FsHistoryProvider \u00b6 FsHistoryProvider is the default ApplicationHistoryProvider for Spark History Server . Creating Instance \u00b6 FsHistoryProvider takes the following to be created: SparkConf Clock (default: SystemClock ) FsHistoryProvider is created when HistoryServer standalone application is started (and no spark.history.provider configuration property was defined). Path of Application History Cache \u00b6 storePath : Option [ File ] FsHistoryProvider uses spark.history.store.path configuration property for the directory to cache application history. With storePath defined, FsHistoryProvider uses a LevelDB as the KVStore . Otherwise, a InMemoryStore . With storePath defined, FsHistoryProvider uses a HistoryServerDiskManager as the disk manager . Disk Manager \u00b6 diskManager : Option [ HistoryServerDiskManager ] FsHistoryProvider creates a HistoryServerDiskManager when created (with storePath defined based on spark.history.store.path configuration property). FsHistoryProvider uses the HistoryServerDiskManager for the following: startPolling getAppUI onUIDetached cleanAppData SparkUI of Spark Application \u00b6 getAppUI ( appId : String , attemptId : Option [ String ]): Option [ LoadedAppUI ] getAppUI is part of the ApplicationHistoryProvider abstraction. getAppUI ...FIXME onUIDetached \u00b6 onUIDetached (): Unit onUIDetached is part of the ApplicationHistoryProvider abstraction. onUIDetached ...FIXME loadDiskStore \u00b6 loadDiskStore ( dm : HistoryServerDiskManager , appId : String , attempt : AttemptInfoWrapper ): KVStore loadDiskStore ...FIXME loadDiskStore is used in getAppUI (with HistoryServerDiskManager available). createInMemoryStore \u00b6 createInMemoryStore ( attempt : AttemptInfoWrapper ): KVStore createInMemoryStore ...FIXME createInMemoryStore is used in getAppUI . rebuildAppStore \u00b6 rebuildAppStore ( store : KVStore , reader : EventLogFileReader , lastUpdated : Long ): Unit rebuildAppStore ...FIXME rebuildAppStore is used in loadDiskStore and createInMemoryStore . cleanAppData \u00b6 cleanAppData ( appId : String , attemptId : Option [ String ], logPath : String ): Unit cleanAppData ...FIXME cleanAppData is used in checkForLogs and deleteAttemptLogs . Polling for Logs \u00b6 startPolling (): Unit startPolling ...FIXME startPolling is used in initialize and startSafeModeCheckThread . Checking Available Event Logs \u00b6 checkForLogs (): Unit checkForLogs ...FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.history.FsHistoryProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.FsHistoryProvider=ALL Refer to Logging .","title":"FsHistoryProvider"},{"location":"history-server/FsHistoryProvider/#fshistoryprovider","text":"FsHistoryProvider is the default ApplicationHistoryProvider for Spark History Server .","title":"FsHistoryProvider"},{"location":"history-server/FsHistoryProvider/#creating-instance","text":"FsHistoryProvider takes the following to be created: SparkConf Clock (default: SystemClock ) FsHistoryProvider is created when HistoryServer standalone application is started (and no spark.history.provider configuration property was defined).","title":"Creating Instance"},{"location":"history-server/FsHistoryProvider/#path-of-application-history-cache","text":"storePath : Option [ File ] FsHistoryProvider uses spark.history.store.path configuration property for the directory to cache application history. With storePath defined, FsHistoryProvider uses a LevelDB as the KVStore . Otherwise, a InMemoryStore . With storePath defined, FsHistoryProvider uses a HistoryServerDiskManager as the disk manager .","title":" Path of Application History Cache"},{"location":"history-server/FsHistoryProvider/#disk-manager","text":"diskManager : Option [ HistoryServerDiskManager ] FsHistoryProvider creates a HistoryServerDiskManager when created (with storePath defined based on spark.history.store.path configuration property). FsHistoryProvider uses the HistoryServerDiskManager for the following: startPolling getAppUI onUIDetached cleanAppData","title":" Disk Manager"},{"location":"history-server/FsHistoryProvider/#sparkui-of-spark-application","text":"getAppUI ( appId : String , attemptId : Option [ String ]): Option [ LoadedAppUI ] getAppUI is part of the ApplicationHistoryProvider abstraction. getAppUI ...FIXME","title":" SparkUI of Spark Application"},{"location":"history-server/FsHistoryProvider/#onuidetached","text":"onUIDetached (): Unit onUIDetached is part of the ApplicationHistoryProvider abstraction. onUIDetached ...FIXME","title":" onUIDetached"},{"location":"history-server/FsHistoryProvider/#loaddiskstore","text":"loadDiskStore ( dm : HistoryServerDiskManager , appId : String , attempt : AttemptInfoWrapper ): KVStore loadDiskStore ...FIXME loadDiskStore is used in getAppUI (with HistoryServerDiskManager available).","title":" loadDiskStore"},{"location":"history-server/FsHistoryProvider/#createinmemorystore","text":"createInMemoryStore ( attempt : AttemptInfoWrapper ): KVStore createInMemoryStore ...FIXME createInMemoryStore is used in getAppUI .","title":" createInMemoryStore"},{"location":"history-server/FsHistoryProvider/#rebuildappstore","text":"rebuildAppStore ( store : KVStore , reader : EventLogFileReader , lastUpdated : Long ): Unit rebuildAppStore ...FIXME rebuildAppStore is used in loadDiskStore and createInMemoryStore .","title":" rebuildAppStore"},{"location":"history-server/FsHistoryProvider/#cleanappdata","text":"cleanAppData ( appId : String , attemptId : Option [ String ], logPath : String ): Unit cleanAppData ...FIXME cleanAppData is used in checkForLogs and deleteAttemptLogs .","title":" cleanAppData"},{"location":"history-server/FsHistoryProvider/#polling-for-logs","text":"startPolling (): Unit startPolling ...FIXME startPolling is used in initialize and startSafeModeCheckThread .","title":" Polling for Logs"},{"location":"history-server/FsHistoryProvider/#checking-available-event-logs","text":"checkForLogs (): Unit checkForLogs ...FIXME","title":" Checking Available Event Logs"},{"location":"history-server/FsHistoryProvider/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.history.FsHistoryProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.FsHistoryProvider=ALL Refer to Logging .","title":"Logging"},{"location":"history-server/HistoryAppStatusStore/","text":"HistoryAppStatusStore \u00b6 HistoryAppStatusStore is an AppStatusStore for SparkUI s in Spark History Server . Creating Instance \u00b6 HistoryAppStatusStore takes the following to be created: SparkConf KVStore HistoryAppStatusStore is created when: FsHistoryProvider is requested for a SparkUI (of a Spark application) ExecutorLogUrlHandler \u00b6 logUrlHandler : ExecutorLogUrlHandler HistoryAppStatusStore creates an ExecutorLogUrlHandler (for the logUrlPattern ) when created . HistoryAppStatusStore uses it when requested to replaceLogUrls . executorList \u00b6 executorList ( exec : v1 . ExecutorSummary , urlPattern : String ): v1 . ExecutorSummary executorList ...FIXME executorList is part of the AppStatusStore abstraction. executorSummary \u00b6 executorSummary ( executorId : String ): v1 . ExecutorSummary executorSummary ...FIXME executorSummary is part of the AppStatusStore abstraction. replaceLogUrls \u00b6 replaceLogUrls ( exec : v1 . ExecutorSummary , urlPattern : String ): v1 . ExecutorSummary replaceLogUrls ...FIXME replaceLogUrls is used when HistoryAppStatusStore is requested to executorList and executorSummary .","title":"HistoryAppStatusStore"},{"location":"history-server/HistoryAppStatusStore/#historyappstatusstore","text":"HistoryAppStatusStore is an AppStatusStore for SparkUI s in Spark History Server .","title":"HistoryAppStatusStore"},{"location":"history-server/HistoryAppStatusStore/#creating-instance","text":"HistoryAppStatusStore takes the following to be created: SparkConf KVStore HistoryAppStatusStore is created when: FsHistoryProvider is requested for a SparkUI (of a Spark application)","title":"Creating Instance"},{"location":"history-server/HistoryAppStatusStore/#executorlogurlhandler","text":"logUrlHandler : ExecutorLogUrlHandler HistoryAppStatusStore creates an ExecutorLogUrlHandler (for the logUrlPattern ) when created . HistoryAppStatusStore uses it when requested to replaceLogUrls .","title":" ExecutorLogUrlHandler"},{"location":"history-server/HistoryAppStatusStore/#executorlist","text":"executorList ( exec : v1 . ExecutorSummary , urlPattern : String ): v1 . ExecutorSummary executorList ...FIXME executorList is part of the AppStatusStore abstraction.","title":" executorList"},{"location":"history-server/HistoryAppStatusStore/#executorsummary","text":"executorSummary ( executorId : String ): v1 . ExecutorSummary executorSummary ...FIXME executorSummary is part of the AppStatusStore abstraction.","title":" executorSummary"},{"location":"history-server/HistoryAppStatusStore/#replacelogurls","text":"replaceLogUrls ( exec : v1 . ExecutorSummary , urlPattern : String ): v1 . ExecutorSummary replaceLogUrls ...FIXME replaceLogUrls is used when HistoryAppStatusStore is requested to executorList and executorSummary .","title":" replaceLogUrls"},{"location":"history-server/HistoryServer/","text":"HistoryServer \u00b6 HistoryServer is an extension of the web UI for reviewing event logs of running (active) and completed Spark applications with event log collection enabled (based on spark.eventLog.enabled configuration property). Starting HistoryServer Standalone Application \u00b6 main ( argStrings : Array [ String ]): Unit main creates a HistoryServerArguments (with the given argStrings arguments). main initializes security. main creates an ApplicationHistoryProvider (based on spark.history.provider configuration property). main creates a HistoryServer (with the ApplicationHistoryProvider and spark.history.ui.port configuration property) and requests it to bind . main requests the ApplicationHistoryProvider to start . main registers a shutdown hook that requests the HistoryServer to stop and sleeps... till the end of the world (giving the daemon thread a go). Creating Instance \u00b6 HistoryServer takes the following to be created: SparkConf ApplicationHistoryProvider SecurityManager Port number When created, HistoryServer initializes itself . HistoryServer is created when HistoryServer standalone application is started. ApplicationCacheOperations \u00b6 HistoryServer is a ApplicationCacheOperations . UIRoot \u00b6 HistoryServer is a UIRoot . Initializing HistoryServer \u00b6 initialize (): Unit initialize is part of the WebUI abstraction. initialize ...FIXME Attaching SparkUI \u00b6 attachSparkUI ( appId : String , attemptId : Option [ String ], ui : SparkUI , completed : Boolean ): Unit attachSparkUI is part of the ApplicationCacheOperations abstraction. attachSparkUI ...FIXME Spark UI \u00b6 getAppUI ( appId : String , attemptId : Option [ String ]): Option [ LoadedAppUI ] getAppUI is part of the ApplicationCacheOperations abstraction. getAppUI requests the ApplicationHistoryProvider for the Spark UI of a Spark application (based on the appId and attemptId ). Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.history.HistoryServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.HistoryServer=ALL Refer to Logging .","title":"HistoryServer"},{"location":"history-server/HistoryServer/#historyserver","text":"HistoryServer is an extension of the web UI for reviewing event logs of running (active) and completed Spark applications with event log collection enabled (based on spark.eventLog.enabled configuration property).","title":"HistoryServer"},{"location":"history-server/HistoryServer/#starting-historyserver-standalone-application","text":"main ( argStrings : Array [ String ]): Unit main creates a HistoryServerArguments (with the given argStrings arguments). main initializes security. main creates an ApplicationHistoryProvider (based on spark.history.provider configuration property). main creates a HistoryServer (with the ApplicationHistoryProvider and spark.history.ui.port configuration property) and requests it to bind . main requests the ApplicationHistoryProvider to start . main registers a shutdown hook that requests the HistoryServer to stop and sleeps... till the end of the world (giving the daemon thread a go).","title":" Starting HistoryServer Standalone Application"},{"location":"history-server/HistoryServer/#creating-instance","text":"HistoryServer takes the following to be created: SparkConf ApplicationHistoryProvider SecurityManager Port number When created, HistoryServer initializes itself . HistoryServer is created when HistoryServer standalone application is started.","title":"Creating Instance"},{"location":"history-server/HistoryServer/#applicationcacheoperations","text":"HistoryServer is a ApplicationCacheOperations .","title":" ApplicationCacheOperations"},{"location":"history-server/HistoryServer/#uiroot","text":"HistoryServer is a UIRoot .","title":" UIRoot"},{"location":"history-server/HistoryServer/#initializing-historyserver","text":"initialize (): Unit initialize is part of the WebUI abstraction. initialize ...FIXME","title":" Initializing HistoryServer"},{"location":"history-server/HistoryServer/#attaching-sparkui","text":"attachSparkUI ( appId : String , attemptId : Option [ String ], ui : SparkUI , completed : Boolean ): Unit attachSparkUI is part of the ApplicationCacheOperations abstraction. attachSparkUI ...FIXME","title":" Attaching SparkUI"},{"location":"history-server/HistoryServer/#spark-ui","text":"getAppUI ( appId : String , attemptId : Option [ String ]): Option [ LoadedAppUI ] getAppUI is part of the ApplicationCacheOperations abstraction. getAppUI requests the ApplicationHistoryProvider for the Spark UI of a Spark application (based on the appId and attemptId ).","title":" Spark UI"},{"location":"history-server/HistoryServer/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.history.HistoryServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.HistoryServer=ALL Refer to Logging .","title":"Logging"},{"location":"history-server/HistoryServerArguments/","text":"== HistoryServerArguments HistoryServerArguments is the command-line parser for the index.md[History Server]. When HistoryServerArguments is executed with a single command-line parameter it is assumed to be the event logs directory. $ ./sbin/start-history-server.sh /tmp/spark-events This is however deprecated since Spark 1.1.0 and you should see the following WARN message in the logs: WARN HistoryServerArguments: Setting log directory through the command line is deprecated as of Spark 1.1.0. Please set this through spark.history.fs.logDirectory instead. The same WARN message shows up for --dir and -d command-line options. --properties-file [propertiesFile] command-line option specifies the file with the custom spark-properties.md[Spark properties]. NOTE: When not specified explicitly, History Server uses the default configuration file, i.e. spark-properties.md#spark-defaults-conf[spark-defaults.conf]. [TIP] \u00b6 Enable WARN logging level for org.apache.spark.deploy.history.HistoryServerArguments logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.HistoryServerArguments=WARN Refer to spark-logging.md[Logging]. \u00b6","title":"HistoryServerArguments"},{"location":"history-server/HistoryServerArguments/#tip","text":"Enable WARN logging level for org.apache.spark.deploy.history.HistoryServerArguments logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.HistoryServerArguments=WARN","title":"[TIP]"},{"location":"history-server/HistoryServerArguments/#refer-to-spark-loggingmdlogging","text":"","title":"Refer to spark-logging.md[Logging]."},{"location":"history-server/HistoryServerDiskManager/","text":"HistoryServerDiskManager \u00b6 HistoryServerDiskManager is a disk manager for FsHistoryProvider . Creating Instance \u00b6 HistoryServerDiskManager takes the following to be created: SparkConf Path KVStore Clock HistoryServerDiskManager is created when: FsHistoryProvider is created (and initializes a diskManager ) Initializing \u00b6 initialize (): Unit initialize ...FIXME initialize is used when: FsHistoryProvider is requested to startPolling Releasing Application Store \u00b6 release ( appId : String , attemptId : Option [ String ], delete : Boolean = false ): Unit release ...FIXME release is used when: FsHistoryProvider is requested to onUIDetached , cleanAppData and loadDiskStore","title":"HistoryServerDiskManager"},{"location":"history-server/HistoryServerDiskManager/#historyserverdiskmanager","text":"HistoryServerDiskManager is a disk manager for FsHistoryProvider .","title":"HistoryServerDiskManager"},{"location":"history-server/HistoryServerDiskManager/#creating-instance","text":"HistoryServerDiskManager takes the following to be created: SparkConf Path KVStore Clock HistoryServerDiskManager is created when: FsHistoryProvider is created (and initializes a diskManager )","title":"Creating Instance"},{"location":"history-server/HistoryServerDiskManager/#initializing","text":"initialize (): Unit initialize ...FIXME initialize is used when: FsHistoryProvider is requested to startPolling","title":" Initializing"},{"location":"history-server/HistoryServerDiskManager/#releasing-application-store","text":"release ( appId : String , attemptId : Option [ String ], delete : Boolean = false ): Unit release ...FIXME release is used when: FsHistoryProvider is requested to onUIDetached , cleanAppData and loadDiskStore","title":" Releasing Application Store"},{"location":"history-server/JsonProtocol/","text":"JsonProtocol Utility \u00b6 JsonProtocol is an utility to convert SparkListenerEvent s to and from JSON format . ObjectMapper \u00b6 JsonProtocol uses an Jackson Databind ObjectMapper for performing conversions to and from JSON. Converting Spark Event to JSON \u00b6 sparkEventToJson ( event : SparkListenerEvent ): JValue sparkEventToJson converts the given SparkListenerEvent to JSON format. sparkEventToJson is used when...FIXME Converting JSON to Spark Event \u00b6 sparkEventFromJson ( json : JValue ): SparkListenerEvent sparkEventFromJson converts a JSON-encoded event to a SparkListenerEvent . sparkEventFromJson is used when...FIXME","title":"JsonProtocol"},{"location":"history-server/JsonProtocol/#jsonprotocol-utility","text":"JsonProtocol is an utility to convert SparkListenerEvent s to and from JSON format .","title":"JsonProtocol Utility"},{"location":"history-server/JsonProtocol/#objectmapper","text":"JsonProtocol uses an Jackson Databind ObjectMapper for performing conversions to and from JSON.","title":" ObjectMapper"},{"location":"history-server/JsonProtocol/#converting-spark-event-to-json","text":"sparkEventToJson ( event : SparkListenerEvent ): JValue sparkEventToJson converts the given SparkListenerEvent to JSON format. sparkEventToJson is used when...FIXME","title":" Converting Spark Event to JSON"},{"location":"history-server/JsonProtocol/#converting-json-to-spark-event","text":"sparkEventFromJson ( json : JValue ): SparkListenerEvent sparkEventFromJson converts a JSON-encoded event to a SparkListenerEvent . sparkEventFromJson is used when...FIXME","title":" Converting JSON to Spark Event"},{"location":"history-server/ReplayListenerBus/","text":"ReplayListenerBus \u00b6 ReplayListenerBus is a SparkListenerBus that can replay JSON-encoded SparkListenerEvent events . ReplayListenerBus is used by FsHistoryProvider . Replaying JSON-encoded SparkListenerEvents \u00b6 replay ( logData : InputStream , sourceName : String , maybeTruncated : Boolean = false ): Unit replay reads JSON-encoded SparkListener.md#SparkListenerEvent[SparkListenerEvent] events from logData (one event per line) and posts them to all registered SparkListenerInterface s. replay uses spark-history-server:JsonProtocol.md#sparkEventFromJson[ JsonProtocol to convert JSON-encoded events to SparkListenerEvent objects]. NOTE: replay uses jackson from http://json4s.org/[json4s ] library to parse the AST for JSON. When there is an exception parsing a JSON event, you may see the following WARN message in the logs (for the last line) or a JsonParseException . WARN Got JsonParseException from log file $sourceName at line [lineNumber], the file might not have finished writing cleanly. Any other non-IO exceptions end up with the following ERROR messages in the logs: ERROR Exception parsing Spark event log: [sourceName] ERROR Malformed line #[lineNumber]: [currentLine] NOTE: The sourceName input argument is only used for messages.","title":"ReplayListenerBus"},{"location":"history-server/ReplayListenerBus/#replaylistenerbus","text":"ReplayListenerBus is a SparkListenerBus that can replay JSON-encoded SparkListenerEvent events . ReplayListenerBus is used by FsHistoryProvider .","title":"ReplayListenerBus"},{"location":"history-server/ReplayListenerBus/#replaying-json-encoded-sparklistenerevents","text":"replay ( logData : InputStream , sourceName : String , maybeTruncated : Boolean = false ): Unit replay reads JSON-encoded SparkListener.md#SparkListenerEvent[SparkListenerEvent] events from logData (one event per line) and posts them to all registered SparkListenerInterface s. replay uses spark-history-server:JsonProtocol.md#sparkEventFromJson[ JsonProtocol to convert JSON-encoded events to SparkListenerEvent objects]. NOTE: replay uses jackson from http://json4s.org/[json4s ] library to parse the AST for JSON. When there is an exception parsing a JSON event, you may see the following WARN message in the logs (for the last line) or a JsonParseException . WARN Got JsonParseException from log file $sourceName at line [lineNumber], the file might not have finished writing cleanly. Any other non-IO exceptions end up with the following ERROR messages in the logs: ERROR Exception parsing Spark event log: [sourceName] ERROR Malformed line #[lineNumber]: [currentLine] NOTE: The sourceName input argument is only used for messages.","title":" Replaying JSON-encoded SparkListenerEvents"},{"location":"history-server/SQLHistoryListener/","text":"== SQLHistoryListener SQLHistoryListener is a custom spark-sql-SQLListener.md[SQLListener] for index.md[History Server]. It attaches spark-sql-webui.md#creating-instance[SQL tab] to History Server's web UI only when the first spark-sql-SQLListener.md#SparkListenerSQLExecutionStart[SparkListenerSQLExecutionStart] arrives and shuts < > off. It also handles < >. NOTE: Support for SQL UI in History Server was added in SPARK-11206 Support SQL UI on the history server. CAUTION: FIXME Add the link to the JIRA. === [[onOtherEvent]] onOtherEvent [source, scala] \u00b6 onOtherEvent(event: SparkListenerEvent): Unit \u00b6 When SparkListenerSQLExecutionStart event comes, onOtherEvent attaches spark-sql-webui.md#creating-instance[SQL tab] to web UI and passes the call to the parent spark-sql-SQLListener.md[SQLListener]. === [[onTaskEnd]] onTaskEnd CAUTION: FIXME === [[creating-instance]] Creating SQLHistoryListener Instance SQLHistoryListener is created using a ( private[sql] ) SQLHistoryListenerFactory class (which is SparkHistoryListenerFactory ). The SQLHistoryListenerFactory class is registered when spark-webui-SparkUI.md#createHistoryUI[ SparkUI creates a web UI for History Server] as a Java service in META-INF/services/org.apache.spark.scheduler.SparkHistoryListenerFactory : org.apache.spark.sql.execution.ui.SQLHistoryListenerFactory NOTE: Loading the service uses Java's https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-[ServiceLoader.load ] method. === [[onExecutorMetricsUpdate]] onExecutorMetricsUpdate onExecutorMetricsUpdate does nothing.","title":"SQLHistoryListener"},{"location":"history-server/SQLHistoryListener/#source-scala","text":"","title":"[source, scala]"},{"location":"history-server/SQLHistoryListener/#onothereventevent-sparklistenerevent-unit","text":"When SparkListenerSQLExecutionStart event comes, onOtherEvent attaches spark-sql-webui.md#creating-instance[SQL tab] to web UI and passes the call to the parent spark-sql-SQLListener.md[SQLListener]. === [[onTaskEnd]] onTaskEnd CAUTION: FIXME === [[creating-instance]] Creating SQLHistoryListener Instance SQLHistoryListener is created using a ( private[sql] ) SQLHistoryListenerFactory class (which is SparkHistoryListenerFactory ). The SQLHistoryListenerFactory class is registered when spark-webui-SparkUI.md#createHistoryUI[ SparkUI creates a web UI for History Server] as a Java service in META-INF/services/org.apache.spark.scheduler.SparkHistoryListenerFactory : org.apache.spark.sql.execution.ui.SQLHistoryListenerFactory NOTE: Loading the service uses Java's https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-[ServiceLoader.load ] method. === [[onExecutorMetricsUpdate]] onExecutorMetricsUpdate onExecutorMetricsUpdate does nothing.","title":"onOtherEvent(event: SparkListenerEvent): Unit"},{"location":"history-server/configuration-properties/","text":"Configuration Properties \u00b6 The following contains the configuration properties of EventLoggingListener and HistoryServer . spark.eventLog.dir \u00b6 Directory where Spark events are logged to (e.g. hdfs://namenode:8021/directory ) Default: /tmp/spark-events The directory must exist before SparkContext is created. spark.eventLog.buffer.kb \u00b6 Size of the buffer to use when writing to output streams. Default: 100 spark.eventLog.compress \u00b6 Whether to enable ( true ) or disable ( false ) event compression (using a CompressionCodec ) Default: false spark.eventLog.enabled \u00b6 Whether to enable ( true ) or disable ( false ) persisting Spark events. Default: false spark.eventLog.logBlockUpdates.enabled \u00b6 Whether EventLoggingListener should log RDD block updates ( true ) or not ( false ) Default: false spark.eventLog.overwrite \u00b6 Whether to enable ( true ) or disable ( false ) deleting (or at least overwriting) an existing .inprogress event log files Default: false spark.history.fs.logDirectory \u00b6 The directory for event log files. The directory has to exist before starting History Server. Default: file:/tmp/spark-events spark.history.kerberos.enabled \u00b6 Whether to enable ( true ) or disable ( false ) security when working with HDFS with security enabled (Kerberos). Default: false spark.history.kerberos.keytab \u00b6 Keytab to use for login to Kerberos. Required when spark.history.kerberos.enabled is enabled. Default: (empty) spark.history.kerberos.principal \u00b6 Kerberos principal. Required when spark.history.kerberos.enabled is enabled. Default: (empty) spark.history.provider \u00b6 Fully-qualified class name of an ApplicationHistoryProvider for HistoryServer . Default: org.apache.spark.deploy.history.FsHistoryProvider spark.history.store.path \u00b6 Local directory where to cache application history information (by ) Default: (undefined) (i.e. all history information will be kept in memory) spark.history.retainedApplications \u00b6 How many Spark applications HistoryServer should retain Default: 50 spark.history.ui.maxApplications \u00b6 How many Spark applications HistoryServer should show in the UI Default: (unbounded) spark.history.ui.port \u00b6 The port of History Server's web UI. Default: 18080","title":"Configuration Properties"},{"location":"history-server/configuration-properties/#configuration-properties","text":"The following contains the configuration properties of EventLoggingListener and HistoryServer .","title":"Configuration Properties"},{"location":"history-server/configuration-properties/#sparkeventlogdir","text":"Directory where Spark events are logged to (e.g. hdfs://namenode:8021/directory ) Default: /tmp/spark-events The directory must exist before SparkContext is created.","title":" spark.eventLog.dir"},{"location":"history-server/configuration-properties/#sparkeventlogbufferkb","text":"Size of the buffer to use when writing to output streams. Default: 100","title":" spark.eventLog.buffer.kb"},{"location":"history-server/configuration-properties/#sparkeventlogcompress","text":"Whether to enable ( true ) or disable ( false ) event compression (using a CompressionCodec ) Default: false","title":" spark.eventLog.compress"},{"location":"history-server/configuration-properties/#sparkeventlogenabled","text":"Whether to enable ( true ) or disable ( false ) persisting Spark events. Default: false","title":" spark.eventLog.enabled"},{"location":"history-server/configuration-properties/#sparkeventloglogblockupdatesenabled","text":"Whether EventLoggingListener should log RDD block updates ( true ) or not ( false ) Default: false","title":" spark.eventLog.logBlockUpdates.enabled"},{"location":"history-server/configuration-properties/#sparkeventlogoverwrite","text":"Whether to enable ( true ) or disable ( false ) deleting (or at least overwriting) an existing .inprogress event log files Default: false","title":" spark.eventLog.overwrite"},{"location":"history-server/configuration-properties/#sparkhistoryfslogdirectory","text":"The directory for event log files. The directory has to exist before starting History Server. Default: file:/tmp/spark-events","title":" spark.history.fs.logDirectory"},{"location":"history-server/configuration-properties/#sparkhistorykerberosenabled","text":"Whether to enable ( true ) or disable ( false ) security when working with HDFS with security enabled (Kerberos). Default: false","title":" spark.history.kerberos.enabled"},{"location":"history-server/configuration-properties/#sparkhistorykerberoskeytab","text":"Keytab to use for login to Kerberos. Required when spark.history.kerberos.enabled is enabled. Default: (empty)","title":" spark.history.kerberos.keytab"},{"location":"history-server/configuration-properties/#sparkhistorykerberosprincipal","text":"Kerberos principal. Required when spark.history.kerberos.enabled is enabled. Default: (empty)","title":" spark.history.kerberos.principal"},{"location":"history-server/configuration-properties/#sparkhistoryprovider","text":"Fully-qualified class name of an ApplicationHistoryProvider for HistoryServer . Default: org.apache.spark.deploy.history.FsHistoryProvider","title":" spark.history.provider"},{"location":"history-server/configuration-properties/#sparkhistorystorepath","text":"Local directory where to cache application history information (by ) Default: (undefined) (i.e. all history information will be kept in memory)","title":" spark.history.store.path"},{"location":"history-server/configuration-properties/#sparkhistoryretainedapplications","text":"How many Spark applications HistoryServer should retain Default: 50","title":" spark.history.retainedApplications"},{"location":"history-server/configuration-properties/#sparkhistoryuimaxapplications","text":"How many Spark applications HistoryServer should show in the UI Default: (unbounded)","title":" spark.history.ui.maxApplications"},{"location":"history-server/configuration-properties/#sparkhistoryuiport","text":"The port of History Server's web UI. Default: 18080","title":" spark.history.ui.port"},{"location":"local/","text":"Spark local \u00b6 Spark local is one of the available runtime environments in Apache Spark. It is the only available runtime with no need for a proper cluster manager (and hence many call it a pseudo-cluster , however such concept do exist in Spark and is a bit different). Spark local is used for the following master URLs (as specified using <<../SparkConf.md#, SparkConf.setMaster>> method or <<../configuration-properties.md#spark.master, spark.master>> configuration property): local (with exactly 1 CPU core) local[n] (with exactly n CPU cores) ++local[ ]++* (with the total number of CPU cores that is the number of available CPU cores on the local machine) local[n, m] (with exactly n CPU cores and m retries when a task fails) ++local[ , m]++* (with the total number of CPU cores that is the number of available CPU cores on the local machine) Internally, Spark local uses < > as the <<../SchedulerBackend.md#, SchedulerBackend>> and executor:ExecutorBackend.md[]. .Architecture of Spark local image::../diagrams/spark-local-architecture.png[align=\"center\"] In this non-distributed multi-threaded runtime environment, Spark spawns all the main execution components - the spark-driver.md[driver] and an executor:Executor.md[] - in the same single JVM. The default parallelism is the number of threads as specified in the < >. This is the only mode where a driver is used for execution (as it acts both as the driver and the only executor). The local mode is very convenient for testing, debugging or demonstration purposes as it requires no earlier setup to launch Spark applications. This mode of operation is also called http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark[Spark in-process] or (less commonly) a local version of Spark . SparkContext.isLocal returns true when Spark runs in local mode. scala> sc.isLocal res0: Boolean = true spark-shell.md[Spark shell] defaults to local mode with local[*] as the spark-deployment-environments.md#master-urls[the master URL]. scala> sc.master res0: String = local[*] Tasks are not re-executed on failure in local mode (unless < > is used). The scheduler:TaskScheduler.md[task scheduler] in local mode works with local/spark-LocalSchedulerBackend.md[LocalSchedulerBackend] task scheduler backend. == [[masterURL]] Master URL You can run Spark in local mode using local , local[n] or the most general local[*] for spark-deployment-environments.md#master-urls[the master URL]. The URL says how many threads can be used in total: local uses 1 thread only. local[n] uses n threads. local[*] uses as many threads as the number of processors available to the Java virtual machine (it uses https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--[Runtime.getRuntime.availableProcessors ()] to know the number). NOTE: What happens when there are less cores than n in local[n] master URL? \"Breaks\" scheduling as Spark assumes more CPU cores available to execute tasks. [[local-with-retries]] local[N, maxFailures] (called local-with-retries ) with N being * or the number of threads to use (as explained above) and maxFailures being the value of <<../configuration-properties.md#spark.task.maxFailures, spark.task.maxFailures>> configuration property. == [[task-submission]] Task Submission a.k.a. reviveOffers .TaskSchedulerImpl.submitTasks in local mode image::taskscheduler-submitTasks-local-mode.png[align=\"center\"] When ReviveOffers or StatusUpdate messages are received, local/spark-LocalEndpoint.md[LocalEndpoint] places an offer to TaskSchedulerImpl (using TaskSchedulerImpl.resourceOffers ). If there is one or more tasks that match the offer, they are launched (using executor.launchTask method). The number of tasks to be launched is controlled by the number of threads as specified in < >. The executor uses threads to spawn the tasks.","title":"Spark local"},{"location":"local/#spark-local","text":"Spark local is one of the available runtime environments in Apache Spark. It is the only available runtime with no need for a proper cluster manager (and hence many call it a pseudo-cluster , however such concept do exist in Spark and is a bit different). Spark local is used for the following master URLs (as specified using <<../SparkConf.md#, SparkConf.setMaster>> method or <<../configuration-properties.md#spark.master, spark.master>> configuration property): local (with exactly 1 CPU core) local[n] (with exactly n CPU cores) ++local[ ]++* (with the total number of CPU cores that is the number of available CPU cores on the local machine) local[n, m] (with exactly n CPU cores and m retries when a task fails) ++local[ , m]++* (with the total number of CPU cores that is the number of available CPU cores on the local machine) Internally, Spark local uses < > as the <<../SchedulerBackend.md#, SchedulerBackend>> and executor:ExecutorBackend.md[]. .Architecture of Spark local image::../diagrams/spark-local-architecture.png[align=\"center\"] In this non-distributed multi-threaded runtime environment, Spark spawns all the main execution components - the spark-driver.md[driver] and an executor:Executor.md[] - in the same single JVM. The default parallelism is the number of threads as specified in the < >. This is the only mode where a driver is used for execution (as it acts both as the driver and the only executor). The local mode is very convenient for testing, debugging or demonstration purposes as it requires no earlier setup to launch Spark applications. This mode of operation is also called http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark[Spark in-process] or (less commonly) a local version of Spark . SparkContext.isLocal returns true when Spark runs in local mode. scala> sc.isLocal res0: Boolean = true spark-shell.md[Spark shell] defaults to local mode with local[*] as the spark-deployment-environments.md#master-urls[the master URL]. scala> sc.master res0: String = local[*] Tasks are not re-executed on failure in local mode (unless < > is used). The scheduler:TaskScheduler.md[task scheduler] in local mode works with local/spark-LocalSchedulerBackend.md[LocalSchedulerBackend] task scheduler backend. == [[masterURL]] Master URL You can run Spark in local mode using local , local[n] or the most general local[*] for spark-deployment-environments.md#master-urls[the master URL]. The URL says how many threads can be used in total: local uses 1 thread only. local[n] uses n threads. local[*] uses as many threads as the number of processors available to the Java virtual machine (it uses https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--[Runtime.getRuntime.availableProcessors ()] to know the number). NOTE: What happens when there are less cores than n in local[n] master URL? \"Breaks\" scheduling as Spark assumes more CPU cores available to execute tasks. [[local-with-retries]] local[N, maxFailures] (called local-with-retries ) with N being * or the number of threads to use (as explained above) and maxFailures being the value of <<../configuration-properties.md#spark.task.maxFailures, spark.task.maxFailures>> configuration property. == [[task-submission]] Task Submission a.k.a. reviveOffers .TaskSchedulerImpl.submitTasks in local mode image::taskscheduler-submitTasks-local-mode.png[align=\"center\"] When ReviveOffers or StatusUpdate messages are received, local/spark-LocalEndpoint.md[LocalEndpoint] places an offer to TaskSchedulerImpl (using TaskSchedulerImpl.resourceOffers ). If there is one or more tasks that match the offer, they are launched (using executor.launchTask method). The number of tasks to be launched is controlled by the number of threads as specified in < >. The executor uses threads to spawn the tasks.","title":"Spark local"},{"location":"local/LauncherBackend/","text":"== [[LauncherBackend]] LauncherBackend LauncherBackend is the < > of < > that can < >. [[contract]] .LauncherBackend Contract (Abstract Methods Only) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | conf a| [[conf]] [source, scala] \u00b6 conf: SparkConf \u00b6 SparkConf.md[] Used exclusively when LauncherBackend is requested to < > (to access configuration-properties.md#spark.launcher.port[spark.launcher.port] and configuration-properties.md#spark.launcher.secret[spark.launcher.secret] configuration properties) | onStopRequest a| [[onStopRequest]] [source, scala] \u00b6 onStopRequest(): Unit \u00b6 Handles stop requests (to stop the Spark application as gracefully as possible) Used exclusively when LauncherBackend is requested to < > |=== [[creating-instance]] LauncherBackend takes no arguments to be created. NOTE: LauncherBackend is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. [[internal-registries]] .LauncherBackend's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | _isConnected a| [[_isConnected]][[isConnected]] Flag that says whether...FIXME ( true ) or not ( false ) Default: false Used when...FIXME | clientThread a| [[clientThread]] Java's https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.lang.Thread ] Used when...FIXME | connection a| [[connection]] BackendConnection Used when...FIXME | lastState a| [[lastState]] SparkAppHandle.State Used when...FIXME |=== [[implementations]] LauncherBackend is < > (as an anonymous class) for the following: Spark on YARN's < > Spark local's < > Spark on Mesos' < > Spark Standalone's < > === [[close]] Closing -- close Method [source, scala] \u00b6 close(): Unit \u00b6 close ...FIXME NOTE: close is used when...FIXME === [[connect]] Connecting -- connect Method [source, scala] \u00b6 connect(): Unit \u00b6 connect ...FIXME [NOTE] \u00b6 connect is used when: Spark Standalone's StandaloneSchedulerBackend is requested to < > (in client deploy mode) Spark local's LocalSchedulerBackend is < > Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to < > (in client deploy mode) * Spark on YARN's Client is requested to < > \u00b6 === [[fireStopRequest]] fireStopRequest Internal Method [source, scala] \u00b6 fireStopRequest(): Unit \u00b6 fireStopRequest ...FIXME NOTE: fireStopRequest is used exclusively when BackendConnection is requested to handle a Stop message. === [[onDisconnected]] Handling Disconnects From Scheduling Backend -- onDisconnected Method [source, scala] \u00b6 onDisconnected(): Unit \u00b6 onDisconnected does nothing by default and is expected to be overriden by < >. NOTE: onDisconnected is used when...FIXME === [[setAppId]] setAppId Method [source, scala] \u00b6 setAppId(appId: String): Unit \u00b6 setAppId ...FIXME NOTE: setAppId is used when...FIXME === [[setState]] setState Method [source, scala] \u00b6 setState(state: SparkAppHandle.State): Unit \u00b6 setState ...FIXME NOTE: setState is used when...FIXME","title":"LauncherBackend"},{"location":"local/LauncherBackend/#source-scala","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#conf-sparkconf","text":"SparkConf.md[] Used exclusively when LauncherBackend is requested to < > (to access configuration-properties.md#spark.launcher.port[spark.launcher.port] and configuration-properties.md#spark.launcher.secret[spark.launcher.secret] configuration properties) | onStopRequest a| [[onStopRequest]]","title":"conf: SparkConf"},{"location":"local/LauncherBackend/#source-scala_1","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#onstoprequest-unit","text":"Handles stop requests (to stop the Spark application as gracefully as possible) Used exclusively when LauncherBackend is requested to < > |=== [[creating-instance]] LauncherBackend takes no arguments to be created. NOTE: LauncherBackend is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. [[internal-registries]] .LauncherBackend's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | _isConnected a| [[_isConnected]][[isConnected]] Flag that says whether...FIXME ( true ) or not ( false ) Default: false Used when...FIXME | clientThread a| [[clientThread]] Java's https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.lang.Thread ] Used when...FIXME | connection a| [[connection]] BackendConnection Used when...FIXME | lastState a| [[lastState]] SparkAppHandle.State Used when...FIXME |=== [[implementations]] LauncherBackend is < > (as an anonymous class) for the following: Spark on YARN's < > Spark local's < > Spark on Mesos' < > Spark Standalone's < > === [[close]] Closing -- close Method","title":"onStopRequest(): Unit"},{"location":"local/LauncherBackend/#source-scala_2","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#close-unit","text":"close ...FIXME NOTE: close is used when...FIXME === [[connect]] Connecting -- connect Method","title":"close(): Unit"},{"location":"local/LauncherBackend/#source-scala_3","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#connect-unit","text":"connect ...FIXME","title":"connect(): Unit"},{"location":"local/LauncherBackend/#note","text":"connect is used when: Spark Standalone's StandaloneSchedulerBackend is requested to < > (in client deploy mode) Spark local's LocalSchedulerBackend is < > Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to < > (in client deploy mode)","title":"[NOTE]"},{"location":"local/LauncherBackend/#spark-on-yarns-client-is-requested-to","text":"=== [[fireStopRequest]] fireStopRequest Internal Method","title":"* Spark on YARN's Client is requested to &lt;&gt;"},{"location":"local/LauncherBackend/#source-scala_4","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#firestoprequest-unit","text":"fireStopRequest ...FIXME NOTE: fireStopRequest is used exclusively when BackendConnection is requested to handle a Stop message. === [[onDisconnected]] Handling Disconnects From Scheduling Backend -- onDisconnected Method","title":"fireStopRequest(): Unit"},{"location":"local/LauncherBackend/#source-scala_5","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#ondisconnected-unit","text":"onDisconnected does nothing by default and is expected to be overriden by < >. NOTE: onDisconnected is used when...FIXME === [[setAppId]] setAppId Method","title":"onDisconnected(): Unit"},{"location":"local/LauncherBackend/#source-scala_6","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#setappidappid-string-unit","text":"setAppId ...FIXME NOTE: setAppId is used when...FIXME === [[setState]] setState Method","title":"setAppId(appId: String): Unit"},{"location":"local/LauncherBackend/#source-scala_7","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#setstatestate-sparkapphandlestate-unit","text":"setState ...FIXME NOTE: setState is used when...FIXME","title":"setState(state: SparkAppHandle.State): Unit"},{"location":"local/LocalEndpoint/","text":"== [[LocalEndpoint]] LocalEndpoint -- RPC Endpoint for LocalSchedulerBackend LocalEndpoint is the <<../index.md#ThreadSafeRpcEndpoint, ThreadSafeRpcEndpoint>> for < > and is registered under the LocalSchedulerBackendEndpoint name. LocalEndpoint is < > exclusively when LocalSchedulerBackend is requested to < >. Put simply, LocalEndpoint is the communication channel between < > and < >. LocalEndpoint is a (thread-safe) rpc:RpcEndpoint.md[RpcEndpoint] that hosts an < > (with driver ID and localhost hostname) for Spark local mode. [[messages]] .LocalEndpoint's RPC Messages [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Message | Description | < > | Requests the < > to executor:Executor.md#killTask[kill a given task] | < > | Calls < > < > | < > | Requests the < > to executor:Executor.md#stop[stop] |=== When a LocalEndpoint starts up (as part of Spark local's initialization) it prints out the following INFO messages to the logs: INFO Executor: Starting executor ID driver on host localhost INFO Executor: Using REPL class URI: http://192.168.1.4:56131 [[executor]] LocalEndpoint creates a single executor:Executor.md[] with the following properties: [[localExecutorId]] driver ID for the executor:Executor.md#executorId[executor ID] [[localExecutorHostname]] localhost for the executor:Executor.md#executorHostname[hostname] < > for the executor:Executor.md#userClassPath[user-defined CLASSPATH] executor:Executor.md#isLocal[isLocal] flag enabled The < > is then used when LocalEndpoint is requested to handle < > and < > RPC messages, and < >. [[internal-registries]] .LocalEndpoint's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | freeCores a| [[freeCores]] The number of CPU cores that are free to use (to schedule tasks) Default: Initial < > (aka totalCores ) Increments when LocalEndpoint is requested to handle < > RPC message with a finished state Decrements when LocalEndpoint is requested to < > and there were tasks to execute NOTE: A single task to execute costs scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[spark.task.cpus] configuration (default: 1 ). Used when LocalEndpoint is requested to < > |=== [[logging]] [TIP] ==== Enable INFO logging level for org.apache.spark.scheduler.local.LocalEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.local.LocalEndpoint=INFO Refer to <<../spark-logging.md#, Logging>>. \u00b6 === [[creating-instance]] Creating LocalEndpoint Instance LocalEndpoint takes the following to be created: [[rpcEnv]] <<../index.md#, RpcEnv>> [[userClassPath]] User-defined class path ( Seq[URL] ) that is the < > configuration property and is used exclusively to create the < > [[scheduler]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] [[executorBackend]] < > [[totalCores]] Number of CPU cores (aka totalCores ) LocalEndpoint initializes the < >. === [[receive]] Processing Receive-Only RPC Messages -- receive Method [source, scala] \u00b6 receive: PartialFunction[Any, Unit] \u00b6 NOTE: receive is part of the rpc:RpcEndpoint.md#receive[RpcEndpoint] abstraction. receive handles ( processes ) < >, < >, and < > RPC messages. ==== [[ReviveOffers]] ReviveOffers RPC Message [source, scala] \u00b6 ReviveOffers() \u00b6 When < >, LocalEndpoint < >. NOTE: ReviveOffers RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. ==== [[StatusUpdate]] StatusUpdate RPC Message [source, scala] \u00b6 StatusUpdate( taskId: Long, state: TaskState, serializedData: ByteBuffer) When < >, LocalEndpoint requests the < > to scheduler:TaskSchedulerImpl.md#statusUpdate[handle a task status update] (given the taskId , the task state and the data). If the given scheduler:Task.md#TaskState[TaskState] is a finished state (one of FINISHED , FAILED , KILLED , LOST states), LocalEndpoint adds scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[spark.task.cpus] configuration (default: 1 ) to the < > registry followed by < >. NOTE: StatusUpdate RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. ==== [[KillTask]] KillTask RPC Message [source, scala] \u00b6 KillTask( taskId: Long, interruptThread: Boolean, reason: String) When < >, LocalEndpoint requests the single < > to executor:Executor.md#killTask[kill a task] (given the taskId , the interruptThread flag and the reason). NOTE: KillTask RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. === [[reviveOffers]] Reviving Offers -- reviveOffers Method [source, scala] \u00b6 reviveOffers(): Unit \u00b6 reviveOffers ...FIXME NOTE: reviveOffers is used when LocalEndpoint is requested to < > (namely < > and < >). === [[receiveAndReply]] Processing Receive-Reply RPC Messages -- receiveAndReply Method [source, scala] \u00b6 receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] \u00b6 NOTE: receiveAndReply is part of the rpc:RpcEndpoint.md#receiveAndReply[RpcEndpoint] abstraction. receiveAndReply handles ( processes ) < > RPC message exclusively. ==== [[StopExecutor]] StopExecutor RPC Message [source, scala] \u00b6 StopExecutor() \u00b6 When < >, LocalEndpoint requests the single < > to executor:Executor.md#stop[stop] and requests the given RpcCallContext to reply with true (as the response). NOTE: StopExecutor RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >.","title":"LocalEndpoint"},{"location":"local/LocalEndpoint/#refer-to-spark-loggingmd-logging","text":"=== [[creating-instance]] Creating LocalEndpoint Instance LocalEndpoint takes the following to be created: [[rpcEnv]] <<../index.md#, RpcEnv>> [[userClassPath]] User-defined class path ( Seq[URL] ) that is the < > configuration property and is used exclusively to create the < > [[scheduler]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] [[executorBackend]] < > [[totalCores]] Number of CPU cores (aka totalCores ) LocalEndpoint initializes the < >. === [[receive]] Processing Receive-Only RPC Messages -- receive Method","title":"Refer to &lt;&lt;../spark-logging.md#, Logging&gt;&gt;."},{"location":"local/LocalEndpoint/#source-scala","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#receive-partialfunctionany-unit","text":"NOTE: receive is part of the rpc:RpcEndpoint.md#receive[RpcEndpoint] abstraction. receive handles ( processes ) < >, < >, and < > RPC messages. ==== [[ReviveOffers]] ReviveOffers RPC Message","title":"receive: PartialFunction[Any, Unit]"},{"location":"local/LocalEndpoint/#source-scala_1","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#reviveoffers","text":"When < >, LocalEndpoint < >. NOTE: ReviveOffers RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. ==== [[StatusUpdate]] StatusUpdate RPC Message","title":"ReviveOffers()"},{"location":"local/LocalEndpoint/#source-scala_2","text":"StatusUpdate( taskId: Long, state: TaskState, serializedData: ByteBuffer) When < >, LocalEndpoint requests the < > to scheduler:TaskSchedulerImpl.md#statusUpdate[handle a task status update] (given the taskId , the task state and the data). If the given scheduler:Task.md#TaskState[TaskState] is a finished state (one of FINISHED , FAILED , KILLED , LOST states), LocalEndpoint adds scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[spark.task.cpus] configuration (default: 1 ) to the < > registry followed by < >. NOTE: StatusUpdate RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. ==== [[KillTask]] KillTask RPC Message","title":"[source, scala]"},{"location":"local/LocalEndpoint/#source-scala_3","text":"KillTask( taskId: Long, interruptThread: Boolean, reason: String) When < >, LocalEndpoint requests the single < > to executor:Executor.md#killTask[kill a task] (given the taskId , the interruptThread flag and the reason). NOTE: KillTask RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. === [[reviveOffers]] Reviving Offers -- reviveOffers Method","title":"[source, scala]"},{"location":"local/LocalEndpoint/#source-scala_4","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#reviveoffers-unit","text":"reviveOffers ...FIXME NOTE: reviveOffers is used when LocalEndpoint is requested to < > (namely < > and < >). === [[receiveAndReply]] Processing Receive-Reply RPC Messages -- receiveAndReply Method","title":"reviveOffers(): Unit"},{"location":"local/LocalEndpoint/#source-scala_5","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#receiveandreplycontext-rpccallcontext-partialfunctionany-unit","text":"NOTE: receiveAndReply is part of the rpc:RpcEndpoint.md#receiveAndReply[RpcEndpoint] abstraction. receiveAndReply handles ( processes ) < > RPC message exclusively. ==== [[StopExecutor]] StopExecutor RPC Message","title":"receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit]"},{"location":"local/LocalEndpoint/#source-scala_6","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#stopexecutor","text":"When < >, LocalEndpoint requests the single < > to executor:Executor.md#stop[stop] and requests the given RpcCallContext to reply with true (as the response). NOTE: StopExecutor RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >.","title":"StopExecutor()"},{"location":"local/LocalSchedulerBackend/","text":"= LocalSchedulerBackend LocalSchedulerBackend is a <<../SchedulerBackend.md#, SchedulerBackend>> and an executor:ExecutorBackend.md[] for the < >. LocalSchedulerBackend is < > when SparkContext is requested to SparkContext.md#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] for the following master URLs: local (with exactly < >) local[n] (with exactly < >) ++local[ ]++* (with the < > that is the number of available CPU cores on the local machine) local[n, m] (with exactly < >) ++local[ , m]++* (with the < > that is the number of available CPU cores on the local machine) While being < >, LocalSchedulerBackend requests the < > to <<../spark-LauncherBackend.md#connect, connect>>. When an executor sends task status updates (using ExecutorBackend.statusUpdate ), they are passed along as < > to < >. .Task status updates flow in local mode image::LocalSchedulerBackend-LocalEndpoint-Executor-task-status-updates.png[align=\"center\"] [[appId]] [[applicationId]] When requested for the <<../SchedulerBackend.md#applicationId, applicationId>>, LocalSchedulerBackend uses local-[currentTimeMillis] . [[maxNumConcurrentTasks]] When requested for the <<../SchedulerBackend.md#maxNumConcurrentTasks, maxNumConcurrentTasks>>, LocalSchedulerBackend simply divides the < > by scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[spark.task.cpus] configuration (default: 1 ). [[defaultParallelism]] When requested for the <<../SchedulerBackend.md#defaultParallelism, defaultParallelism>>, LocalSchedulerBackend uses <<../configuration-properties.md#spark.default.parallelism, spark.default.parallelism>> configuration (if defined) or the < >. [[userClassPath]] When < >, LocalSchedulerBackend < > the <<../configuration-properties.md#spark.executor.extraClassPath, spark.executor.extraClassPath>> configuration property (in the given < >) for the user-defined class path for executors that is used exclusively when LocalSchedulerBackend is requested to < > (and creates a < > that in turn uses it to create the one < >). [[creating-instance]] LocalSchedulerBackend takes the following to be created: [[conf]] <<../SparkConf.md#, SparkConf>> [[scheduler]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] [[totalCores]] Total number of CPU cores (aka totalCores ) [[internal-registries]] .LocalSchedulerBackend's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | localEndpoint a| [[localEndpoint]] rpc:RpcEndpointRef.md[RpcEndpointRef] to LocalSchedulerBackendEndpoint RPC endpoint (that is < > which LocalSchedulerBackend registers when < >) Used when LocalSchedulerBackend is requested for the following: < > (and sends a < > one-way asynchronous message) < > (and sends a < > one-way asynchronous message) < > (and sends a < > one-way asynchronous message) < > (and sends a < > asynchronous message) | launcherBackend a| [[launcherBackend]] <<../spark-LauncherBackend.md#, LauncherBackend>> Used when LocalSchedulerBackend is < >, < > and < > | listenerBus a| [[listenerBus]] scheduler:LiveListenerBus.md[] that is used exclusively when LocalSchedulerBackend is requested to < > |=== [[logging]] [TIP] ==== Enable INFO logging level for org.apache.spark.scheduler.local.LocalSchedulerBackend logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.local.LocalSchedulerBackend=INFO Refer to <<../spark-logging.md#, Logging>>. \u00b6 == [[start]] Starting Scheduling Backend -- start Method [source, scala] \u00b6 start(): Unit \u00b6 NOTE: start is part of the <<../SchedulerBackend.md#start, SchedulerBackend Contract>> to start the scheduling backend. start requests the SparkEnv object for the current core:SparkEnv.md#rpcEnv[RpcEnv]. start then creates a < > and requests the RpcEnv to rpc:RpcEnv.md#setupEndpoint[register it] as LocalSchedulerBackendEndpoint RPC endpoint. start requests the < > to scheduler:LiveListenerBus.md#post[post] a SparkListener.md#SparkListenerExecutorAdded[SparkListenerExecutorAdded] event. In the end, start requests the < > to <<../spark-LauncherBackend.md#setAppId, setAppId>> as the < > and <<../spark-LauncherBackend.md#setState, setState>> as RUNNING . == [[reviveOffers]] reviveOffers Method [source, scala] \u00b6 reviveOffers(): Unit \u00b6 NOTE: reviveOffers is part of the <<../SchedulerBackend.md#reviveOffers, SchedulerBackend Contract>> to...FIXME. reviveOffers ...FIXME == [[killTask]] killTask Method [source, scala] \u00b6 killTask( taskId: Long, executorId: String, interruptThread: Boolean, reason: String): Unit NOTE: killTask is part of the <<../SchedulerBackend.md#killTask, SchedulerBackend Contract>> to kill a task. killTask ...FIXME == [[statusUpdate]] statusUpdate Method [source, scala] \u00b6 statusUpdate( taskId: Long, state: TaskState, data: ByteBuffer): Unit NOTE: statusUpdate is part of the executor:ExecutorBackend.md#statusUpdate[ExecutorBackend] abstraction. statusUpdate ...FIXME == [[stop]] Stopping Scheduling Backend -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 NOTE: stop is part of the <<../SchedulerBackend.md#stop, SchedulerBackend Contract>> to stop a scheduling backend. stop ...FIXME == [[getUserClasspath]] User-Defined Class Path for Executors -- getUserClasspath Method [source, scala] \u00b6 getUserClasspath(conf: SparkConf): Seq[URL] \u00b6 getUserClasspath simply requests the given SparkConf for the <<../configuration-properties.md#spark.executor.extraClassPath, spark.executor.extraClassPath>> configuration property and converts the entries (separated by the system-dependent path separator) to URLs. NOTE: getUserClasspath is used exclusively when LocalSchedulerBackend is < >.","title":"LocalSchedulerBackend"},{"location":"local/LocalSchedulerBackend/#refer-to-spark-loggingmd-logging","text":"== [[start]] Starting Scheduling Backend -- start Method","title":"Refer to &lt;&lt;../spark-logging.md#, Logging&gt;&gt;."},{"location":"local/LocalSchedulerBackend/#source-scala","text":"","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#start-unit","text":"NOTE: start is part of the <<../SchedulerBackend.md#start, SchedulerBackend Contract>> to start the scheduling backend. start requests the SparkEnv object for the current core:SparkEnv.md#rpcEnv[RpcEnv]. start then creates a < > and requests the RpcEnv to rpc:RpcEnv.md#setupEndpoint[register it] as LocalSchedulerBackendEndpoint RPC endpoint. start requests the < > to scheduler:LiveListenerBus.md#post[post] a SparkListener.md#SparkListenerExecutorAdded[SparkListenerExecutorAdded] event. In the end, start requests the < > to <<../spark-LauncherBackend.md#setAppId, setAppId>> as the < > and <<../spark-LauncherBackend.md#setState, setState>> as RUNNING . == [[reviveOffers]] reviveOffers Method","title":"start(): Unit"},{"location":"local/LocalSchedulerBackend/#source-scala_1","text":"","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#reviveoffers-unit","text":"NOTE: reviveOffers is part of the <<../SchedulerBackend.md#reviveOffers, SchedulerBackend Contract>> to...FIXME. reviveOffers ...FIXME == [[killTask]] killTask Method","title":"reviveOffers(): Unit"},{"location":"local/LocalSchedulerBackend/#source-scala_2","text":"killTask( taskId: Long, executorId: String, interruptThread: Boolean, reason: String): Unit NOTE: killTask is part of the <<../SchedulerBackend.md#killTask, SchedulerBackend Contract>> to kill a task. killTask ...FIXME == [[statusUpdate]] statusUpdate Method","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#source-scala_3","text":"statusUpdate( taskId: Long, state: TaskState, data: ByteBuffer): Unit NOTE: statusUpdate is part of the executor:ExecutorBackend.md#statusUpdate[ExecutorBackend] abstraction. statusUpdate ...FIXME == [[stop]] Stopping Scheduling Backend -- stop Method","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#source-scala_4","text":"","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#stop-unit","text":"NOTE: stop is part of the <<../SchedulerBackend.md#stop, SchedulerBackend Contract>> to stop a scheduling backend. stop ...FIXME == [[getUserClasspath]] User-Defined Class Path for Executors -- getUserClasspath Method","title":"stop(): Unit"},{"location":"local/LocalSchedulerBackend/#source-scala_5","text":"","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#getuserclasspathconf-sparkconf-sequrl","text":"getUserClasspath simply requests the given SparkConf for the <<../configuration-properties.md#spark.executor.extraClassPath, spark.executor.extraClassPath>> configuration property and converts the entries (separated by the system-dependent path separator) to URLs. NOTE: getUserClasspath is used exclusively when LocalSchedulerBackend is < >.","title":"getUserClasspath(conf: SparkConf): Seq[URL]"},{"location":"memory/","text":"Memory System \u00b6 Memory System is a core component of Apache Spark that is based on UnifiedMemoryManager . Resources \u00b6 SPARK-10000: Consolidate storage and execution memory management Videos \u00b6 Deep Dive: Apache Spark Memory Management Deep Dive into Project Tungsten Spark Performance: What's Next","title":"Memory System"},{"location":"memory/#memory-system","text":"Memory System is a core component of Apache Spark that is based on UnifiedMemoryManager .","title":"Memory System"},{"location":"memory/#resources","text":"SPARK-10000: Consolidate storage and execution memory management","title":"Resources"},{"location":"memory/#videos","text":"Deep Dive: Apache Spark Memory Management Deep Dive into Project Tungsten Spark Performance: What's Next","title":"Videos"},{"location":"memory/BytesToBytesMap/","text":"= [[BytesToBytesMap]] BytesToBytesMap BytesToBytesMap is a MemoryConsumer.md[MemoryConsumer]. BytesToBytesMap is used to create Spark SQL's UnsafeKVExternalSorter and UnsafeHashedRelation. == [[creating-instance]] Creating Instance BytesToBytesMap takes the following to be created: [[taskMemoryManager]] memory:TaskMemoryManager.md[TaskMemoryManager] [[blockManager]] storage:BlockManager.md[BlockManager] < > [[initialCapacity]] Initial capacity [[loadFactor]] Load factor (default: 0.5) [[pageSizeBytes]] Page size (in bytes) [[enablePerfMetrics]] enablePerfMetrics flag BytesToBytesMap is created for Spark SQL's UnsafeFixedWidthAggregationMap and UnsafeHashedRelation. == [[serializerManager]] SerializerManager BytesToBytesMap is given a serializer:SerializerManager.md[SerializerManager] when < >. BytesToBytesMap uses the SerializerManager when (MapIterator is) requested to advanceToNextPage (to request UnsafeSorterSpillWriter for a memory:UnsafeSorterSpillWriter.md#getReader[UnsafeSorterSpillReader]). == [[MAX_CAPACITY]] Maximum Supported Capacity BytesToBytesMap supports up to 1 << 29 keys. == [[spillWriters]] UnsafeSorterSpillWriters BytesToBytesMap manages UnsafeSorterSpillWriter.md[UnsafeSorterSpillWriters]. BytesToBytesMap registers a new UnsafeSorterSpillWriter when requested to < >. BytesToBytesMap uses the UnsafeSorterSpillWriters when: < > FIXME == [[destructiveIterator]] MapIterator BytesToBytesMap manages a \"destructive\" MapIterator. BytesToBytesMap creates it when requested for < >. BytesToBytesMap requests it to spill when requested to < >. == [[destructiveIterator]] Creating Destructive MapIterator [source, java] \u00b6 MapIterator destructiveIterator() \u00b6 destructiveIterator < > and creates a MapIterator (with the < > and < >). destructiveIterator is used when Spark SQL's UnsafeFixedWidthAggregationMap is requested for a key-value iterator. == [[allocate]] Allocating [source, java] \u00b6 void allocate( int capacity) allocate uses the input capacity to compute a number that is a power of 2 and greater or equal than capacity, but not greater than < >. The computed number is at least 64. [source,scala] \u00b6 def _c(capacity: Int) = { import org.apache.spark.unsafe.array.ByteArrayMethods val MAX_CAPACITY = (1 << 29) Math.max(Math.min(MAX_CAPACITY, ByteArrayMethods.nextPowerOf2(capacity)), 64) } allocate MemoryConsumer.md#allocateArray[allocates an array] twice as big as the power-of-two capacity and fills it all with 0s. allocate initializes the < > and < > internal properties. allocate requires that the input capacity is positive. allocate is used when...FIXME == [[spill]] Spilling [source, java] \u00b6 long spill( long size, MemoryConsumer trigger) NOTE: spill is part of the memory:MemoryConsumer.md#spill[MemoryConsumer] abstraction. spill requests the < > to spill when the given MemoryConsumer is not this BytesToBytesMap and the MapIterator is available. == [[free]] Freeing Up Allocated Memory [source, java] \u00b6 void free() \u00b6 free...FIXME free is used when...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | growthThreshold a| [[growthThreshold]] Growth threshold | mask a| [[mask]] Mask for truncating hashcodes so that they do not exceed the long array's size |===","title":"BytesToBytesMap"},{"location":"memory/BytesToBytesMap/#source-java","text":"","title":"[source, java]"},{"location":"memory/BytesToBytesMap/#mapiterator-destructiveiterator","text":"destructiveIterator < > and creates a MapIterator (with the < > and < >). destructiveIterator is used when Spark SQL's UnsafeFixedWidthAggregationMap is requested for a key-value iterator. == [[allocate]] Allocating","title":"MapIterator destructiveIterator()"},{"location":"memory/BytesToBytesMap/#source-java_1","text":"void allocate( int capacity) allocate uses the input capacity to compute a number that is a power of 2 and greater or equal than capacity, but not greater than < >. The computed number is at least 64.","title":"[source, java]"},{"location":"memory/BytesToBytesMap/#sourcescala","text":"def _c(capacity: Int) = { import org.apache.spark.unsafe.array.ByteArrayMethods val MAX_CAPACITY = (1 << 29) Math.max(Math.min(MAX_CAPACITY, ByteArrayMethods.nextPowerOf2(capacity)), 64) } allocate MemoryConsumer.md#allocateArray[allocates an array] twice as big as the power-of-two capacity and fills it all with 0s. allocate initializes the < > and < > internal properties. allocate requires that the input capacity is positive. allocate is used when...FIXME == [[spill]] Spilling","title":"[source,scala]"},{"location":"memory/BytesToBytesMap/#source-java_2","text":"long spill( long size, MemoryConsumer trigger) NOTE: spill is part of the memory:MemoryConsumer.md#spill[MemoryConsumer] abstraction. spill requests the < > to spill when the given MemoryConsumer is not this BytesToBytesMap and the MapIterator is available. == [[free]] Freeing Up Allocated Memory","title":"[source, java]"},{"location":"memory/BytesToBytesMap/#source-java_3","text":"","title":"[source, java]"},{"location":"memory/BytesToBytesMap/#void-free","text":"free...FIXME free is used when...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | growthThreshold a| [[growthThreshold]] Growth threshold | mask a| [[mask]] Mask for truncating hashcodes so that they do not exceed the long array's size |===","title":"void free()"},{"location":"memory/ExecutionMemoryPool/","text":"ExecutionMemoryPool \u00b6 ExecutionMemoryPool is a MemoryPool . Creating Instance \u00b6 ExecutionMemoryPool takes the following to be created: Lock Object MemoryMode ( ON_HEAP or OFF_HEAP ) ExecutionMemoryPool is created when: MemoryManager is created (and initializes on-heap and off-heap execution memory pools)","title":"ExecutionMemoryPool"},{"location":"memory/ExecutionMemoryPool/#executionmemorypool","text":"ExecutionMemoryPool is a MemoryPool .","title":"ExecutionMemoryPool"},{"location":"memory/ExecutionMemoryPool/#creating-instance","text":"ExecutionMemoryPool takes the following to be created: Lock Object MemoryMode ( ON_HEAP or OFF_HEAP ) ExecutionMemoryPool is created when: MemoryManager is created (and initializes on-heap and off-heap execution memory pools)","title":"Creating Instance"},{"location":"memory/MemoryConsumer/","text":"MemoryConsumer \u00b6 MemoryConsumer is an abstraction of spillable memory consumers (of TaskMemoryManager ). MemoryConsumer s correspond to individual operators and data structures within a task. TaskMemoryManager receives memory allocation requests from MemoryConsumer s and issues callbacks to consumers in order to trigger spilling when running low on memory. A MemoryConsumer basically tracks how much memory is allocated . Contract \u00b6 spill \u00b6 void spill () // (1) long spill ( long size , MemoryConsumer trigger ) Uses MAX_VALUE for the size and this MemoryConsumer Used when: TaskMemoryManager is requested to acquireExecutionMemory UnsafeExternalSorter is requested to createWithExistingInMemorySorter Implementations \u00b6 BytesToBytesMap LongToUnsafeRowMap RowBasedKeyValueBatch ShuffleExternalSorter Spillable UnsafeExternalSorter Creating Instance \u00b6 MemoryConsumer takes the following to be created: TaskMemoryManager Page Size MemoryMode ( ON_HEAP or OFF_HEAP ) Abstract Class MemoryConsumer is an abstract class and cannot be created directly. It is created indirectly for the concrete MemoryConsumers .","title":"MemoryConsumer"},{"location":"memory/MemoryConsumer/#memoryconsumer","text":"MemoryConsumer is an abstraction of spillable memory consumers (of TaskMemoryManager ). MemoryConsumer s correspond to individual operators and data structures within a task. TaskMemoryManager receives memory allocation requests from MemoryConsumer s and issues callbacks to consumers in order to trigger spilling when running low on memory. A MemoryConsumer basically tracks how much memory is allocated .","title":"MemoryConsumer"},{"location":"memory/MemoryConsumer/#contract","text":"","title":"Contract"},{"location":"memory/MemoryConsumer/#spill","text":"void spill () // (1) long spill ( long size , MemoryConsumer trigger ) Uses MAX_VALUE for the size and this MemoryConsumer Used when: TaskMemoryManager is requested to acquireExecutionMemory UnsafeExternalSorter is requested to createWithExistingInMemorySorter","title":" spill"},{"location":"memory/MemoryConsumer/#implementations","text":"BytesToBytesMap LongToUnsafeRowMap RowBasedKeyValueBatch ShuffleExternalSorter Spillable UnsafeExternalSorter","title":"Implementations"},{"location":"memory/MemoryConsumer/#creating-instance","text":"MemoryConsumer takes the following to be created: TaskMemoryManager Page Size MemoryMode ( ON_HEAP or OFF_HEAP ) Abstract Class MemoryConsumer is an abstract class and cannot be created directly. It is created indirectly for the concrete MemoryConsumers .","title":"Creating Instance"},{"location":"memory/MemoryManager/","text":"MemoryManager \u00b6 MemoryManager is an abstraction of memory managers that can enforce how memory is shared between task execution ( TaskMemoryManager ) and storage ( BlockManager ). MemoryManager splits assigned memory into two regions: Execution Memory for shuffles, joins, sorts and aggregations Storage Memory for caching and propagating internal data across Spark nodes (in on - and off-heap modes) MemoryManager is used to create BlockManager (and MemoryStore ) and TaskMemoryManager . Contract \u00b6 Acquiring Execution Memory for Task \u00b6 acquireExecutionMemory ( numBytes : Long , taskAttemptId : Long , memoryMode : MemoryMode ): Long Used when: TaskMemoryManager is requested to acquireExecutionMemory Acquiring Storage Memory for Block \u00b6 acquireStorageMemory ( blockId : BlockId , numBytes : Long , memoryMode : MemoryMode ): Boolean Used when: MemoryStore is requested for the putBytes and putIterator Acquiring Unroll Memory for Block \u00b6 acquireUnrollMemory ( blockId : BlockId , numBytes : Long , memoryMode : MemoryMode ): Boolean Used when: MemoryStore is requested for the reserveUnrollMemoryForThisTask Total Available Off-Heap Storage Memory \u00b6 maxOffHeapStorageMemory : Long May vary over time Used when: BlockManager is created MemoryStore is requested for the maxMemory Total Available On-Heap Storage Memory \u00b6 maxOnHeapStorageMemory : Long May vary over time Used when: BlockManager is created MemoryStore is requested for the maxMemory Implementations \u00b6 UnifiedMemoryManager Creating Instance \u00b6 MemoryManager takes the following to be created: SparkConf Number of CPU Cores Size of the On-Heap Storage Memory Size of the On-Heap Execution Memory Abstract Class MemoryManager is an abstract class and cannot be created directly. It is created indirectly for the concrete MemoryManagers . Accessing MemoryManager (SparkEnv) \u00b6 MemoryManager is available as SparkEnv.memoryManager on the driver and executors. import org.apache.spark.SparkEnv val mm = SparkEnv.get.memoryManager scala> :type mm org.apache.spark.memory.MemoryManager Associating MemoryStore with Storage Memory Pools \u00b6 setMemoryStore ( store : MemoryStore ): Unit setMemoryStore requests the on-heap and off-heap storage memory pools to use the given MemoryStore . setMemoryStore is used when: BlockManager is created Storage Memory Pools \u00b6 On-Heap \u00b6 onHeapStorageMemoryPool : StorageMemoryPool MemoryManager creates a StorageMemoryPool with ON_HEAP memory mode when created and immediately requests it to incrementPoolSize to onHeapExecutionMemory . onHeapStorageMemoryPool is requested to setMemoryStore when MemoryManager is requested to setMemoryStore . onHeapStorageMemoryPool is requested to release memory when MemoryManager is requested to release on-heap storage memory . onHeapStorageMemoryPool is requested to release all memory when MemoryManager is requested to release all storage memory . onHeapStorageMemoryPool is used when: MemoryManager is requested for the storageMemoryUsed and onHeapStorageMemoryUsed UnifiedMemoryManager is requested to acquire on-heap execution and storage memory Off-Heap \u00b6 offHeapStorageMemoryPool : StorageMemoryPool MemoryManager creates a StorageMemoryPool with OFF_HEAP memory mode when created and immediately requested it to incrementPoolSize to offHeapStorageMemory . MemoryManager requests the MemoryPool s to use a given MemoryStore when requested to setMemoryStore . MemoryManager requests the MemoryPool s to release memory when requested to releaseStorageMemory . MemoryManager requests the MemoryPool s to release all memory when requested to release all storage memory . MemoryManager requests the MemoryPool s for the memoryUsed when requested for storageMemoryUsed . offHeapStorageMemoryPool is used when: MemoryManager is requested for the offHeapStorageMemoryUsed UnifiedMemoryManager is requested to acquire off-heap execution and storage memory Total Storage Memory Used \u00b6 storageMemoryUsed : Long storageMemoryUsed is the sum of the memory used of the on-heap and off-heap storage memory pools. storageMemoryUsed is used when: TaskMemoryManager is requested to showMemoryUsage MemoryStore is requested to memoryUsed MemoryMode \u00b6 tungstenMemoryMode : MemoryMode tungstenMemoryMode tracks whether Tungsten memory will be allocated on the JVM heap or off-heap (using sun.misc.Unsafe ). tungstenMemoryMode is OFF_HEAP when the following are all met: spark.memory.offHeap.enabled configuration property is enabled spark.memory.offHeap.size configuration property is greater than 0 JVM supports unaligned memory access (aka unaligned Unsafe , i.e. sun.misc.Unsafe package is available and the underlying system has unaligned-access capability) Otherwise, tungstenMemoryMode is ON_HEAP . Note Given that spark.memory.offHeap.enabled configuration property is turned off by default and spark.memory.offHeap.size configuration property is 0 by default, Apache Spark seems to encourage using Tungsten memory allocated on the JVM heap ( ON_HEAP ). tungstenMemoryMode is used when: MemoryManager is created (and initializes the pageSizeBytes and tungstenMemoryAllocator internal properties) TaskMemoryManager is created","title":"MemoryManager"},{"location":"memory/MemoryManager/#memorymanager","text":"MemoryManager is an abstraction of memory managers that can enforce how memory is shared between task execution ( TaskMemoryManager ) and storage ( BlockManager ). MemoryManager splits assigned memory into two regions: Execution Memory for shuffles, joins, sorts and aggregations Storage Memory for caching and propagating internal data across Spark nodes (in on - and off-heap modes) MemoryManager is used to create BlockManager (and MemoryStore ) and TaskMemoryManager .","title":"MemoryManager"},{"location":"memory/MemoryManager/#contract","text":"","title":"Contract"},{"location":"memory/MemoryManager/#acquiring-execution-memory-for-task","text":"acquireExecutionMemory ( numBytes : Long , taskAttemptId : Long , memoryMode : MemoryMode ): Long Used when: TaskMemoryManager is requested to acquireExecutionMemory","title":" Acquiring Execution Memory for Task"},{"location":"memory/MemoryManager/#acquiring-storage-memory-for-block","text":"acquireStorageMemory ( blockId : BlockId , numBytes : Long , memoryMode : MemoryMode ): Boolean Used when: MemoryStore is requested for the putBytes and putIterator","title":" Acquiring Storage Memory for Block"},{"location":"memory/MemoryManager/#acquiring-unroll-memory-for-block","text":"acquireUnrollMemory ( blockId : BlockId , numBytes : Long , memoryMode : MemoryMode ): Boolean Used when: MemoryStore is requested for the reserveUnrollMemoryForThisTask","title":" Acquiring Unroll Memory for Block"},{"location":"memory/MemoryManager/#total-available-off-heap-storage-memory","text":"maxOffHeapStorageMemory : Long May vary over time Used when: BlockManager is created MemoryStore is requested for the maxMemory","title":" Total Available Off-Heap Storage Memory"},{"location":"memory/MemoryManager/#total-available-on-heap-storage-memory","text":"maxOnHeapStorageMemory : Long May vary over time Used when: BlockManager is created MemoryStore is requested for the maxMemory","title":" Total Available On-Heap Storage Memory"},{"location":"memory/MemoryManager/#implementations","text":"UnifiedMemoryManager","title":"Implementations"},{"location":"memory/MemoryManager/#creating-instance","text":"MemoryManager takes the following to be created: SparkConf Number of CPU Cores Size of the On-Heap Storage Memory Size of the On-Heap Execution Memory Abstract Class MemoryManager is an abstract class and cannot be created directly. It is created indirectly for the concrete MemoryManagers .","title":"Creating Instance"},{"location":"memory/MemoryManager/#accessing-memorymanager-sparkenv","text":"MemoryManager is available as SparkEnv.memoryManager on the driver and executors. import org.apache.spark.SparkEnv val mm = SparkEnv.get.memoryManager scala> :type mm org.apache.spark.memory.MemoryManager","title":" Accessing MemoryManager (SparkEnv)"},{"location":"memory/MemoryManager/#associating-memorystore-with-storage-memory-pools","text":"setMemoryStore ( store : MemoryStore ): Unit setMemoryStore requests the on-heap and off-heap storage memory pools to use the given MemoryStore . setMemoryStore is used when: BlockManager is created","title":" Associating MemoryStore with Storage Memory Pools"},{"location":"memory/MemoryManager/#storage-memory-pools","text":"","title":"Storage Memory Pools"},{"location":"memory/MemoryManager/#on-heap","text":"onHeapStorageMemoryPool : StorageMemoryPool MemoryManager creates a StorageMemoryPool with ON_HEAP memory mode when created and immediately requests it to incrementPoolSize to onHeapExecutionMemory . onHeapStorageMemoryPool is requested to setMemoryStore when MemoryManager is requested to setMemoryStore . onHeapStorageMemoryPool is requested to release memory when MemoryManager is requested to release on-heap storage memory . onHeapStorageMemoryPool is requested to release all memory when MemoryManager is requested to release all storage memory . onHeapStorageMemoryPool is used when: MemoryManager is requested for the storageMemoryUsed and onHeapStorageMemoryUsed UnifiedMemoryManager is requested to acquire on-heap execution and storage memory","title":" On-Heap"},{"location":"memory/MemoryManager/#off-heap","text":"offHeapStorageMemoryPool : StorageMemoryPool MemoryManager creates a StorageMemoryPool with OFF_HEAP memory mode when created and immediately requested it to incrementPoolSize to offHeapStorageMemory . MemoryManager requests the MemoryPool s to use a given MemoryStore when requested to setMemoryStore . MemoryManager requests the MemoryPool s to release memory when requested to releaseStorageMemory . MemoryManager requests the MemoryPool s to release all memory when requested to release all storage memory . MemoryManager requests the MemoryPool s for the memoryUsed when requested for storageMemoryUsed . offHeapStorageMemoryPool is used when: MemoryManager is requested for the offHeapStorageMemoryUsed UnifiedMemoryManager is requested to acquire off-heap execution and storage memory","title":" Off-Heap"},{"location":"memory/MemoryManager/#total-storage-memory-used","text":"storageMemoryUsed : Long storageMemoryUsed is the sum of the memory used of the on-heap and off-heap storage memory pools. storageMemoryUsed is used when: TaskMemoryManager is requested to showMemoryUsage MemoryStore is requested to memoryUsed","title":" Total Storage Memory Used"},{"location":"memory/MemoryManager/#memorymode","text":"tungstenMemoryMode : MemoryMode tungstenMemoryMode tracks whether Tungsten memory will be allocated on the JVM heap or off-heap (using sun.misc.Unsafe ). tungstenMemoryMode is OFF_HEAP when the following are all met: spark.memory.offHeap.enabled configuration property is enabled spark.memory.offHeap.size configuration property is greater than 0 JVM supports unaligned memory access (aka unaligned Unsafe , i.e. sun.misc.Unsafe package is available and the underlying system has unaligned-access capability) Otherwise, tungstenMemoryMode is ON_HEAP . Note Given that spark.memory.offHeap.enabled configuration property is turned off by default and spark.memory.offHeap.size configuration property is 0 by default, Apache Spark seems to encourage using Tungsten memory allocated on the JVM heap ( ON_HEAP ). tungstenMemoryMode is used when: MemoryManager is created (and initializes the pageSizeBytes and tungstenMemoryAllocator internal properties) TaskMemoryManager is created","title":" MemoryMode"},{"location":"memory/MemoryPool/","text":"MemoryPool \u00b6 MemoryPool is an abstraction of memory pools . Contract \u00b6 Memory Used \u00b6 memoryUsed : Long Used when: MemoryPool is requested for the amount of free memory and decrementPoolSize Implementations \u00b6 ExecutionMemoryPool StorageMemoryPool Creating Instance \u00b6 MemoryPool takes the following to be created: Lock Object Abstract Class MemoryPool is an abstract class and cannot be created directly. It is created indirectly for the concrete MemoryPools . Free Memory \u00b6 memoryFree memoryFree ...FIXME memoryFree is used when: ExecutionMemoryPool is requested to acquireMemory StorageMemoryPool is requested to acquireMemory and freeSpaceToShrinkPool UnifiedMemoryManager is requested to acquire execution and storage memory decrementPoolSize \u00b6 decrementPoolSize ( delta : Long ): Unit decrementPoolSize ...FIXME decrementPoolSize is used when: UnifiedMemoryManager is requested to acquireExecutionMemory and acquireStorageMemory","title":"MemoryPool"},{"location":"memory/MemoryPool/#memorypool","text":"MemoryPool is an abstraction of memory pools .","title":"MemoryPool"},{"location":"memory/MemoryPool/#contract","text":"","title":"Contract"},{"location":"memory/MemoryPool/#memory-used","text":"memoryUsed : Long Used when: MemoryPool is requested for the amount of free memory and decrementPoolSize","title":" Memory Used"},{"location":"memory/MemoryPool/#implementations","text":"ExecutionMemoryPool StorageMemoryPool","title":"Implementations"},{"location":"memory/MemoryPool/#creating-instance","text":"MemoryPool takes the following to be created: Lock Object Abstract Class MemoryPool is an abstract class and cannot be created directly. It is created indirectly for the concrete MemoryPools .","title":"Creating Instance"},{"location":"memory/MemoryPool/#free-memory","text":"memoryFree memoryFree ...FIXME memoryFree is used when: ExecutionMemoryPool is requested to acquireMemory StorageMemoryPool is requested to acquireMemory and freeSpaceToShrinkPool UnifiedMemoryManager is requested to acquire execution and storage memory","title":" Free Memory"},{"location":"memory/MemoryPool/#decrementpoolsize","text":"decrementPoolSize ( delta : Long ): Unit decrementPoolSize ...FIXME decrementPoolSize is used when: UnifiedMemoryManager is requested to acquireExecutionMemory and acquireStorageMemory","title":" decrementPoolSize"},{"location":"memory/StorageMemoryPool/","text":"StorageMemoryPool \u00b6 StorageMemoryPool is a MemoryPool . Creating Instance \u00b6 StorageMemoryPool takes the following to be created: Lock Object MemoryMode ( ON_HEAP or OFF_HEAP ) StorageMemoryPool is created when: MemoryManager is created (and initializes on-heap and off-heap storage memory pools) MemoryStore \u00b6 StorageMemoryPool is given a MemoryStore when MemoryManager is requested to associate one with storage memory pools . MemoryStore is used when StorageMemoryPool is requested to: Acquire Memory Free Space to Shrink Pool Acquiring Memory \u00b6 acquireMemory ( blockId : BlockId , numBytes : Long ): Boolean acquireMemory ( blockId : BlockId , numBytesToAcquire : Long , numBytesToFree : Long ): Boolean acquireMemory ...FIXME acquireMemory is used when: UnifiedMemoryManager is requested to acquire storage memory freeSpaceToShrinkPool \u00b6 freeSpaceToShrinkPool ( spaceToFree : Long ): Long freeSpaceToShrinkPool ...FIXME freeSpaceToShrinkPool is used when: UnifiedMemoryManager is requested to acquire execution memory","title":"StorageMemoryPool"},{"location":"memory/StorageMemoryPool/#storagememorypool","text":"StorageMemoryPool is a MemoryPool .","title":"StorageMemoryPool"},{"location":"memory/StorageMemoryPool/#creating-instance","text":"StorageMemoryPool takes the following to be created: Lock Object MemoryMode ( ON_HEAP or OFF_HEAP ) StorageMemoryPool is created when: MemoryManager is created (and initializes on-heap and off-heap storage memory pools)","title":"Creating Instance"},{"location":"memory/StorageMemoryPool/#memorystore","text":"StorageMemoryPool is given a MemoryStore when MemoryManager is requested to associate one with storage memory pools . MemoryStore is used when StorageMemoryPool is requested to: Acquire Memory Free Space to Shrink Pool","title":" MemoryStore"},{"location":"memory/StorageMemoryPool/#acquiring-memory","text":"acquireMemory ( blockId : BlockId , numBytes : Long ): Boolean acquireMemory ( blockId : BlockId , numBytesToAcquire : Long , numBytesToFree : Long ): Boolean acquireMemory ...FIXME acquireMemory is used when: UnifiedMemoryManager is requested to acquire storage memory","title":" Acquiring Memory"},{"location":"memory/StorageMemoryPool/#freespacetoshrinkpool","text":"freeSpaceToShrinkPool ( spaceToFree : Long ): Long freeSpaceToShrinkPool ...FIXME freeSpaceToShrinkPool is used when: UnifiedMemoryManager is requested to acquire execution memory","title":" freeSpaceToShrinkPool"},{"location":"memory/TaskMemoryManager/","text":"TaskMemoryManager \u00b6 TaskMemoryManager manages the memory allocated to execute a single < > (using < >). TaskMemoryManager is < > when TaskRunner is requested to executor:TaskRunner.md#run[run]. TaskMemoryManager assumes that: The number of bits to address pages (aka PAGE_NUMBER_BITS ) is 13 The number of bits to encode offsets in data pages (aka OFFSET_BITS ) is 51 (i.e. 64 bits - PAGE_NUMBER_BITS ) The number of entries in the < > and < > (aka PAGE_TABLE_SIZE ) is 8192 (i.e. 1 << PAGE_NUMBER_BITS ) The maximum page size (aka MAXIMUM_PAGE_SIZE_BYTES ) is 15GB (i.e. ((1L << 31) - 1) * 8L ) == [[creating-instance]] Creating Instance TaskMemoryManager takes the following to be created: < > [[taskAttemptId]] executor:TaskRunner.md#taskId[Task attempt ID] TaskMemoryManager initializes the < >. == [[consumers]] Spillable Memory Consumers TaskMemoryManager tracks memory:MemoryConsumer.md[spillable memory consumers]. TaskMemoryManager registers a new memory consumer when requested to < >. TaskMemoryManager removes ( clears ) all registered memory consumer when requested to < >. Memory consumers are used to report memory usage when TaskMemoryManager is requested to < >. == [[memoryManager]] MemoryManager TaskMemoryManager is given a memory:MemoryManager.md[MemoryManager] when < >. TaskMemoryManager uses the MemoryManager for the following: < >, < > or < > execution memory < > < > < > < > < > == [[cleanUpAllAllocatedMemory]] Cleaning Up All Allocated Memory [source, java] \u00b6 long cleanUpAllAllocatedMemory() \u00b6 cleanUpAllAllocatedMemory clears < >. CAUTION: FIXME All recorded < > are queried for the size of used memory. If the memory used is greater than 0, the following WARN message is printed out to the logs: WARN TaskMemoryManager: leak [bytes] memory from [consumer] The consumers collection is then cleared. MemoryManager.md#releaseExecutionMemory[MemoryManager.releaseExecutionMemory] is executed to release the memory that is not used by any consumer. Before cleanUpAllAllocatedMemory returns, it calls MemoryManager.md#releaseAllExecutionMemoryForTask[MemoryManager.releaseAllExecutionMemoryForTask] that in turn becomes the return value. CAUTION: FIXME Image with the interactions to MemoryManager . NOTE: cleanUpAllAllocatedMemory is used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (and cleans up after itself). == [[acquireExecutionMemory]] Acquiring Execution Memory [source, java] \u00b6 long acquireExecutionMemory( long required, MemoryConsumer consumer) acquireExecutionMemory allocates up to required size of memory for the memory:MemoryConsumer.md[MemoryConsumer]. When no memory could be allocated, it calls spill on every consumer, itself including. Finally, acquireExecutionMemory returns the allocated memory. NOTE: acquireExecutionMemory synchronizes on itself, and so no other calls on the object could be completed. NOTE: memory:MemoryConsumer.md[MemoryConsumer] knows its mode -- on- or off-heap. acquireExecutionMemory first calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) . TIP: TaskMemoryManager is a mere wrapper of MemoryManager to track < >? CAUTION: FIXME When the memory obtained is less than requested (by required ), acquireExecutionMemory requests all < > to MemoryConsumer.md#spill[release memory (by spilling it to disk)]. NOTE: acquireExecutionMemory requests memory from consumers that work in the same mode except the requesting one. You may see the following DEBUG message when spill released some memory: DEBUG Task [taskAttemptId] released [bytes] from [consumer] for [consumer] acquireExecutionMemory calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) again (it called it at the beginning). It does the memory acquisition until it gets enough memory or there are no more consumers to request spill from. You may also see the following ERROR message in the logs when there is an error while requesting spill with OutOfMemoryError followed. ERROR error while calling spill() on [consumer] If the earlier spill on the consumers did not work out and there is still memory to be acquired, acquireExecutionMemory MemoryConsumer.md#spill[requests the input consumer to spill memory to disk] (that in fact requested more memory!) If the consumer releases some memory, you should see the following DEBUG message in the logs: DEBUG Task [taskAttemptId] released [bytes] from itself ([consumer]) acquireExecutionMemory calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) once more. NOTE: memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) could have been called \"three\" times, i.e. at the very beginning, for each consumer, and on itself. It records the consumer in < > registry. You should see the following DEBUG message in the logs: DEBUG Task [taskAttemptId] acquired [bytes] for [consumer] acquireExecutionMemory is used when: MemoryConsumer is requested to memory:MemoryConsumer.md#acquireMemory[acquire execution memory] TaskMemoryManager is requested to < > == [[allocatePage]] Allocating Memory Block for Tungsten Consumers [source, java] \u00b6 MemoryBlock allocatePage( long size, MemoryConsumer consumer) NOTE: It only handles Tungsten Consumers , i.e. MemoryConsumer.md[MemoryConsumers] in tungstenMemoryMode mode. allocatePage allocates a block of memory (aka page ) smaller than MAXIMUM_PAGE_SIZE_BYTES maximum size. It checks size against the internal MAXIMUM_PAGE_SIZE_BYTES maximum size. If it is greater than the maximum size, the following IllegalArgumentException is thrown: Cannot allocate a page with more than [MAXIMUM_PAGE_SIZE_BYTES] bytes It then < > (for the input size and consumer ). It finishes by returning null when no execution memory could be acquired. With the execution memory acquired, it finds the smallest unallocated page index and records the page number (using < > registry). If the index is PAGE_TABLE_SIZE or higher, < > is called and then the following IllegalStateException is thrown: Have already allocated a maximum of [PAGE_TABLE_SIZE] pages It then attempts to allocate a MemoryBlock from Tungsten MemoryAllocator (calling memoryManager.tungstenMemoryAllocator().allocate(acquired) ). CAUTION: FIXME What is MemoryAllocator ? When successful, MemoryBlock gets assigned pageNumber and it gets added to the internal < > registry. You should see the following TRACE message in the logs: TRACE Allocate page number [pageNumber] ([acquired] bytes) The page is returned. If a OutOfMemoryError is thrown when allocating a MemoryBlock page, the following WARN message is printed out to the logs: WARN Failed to allocate a page ([acquired] bytes), try again. And acquiredButNotUsed gets acquired memory space with the pageNumber cleared in < > (i.e. the index for pageNumber gets false ). CAUTION: FIXME Why is the code tracking acquiredButNotUsed ? Another < > attempt is recursively tried. CAUTION: FIXME Why is there a hope for being able to allocate a page? == [[releaseExecutionMemory]] releaseExecutionMemory Method [source, java] \u00b6 void releaseExecutionMemory(long size, MemoryConsumer consumer) \u00b6 releaseExecutionMemory ...FIXME [NOTE] \u00b6 releaseExecutionMemory is used when: MemoryConsumer is requested to MemoryConsumer.md#freeMemory[freeMemory] * TaskMemoryManager is requested to < > and < > \u00b6 == [[getMemoryConsumptionForThisTask]] getMemoryConsumptionForThisTask Method [source, java] \u00b6 long getMemoryConsumptionForThisTask() \u00b6 getMemoryConsumptionForThisTask ...FIXME NOTE: getMemoryConsumptionForThisTask is used exclusively in Spark tests. == [[showMemoryUsage]] Displaying Memory Usage [source, java] \u00b6 void showMemoryUsage() \u00b6 showMemoryUsage prints out the following INFO message to the logs (with the < >): [source,plaintext] \u00b6 Memory used in task [taskAttemptId] \u00b6 showMemoryUsage requests every < > to memory:MemoryConsumer.md#getUsed[report memory used]. showMemoryUsage prints out the following INFO message to the logs for a MemoryConsumer with some memory usage (and excludes zero-memory consumers): [source,plaintext] \u00b6 Acquired by [consumer]: [memUsage] \u00b6 showMemoryUsage prints out the following INFO messages to the logs: [source,plaintext] \u00b6 [amount] bytes of memory were used by task [taskAttemptId] but are not associated with specific consumers \u00b6 [source,plaintext] \u00b6 [executionMemoryUsed] bytes of memory are used for execution and [storageMemoryUsed] bytes of memory are used for storage \u00b6 showMemoryUsage is used when MemoryConsumer is requested to memory:MemoryConsumer.md#throwOom[throw an OutOfMemoryError]. == [[pageSizeBytes]] pageSizeBytes Method [source, java] \u00b6 long pageSizeBytes() \u00b6 pageSizeBytes simply requests the < > for MemoryManager.md#pageSizeBytes[pageSizeBytes]. NOTE: pageSizeBytes is used when...FIXME == [[freePage]] Freeing Memory Page -- freePage Method [source, java] \u00b6 void freePage(MemoryBlock page, MemoryConsumer consumer) \u00b6 pageSizeBytes simply requests the < > for MemoryManager.md#pageSizeBytes[pageSizeBytes]. NOTE: pageSizeBytes is used when MemoryConsumer is requested to MemoryConsumer.md#freePage[freePage] and MemoryConsumer.md#throwOom[throwOom]. == [[getPage]] Getting Page -- getPage Method [source, java] \u00b6 Object getPage(long pagePlusOffsetAddress) \u00b6 getPage ...FIXME NOTE: getPage is used when...FIXME == [[getOffsetInPage]] Getting Page Offset -- getOffsetInPage Method [source, java] \u00b6 long getOffsetInPage(long pagePlusOffsetAddress) \u00b6 getPage ...FIXME NOTE: getPage is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.memory.TaskMemoryManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.memory.TaskMemoryManager=ALL \u00b6 Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | acquiredButNotUsed | [[acquiredButNotUsed]] The size of memory allocated but not used. | allocatedPages | [[allocatedPages]] Collection of flags ( true or false values) of size PAGE_TABLE_SIZE with all bits initially disabled (i.e. false ). TIP: allocatedPages is https://docs.oracle.com/javase/8/docs/api/java/util/BitSet.html[java.util.BitSet ]. When < > is called, it will record the page in the registry by setting the bit at the specified index (that corresponds to the allocated page) to true . | pageTable | [[pageTable]] The array of size PAGE_TABLE_SIZE with indices being MemoryBlock objects. When < MemoryBlock page for Tungsten consumers>>, the index corresponds to pageNumber that points to the MemoryBlock page allocated. | tungstenMemoryMode | [[tungstenMemoryMode]] MemoryMode (i.e. OFF_HEAP or ON_HEAP ) Set to the MemoryManager.md#tungstenMemoryMode[tungstenMemoryMode] of the < > while TaskMemoryManager is < > |===","title":"TaskMemoryManager"},{"location":"memory/TaskMemoryManager/#taskmemorymanager","text":"TaskMemoryManager manages the memory allocated to execute a single < > (using < >). TaskMemoryManager is < > when TaskRunner is requested to executor:TaskRunner.md#run[run]. TaskMemoryManager assumes that: The number of bits to address pages (aka PAGE_NUMBER_BITS ) is 13 The number of bits to encode offsets in data pages (aka OFFSET_BITS ) is 51 (i.e. 64 bits - PAGE_NUMBER_BITS ) The number of entries in the < > and < > (aka PAGE_TABLE_SIZE ) is 8192 (i.e. 1 << PAGE_NUMBER_BITS ) The maximum page size (aka MAXIMUM_PAGE_SIZE_BYTES ) is 15GB (i.e. ((1L << 31) - 1) * 8L ) == [[creating-instance]] Creating Instance TaskMemoryManager takes the following to be created: < > [[taskAttemptId]] executor:TaskRunner.md#taskId[Task attempt ID] TaskMemoryManager initializes the < >. == [[consumers]] Spillable Memory Consumers TaskMemoryManager tracks memory:MemoryConsumer.md[spillable memory consumers]. TaskMemoryManager registers a new memory consumer when requested to < >. TaskMemoryManager removes ( clears ) all registered memory consumer when requested to < >. Memory consumers are used to report memory usage when TaskMemoryManager is requested to < >. == [[memoryManager]] MemoryManager TaskMemoryManager is given a memory:MemoryManager.md[MemoryManager] when < >. TaskMemoryManager uses the MemoryManager for the following: < >, < > or < > execution memory < > < > < > < > < > == [[cleanUpAllAllocatedMemory]] Cleaning Up All Allocated Memory","title":"TaskMemoryManager"},{"location":"memory/TaskMemoryManager/#source-java","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#long-cleanupallallocatedmemory","text":"cleanUpAllAllocatedMemory clears < >. CAUTION: FIXME All recorded < > are queried for the size of used memory. If the memory used is greater than 0, the following WARN message is printed out to the logs: WARN TaskMemoryManager: leak [bytes] memory from [consumer] The consumers collection is then cleared. MemoryManager.md#releaseExecutionMemory[MemoryManager.releaseExecutionMemory] is executed to release the memory that is not used by any consumer. Before cleanUpAllAllocatedMemory returns, it calls MemoryManager.md#releaseAllExecutionMemoryForTask[MemoryManager.releaseAllExecutionMemoryForTask] that in turn becomes the return value. CAUTION: FIXME Image with the interactions to MemoryManager . NOTE: cleanUpAllAllocatedMemory is used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (and cleans up after itself). == [[acquireExecutionMemory]] Acquiring Execution Memory","title":"long cleanUpAllAllocatedMemory()"},{"location":"memory/TaskMemoryManager/#source-java_1","text":"long acquireExecutionMemory( long required, MemoryConsumer consumer) acquireExecutionMemory allocates up to required size of memory for the memory:MemoryConsumer.md[MemoryConsumer]. When no memory could be allocated, it calls spill on every consumer, itself including. Finally, acquireExecutionMemory returns the allocated memory. NOTE: acquireExecutionMemory synchronizes on itself, and so no other calls on the object could be completed. NOTE: memory:MemoryConsumer.md[MemoryConsumer] knows its mode -- on- or off-heap. acquireExecutionMemory first calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) . TIP: TaskMemoryManager is a mere wrapper of MemoryManager to track < >? CAUTION: FIXME When the memory obtained is less than requested (by required ), acquireExecutionMemory requests all < > to MemoryConsumer.md#spill[release memory (by spilling it to disk)]. NOTE: acquireExecutionMemory requests memory from consumers that work in the same mode except the requesting one. You may see the following DEBUG message when spill released some memory: DEBUG Task [taskAttemptId] released [bytes] from [consumer] for [consumer] acquireExecutionMemory calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) again (it called it at the beginning). It does the memory acquisition until it gets enough memory or there are no more consumers to request spill from. You may also see the following ERROR message in the logs when there is an error while requesting spill with OutOfMemoryError followed. ERROR error while calling spill() on [consumer] If the earlier spill on the consumers did not work out and there is still memory to be acquired, acquireExecutionMemory MemoryConsumer.md#spill[requests the input consumer to spill memory to disk] (that in fact requested more memory!) If the consumer releases some memory, you should see the following DEBUG message in the logs: DEBUG Task [taskAttemptId] released [bytes] from itself ([consumer]) acquireExecutionMemory calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) once more. NOTE: memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) could have been called \"three\" times, i.e. at the very beginning, for each consumer, and on itself. It records the consumer in < > registry. You should see the following DEBUG message in the logs: DEBUG Task [taskAttemptId] acquired [bytes] for [consumer] acquireExecutionMemory is used when: MemoryConsumer is requested to memory:MemoryConsumer.md#acquireMemory[acquire execution memory] TaskMemoryManager is requested to < > == [[allocatePage]] Allocating Memory Block for Tungsten Consumers","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#source-java_2","text":"MemoryBlock allocatePage( long size, MemoryConsumer consumer) NOTE: It only handles Tungsten Consumers , i.e. MemoryConsumer.md[MemoryConsumers] in tungstenMemoryMode mode. allocatePage allocates a block of memory (aka page ) smaller than MAXIMUM_PAGE_SIZE_BYTES maximum size. It checks size against the internal MAXIMUM_PAGE_SIZE_BYTES maximum size. If it is greater than the maximum size, the following IllegalArgumentException is thrown: Cannot allocate a page with more than [MAXIMUM_PAGE_SIZE_BYTES] bytes It then < > (for the input size and consumer ). It finishes by returning null when no execution memory could be acquired. With the execution memory acquired, it finds the smallest unallocated page index and records the page number (using < > registry). If the index is PAGE_TABLE_SIZE or higher, < > is called and then the following IllegalStateException is thrown: Have already allocated a maximum of [PAGE_TABLE_SIZE] pages It then attempts to allocate a MemoryBlock from Tungsten MemoryAllocator (calling memoryManager.tungstenMemoryAllocator().allocate(acquired) ). CAUTION: FIXME What is MemoryAllocator ? When successful, MemoryBlock gets assigned pageNumber and it gets added to the internal < > registry. You should see the following TRACE message in the logs: TRACE Allocate page number [pageNumber] ([acquired] bytes) The page is returned. If a OutOfMemoryError is thrown when allocating a MemoryBlock page, the following WARN message is printed out to the logs: WARN Failed to allocate a page ([acquired] bytes), try again. And acquiredButNotUsed gets acquired memory space with the pageNumber cleared in < > (i.e. the index for pageNumber gets false ). CAUTION: FIXME Why is the code tracking acquiredButNotUsed ? Another < > attempt is recursively tried. CAUTION: FIXME Why is there a hope for being able to allocate a page? == [[releaseExecutionMemory]] releaseExecutionMemory Method","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#source-java_3","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#void-releaseexecutionmemorylong-size-memoryconsumer-consumer","text":"releaseExecutionMemory ...FIXME","title":"void releaseExecutionMemory(long size, MemoryConsumer consumer)"},{"location":"memory/TaskMemoryManager/#note","text":"releaseExecutionMemory is used when: MemoryConsumer is requested to MemoryConsumer.md#freeMemory[freeMemory]","title":"[NOTE]"},{"location":"memory/TaskMemoryManager/#taskmemorymanager-is-requested-to-and","text":"== [[getMemoryConsumptionForThisTask]] getMemoryConsumptionForThisTask Method","title":"* TaskMemoryManager is requested to &lt;&gt; and &lt;&gt;"},{"location":"memory/TaskMemoryManager/#source-java_4","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#long-getmemoryconsumptionforthistask","text":"getMemoryConsumptionForThisTask ...FIXME NOTE: getMemoryConsumptionForThisTask is used exclusively in Spark tests. == [[showMemoryUsage]] Displaying Memory Usage","title":"long getMemoryConsumptionForThisTask()"},{"location":"memory/TaskMemoryManager/#source-java_5","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#void-showmemoryusage","text":"showMemoryUsage prints out the following INFO message to the logs (with the < >):","title":"void showMemoryUsage()"},{"location":"memory/TaskMemoryManager/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"memory/TaskMemoryManager/#memory-used-in-task-taskattemptid","text":"showMemoryUsage requests every < > to memory:MemoryConsumer.md#getUsed[report memory used]. showMemoryUsage prints out the following INFO message to the logs for a MemoryConsumer with some memory usage (and excludes zero-memory consumers):","title":"Memory used in task [taskAttemptId]"},{"location":"memory/TaskMemoryManager/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"memory/TaskMemoryManager/#acquired-by-consumer-memusage","text":"showMemoryUsage prints out the following INFO messages to the logs:","title":"Acquired by [consumer]: [memUsage]"},{"location":"memory/TaskMemoryManager/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"memory/TaskMemoryManager/#amount-bytes-of-memory-were-used-by-task-taskattemptid-but-are-not-associated-with-specific-consumers","text":"","title":"[amount] bytes of memory were used by task [taskAttemptId] but are not associated with specific consumers"},{"location":"memory/TaskMemoryManager/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"memory/TaskMemoryManager/#executionmemoryused-bytes-of-memory-are-used-for-execution-and-storagememoryused-bytes-of-memory-are-used-for-storage","text":"showMemoryUsage is used when MemoryConsumer is requested to memory:MemoryConsumer.md#throwOom[throw an OutOfMemoryError]. == [[pageSizeBytes]] pageSizeBytes Method","title":"[executionMemoryUsed] bytes of memory are used for execution and [storageMemoryUsed] bytes of memory are used for storage"},{"location":"memory/TaskMemoryManager/#source-java_6","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#long-pagesizebytes","text":"pageSizeBytes simply requests the < > for MemoryManager.md#pageSizeBytes[pageSizeBytes]. NOTE: pageSizeBytes is used when...FIXME == [[freePage]] Freeing Memory Page -- freePage Method","title":"long pageSizeBytes()"},{"location":"memory/TaskMemoryManager/#source-java_7","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#void-freepagememoryblock-page-memoryconsumer-consumer","text":"pageSizeBytes simply requests the < > for MemoryManager.md#pageSizeBytes[pageSizeBytes]. NOTE: pageSizeBytes is used when MemoryConsumer is requested to MemoryConsumer.md#freePage[freePage] and MemoryConsumer.md#throwOom[throwOom]. == [[getPage]] Getting Page -- getPage Method","title":"void freePage(MemoryBlock page, MemoryConsumer consumer)"},{"location":"memory/TaskMemoryManager/#source-java_8","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#object-getpagelong-pageplusoffsetaddress","text":"getPage ...FIXME NOTE: getPage is used when...FIXME == [[getOffsetInPage]] Getting Page Offset -- getOffsetInPage Method","title":"Object getPage(long pagePlusOffsetAddress)"},{"location":"memory/TaskMemoryManager/#source-java_9","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#long-getoffsetinpagelong-pageplusoffsetaddress","text":"getPage ...FIXME NOTE: getPage is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.memory.TaskMemoryManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"long getOffsetInPage(long pagePlusOffsetAddress)"},{"location":"memory/TaskMemoryManager/#source","text":"","title":"[source]"},{"location":"memory/TaskMemoryManager/#log4jloggerorgapachesparkmemorytaskmemorymanagerall","text":"Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | acquiredButNotUsed | [[acquiredButNotUsed]] The size of memory allocated but not used. | allocatedPages | [[allocatedPages]] Collection of flags ( true or false values) of size PAGE_TABLE_SIZE with all bits initially disabled (i.e. false ). TIP: allocatedPages is https://docs.oracle.com/javase/8/docs/api/java/util/BitSet.html[java.util.BitSet ]. When < > is called, it will record the page in the registry by setting the bit at the specified index (that corresponds to the allocated page) to true . | pageTable | [[pageTable]] The array of size PAGE_TABLE_SIZE with indices being MemoryBlock objects. When < MemoryBlock page for Tungsten consumers>>, the index corresponds to pageNumber that points to the MemoryBlock page allocated. | tungstenMemoryMode | [[tungstenMemoryMode]] MemoryMode (i.e. OFF_HEAP or ON_HEAP ) Set to the MemoryManager.md#tungstenMemoryMode[tungstenMemoryMode] of the < > while TaskMemoryManager is < > |===","title":"log4j.logger.org.apache.spark.memory.TaskMemoryManager=ALL"},{"location":"memory/UnifiedMemoryManager/","text":"UnifiedMemoryManager \u00b6 UnifiedMemoryManager is a MemoryManager (with the onHeapExecutionMemory being the Maximum Heap Memory with the onHeapStorageRegionSize taken out). UnifiedMemoryManager allows for soft boundaries between storage and execution memory (allowing requests for memory in one region to be fulfilled by borrowing memory from the other). Creating Instance \u00b6 UnifiedMemoryManager takes the following to be created: SparkConf Maximum Heap Memory Size of the On-Heap Storage Region Number of CPU Cores While being created, UnifiedMemoryManager asserts the invariants . UnifiedMemoryManager is created using apply factory. Invariants \u00b6 UnifiedMemoryManager asserts the following: Sum of the pool size of the on-heap ExecutionMemoryPool and on-heap StorageMemoryPool is exactly the maximum heap memory Sum of the pool size of the off-heap ExecutionMemoryPool and off-heap StorageMemoryPool is exactly the maximum off-heap memory Creating UnifiedMemoryManager \u00b6 apply ( conf : SparkConf , numCores : Int ): UnifiedMemoryManager apply creates a UnifiedMemoryManager with the following: Property Value Maximum Heap Memory Maximum Memory onHeapStorageRegionSize spark.memory.storageFraction of the Maximum Memory apply is used when: SparkEnv utility is used to create a base SparkEnv (for the driver and executors) Maximum Heap Memory \u00b6 getMaxMemory ( conf : SparkConf ): Long getMaxMemory calculates the maximum memory to use for execution and storage based on...FIXME Demo \u00b6 // local mode with --conf spark.driver.memory=2g scala> sc.getConf.getSizeAsBytes(\"spark.driver.memory\") res0: Long = 2147483648 scala> val systemMemory = Runtime.getRuntime.maxMemory // fixed amount of memory for non-storage, non-execution purposes val reservedMemory = 300 * 1024 * 1024 // minimum system memory required val minSystemMemory = (reservedMemory * 1.5).ceil.toLong val usableMemory = systemMemory - reservedMemory val memoryFraction = sc.getConf.getDouble(\"spark.memory.fraction\", 0.6) scala> val maxMemory = (usableMemory * memoryFraction).toLong maxMemory: Long = 956615884 import org.apache.spark.network.util.JavaUtils scala> JavaUtils.byteStringAsMb(maxMemory + \"b\") res1: Long = 912 Total Available On-Heap Memory for Storage \u00b6 maxOnHeapStorageMemory : Long maxOnHeapStorageMemory is part of the MemoryManager abstraction. maxOnHeapStorageMemory is the difference between Maximum Heap Memory and the memory used in the on-heap execution memory pool .","title":"UnifiedMemoryManager"},{"location":"memory/UnifiedMemoryManager/#unifiedmemorymanager","text":"UnifiedMemoryManager is a MemoryManager (with the onHeapExecutionMemory being the Maximum Heap Memory with the onHeapStorageRegionSize taken out). UnifiedMemoryManager allows for soft boundaries between storage and execution memory (allowing requests for memory in one region to be fulfilled by borrowing memory from the other).","title":"UnifiedMemoryManager"},{"location":"memory/UnifiedMemoryManager/#creating-instance","text":"UnifiedMemoryManager takes the following to be created: SparkConf Maximum Heap Memory Size of the On-Heap Storage Region Number of CPU Cores While being created, UnifiedMemoryManager asserts the invariants . UnifiedMemoryManager is created using apply factory.","title":"Creating Instance"},{"location":"memory/UnifiedMemoryManager/#invariants","text":"UnifiedMemoryManager asserts the following: Sum of the pool size of the on-heap ExecutionMemoryPool and on-heap StorageMemoryPool is exactly the maximum heap memory Sum of the pool size of the off-heap ExecutionMemoryPool and off-heap StorageMemoryPool is exactly the maximum off-heap memory","title":" Invariants"},{"location":"memory/UnifiedMemoryManager/#creating-unifiedmemorymanager","text":"apply ( conf : SparkConf , numCores : Int ): UnifiedMemoryManager apply creates a UnifiedMemoryManager with the following: Property Value Maximum Heap Memory Maximum Memory onHeapStorageRegionSize spark.memory.storageFraction of the Maximum Memory apply is used when: SparkEnv utility is used to create a base SparkEnv (for the driver and executors)","title":" Creating UnifiedMemoryManager"},{"location":"memory/UnifiedMemoryManager/#maximum-heap-memory","text":"getMaxMemory ( conf : SparkConf ): Long getMaxMemory calculates the maximum memory to use for execution and storage based on...FIXME","title":" Maximum Heap Memory"},{"location":"memory/UnifiedMemoryManager/#demo","text":"// local mode with --conf spark.driver.memory=2g scala> sc.getConf.getSizeAsBytes(\"spark.driver.memory\") res0: Long = 2147483648 scala> val systemMemory = Runtime.getRuntime.maxMemory // fixed amount of memory for non-storage, non-execution purposes val reservedMemory = 300 * 1024 * 1024 // minimum system memory required val minSystemMemory = (reservedMemory * 1.5).ceil.toLong val usableMemory = systemMemory - reservedMemory val memoryFraction = sc.getConf.getDouble(\"spark.memory.fraction\", 0.6) scala> val maxMemory = (usableMemory * memoryFraction).toLong maxMemory: Long = 956615884 import org.apache.spark.network.util.JavaUtils scala> JavaUtils.byteStringAsMb(maxMemory + \"b\") res1: Long = 912","title":"Demo"},{"location":"memory/UnifiedMemoryManager/#total-available-on-heap-memory-for-storage","text":"maxOnHeapStorageMemory : Long maxOnHeapStorageMemory is part of the MemoryManager abstraction. maxOnHeapStorageMemory is the difference between Maximum Heap Memory and the memory used in the on-heap execution memory pool .","title":" Total Available On-Heap Memory for Storage"},{"location":"memory/UnsafeExternalSorter/","text":"UnsafeExternalSorter \u00b6 UnsafeExternalSorter is a MemoryConsumer . Creating Instance \u00b6 UnsafeExternalSorter takes the following to be created: TaskMemoryManager BlockManager SerializerManager TaskContext RecordComparator Supplier PrefixComparator Initial Size Page size (in bytes) numElementsForSpillThreshold UnsafeInMemorySorter canUseRadixSort flag UnsafeExternalSorter is created when: UnsafeExternalSorter utility is used to createWithExistingInMemorySorter and create createWithExistingInMemorySorter \u00b6 UnsafeExternalSorter createWithExistingInMemorySorter ( TaskMemoryManager taskMemoryManager , BlockManager blockManager , SerializerManager serializerManager , TaskContext taskContext , Supplier < RecordComparator > recordComparatorSupplier , PrefixComparator prefixComparator , int initialSize , long pageSizeBytes , int numElementsForSpillThreshold , UnsafeInMemorySorter inMemorySorter , long existingMemoryConsumption ) createWithExistingInMemorySorter ...FIXME createWithExistingInMemorySorter is used when: UnsafeKVExternalSorter is created create \u00b6 UnsafeExternalSorter create ( TaskMemoryManager taskMemoryManager , BlockManager blockManager , SerializerManager serializerManager , TaskContext taskContext , Supplier < RecordComparator > recordComparatorSupplier , PrefixComparator prefixComparator , int initialSize , long pageSizeBytes , int numElementsForSpillThreshold , boolean canUseRadixSort ) create creates a new UnsafeExternalSorter with no UnsafeInMemorySorter given ( null ). create is used when: UnsafeExternalRowSorter and UnsafeKVExternalSorter are created","title":"UnsafeExternalSorter"},{"location":"memory/UnsafeExternalSorter/#unsafeexternalsorter","text":"UnsafeExternalSorter is a MemoryConsumer .","title":"UnsafeExternalSorter"},{"location":"memory/UnsafeExternalSorter/#creating-instance","text":"UnsafeExternalSorter takes the following to be created: TaskMemoryManager BlockManager SerializerManager TaskContext RecordComparator Supplier PrefixComparator Initial Size Page size (in bytes) numElementsForSpillThreshold UnsafeInMemorySorter canUseRadixSort flag UnsafeExternalSorter is created when: UnsafeExternalSorter utility is used to createWithExistingInMemorySorter and create","title":"Creating Instance"},{"location":"memory/UnsafeExternalSorter/#createwithexistinginmemorysorter","text":"UnsafeExternalSorter createWithExistingInMemorySorter ( TaskMemoryManager taskMemoryManager , BlockManager blockManager , SerializerManager serializerManager , TaskContext taskContext , Supplier < RecordComparator > recordComparatorSupplier , PrefixComparator prefixComparator , int initialSize , long pageSizeBytes , int numElementsForSpillThreshold , UnsafeInMemorySorter inMemorySorter , long existingMemoryConsumption ) createWithExistingInMemorySorter ...FIXME createWithExistingInMemorySorter is used when: UnsafeKVExternalSorter is created","title":" createWithExistingInMemorySorter"},{"location":"memory/UnsafeExternalSorter/#create","text":"UnsafeExternalSorter create ( TaskMemoryManager taskMemoryManager , BlockManager blockManager , SerializerManager serializerManager , TaskContext taskContext , Supplier < RecordComparator > recordComparatorSupplier , PrefixComparator prefixComparator , int initialSize , long pageSizeBytes , int numElementsForSpillThreshold , boolean canUseRadixSort ) create creates a new UnsafeExternalSorter with no UnsafeInMemorySorter given ( null ). create is used when: UnsafeExternalRowSorter and UnsafeKVExternalSorter are created","title":" create"},{"location":"memory/UnsafeInMemorySorter/","text":"UnsafeInMemorySorter \u00b6 Creating Instance \u00b6 UnsafeInMemorySorter takes the following to be created: MemoryConsumer TaskMemoryManager RecordComparator PrefixComparator Long Array or Size canUseRadixSort flag UnsafeInMemorySorter is created when: UnsafeExternalSorter is created UnsafeKVExternalSorter is created","title":"UnsafeInMemorySorter"},{"location":"memory/UnsafeInMemorySorter/#unsafeinmemorysorter","text":"","title":"UnsafeInMemorySorter"},{"location":"memory/UnsafeInMemorySorter/#creating-instance","text":"UnsafeInMemorySorter takes the following to be created: MemoryConsumer TaskMemoryManager RecordComparator PrefixComparator Long Array or Size canUseRadixSort flag UnsafeInMemorySorter is created when: UnsafeExternalSorter is created UnsafeKVExternalSorter is created","title":"Creating Instance"},{"location":"memory/UnsafeSorterSpillReader/","text":"= UnsafeSorterSpillReader UnsafeSorterSpillReader is...FIXME","title":"UnsafeSorterSpillReader"},{"location":"memory/UnsafeSorterSpillWriter/","text":"= [[UnsafeSorterSpillWriter]] UnsafeSorterSpillWriter UnsafeSorterSpillWriter is...FIXME","title":"UnsafeSorterSpillWriter"},{"location":"metrics/","text":"Spark Metrics \u00b6 Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances , e.g. the driver of a Spark application or the master of a Spark Standalone cluster). Spark Metrics uses Dropwizard Metrics Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment . Metrics Systems \u00b6 applicationMaster \u00b6 Registered when ApplicationMaster (Hadoop YARN) is requested to createAllocator applications \u00b6 Registered when Master (Spark Standalone) is created driver \u00b6 Registered when SparkEnv is created for the driver executor \u00b6 Registered when SparkEnv is created for an executor master \u00b6 Registered when Master (Spark Standalone) is created mesos_cluster \u00b6 Registered when MesosClusterScheduler (Apache Mesos) is created shuffleService \u00b6 Registered when ExternalShuffleService is created worker \u00b6 Registered when Worker (Spark Standalone) is created MetricsSystem \u00b6 Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' MetricRegistry that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem MetricsConfig \u00b6 MetricsConfig is the configuration of the MetricsSystem (i.e. metrics spark-metrics-Source.md[sources] and spark-metrics-Sink.md[sinks]). metrics.properties is the default metrics configuration file. It is configured using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsServlet Metrics Sink \u00b6 Among the metrics sinks is spark-metrics-MetricsServlet.md[MetricsServlet] that is used when sink.servlet metrics sink is configured in spark-metrics-MetricsConfig.md[metrics configuration]. CAUTION: FIXME Describe configuration files and properties JmxSink Metrics Sink \u00b6 Enable org.apache.spark.metrics.sink.JmxSink in spark-metrics-MetricsConfig.md[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink JSON URI Path \u00b6 Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output. Spark Standalone Master \u00b6 $ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Metrics"},{"location":"metrics/#spark-metrics","text":"Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances , e.g. the driver of a Spark application or the master of a Spark Standalone cluster). Spark Metrics uses Dropwizard Metrics Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment .","title":"Spark Metrics"},{"location":"metrics/#metrics-systems","text":"","title":"Metrics Systems"},{"location":"metrics/#applicationmaster","text":"Registered when ApplicationMaster (Hadoop YARN) is requested to createAllocator","title":"applicationMaster"},{"location":"metrics/#applications","text":"Registered when Master (Spark Standalone) is created","title":"applications"},{"location":"metrics/#driver","text":"Registered when SparkEnv is created for the driver","title":"driver"},{"location":"metrics/#executor","text":"Registered when SparkEnv is created for an executor","title":"executor"},{"location":"metrics/#master","text":"Registered when Master (Spark Standalone) is created","title":"master"},{"location":"metrics/#mesos_cluster","text":"Registered when MesosClusterScheduler (Apache Mesos) is created","title":"mesos_cluster"},{"location":"metrics/#shuffleservice","text":"Registered when ExternalShuffleService is created","title":"shuffleService"},{"location":"metrics/#worker","text":"Registered when Worker (Spark Standalone) is created","title":"worker"},{"location":"metrics/#metricssystem","text":"Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' MetricRegistry that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem","title":" MetricsSystem"},{"location":"metrics/#metricsconfig","text":"MetricsConfig is the configuration of the MetricsSystem (i.e. metrics spark-metrics-Source.md[sources] and spark-metrics-Sink.md[sinks]). metrics.properties is the default metrics configuration file. It is configured using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration.","title":" MetricsConfig"},{"location":"metrics/#metricsservlet-metrics-sink","text":"Among the metrics sinks is spark-metrics-MetricsServlet.md[MetricsServlet] that is used when sink.servlet metrics sink is configured in spark-metrics-MetricsConfig.md[metrics configuration]. CAUTION: FIXME Describe configuration files and properties","title":" MetricsServlet Metrics Sink"},{"location":"metrics/#jmxsink-metrics-sink","text":"Enable org.apache.spark.metrics.sink.JmxSink in spark-metrics-MetricsConfig.md[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink","title":" JmxSink Metrics Sink"},{"location":"metrics/#json-uri-path","text":"Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output.","title":"JSON URI Path"},{"location":"metrics/#spark-standalone-master","text":"$ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Standalone Master"},{"location":"metrics/JvmSource/","text":"JvmSource \u00b6 JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codahale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/JvmSource/#jvmsource","text":"JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codahale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/MetricsConfig/","text":"MetricsConfig \u00b6 MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when MetricsSystem is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |=== [NOTE] \u00b6 The order of precedence of metrics configuration settings is as follows: . < > . spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a SparkConf.md[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 initialize < > and < > (that is defined using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is created . === [[setDefaultProperties]] setDefaultProperties Internal Method [source, scala] \u00b6 setDefaultProperties(prop: Properties): Unit \u00b6 setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method [source, scala] \u00b6 loadPropertiesFromFile(path: Option[String]): Unit \u00b6 loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method [source, scala] \u00b6 subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties] \u00b6 subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es). [source, scala] \u00b6 driver.hello.world => (driver, (hello.world)) \u00b6 NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem registers metrics sources and sinks . === [[getInstance]] getInstance Method [source, scala] \u00b6 getInstance(inst: String): Properties \u00b6 getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#metricsconfig","text":"MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when MetricsSystem is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |===","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#note","text":"The order of precedence of metrics configuration settings is as follows: . < > . spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a SparkConf.md[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method","title":"[NOTE]"},{"location":"metrics/MetricsConfig/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#initialize-unit","text":"initialize < > and < > (that is defined using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is created . === [[setDefaultProperties]] setDefaultProperties Internal Method","title":"initialize(): Unit"},{"location":"metrics/MetricsConfig/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#setdefaultpropertiesprop-properties-unit","text":"setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method","title":"setDefaultProperties(prop: Properties): Unit"},{"location":"metrics/MetricsConfig/#source-scala_2","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#loadpropertiesfromfilepath-optionstring-unit","text":"loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method","title":"loadPropertiesFromFile(path: Option[String]): Unit"},{"location":"metrics/MetricsConfig/#source-scala_3","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#subpropertiesprop-properties-regex-regex-mutablehashmapstring-properties","text":"subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es).","title":"subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties]"},{"location":"metrics/MetricsConfig/#source-scala_4","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#driverhelloworld-driver-helloworld","text":"NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem registers metrics sources and sinks . === [[getInstance]] getInstance Method","title":"driver.hello.world =&gt; (driver, (hello.world))"},{"location":"metrics/MetricsConfig/#source-scala_5","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#getinstanceinst-string-properties","text":"getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"getInstance(inst: String): Properties"},{"location":"metrics/MetricsServlet/","text":"MetricsServlet JSON Metrics Sink \u00b6 MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is started (and requested to register metrics sinks ). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in spark-metrics-MetricsConfig.md[metrics configuration]). That is not required since MetricsConfig spark-metrics-MetricsConfig.md#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with Dropwizard Metrics library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale com.codahale.metrics.json.MetricsModule . Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale com.codahale.metrics.json.MetricsModule . |=== Creating Instance \u00b6 MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] MetricRegistry ( Dropwizard Metrics [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method [source, scala] \u00b6 getMetricsSnapshot(request: HttpServletRequest): String \u00b6 getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using ++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method [source, scala] \u00b6 getHandlers(conf: SparkConf): Array[ServletContextHandler] \u00b6 getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for MetricsSystem.md#getServletHandlers[metrics ServletContextHandlers].","title":"MetricsServlet"},{"location":"metrics/MetricsServlet/#metricsservlet-json-metrics-sink","text":"MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is started (and requested to register metrics sinks ). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in spark-metrics-MetricsConfig.md[metrics configuration]). That is not required since MetricsConfig spark-metrics-MetricsConfig.md#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with Dropwizard Metrics library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale com.codahale.metrics.json.MetricsModule . Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale com.codahale.metrics.json.MetricsModule . |===","title":"MetricsServlet JSON Metrics Sink"},{"location":"metrics/MetricsServlet/#creating-instance","text":"MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] MetricRegistry ( Dropwizard Metrics [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method","title":"Creating Instance"},{"location":"metrics/MetricsServlet/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#getmetricssnapshotrequest-httpservletrequest-string","text":"getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using ++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method","title":"getMetricsSnapshot(request: HttpServletRequest): String"},{"location":"metrics/MetricsServlet/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#gethandlersconf-sparkconf-arrayservletcontexthandler","text":"getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for MetricsSystem.md#getServletHandlers[metrics ServletContextHandlers].","title":"getHandlers(conf: SparkConf): Array[ServletContextHandler]"},{"location":"metrics/MetricsSystem/","text":"MetricsSystem \u00b6 MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem . Creating Instance \u00b6 MetricsSystem takes the following to be created: Instance Name SparkConf SecurityManager While being created, MetricsSystem requests the MetricsConfig to initialize . MetricsSystem is created (using createMetricsSystem utility) for the Metrics Systems . Creating MetricsSystem \u00b6 createMetricsSystem ( instance : String conf : SparkConf securityMgr : SecurityManager ): MetricsSystem createMetricsSystem creates a new MetricsSystem (for the given parameters). createMetricsSystem is used to create metrics systems . Metrics Sources for Spark SQL \u00b6 CodegenMetrics HiveCatalogMetrics Registering Metrics Source \u00b6 registerSource ( source : Source ): Unit registerSource adds source to the sources internal registry. registerSource creates an identifier for the metrics source and registers it with the MetricRegistry . registerSource registers the metrics source under a given name. registerSource prints out the following INFO message to the logs when registering a name more than once: Metrics already registered Building Metrics Source Identifier \u00b6 buildRegistryName ( source : Source ): String buildRegistryName uses spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace] and executor:Executor.md#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace], executor:Executor.md#spark.executor.id[spark.executor.id] and the name of the source . FIXME Finish for the other components. buildRegistryName is used when MetricsSystem is requested to register or remove a metrics source. Registering Metrics Sources for Spark Instance \u00b6 registerSources (): Unit registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the spark-metrics-Source.md[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. Source class [classPath] cannot be instantiated registerSources is used when MetricsSystem is requested to start . Requesting JSON Servlet Handler \u00b6 getServletHandlers : Array [ ServletContextHandler ] If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the spark-metrics-MetricsServlet.md#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem getServletHandlers is used when: SparkContext is created (Spark Standalone) Master and Worker are requested to start Registering Metrics Sinks \u00b6 registerSinks (): Unit registerSinks requests the < > for the spark-metrics-MetricsConfig.md#getInstance[configuration] of the < >. registerSinks requests the < > for the spark-metrics-MetricsConfig.md#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a spark-metrics-MetricsServlet.md[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated registerSinks is used when MetricsSystem is requested to start . Stopping \u00b6 stop (): Unit stop ...FIXME Reporting Metrics \u00b6 report (): Unit report simply requests the registered metrics sinks to report metrics . Starting \u00b6 start (): Unit start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to spark-metrics-Sink.md#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running Logging \u00b6 Enable ALL logging level for org.apache.spark.metrics.MetricsSystem logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=ALL Refer to Logging . Internal Registries \u00b6 MetricRegistry \u00b6 Integration point to Dropwizard Metrics' MetricRegistry Used when MetricsSystem is requested to: Register or remove a metrics source Start (that in turn registers metrics sinks ) MetricsConfig \u00b6 MetricsConfig Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >. MetricsServlet \u00b6 MetricsServlet JSON metrics sink that is only available for the < > with a web UI (i.e. the driver of a Spark application and Spark Standalone's Master ). MetricsSystem may have at most one MetricsServlet JSON metrics sink (which is registered by default ). Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used when MetricsSystem is requested for a < >. running Flag \u00b6 Indicates whether MetricsSystem has been started ( true ) or not ( false ) Default: false sinks \u00b6 Metrics sinks Used when MetricsSystem < > and < >. sources \u00b6 Metrics sources Used when MetricsSystem < >.","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#metricssystem","text":"MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem .","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#creating-instance","text":"MetricsSystem takes the following to be created: Instance Name SparkConf SecurityManager While being created, MetricsSystem requests the MetricsConfig to initialize . MetricsSystem is created (using createMetricsSystem utility) for the Metrics Systems .","title":"Creating Instance"},{"location":"metrics/MetricsSystem/#creating-metricssystem","text":"createMetricsSystem ( instance : String conf : SparkConf securityMgr : SecurityManager ): MetricsSystem createMetricsSystem creates a new MetricsSystem (for the given parameters). createMetricsSystem is used to create metrics systems .","title":" Creating MetricsSystem"},{"location":"metrics/MetricsSystem/#metrics-sources-for-spark-sql","text":"CodegenMetrics HiveCatalogMetrics","title":" Metrics Sources for Spark SQL"},{"location":"metrics/MetricsSystem/#registering-metrics-source","text":"registerSource ( source : Source ): Unit registerSource adds source to the sources internal registry. registerSource creates an identifier for the metrics source and registers it with the MetricRegistry . registerSource registers the metrics source under a given name. registerSource prints out the following INFO message to the logs when registering a name more than once: Metrics already registered","title":" Registering Metrics Source"},{"location":"metrics/MetricsSystem/#building-metrics-source-identifier","text":"buildRegistryName ( source : Source ): String buildRegistryName uses spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace] and executor:Executor.md#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace], executor:Executor.md#spark.executor.id[spark.executor.id] and the name of the source . FIXME Finish for the other components. buildRegistryName is used when MetricsSystem is requested to register or remove a metrics source.","title":" Building Metrics Source Identifier"},{"location":"metrics/MetricsSystem/#registering-metrics-sources-for-spark-instance","text":"registerSources (): Unit registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the spark-metrics-Source.md[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. Source class [classPath] cannot be instantiated registerSources is used when MetricsSystem is requested to start .","title":" Registering Metrics Sources for Spark Instance"},{"location":"metrics/MetricsSystem/#requesting-json-servlet-handler","text":"getServletHandlers : Array [ ServletContextHandler ] If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the spark-metrics-MetricsServlet.md#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem getServletHandlers is used when: SparkContext is created (Spark Standalone) Master and Worker are requested to start","title":" Requesting JSON Servlet Handler"},{"location":"metrics/MetricsSystem/#registering-metrics-sinks","text":"registerSinks (): Unit registerSinks requests the < > for the spark-metrics-MetricsConfig.md#getInstance[configuration] of the < >. registerSinks requests the < > for the spark-metrics-MetricsConfig.md#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a spark-metrics-MetricsServlet.md[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated registerSinks is used when MetricsSystem is requested to start .","title":" Registering Metrics Sinks"},{"location":"metrics/MetricsSystem/#stopping","text":"stop (): Unit stop ...FIXME","title":" Stopping"},{"location":"metrics/MetricsSystem/#reporting-metrics","text":"report (): Unit report simply requests the registered metrics sinks to report metrics .","title":" Reporting Metrics"},{"location":"metrics/MetricsSystem/#starting","text":"start (): Unit start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to spark-metrics-Sink.md#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running","title":" Starting"},{"location":"metrics/MetricsSystem/#logging","text":"Enable ALL logging level for org.apache.spark.metrics.MetricsSystem logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=ALL Refer to Logging .","title":"Logging"},{"location":"metrics/MetricsSystem/#internal-registries","text":"","title":"Internal Registries"},{"location":"metrics/MetricsSystem/#metricregistry","text":"Integration point to Dropwizard Metrics' MetricRegistry Used when MetricsSystem is requested to: Register or remove a metrics source Start (that in turn registers metrics sinks )","title":" MetricRegistry"},{"location":"metrics/MetricsSystem/#metricsconfig","text":"MetricsConfig Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >.","title":" MetricsConfig"},{"location":"metrics/MetricsSystem/#metricsservlet","text":"MetricsServlet JSON metrics sink that is only available for the < > with a web UI (i.e. the driver of a Spark application and Spark Standalone's Master ). MetricsSystem may have at most one MetricsServlet JSON metrics sink (which is registered by default ). Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used when MetricsSystem is requested for a < >.","title":" MetricsServlet"},{"location":"metrics/MetricsSystem/#running-flag","text":"Indicates whether MetricsSystem has been started ( true ) or not ( false ) Default: false","title":" running Flag"},{"location":"metrics/MetricsSystem/#sinks","text":"Metrics sinks Used when MetricsSystem < > and < >.","title":" sinks"},{"location":"metrics/MetricsSystem/#sources","text":"Metrics sources Used when MetricsSystem < >.","title":" sources"},{"location":"metrics/Sink/","text":"Sink \u00b6 Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | spark-metrics-MetricsServlet.md[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Sink/#sink","text":"Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | spark-metrics-MetricsServlet.md[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Source/","text":"Source \u00b6 Source is an abstraction of metrics sources . Contract \u00b6 MetricRegistry \u00b6 metricRegistry : MetricRegistry MetricRegistry ( Codahale Metrics ) Used when: MetricsSystem is requested to register a metrics source Source Name \u00b6 sourceName : String Used when: MetricsSystem is requested to build a metrics source identifier and getSourcesByName Implementations \u00b6 AccumulatorSource ApplicationMasterSource ApplicationSource AppStatusSource BlockManagerSource CacheMetrics CodegenMetrics DAGSchedulerSource ExecutorAllocationManagerSource ExecutorMetricsSource ExecutorSource ExternalShuffleServiceSource HiveCatalogMetrics JVMCPUSource JvmSource LiveListenerBusMetrics MasterSource PluginMetricsSource ShuffleMetricsSource WorkerSource","title":"Source"},{"location":"metrics/Source/#source","text":"Source is an abstraction of metrics sources .","title":"Source"},{"location":"metrics/Source/#contract","text":"","title":"Contract"},{"location":"metrics/Source/#metricregistry","text":"metricRegistry : MetricRegistry MetricRegistry ( Codahale Metrics ) Used when: MetricsSystem is requested to register a metrics source","title":" MetricRegistry"},{"location":"metrics/Source/#source-name","text":"sourceName : String Used when: MetricsSystem is requested to build a metrics source identifier and getSourcesByName","title":" Source Name"},{"location":"metrics/Source/#implementations","text":"AccumulatorSource ApplicationMasterSource ApplicationSource AppStatusSource BlockManagerSource CacheMetrics CodegenMetrics DAGSchedulerSource ExecutorAllocationManagerSource ExecutorMetricsSource ExecutorSource ExternalShuffleServiceSource HiveCatalogMetrics JVMCPUSource JvmSource LiveListenerBusMetrics MasterSource PluginMetricsSource ShuffleMetricsSource WorkerSource","title":"Implementations"},{"location":"metrics/configuration-properties/","text":"Configuration Properties \u00b6 spark.metrics.appStatusSource.enabled \u00b6 Enables Dropwizard/Codahale metrics with the status of a live Spark application Default: false Used when: AppStatusSource utility is used to create an AppStatusSource spark.metrics.conf \u00b6 The metrics configuration file Default: metrics.properties spark.metrics.namespace \u00b6 Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#sparkmetricsappstatussourceenabled","text":"Enables Dropwizard/Codahale metrics with the status of a live Spark application Default: false Used when: AppStatusSource utility is used to create an AppStatusSource","title":" spark.metrics.appStatusSource.enabled"},{"location":"metrics/configuration-properties/#sparkmetricsconf","text":"The metrics configuration file Default: metrics.properties","title":" spark.metrics.conf"},{"location":"metrics/configuration-properties/#sparkmetricsnamespace","text":"Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":" spark.metrics.namespace"},{"location":"network/ManagedBuffer/","text":"ManagedBuffer \u00b6 ManagedBuffer is the < > of < > that < >. == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | convertToNetty a| [[convertToNetty]] [source, java] \u00b6 Object convertToNetty() \u00b6 Used exclusively when MessageEncoder is requested to encode a message | createInputStream a| [[createInputStream]] [source, java] \u00b6 InputStream createInputStream() \u00b6 Used exclusively when ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#next[retrieve the next element] | nioByteBuffer a| [[nioByteBuffer]] [source, java] \u00b6 ByteBuffer nioByteBuffer() \u00b6 Used when...FIXME | release a| [[release]] [source, java] \u00b6 ManagedBuffer release() \u00b6 Used when...FIXME | retain a| [[retain]] [source, java] \u00b6 ManagedBuffer retain() \u00b6 Used when: MessageWithHeader is requested to retain ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#sendRequest[send a remote shuffle block fetch request] and storage:ShuffleBlockFetcherIterator.md#fetchLocalBlocks[fetchLocalBlocks] | size a| [[size]] [source, java] \u00b6 long size() \u00b6 Number of bytes of the data Used when...FIXME |=== == [[implementations]] Available ManagedBuffers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | ManagedBuffer | Description | BlockManagerManagedBuffer | [[BlockManagerManagedBuffer]] | EncryptedManagedBuffer | [[EncryptedManagedBuffer]] | FileSegmentManagedBuffer | [[FileSegmentManagedBuffer]] | NettyManagedBuffer | [[NettyManagedBuffer]] | NioManagedBuffer | [[NioManagedBuffer]] |===","title":"ManagedBuffer"},{"location":"network/ManagedBuffer/#managedbuffer","text":"ManagedBuffer is the < > of < > that < >. == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | convertToNetty a| [[convertToNetty]]","title":"ManagedBuffer"},{"location":"network/ManagedBuffer/#source-java","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#object-converttonetty","text":"Used exclusively when MessageEncoder is requested to encode a message | createInputStream a| [[createInputStream]]","title":"Object convertToNetty()"},{"location":"network/ManagedBuffer/#source-java_1","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#inputstream-createinputstream","text":"Used exclusively when ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#next[retrieve the next element] | nioByteBuffer a| [[nioByteBuffer]]","title":"InputStream createInputStream()"},{"location":"network/ManagedBuffer/#source-java_2","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#bytebuffer-niobytebuffer","text":"Used when...FIXME | release a| [[release]]","title":"ByteBuffer nioByteBuffer()"},{"location":"network/ManagedBuffer/#source-java_3","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#managedbuffer-release","text":"Used when...FIXME | retain a| [[retain]]","title":"ManagedBuffer release()"},{"location":"network/ManagedBuffer/#source-java_4","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#managedbuffer-retain","text":"Used when: MessageWithHeader is requested to retain ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#sendRequest[send a remote shuffle block fetch request] and storage:ShuffleBlockFetcherIterator.md#fetchLocalBlocks[fetchLocalBlocks] | size a| [[size]]","title":"ManagedBuffer retain()"},{"location":"network/ManagedBuffer/#source-java_5","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#long-size","text":"Number of bytes of the data Used when...FIXME |=== == [[implementations]] Available ManagedBuffers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | ManagedBuffer | Description | BlockManagerManagedBuffer | [[BlockManagerManagedBuffer]] | EncryptedManagedBuffer | [[EncryptedManagedBuffer]] | FileSegmentManagedBuffer | [[FileSegmentManagedBuffer]] | NettyManagedBuffer | [[NettyManagedBuffer]] | NioManagedBuffer | [[NioManagedBuffer]] |===","title":"long size()"},{"location":"network/MessageHandler/","text":"= MessageHandler MessageHandler is a < > of < > that can < > messages. [[contract]] [source, java] package org.apache.spark.network.server; abstract class MessageHandler { abstract void handle(T message) throws Exception; abstract void channelActive(); abstract void exceptionCaught(Throwable cause); abstract void channelInactive(); } .MessageHandler Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | handle | [[handle]] Used when...FIXME | channelActive | [[channelActive]] Used when...FIXME | exceptionCaught | [[exceptionCaught]] Used when...FIXME | channelInactive | [[channelInactive]] Used when...FIXME |=== == [[implementations]] MessageHandlers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | MessageHandler | Description | network:TransportRequestHandler.md[] | [[TransportRequestHandler]] | TransportResponseHandler | [[TransportResponseHandler]] |===","title":"MessageHandler"},{"location":"network/OneForOneStreamManager/","text":"OneForOneStreamManager \u00b6 OneForOneStreamManager is a StreamManager . Creating Instance \u00b6 OneForOneStreamManager takes no arguments to be created. OneForOneStreamManager is created when...FIXME == [[registerStream]] registerStream Method [source,java] \u00b6 long registerStream( String appId, Iterator buffers) registerStream...FIXME registerStream is used when...FIXME","title":"OneForOneStreamManager"},{"location":"network/OneForOneStreamManager/#oneforonestreammanager","text":"OneForOneStreamManager is a StreamManager .","title":"OneForOneStreamManager"},{"location":"network/OneForOneStreamManager/#creating-instance","text":"OneForOneStreamManager takes no arguments to be created. OneForOneStreamManager is created when...FIXME == [[registerStream]] registerStream Method","title":"Creating Instance"},{"location":"network/OneForOneStreamManager/#sourcejava","text":"long registerStream( String appId, Iterator buffers) registerStream...FIXME registerStream is used when...FIXME","title":"[source,java]"},{"location":"network/RpcHandler/","text":"RpcHandler \u00b6 Review Me \u00b6 RpcHandler is the < > of...FIXME [[ONE_WAY_CALLBACK]] RpcHandler uses a < > that...FIXME [[contract]] [source, java] package org.apache.spark.network.server; abstract class RpcHandler { // only required methods that have no implementation // the others follow abstract void receive( TransportClient client, ByteBuffer message, RpcResponseCallback callback); abstract StreamManager getStreamManager(); } .(Subset of) RpcHandler Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | receive a| [[receive]] Used when: AuthRpcHandler is requested to receive SaslRpcHandler is requested to receive (after authentication is complete) TransportRequestHandler is requested to network:TransportRequestHandler.md#processRpcRequest[processRpcRequest] | getStreamManager | [[getStreamManager]] Used when...FIXME |=== [[implementations]] .RpcHandlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | RpcHandler | Description | AuthRpcHandler | [[AuthRpcHandler]] | storage:NettyBlockRpcServer.md[] | [[NettyBlockRpcServer]] | NettyRpcHandler | [[NettyRpcHandler]] | NoOpRpcHandler | [[NoOpRpcHandler]] | SaslRpcHandler | [[SaslRpcHandler]] |=== == [[OneWayRpcCallback]] OneWayRpcCallback RpcResponseCallback OneWayRpcCallback is a RpcResponseCallback that simply prints out the WARN and ERROR for the following methods onSuccess and onFailure respectively. .void onSuccess(ByteBuffer response) Response provided for one-way RPC. .void onFailure(Throwable e) Error response provided for one-way RPC.","title":"RpcHandler"},{"location":"network/RpcHandler/#rpchandler","text":"","title":"RpcHandler"},{"location":"network/RpcHandler/#review-me","text":"RpcHandler is the < > of...FIXME [[ONE_WAY_CALLBACK]] RpcHandler uses a < > that...FIXME [[contract]] [source, java] package org.apache.spark.network.server; abstract class RpcHandler { // only required methods that have no implementation // the others follow abstract void receive( TransportClient client, ByteBuffer message, RpcResponseCallback callback); abstract StreamManager getStreamManager(); } .(Subset of) RpcHandler Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | receive a| [[receive]] Used when: AuthRpcHandler is requested to receive SaslRpcHandler is requested to receive (after authentication is complete) TransportRequestHandler is requested to network:TransportRequestHandler.md#processRpcRequest[processRpcRequest] | getStreamManager | [[getStreamManager]] Used when...FIXME |=== [[implementations]] .RpcHandlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | RpcHandler | Description | AuthRpcHandler | [[AuthRpcHandler]] | storage:NettyBlockRpcServer.md[] | [[NettyBlockRpcServer]] | NettyRpcHandler | [[NettyRpcHandler]] | NoOpRpcHandler | [[NoOpRpcHandler]] | SaslRpcHandler | [[SaslRpcHandler]] |=== == [[OneWayRpcCallback]] OneWayRpcCallback RpcResponseCallback OneWayRpcCallback is a RpcResponseCallback that simply prints out the WARN and ERROR for the following methods onSuccess and onFailure respectively. .void onSuccess(ByteBuffer response) Response provided for one-way RPC. .void onFailure(Throwable e) Error response provided for one-way RPC.","title":"Review Me"},{"location":"network/RpcResponseCallback/","text":"= RpcResponseCallback RpcResponseCallback is the < > of...FIXME [[contract]] [source, java] package org.apache.spark.network.client; interface RpcResponseCallback { void onSuccess(ByteBuffer response); void onFailure(Throwable e); } .RpcResponseCallback Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | onSuccess a| [[onSuccess]] Used when: NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#receive[receive RPC messages] (i.e. OpenBlocks and UploadBlock messages) RemoteNettyRpcCallContext is requested to send TransportResponseHandler is requested to handle a RpcResponse message AuthRpcHandler and SaslRpcHandler are requested to receive | onFailure | [[onFailure]] Used when...FIXME |=== [[implementations]] .RpcResponseCallbacks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | RpcResponseCallback | Description \"Unnamed\" in NettyBlockTransferService \"Unnamed\" in TransportRequestHandler \"Unnamed\" in TransportClient \"Unnamed\" in storage:OneForOneBlockFetcher.md[] | OneWayRpcCallback | [[OneWayRpcCallback]] | RegisterDriverCallback | [[RegisterDriverCallback]] | RpcOutboxMessage | [[RpcOutboxMessage]] |===","title":"RpcResponseCallback"},{"location":"network/SparkTransportConf/","text":"= SparkTransportConf SparkTransportConf is...FIXME == [[fromSparkConf]] Creating TransportConf [source,scala] \u00b6 fromSparkConf( _conf: SparkConf, module: String, numUsableCores: Int = 0): TransportConf fromSparkConf...FIXME fromSparkConf is used when...FIXME == [[defaultNumThreads]] Calculating Default Number of Threads [source, scala] \u00b6 defaultNumThreads( numUsableCores: Int): Int defaultNumThreads calculates the default number of threads for both the Netty client and server thread pools that is 8 maximum or numUsableCores is smaller. If numUsableCores is not specified, defaultNumThreads uses the number of processors available to the Java virtual machine. NOTE: 8 is the maximum number of threads for Netty and is not configurable. NOTE: defaultNumThreads uses ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--++[Java's Runtime for the number of processors in JVM].","title":"SparkTransportConf"},{"location":"network/SparkTransportConf/#sourcescala","text":"fromSparkConf( _conf: SparkConf, module: String, numUsableCores: Int = 0): TransportConf fromSparkConf...FIXME fromSparkConf is used when...FIXME == [[defaultNumThreads]] Calculating Default Number of Threads","title":"[source,scala]"},{"location":"network/SparkTransportConf/#source-scala","text":"defaultNumThreads( numUsableCores: Int): Int defaultNumThreads calculates the default number of threads for both the Netty client and server thread pools that is 8 maximum or numUsableCores is smaller. If numUsableCores is not specified, defaultNumThreads uses the number of processors available to the Java virtual machine. NOTE: 8 is the maximum number of threads for Netty and is not configurable. NOTE: defaultNumThreads uses ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--++[Java's Runtime for the number of processors in JVM].","title":"[source, scala]"},{"location":"network/StreamManager/","text":"= StreamManager StreamManager is an abstraction of...FIXME == [[implementations]] Available StreamManagers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StreamManager | Description | rpc:NettyStreamManager.md[] | [[NettyStreamManager]] | network:OneForOneStreamManager.md[OneForOneStreamManager] | [[OneForOneStreamManager]] |===","title":"StreamManager"},{"location":"network/TransportClientFactory/","text":"= TransportClientFactory TransportClientFactory is...FIXME == [[createUnmanagedClient]] createUnmanagedClient Method [source, java] \u00b6 TransportClient createUnmanagedClient(String remoteHost, int remotePort) throws IOException, InterruptedException createUnmanagedClient ...FIXME NOTE: createUnmanagedClient is used when...FIXME == [[createClient]] createClient Internal Method [source, java] \u00b6 TransportClient createClient(String remoteHost, int remotePort) throws IOException, InterruptedException TransportClient createClient(InetSocketAddress address) throws IOException, InterruptedException createClient ...FIXME createClient is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] and storage:NettyBlockTransferService.md#uploadBlock[uploadBlock] NettyRpcEnv is requested to createClient and downloadClient TransportClientFactory is requested to < >.","title":"TransportClientFactory"},{"location":"network/TransportClientFactory/#source-java","text":"TransportClient createUnmanagedClient(String remoteHost, int remotePort) throws IOException, InterruptedException createUnmanagedClient ...FIXME NOTE: createUnmanagedClient is used when...FIXME == [[createClient]] createClient Internal Method","title":"[source, java]"},{"location":"network/TransportClientFactory/#source-java_1","text":"TransportClient createClient(String remoteHost, int remotePort) throws IOException, InterruptedException TransportClient createClient(InetSocketAddress address) throws IOException, InterruptedException createClient ...FIXME createClient is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] and storage:NettyBlockTransferService.md#uploadBlock[uploadBlock] NettyRpcEnv is requested to createClient and downloadClient TransportClientFactory is requested to < >.","title":"[source, java]"},{"location":"network/TransportConf/","text":"TransportConf \u00b6 TransportConf is a class for the transport-related network configuration for modules, e.g. ExternalShuffleService or spark-on-yarn:spark-yarn-YarnShuffleService.md[YarnShuffleService]. TransportConf exposes methods to access settings for a single module as < > or < >. == [[spark.module.prefix]] spark.module.prefix Settings The settings can be in the form of spark.[module].[prefix] with the following prefixes: io.mode (default: NIO ) -- the IO mode: nio or epoll . io.preferDirectBufs (default: true ) -- a flag to control whether Spark prefers allocating off-heap byte buffers within Netty ( true ) or not ( false ). io.connectionTimeout (default: rpc:index.md#spark.network.timeout[spark.network.timeout] or 120s ) -- the connection timeout in milliseconds. io.backLog (default: -1 for no backlog) -- the requested maximum length of the queue of incoming connections. io.numConnectionsPerPeer (default: 1 ) -- the number of concurrent connections between two nodes for fetching data. io.serverThreads (default: 0 i.e. 2x#cores) -- the number of threads used in the server thread pool. io.clientThreads (default: 0 i.e. 2x#cores) -- the number of threads used in the client thread pool. io.receiveBuffer (default: -1 ) -- the receive buffer size (SO_RCVBUF). io.sendBuffer (default: -1 ) -- the send buffer size (SO_SNDBUF). sasl.timeout (default: 30s ) -- the timeout (in milliseconds) for a single round trip of SASL token exchange. [[io.maxRetries]] io.maxRetries (default: 3 ) -- the maximum number of times Spark will try IO exceptions (such as connection timeouts) per request. If set to 0 , Spark will not do any retries. io.retryWait (default: 5s ) -- the time (in milliseconds) that Spark will wait in order to perform a retry after an IOException . Only relevant if io.maxRetries > 0. io.lazyFD (default: true ) -- controls whether to initialize FileDescriptor lazily ( true ) or not ( false ). If true , file descriptors are created only when data is going to be transferred. This can reduce the number of open files. == [[general-settings]] General Network-Related Settings === [[spark.storage.memoryMapThreshold]] spark.storage.memoryMapThreshold spark.storage.memoryMapThreshold (default: 2m ) is the minimum size of a block that we should start using memory map rather than reading in through normal IO operations. This prevents Spark from memory mapping very small blocks. In general, memory mapping has high overhead for blocks close to or below the page size of the OS. === [[spark.network.sasl.maxEncryptedBlockSize]] spark.network.sasl.maxEncryptedBlockSize spark.network.sasl.maxEncryptedBlockSize (default: 64k ) is the maximum number of bytes to be encrypted at a time when SASL encryption is enabled. === [[spark.network.sasl.serverAlwaysEncrypt]] spark.network.sasl.serverAlwaysEncrypt spark.network.sasl.serverAlwaysEncrypt (default: false ) controls whether the server should enforce encryption on SASL-authenticated connections ( true ) or not ( false ).","title":"TransportConf"},{"location":"network/TransportConf/#transportconf","text":"TransportConf is a class for the transport-related network configuration for modules, e.g. ExternalShuffleService or spark-on-yarn:spark-yarn-YarnShuffleService.md[YarnShuffleService]. TransportConf exposes methods to access settings for a single module as < > or < >. == [[spark.module.prefix]] spark.module.prefix Settings The settings can be in the form of spark.[module].[prefix] with the following prefixes: io.mode (default: NIO ) -- the IO mode: nio or epoll . io.preferDirectBufs (default: true ) -- a flag to control whether Spark prefers allocating off-heap byte buffers within Netty ( true ) or not ( false ). io.connectionTimeout (default: rpc:index.md#spark.network.timeout[spark.network.timeout] or 120s ) -- the connection timeout in milliseconds. io.backLog (default: -1 for no backlog) -- the requested maximum length of the queue of incoming connections. io.numConnectionsPerPeer (default: 1 ) -- the number of concurrent connections between two nodes for fetching data. io.serverThreads (default: 0 i.e. 2x#cores) -- the number of threads used in the server thread pool. io.clientThreads (default: 0 i.e. 2x#cores) -- the number of threads used in the client thread pool. io.receiveBuffer (default: -1 ) -- the receive buffer size (SO_RCVBUF). io.sendBuffer (default: -1 ) -- the send buffer size (SO_SNDBUF). sasl.timeout (default: 30s ) -- the timeout (in milliseconds) for a single round trip of SASL token exchange. [[io.maxRetries]] io.maxRetries (default: 3 ) -- the maximum number of times Spark will try IO exceptions (such as connection timeouts) per request. If set to 0 , Spark will not do any retries. io.retryWait (default: 5s ) -- the time (in milliseconds) that Spark will wait in order to perform a retry after an IOException . Only relevant if io.maxRetries > 0. io.lazyFD (default: true ) -- controls whether to initialize FileDescriptor lazily ( true ) or not ( false ). If true , file descriptors are created only when data is going to be transferred. This can reduce the number of open files. == [[general-settings]] General Network-Related Settings === [[spark.storage.memoryMapThreshold]] spark.storage.memoryMapThreshold spark.storage.memoryMapThreshold (default: 2m ) is the minimum size of a block that we should start using memory map rather than reading in through normal IO operations. This prevents Spark from memory mapping very small blocks. In general, memory mapping has high overhead for blocks close to or below the page size of the OS. === [[spark.network.sasl.maxEncryptedBlockSize]] spark.network.sasl.maxEncryptedBlockSize spark.network.sasl.maxEncryptedBlockSize (default: 64k ) is the maximum number of bytes to be encrypted at a time when SASL encryption is enabled. === [[spark.network.sasl.serverAlwaysEncrypt]] spark.network.sasl.serverAlwaysEncrypt spark.network.sasl.serverAlwaysEncrypt (default: false ) controls whether the server should enforce encryption on SASL-authenticated connections ( true ) or not ( false ).","title":"TransportConf"},{"location":"network/TransportContext/","text":"TransportContext \u00b6 Creating Instance \u00b6 TransportContext takes the following to be created: TransportConf RpcHandler closeIdleConnections flag isClientOnly flag TransportContext is created when: ExternalBlockStoreClient is requested to init ExternalShuffleService is requested to start NettyBlockTransferService is requested to init NettyRpcEnv is created and requested to downloadClient YarnShuffleService (Spark on YARN) is requested to serviceInit Creating Server \u00b6 TransportServer createServer ( int port , List < TransportServerBootstrap > bootstraps ) TransportServer createServer ( String host , int port , List < TransportServerBootstrap > bootstraps ) createServer creates a TransportServer (with the RpcHandler and the input arguments). createServer is used when: YarnShuffleService (Spark on YARN) is requested to serviceInit ExternalShuffleService is requested to start NettyBlockTransferService is requested to createServer NettyRpcEnv is requested to startServer Creating TransportClientFactory \u00b6 TransportClientFactory createClientFactory ( List < TransportClientBootstrap > bootstraps ) createClientFactory ...FIXME createClientFactory is used when: ExternalBlockStoreClient is requested to init NettyBlockTransferService is requested to init NettyRpcEnv is created and requested to downloadClient","title":"TransportContext"},{"location":"network/TransportContext/#transportcontext","text":"","title":"TransportContext"},{"location":"network/TransportContext/#creating-instance","text":"TransportContext takes the following to be created: TransportConf RpcHandler closeIdleConnections flag isClientOnly flag TransportContext is created when: ExternalBlockStoreClient is requested to init ExternalShuffleService is requested to start NettyBlockTransferService is requested to init NettyRpcEnv is created and requested to downloadClient YarnShuffleService (Spark on YARN) is requested to serviceInit","title":"Creating Instance"},{"location":"network/TransportContext/#creating-server","text":"TransportServer createServer ( int port , List < TransportServerBootstrap > bootstraps ) TransportServer createServer ( String host , int port , List < TransportServerBootstrap > bootstraps ) createServer creates a TransportServer (with the RpcHandler and the input arguments). createServer is used when: YarnShuffleService (Spark on YARN) is requested to serviceInit ExternalShuffleService is requested to start NettyBlockTransferService is requested to createServer NettyRpcEnv is requested to startServer","title":" Creating Server"},{"location":"network/TransportContext/#creating-transportclientfactory","text":"TransportClientFactory createClientFactory ( List < TransportClientBootstrap > bootstraps ) createClientFactory ...FIXME createClientFactory is used when: ExternalBlockStoreClient is requested to init NettyBlockTransferService is requested to init NettyRpcEnv is created and requested to downloadClient","title":" Creating TransportClientFactory"},{"location":"network/TransportRequestHandler/","text":"TransportRequestHandler \u00b6 TransportRequestHandler is a network:MessageHandler.md[] of < > from Netty's < >. == [[creating-instance]] Creating Instance TransportRequestHandler takes the following to be created: [[channel]] Netty's https://netty.io/4.1/api/io/netty/channel/Channel.html[Channel ] [[reverseClient]] TransportClient [[rpcHandler]] network:RpcHandler.md[] [[maxChunksBeingTransferred]] Maximum number of chunks allowed to be transferred at the same time TransportRequestHandler is created when TransportContext is requested to network:TransportContext.md#createChannelHandler[create a ChannelHandler]. == [[processRpcRequest]] processRpcRequest Internal Method [source, java] \u00b6 void processRpcRequest( RpcRequest req) processRpcRequest...FIXME processRpcRequest is used when TransportRequestHandler is requested to < >. == [[processFetchRequest]] processFetchRequest Internal Method [source, java] \u00b6 void processFetchRequest( ChunkFetchRequest req) processFetchRequest...FIXME processFetchRequest is used when TransportRequestHandler is requested to < >. == [[processOneWayMessage]] processOneWayMessage Internal Method [source, java] \u00b6 void processOneWayMessage(OneWayMessage req) \u00b6 processOneWayMessage ...FIXME NOTE: processOneWayMessage is used exclusively when TransportRequestHandler is requested to < > a OneWayMessage request. == [[processStreamRequest]] processStreamRequest Internal Method [source, java] \u00b6 void processStreamRequest(final StreamRequest req) \u00b6 processStreamRequest ...FIXME NOTE: processStreamRequest is used exclusively when TransportRequestHandler is requested to < > a StreamRequest request. == [[handle]] Handling RequestMessages -- handle Method [source, java] \u00b6 void handle(RequestMessage request) \u00b6 handle branches off per the type of the input RequestMessage : For ChunkFetchRequest requests, handle < > For RpcRequest requests, handle < > For OneWayMessage requests, handle < > For StreamRequest requests, handle < > For unknown requests, handle simply throws a IllegalArgumentException . Unknown request type: [request] handle is part of network:MessageHandler.md#handle[MessageHandler] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.server.TransportRequestHandler logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.network.server.TransportRequestHandler=ALL \u00b6 Refer to spark-logging.md[Logging].","title":"TransportRequestHandler"},{"location":"network/TransportRequestHandler/#transportrequesthandler","text":"TransportRequestHandler is a network:MessageHandler.md[] of < > from Netty's < >. == [[creating-instance]] Creating Instance TransportRequestHandler takes the following to be created: [[channel]] Netty's https://netty.io/4.1/api/io/netty/channel/Channel.html[Channel ] [[reverseClient]] TransportClient [[rpcHandler]] network:RpcHandler.md[] [[maxChunksBeingTransferred]] Maximum number of chunks allowed to be transferred at the same time TransportRequestHandler is created when TransportContext is requested to network:TransportContext.md#createChannelHandler[create a ChannelHandler]. == [[processRpcRequest]] processRpcRequest Internal Method","title":"TransportRequestHandler"},{"location":"network/TransportRequestHandler/#source-java","text":"void processRpcRequest( RpcRequest req) processRpcRequest...FIXME processRpcRequest is used when TransportRequestHandler is requested to < >. == [[processFetchRequest]] processFetchRequest Internal Method","title":"[source, java]"},{"location":"network/TransportRequestHandler/#source-java_1","text":"void processFetchRequest( ChunkFetchRequest req) processFetchRequest...FIXME processFetchRequest is used when TransportRequestHandler is requested to < >. == [[processOneWayMessage]] processOneWayMessage Internal Method","title":"[source, java]"},{"location":"network/TransportRequestHandler/#source-java_2","text":"","title":"[source, java]"},{"location":"network/TransportRequestHandler/#void-processonewaymessageonewaymessage-req","text":"processOneWayMessage ...FIXME NOTE: processOneWayMessage is used exclusively when TransportRequestHandler is requested to < > a OneWayMessage request. == [[processStreamRequest]] processStreamRequest Internal Method","title":"void processOneWayMessage(OneWayMessage req)"},{"location":"network/TransportRequestHandler/#source-java_3","text":"","title":"[source, java]"},{"location":"network/TransportRequestHandler/#void-processstreamrequestfinal-streamrequest-req","text":"processStreamRequest ...FIXME NOTE: processStreamRequest is used exclusively when TransportRequestHandler is requested to < > a StreamRequest request. == [[handle]] Handling RequestMessages -- handle Method","title":"void processStreamRequest(final StreamRequest req)"},{"location":"network/TransportRequestHandler/#source-java_4","text":"","title":"[source, java]"},{"location":"network/TransportRequestHandler/#void-handlerequestmessage-request","text":"handle branches off per the type of the input RequestMessage : For ChunkFetchRequest requests, handle < > For RpcRequest requests, handle < > For OneWayMessage requests, handle < > For StreamRequest requests, handle < > For unknown requests, handle simply throws a IllegalArgumentException . Unknown request type: [request] handle is part of network:MessageHandler.md#handle[MessageHandler] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.server.TransportRequestHandler logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"void handle(RequestMessage request)"},{"location":"network/TransportRequestHandler/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"network/TransportRequestHandler/#log4jloggerorgapachesparknetworkservertransportrequesthandlerall","text":"Refer to spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.network.server.TransportRequestHandler=ALL"},{"location":"network/TransportServer/","text":"TransportServer \u00b6 == [[creating-instance]] Creating TransportServer Instance TransportServer takes the following when created: [[context]] network:TransportContext.md[] [[hostToBind]] Host name to bind to [[portToBind]] Port number to bind to [[appRpcHandler]] network:RpcHandler.md[] [[bootstraps]] TransportServerBootstraps When created, TransportServer < > with the < > and < > to bind to. TransportServer is created when TransportContext is requested to network:TransportContext.md#createServer[create a server]. == [[init]] init Internal Method [source, java] \u00b6 void init(String hostToBind, int portToBind) \u00b6 init ...FIXME NOTE: init is used exclusively when TransportServer is < >. == [[getPort]] getPort Method [source, java] \u00b6 int getPort() \u00b6 getPort ...FIXME [NOTE] \u00b6 getPort is used when: NettyRpcEnv is requested for the address * Spark on YARN's YarnShuffleService is requested to serviceInit \u00b6","title":"TransportServer"},{"location":"network/TransportServer/#transportserver","text":"== [[creating-instance]] Creating TransportServer Instance TransportServer takes the following when created: [[context]] network:TransportContext.md[] [[hostToBind]] Host name to bind to [[portToBind]] Port number to bind to [[appRpcHandler]] network:RpcHandler.md[] [[bootstraps]] TransportServerBootstraps When created, TransportServer < > with the < > and < > to bind to. TransportServer is created when TransportContext is requested to network:TransportContext.md#createServer[create a server]. == [[init]] init Internal Method","title":"TransportServer"},{"location":"network/TransportServer/#source-java","text":"","title":"[source, java]"},{"location":"network/TransportServer/#void-initstring-hosttobind-int-porttobind","text":"init ...FIXME NOTE: init is used exclusively when TransportServer is < >. == [[getPort]] getPort Method","title":"void init(String hostToBind, int portToBind)"},{"location":"network/TransportServer/#source-java_1","text":"","title":"[source, java]"},{"location":"network/TransportServer/#int-getport","text":"getPort ...FIXME","title":"int getPort()"},{"location":"network/TransportServer/#note","text":"getPort is used when: NettyRpcEnv is requested for the address","title":"[NOTE]"},{"location":"network/TransportServer/#spark-on-yarns-yarnshuffleservice-is-requested-to-serviceinit","text":"","title":"* Spark on YARN's YarnShuffleService is requested to serviceInit"},{"location":"plugins/","text":"Plugin Framework \u00b6 Plugin Framework is an API for registering custom extensions ( plugins ) to be executed on the driver and executors. Plugin Framework uses the following main abstractions: PluginContainer SparkPlugin Plugin Framework was introduced in Spark 2.4.4 (that only offered an API for executors) with further changes in Spark 3.0.0 (to cover the driver). Resources \u00b6 Advanced Instrumentation in the official documentation of Apache Spark Commit for SPARK-29397 Spark Plugin Framework in 3.0 - Part 1: Introduction by Madhukara Phatak Spark Memory Monitor by squito SparkPlugins by Luca Canali (CERN)","title":"Plugin Framework"},{"location":"plugins/#plugin-framework","text":"Plugin Framework is an API for registering custom extensions ( plugins ) to be executed on the driver and executors. Plugin Framework uses the following main abstractions: PluginContainer SparkPlugin Plugin Framework was introduced in Spark 2.4.4 (that only offered an API for executors) with further changes in Spark 3.0.0 (to cover the driver).","title":"Plugin Framework"},{"location":"plugins/#resources","text":"Advanced Instrumentation in the official documentation of Apache Spark Commit for SPARK-29397 Spark Plugin Framework in 3.0 - Part 1: Introduction by Madhukara Phatak Spark Memory Monitor by squito SparkPlugins by Luca Canali (CERN)","title":"Resources"},{"location":"plugins/DriverPlugin/","text":"DriverPlugin \u00b6 DriverPlugin is...FIXME","title":"DriverPlugin"},{"location":"plugins/DriverPlugin/#driverplugin","text":"DriverPlugin is...FIXME","title":"DriverPlugin"},{"location":"plugins/DriverPluginContainer/","text":"DriverPluginContainer \u00b6 DriverPluginContainer is a PluginContainer . Creating Instance \u00b6 DriverPluginContainer takes the following to be created: SparkContext Resources ( Map[String, ResourceInformation] ) SparkPlugin s DriverPluginContainer is created when: PluginContainer utility is used for a PluginContainer (at SparkContext startup) Registering Metrics \u00b6 registerMetrics ( appId : String ): Unit registerMetrics is part of the PluginContainer abstraction. For every driver plugin , registerMetrics requests it to register metrics and the associated PluginContextImpl for the same . Logging \u00b6 Enable ALL logging level for org.apache.spark.internal.plugin.DriverPluginContainer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.internal.plugin.DriverPluginContainer=ALL Refer to Logging .","title":"DriverPluginContainer"},{"location":"plugins/DriverPluginContainer/#driverplugincontainer","text":"DriverPluginContainer is a PluginContainer .","title":"DriverPluginContainer"},{"location":"plugins/DriverPluginContainer/#creating-instance","text":"DriverPluginContainer takes the following to be created: SparkContext Resources ( Map[String, ResourceInformation] ) SparkPlugin s DriverPluginContainer is created when: PluginContainer utility is used for a PluginContainer (at SparkContext startup)","title":"Creating Instance"},{"location":"plugins/DriverPluginContainer/#registering-metrics","text":"registerMetrics ( appId : String ): Unit registerMetrics is part of the PluginContainer abstraction. For every driver plugin , registerMetrics requests it to register metrics and the associated PluginContextImpl for the same .","title":" Registering Metrics"},{"location":"plugins/DriverPluginContainer/#logging","text":"Enable ALL logging level for org.apache.spark.internal.plugin.DriverPluginContainer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.internal.plugin.DriverPluginContainer=ALL Refer to Logging .","title":"Logging"},{"location":"plugins/ExecutorPlugin/","text":"ExecutorPlugin \u00b6 ExecutorPlugin is...FIXME","title":"ExecutorPlugin"},{"location":"plugins/ExecutorPlugin/#executorplugin","text":"ExecutorPlugin is...FIXME","title":"ExecutorPlugin"},{"location":"plugins/ExecutorPluginContainer/","text":"ExecutorPluginContainer \u00b6 ExecutorPluginContainer is...FIXME","title":"ExecutorPluginContainer"},{"location":"plugins/ExecutorPluginContainer/#executorplugincontainer","text":"ExecutorPluginContainer is...FIXME","title":"ExecutorPluginContainer"},{"location":"plugins/PluginContainer/","text":"PluginContainer \u00b6 PluginContainer is an abstraction of plugin containers that can registerMetrics (for the driver and executors). PluginContainer is created for the driver and executors using apply utility. Contract \u00b6 Listening to Task Failures \u00b6 onTaskFailed ( failureReason : TaskFailedReason ): Unit For ExecutorPluginContainer only Used when: TaskRunner is requested to run (and the task has failed) Listening to Task Start \u00b6 onTaskStart (): Unit For ExecutorPluginContainer only Used when: TaskRunner is requested to run (and the task has just started) Listening to Task Success \u00b6 onTaskSucceeded (): Unit For ExecutorPluginContainer only Used when: TaskRunner is requested to run (and the task has finished successfully) Registering Metrics \u00b6 registerMetrics ( appId : String ): Unit Registers metrics for the application ID For DriverPluginContainer only Used when: SparkContext is created Shutdown \u00b6 shutdown (): Unit Used when: SparkContext is requested to stop Executor is requested to stop Implementations \u00b6 Sealed Abstract Class PluginContainer is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). DriverPluginContainer ExecutorPluginContainer Creating PluginContainer \u00b6 // the driver apply ( sc : SparkContext , resources : java . util . Map [ String , ResourceInformation ]): Option [ PluginContainer ] // executors apply ( env : SparkEnv , resources : java . util . Map [ String , ResourceInformation ]): Option [ PluginContainer ] // private helper apply ( ctx : Either [ SparkContext , SparkEnv ], resources : java . util . Map [ String , ResourceInformation ]): Option [ PluginContainer ] apply creates a PluginContainer for the driver or executors (based on the type of the first input argument, i.e. SparkContext or SparkEnv , respectively). apply first loads the SparkPlugin s defined by spark.plugins configuration property. Only when there was at least one plugin loaded, apply creates a DriverPluginContainer or ExecutorPluginContainer . apply is used when: SparkContext is created Executor is created","title":"PluginContainer"},{"location":"plugins/PluginContainer/#plugincontainer","text":"PluginContainer is an abstraction of plugin containers that can registerMetrics (for the driver and executors). PluginContainer is created for the driver and executors using apply utility.","title":"PluginContainer"},{"location":"plugins/PluginContainer/#contract","text":"","title":"Contract"},{"location":"plugins/PluginContainer/#listening-to-task-failures","text":"onTaskFailed ( failureReason : TaskFailedReason ): Unit For ExecutorPluginContainer only Used when: TaskRunner is requested to run (and the task has failed)","title":" Listening to Task Failures"},{"location":"plugins/PluginContainer/#listening-to-task-start","text":"onTaskStart (): Unit For ExecutorPluginContainer only Used when: TaskRunner is requested to run (and the task has just started)","title":" Listening to Task Start"},{"location":"plugins/PluginContainer/#listening-to-task-success","text":"onTaskSucceeded (): Unit For ExecutorPluginContainer only Used when: TaskRunner is requested to run (and the task has finished successfully)","title":" Listening to Task Success"},{"location":"plugins/PluginContainer/#registering-metrics","text":"registerMetrics ( appId : String ): Unit Registers metrics for the application ID For DriverPluginContainer only Used when: SparkContext is created","title":" Registering Metrics"},{"location":"plugins/PluginContainer/#shutdown","text":"shutdown (): Unit Used when: SparkContext is requested to stop Executor is requested to stop","title":" Shutdown"},{"location":"plugins/PluginContainer/#implementations","text":"Sealed Abstract Class PluginContainer is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). DriverPluginContainer ExecutorPluginContainer","title":"Implementations"},{"location":"plugins/PluginContainer/#creating-plugincontainer","text":"// the driver apply ( sc : SparkContext , resources : java . util . Map [ String , ResourceInformation ]): Option [ PluginContainer ] // executors apply ( env : SparkEnv , resources : java . util . Map [ String , ResourceInformation ]): Option [ PluginContainer ] // private helper apply ( ctx : Either [ SparkContext , SparkEnv ], resources : java . util . Map [ String , ResourceInformation ]): Option [ PluginContainer ] apply creates a PluginContainer for the driver or executors (based on the type of the first input argument, i.e. SparkContext or SparkEnv , respectively). apply first loads the SparkPlugin s defined by spark.plugins configuration property. Only when there was at least one plugin loaded, apply creates a DriverPluginContainer or ExecutorPluginContainer . apply is used when: SparkContext is created Executor is created","title":" Creating PluginContainer"},{"location":"plugins/PluginContextImpl/","text":"PluginContextImpl \u00b6 PluginContextImpl is...FIXME","title":"PluginContextImpl"},{"location":"plugins/PluginContextImpl/#plugincontextimpl","text":"PluginContextImpl is...FIXME","title":"PluginContextImpl"},{"location":"plugins/SparkPlugin/","text":"SparkPlugin \u00b6 SparkPlugin is an abstraction of custom extensions for Spark applications. Contract \u00b6 Driver-side Component \u00b6 DriverPlugin driverPlugin () Used when: DriverPluginContainer is created Executor-side Component \u00b6 ExecutorPlugin executorPlugin () Used when: ExecutorPluginContainer is created","title":"SparkPlugin"},{"location":"plugins/SparkPlugin/#sparkplugin","text":"SparkPlugin is an abstraction of custom extensions for Spark applications.","title":"SparkPlugin"},{"location":"plugins/SparkPlugin/#contract","text":"","title":"Contract"},{"location":"plugins/SparkPlugin/#driver-side-component","text":"DriverPlugin driverPlugin () Used when: DriverPluginContainer is created","title":" Driver-side Component"},{"location":"plugins/SparkPlugin/#executor-side-component","text":"ExecutorPlugin executorPlugin () Used when: ExecutorPluginContainer is created","title":" Executor-side Component"},{"location":"rdd/","text":"Resilient Distributed Dataset (RDD) \u00b6 Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a spark-cluster.md[cluster]. Dataset is a collection of spark-rdd-partitions.md[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's scheduler:DAGScheduler.md#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage spark-data-locality.md[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of spark-rdd-partitions.md[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] spark-rdd-partitions.md[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of spark-rdd-shuffle.md[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka rdd:RDD.md#dependencies[RDD dependencies]) An array of spark-rdd-partitions.md[partitions] that a dataset is divided to. A rdd:RDD.md#compute[compute] function to do a computation on partitions. An optional rdd:Partitioner.md[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a SparkContext.md[] (available as context property). RDDs live in one and only one SparkContext.md[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see SparkContext.md#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using executor:Executor.md[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in rdd:spark-rdd-transformations.md[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section spark-rdd-actions.md[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You spark-rdd-caching.md[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to spark-io.md[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for spark-shell.md[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ): [source,scala] \u00b6 val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"Resilient Distributed Dataset (RDD)"},{"location":"rdd/#resilient-distributed-dataset-rdd","text":"Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a spark-cluster.md[cluster]. Dataset is a collection of spark-rdd-partitions.md[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's scheduler:DAGScheduler.md#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage spark-data-locality.md[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of spark-rdd-partitions.md[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] spark-rdd-partitions.md[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of spark-rdd-shuffle.md[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka rdd:RDD.md#dependencies[RDD dependencies]) An array of spark-rdd-partitions.md[partitions] that a dataset is divided to. A rdd:RDD.md#compute[compute] function to do a computation on partitions. An optional rdd:Partitioner.md[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a SparkContext.md[] (available as context property). RDDs live in one and only one SparkContext.md[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see SparkContext.md#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using executor:Executor.md[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in rdd:spark-rdd-transformations.md[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section spark-rdd-actions.md[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You spark-rdd-caching.md[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to spark-io.md[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for spark-shell.md[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ):","title":"Resilient Distributed Dataset (RDD)"},{"location":"rdd/#sourcescala","text":"val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"[source,scala]"},{"location":"rdd/Aggregator/","text":"Aggregator \u00b6 Aggregator is a set of < > used to aggregate data using rdd:PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation. Aggregator[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. [[creating-instance]][[aggregation-functions]] Aggregator transforms an RDD[(K, V)] into an RDD[(K, C)] (for a \"combined type\" C) using the functions: [[createCombiner]] createCombiner: V => C [[mergeValue]] mergeValue: (C, V) => C [[mergeCombiners]] mergeCombiners: (C, C) => C Aggregator is used to create a ShuffleDependency and ExternalSorter . == [[combineValuesByKey]] combineValuesByKey Method [source, scala] \u00b6 combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to shuffle:ExternalAppendOnlyMap.md#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an shuffle:ExternalAppendOnlyMap.md#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: rdd:PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with the Map-Size Partial Aggregation Flag off) == [[combineCombinersByKey]] combineCombinersByKey Method [source, scala] \u00b6 combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with the Map-Size Partial Aggregation Flag on). == [[updateMetrics]] Updating Task Metrics [source, scala] \u00b6 updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input TaskContext for the TaskMetrics to update the metrics based on the metrics of the input ExternalAppendOnlyMap : executor:TaskMetrics.md#incMemoryBytesSpilled[Increment memory bytes spilled] executor:TaskMetrics.md#incDiskBytesSpilled[Increment disk bytes spilled] executor:TaskMetrics.md#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"Aggregator"},{"location":"rdd/Aggregator/#aggregator","text":"Aggregator is a set of < > used to aggregate data using rdd:PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation. Aggregator[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. [[creating-instance]][[aggregation-functions]] Aggregator transforms an RDD[(K, V)] into an RDD[(K, C)] (for a \"combined type\" C) using the functions: [[createCombiner]] createCombiner: V => C [[mergeValue]] mergeValue: (C, V) => C [[mergeCombiners]] mergeCombiners: (C, C) => C Aggregator is used to create a ShuffleDependency and ExternalSorter . == [[combineValuesByKey]] combineValuesByKey Method","title":"Aggregator"},{"location":"rdd/Aggregator/#source-scala","text":"combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to shuffle:ExternalAppendOnlyMap.md#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an shuffle:ExternalAppendOnlyMap.md#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: rdd:PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with the Map-Size Partial Aggregation Flag off) == [[combineCombinersByKey]] combineCombinersByKey Method","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_1","text":"combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with the Map-Size Partial Aggregation Flag on). == [[updateMetrics]] Updating Task Metrics","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_2","text":"updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input TaskContext for the TaskMetrics to update the metrics based on the metrics of the input ExternalAppendOnlyMap : executor:TaskMetrics.md#incMemoryBytesSpilled[Increment memory bytes spilled] executor:TaskMetrics.md#incDiskBytesSpilled[Increment disk bytes spilled] executor:TaskMetrics.md#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"[source, scala]"},{"location":"rdd/AsyncRDDActions/","text":"AsyncRDDActions \u00b6 AsyncRDDActions is...FIXME","title":"AsyncRDDActions"},{"location":"rdd/AsyncRDDActions/#asyncrddactions","text":"AsyncRDDActions is...FIXME","title":"AsyncRDDActions"},{"location":"rdd/CheckpointRDD/","text":"CheckpointRDD \u00b6 CheckpointRDD is an extension of the RDD abstraction for RDDs that recovers checkpointed data from storage. CheckpointRDD cannot be checkpointed again (and doCheckpoint , checkpoint , and localCheckpoint are simply noops). getPartitions and compute throw an NotImplementedError and are supposed to be overriden by the implementations . Implementations \u00b6 LocalCheckpointRDD ReliableCheckpointRDD","title":"CheckpointRDD"},{"location":"rdd/CheckpointRDD/#checkpointrdd","text":"CheckpointRDD is an extension of the RDD abstraction for RDDs that recovers checkpointed data from storage. CheckpointRDD cannot be checkpointed again (and doCheckpoint , checkpoint , and localCheckpoint are simply noops). getPartitions and compute throw an NotImplementedError and are supposed to be overriden by the implementations .","title":"CheckpointRDD"},{"location":"rdd/CheckpointRDD/#implementations","text":"LocalCheckpointRDD ReliableCheckpointRDD","title":"Implementations"},{"location":"rdd/CoGroupedRDD/","text":"CoGroupedRDD \u00b6 CoGroupedRDD[K] is an RDD that cogroups the parent RDDs . RDD [( K , Array [ Iterable [ _ ]])] For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key. Creating Instance \u00b6 CoGroupedRDD takes the following to be created: Key-Value RDD s ( Seq[RDD[_ <: Product2[K, _]]] ) Partitioner CoGroupedRDD is created when: RDD.cogroup operator is used","title":"CoGroupedRDD"},{"location":"rdd/CoGroupedRDD/#cogroupedrdd","text":"CoGroupedRDD[K] is an RDD that cogroups the parent RDDs . RDD [( K , Array [ Iterable [ _ ]])] For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key.","title":"CoGroupedRDD"},{"location":"rdd/CoGroupedRDD/#creating-instance","text":"CoGroupedRDD takes the following to be created: Key-Value RDD s ( Seq[RDD[_ <: Product2[K, _]]] ) Partitioner CoGroupedRDD is created when: RDD.cogroup operator is used","title":"Creating Instance"},{"location":"rdd/CoalescedRDD/","text":"CoalescedRDD \u00b6 CoalescedRDD is...FIXME","title":"CoalescedRDD"},{"location":"rdd/CoalescedRDD/#coalescedrdd","text":"CoalescedRDD is...FIXME","title":"CoalescedRDD"},{"location":"rdd/Dependency/","text":"RDD Dependencies \u00b6 Dependency class is the base (abstract) class to model a dependency relationship between two or more RDDs. [[rdd]] Dependency has a single method rdd to access the RDD that is behind a dependency. [source, scala] \u00b6 def rdd: RDD[T] \u00b6 Whenever you apply a spark-rdd-transformations.md[transformation] (e.g. map , flatMap ) to a RDD you build the so-called spark-rdd-lineage.md[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: NarrowDependency and ShuffleDependency are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | NarrowDependency | | ShuffleDependency | | OneToOneDependency | | PruneDependency | | RangeDependency | |=== [NOTE] \u00b6 The dependencies of a RDD are available using rdd:index.md#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use spark-rdd-lineage.md#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way. scala> myRdd.toDebugString res6: String = (8) ShuffledRDD[8] at groupBy at <console>:24 [] +-(8) MapPartitionsRDD[7] at groupBy at <console>:24 [] | ParallelCollectionRDD[6] at parallelize at <console>:24 [] \u00b6","title":"Dependencies"},{"location":"rdd/Dependency/#rdd-dependencies","text":"Dependency class is the base (abstract) class to model a dependency relationship between two or more RDDs. [[rdd]] Dependency has a single method rdd to access the RDD that is behind a dependency.","title":"RDD Dependencies"},{"location":"rdd/Dependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/Dependency/#def-rdd-rddt","text":"Whenever you apply a spark-rdd-transformations.md[transformation] (e.g. map , flatMap ) to a RDD you build the so-called spark-rdd-lineage.md[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: NarrowDependency and ShuffleDependency are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | NarrowDependency | | ShuffleDependency | | OneToOneDependency | | PruneDependency | | RangeDependency | |===","title":"def rdd: RDD[T]"},{"location":"rdd/Dependency/#note","text":"The dependencies of a RDD are available using rdd:index.md#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use spark-rdd-lineage.md#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way.","title":"[NOTE]"},{"location":"rdd/Dependency/#scala-myrddtodebugstring-res6-string-8-shuffledrdd8-at-groupby-at-console24-8-mappartitionsrdd7-at-groupby-at-console24-parallelcollectionrdd6-at-parallelize-at-console24","text":"","title":"scala&gt; myRdd.toDebugString\nres6: String =\n(8) ShuffledRDD[8] at groupBy at &lt;console&gt;:24 []\n +-(8) MapPartitionsRDD[7] at groupBy at &lt;console&gt;:24 []\n    |  ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24 []\n"},{"location":"rdd/HadoopRDD/","text":"HadoopRDD \u00b6 https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.HadoopRDD[HadoopRDD ] is an RDD that provides core functionality for reading data stored in HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI using the older MapReduce API ( https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/package-summary.html[org.apache.hadoop.mapred ]). HadoopRDD is created as a result of calling the following methods in SparkContext.md[]: hadoopFile textFile (the most often used in examples!) sequenceFile Partitions are of type HadoopPartition . When an HadoopRDD is computed, i.e. an action is called, you should see the INFO message Input split: in the logs. scala> sc.textFile(\"README.md\").count ... 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:1784+1784 ... The following properties are set upon partition execution: mapred.tip.id - task id of this task's attempt mapred.task.id - task attempt's id mapred.task.is.map as true mapred.task.partition - split id mapred.job.id Spark settings for HadoopRDD : spark.hadoop.cloneConf (default: false ) - shouldCloneJobConf - should a Hadoop job configuration JobConf object be cloned before spawning a Hadoop job. Refer to https://issues.apache.org/jira/browse/SPARK-2546[[SPARK-2546 ] Configuration object thread safety issue]. When true , you should see a DEBUG message Cloning Hadoop Configuration . You can register callbacks on TaskContext . HadoopRDDs are not checkpointed. They do nothing when checkpoint() is called. [CAUTION] \u00b6 FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they? What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits \u00b6 === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"HadoopRDD"},{"location":"rdd/HadoopRDD/#hadooprdd","text":"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.HadoopRDD[HadoopRDD ] is an RDD that provides core functionality for reading data stored in HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI using the older MapReduce API ( https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/package-summary.html[org.apache.hadoop.mapred ]). HadoopRDD is created as a result of calling the following methods in SparkContext.md[]: hadoopFile textFile (the most often used in examples!) sequenceFile Partitions are of type HadoopPartition . When an HadoopRDD is computed, i.e. an action is called, you should see the INFO message Input split: in the logs. scala> sc.textFile(\"README.md\").count ... 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:1784+1784 ... The following properties are set upon partition execution: mapred.tip.id - task id of this task's attempt mapred.task.id - task attempt's id mapred.task.is.map as true mapred.task.partition - split id mapred.job.id Spark settings for HadoopRDD : spark.hadoop.cloneConf (default: false ) - shouldCloneJobConf - should a Hadoop job configuration JobConf object be cloned before spawning a Hadoop job. Refer to https://issues.apache.org/jira/browse/SPARK-2546[[SPARK-2546 ] Configuration object thread safety issue]. When true , you should see a DEBUG message Cloning Hadoop Configuration . You can register callbacks on TaskContext . HadoopRDDs are not checkpointed. They do nothing when checkpoint() is called.","title":"HadoopRDD"},{"location":"rdd/HadoopRDD/#caution","text":"FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they?","title":"[CAUTION]"},{"location":"rdd/HadoopRDD/#whats-hadoop-split-input-splits-for-hadoop-reads-see-inputformatgetsplits","text":"=== [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits"},{"location":"rdd/HashPartitioner/","text":"HashPartitioner \u00b6 HashPartitioner is a Partitioner for hash-based partitioning . Important HashPartitioner places null keys in 0 th partition . HashPartitioner is used as the default Partitioner . Creating Instance \u00b6 HashPartitioner takes the following to be created: Number of partitions Number of Partitions \u00b6 numPartitions : Int numPartitions returns the given number of partitions . numPartitions is part of the Partitioner abstraction. Partition for Key \u00b6 getPartition ( key : Any ): Int For null keys getPartition simply returns 0 . For non- null keys, getPartition uses the Object.hashCode of the key modulo the number of partitions . For negative results, getPartition adds the number of partitions to make it non-negative. getPartition is part of the Partitioner abstraction.","title":"HashPartitioner"},{"location":"rdd/HashPartitioner/#hashpartitioner","text":"HashPartitioner is a Partitioner for hash-based partitioning . Important HashPartitioner places null keys in 0 th partition . HashPartitioner is used as the default Partitioner .","title":"HashPartitioner"},{"location":"rdd/HashPartitioner/#creating-instance","text":"HashPartitioner takes the following to be created: Number of partitions","title":"Creating Instance"},{"location":"rdd/HashPartitioner/#number-of-partitions","text":"numPartitions : Int numPartitions returns the given number of partitions . numPartitions is part of the Partitioner abstraction.","title":" Number of Partitions"},{"location":"rdd/HashPartitioner/#partition-for-key","text":"getPartition ( key : Any ): Int For null keys getPartition simply returns 0 . For non- null keys, getPartition uses the Object.hashCode of the key modulo the number of partitions . For negative results, getPartition adds the number of partitions to make it non-negative. getPartition is part of the Partitioner abstraction.","title":" Partition for Key"},{"location":"rdd/LocalCheckpointRDD/","text":"LocalCheckpointRDD \u00b6 LocalCheckpointRDD[T] is a CheckpointRDD . Creating Instance \u00b6 LocalCheckpointRDD takes the following to be created: RDD SparkContext RDD ID Number of Partitions LocalCheckpointRDD is created when: LocalRDDCheckpointData is requested to doCheckpoint Partitions \u00b6 getPartitions : Array [ Partition ] getPartitions is part of the RDD abstraction. getPartitions creates a CheckpointRDDPartition for every input partition (index). Computing Partition \u00b6 compute ( partition : Partition , context : TaskContext ): Iterator [ T ] compute is part of the RDD abstraction. compute merely throws an SparkException (that explains the reason): Checkpoint block [RDDBlockId] not found! Either the executor that originally checkpointed this partition is no longer alive, or the original RDD is unpersisted. If this problem persists, you may consider using `rdd.checkpoint()` instead, which is slower than local checkpointing but more fault-tolerant.\"","title":"LocalCheckpointRDD"},{"location":"rdd/LocalCheckpointRDD/#localcheckpointrdd","text":"LocalCheckpointRDD[T] is a CheckpointRDD .","title":"LocalCheckpointRDD"},{"location":"rdd/LocalCheckpointRDD/#creating-instance","text":"LocalCheckpointRDD takes the following to be created: RDD SparkContext RDD ID Number of Partitions LocalCheckpointRDD is created when: LocalRDDCheckpointData is requested to doCheckpoint","title":"Creating Instance"},{"location":"rdd/LocalCheckpointRDD/#partitions","text":"getPartitions : Array [ Partition ] getPartitions is part of the RDD abstraction. getPartitions creates a CheckpointRDDPartition for every input partition (index).","title":" Partitions"},{"location":"rdd/LocalCheckpointRDD/#computing-partition","text":"compute ( partition : Partition , context : TaskContext ): Iterator [ T ] compute is part of the RDD abstraction. compute merely throws an SparkException (that explains the reason): Checkpoint block [RDDBlockId] not found! Either the executor that originally checkpointed this partition is no longer alive, or the original RDD is unpersisted. If this problem persists, you may consider using `rdd.checkpoint()` instead, which is slower than local checkpointing but more fault-tolerant.\"","title":" Computing Partition"},{"location":"rdd/LocalRDDCheckpointData/","text":"LocalRDDCheckpointData \u00b6 LocalRDDCheckpointData is a RDDCheckpointData . Creating Instance \u00b6 LocalRDDCheckpointData takes the following to be created: RDD LocalRDDCheckpointData is created when: RDD is requested to localCheckpoint doCheckpoint \u00b6 doCheckpoint (): CheckpointRDD [ T ] doCheckpoint is part of the RDDCheckpointData abstraction. doCheckpoint creates a LocalCheckpointRDD with the RDD . doCheckpoint triggers caching any missing partitions (by checking availability of the RDDBlockId s for the partitions in the BlockManagerMaster ). Extra Spark Job If there are any missing partitions ( RDDBlockId s) doCheckpoint requests the SparkContext to run a Spark job with the RDD and the missing partitions. doCheckpoint makes sure that the StorageLevel of the RDD uses disk (among other persistence storages). If not, doCheckpoint throws an AssertionError : Storage level [level] is not appropriate for local checkpointing","title":"LocalRDDCheckpointData"},{"location":"rdd/LocalRDDCheckpointData/#localrddcheckpointdata","text":"LocalRDDCheckpointData is a RDDCheckpointData .","title":"LocalRDDCheckpointData"},{"location":"rdd/LocalRDDCheckpointData/#creating-instance","text":"LocalRDDCheckpointData takes the following to be created: RDD LocalRDDCheckpointData is created when: RDD is requested to localCheckpoint","title":"Creating Instance"},{"location":"rdd/LocalRDDCheckpointData/#docheckpoint","text":"doCheckpoint (): CheckpointRDD [ T ] doCheckpoint is part of the RDDCheckpointData abstraction. doCheckpoint creates a LocalCheckpointRDD with the RDD . doCheckpoint triggers caching any missing partitions (by checking availability of the RDDBlockId s for the partitions in the BlockManagerMaster ). Extra Spark Job If there are any missing partitions ( RDDBlockId s) doCheckpoint requests the SparkContext to run a Spark job with the RDD and the missing partitions. doCheckpoint makes sure that the StorageLevel of the RDD uses disk (among other persistence storages). If not, doCheckpoint throws an AssertionError : Storage level [level] is not appropriate for local checkpointing","title":" doCheckpoint"},{"location":"rdd/MapPartitionsRDD/","text":"MapPartitionsRDD \u00b6 MapPartitionsRDD is an RDD that has exactly one-to-one narrow dependency on the < > and \"describes\" a distributed computation of the given < > to every RDD partition. MapPartitionsRDD is < > when: PairRDDFunctions ( RDD[(K, V)] ) is requested to rdd:PairRDDFunctions.md#mapValues[mapValues] and rdd:PairRDDFunctions.md#flatMapValues[flatMapValues] (with the < > flag enabled) RDD[T] is requested to < >, < >, < >, < >, < >, < >, < >, and < > RDDBarrier[T] is requested to < > (with the < > flag enabled) By default, it does not preserve partitioning -- the last input parameter preservesPartitioning is false . If it is true , it retains the original RDD's partitioning. MapPartitionsRDD is the result of the following transformations: filter glom spark-rdd-transformations.md#mapPartitions[mapPartitions] mapPartitionsWithIndex rdd:PairRDDFunctions.md#mapValues[PairRDDFunctions.mapValues] rdd:PairRDDFunctions.md#flatMapValues[PairRDDFunctions.flatMapValues] [[isBarrier_]] When requested for the rdd:RDD.md#isBarrier_[isBarrier_] flag, MapPartitionsRDD gives the < > flag or check whether any of the RDDs of the rdd:RDD.md#dependencies[RDD dependencies] are rdd:RDD.md#isBarrier[barrier-enabled]. === [[creating-instance]] Creating MapPartitionsRDD Instance MapPartitionsRDD takes the following to be created: [[prev]] Parent rdd:RDD.md[RDD] ( RDD[T] ) [[f]] Function to execute on partitions + (TaskContext, partitionID, Iterator[T]) => Iterator[U] [[preservesPartitioning]] preservesPartitioning flag (default: false ) [[isFromBarrier]] isFromBarrier flag for < > (default: false ) [[isOrderSensitive]] isOrderSensitive flag (default: false )","title":"MapPartitionsRDD"},{"location":"rdd/MapPartitionsRDD/#mappartitionsrdd","text":"MapPartitionsRDD is an RDD that has exactly one-to-one narrow dependency on the < > and \"describes\" a distributed computation of the given < > to every RDD partition. MapPartitionsRDD is < > when: PairRDDFunctions ( RDD[(K, V)] ) is requested to rdd:PairRDDFunctions.md#mapValues[mapValues] and rdd:PairRDDFunctions.md#flatMapValues[flatMapValues] (with the < > flag enabled) RDD[T] is requested to < >, < >, < >, < >, < >, < >, < >, and < > RDDBarrier[T] is requested to < > (with the < > flag enabled) By default, it does not preserve partitioning -- the last input parameter preservesPartitioning is false . If it is true , it retains the original RDD's partitioning. MapPartitionsRDD is the result of the following transformations: filter glom spark-rdd-transformations.md#mapPartitions[mapPartitions] mapPartitionsWithIndex rdd:PairRDDFunctions.md#mapValues[PairRDDFunctions.mapValues] rdd:PairRDDFunctions.md#flatMapValues[PairRDDFunctions.flatMapValues] [[isBarrier_]] When requested for the rdd:RDD.md#isBarrier_[isBarrier_] flag, MapPartitionsRDD gives the < > flag or check whether any of the RDDs of the rdd:RDD.md#dependencies[RDD dependencies] are rdd:RDD.md#isBarrier[barrier-enabled]. === [[creating-instance]] Creating MapPartitionsRDD Instance MapPartitionsRDD takes the following to be created: [[prev]] Parent rdd:RDD.md[RDD] ( RDD[T] ) [[f]] Function to execute on partitions + (TaskContext, partitionID, Iterator[T]) => Iterator[U] [[preservesPartitioning]] preservesPartitioning flag (default: false ) [[isFromBarrier]] isFromBarrier flag for < > (default: false ) [[isOrderSensitive]] isOrderSensitive flag (default: false )","title":"MapPartitionsRDD"},{"location":"rdd/NarrowDependency/","text":"NarrowDependency \u00b6 NarrowDependency is a base (abstract) Dependency.md[Dependency] with narrow (limited) number of partitions of the parent RDD that are required to compute a partition of the child RDD. NOTE: Narrow dependencies allow for pipelined execution. .Concrete NarrowDependency -ies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | < > | | < > | | < > | |=== === [[contract]] NarrowDependency Contract NarrowDependency contract assumes that extensions implement getParents method. [source, scala] \u00b6 def getParents(partitionId: Int): Seq[Int] \u00b6 getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"NarrowDependency"},{"location":"rdd/NarrowDependency/#narrowdependency","text":"NarrowDependency is a base (abstract) Dependency.md[Dependency] with narrow (limited) number of partitions of the parent RDD that are required to compute a partition of the child RDD. NOTE: Narrow dependencies allow for pipelined execution. .Concrete NarrowDependency -ies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | < > | | < > | | < > | |=== === [[contract]] NarrowDependency Contract NarrowDependency contract assumes that extensions implement getParents method.","title":"NarrowDependency"},{"location":"rdd/NarrowDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/NarrowDependency/#def-getparentspartitionid-int-seqint","text":"getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"def getParents(partitionId: Int): Seq[Int]"},{"location":"rdd/NewHadoopRDD/","text":"== [[NewHadoopRDD]] NewHadoopRDD NewHadoopRDD is an rdd:index.md[RDD] of K keys and V values. < NewHadoopRDD is created>> when: SparkContext.newAPIHadoopFile SparkContext.newAPIHadoopRDD (indirectly) SparkContext.binaryFiles (indirectly) SparkContext.wholeTextFiles NOTE: NewHadoopRDD is the base RDD of BinaryFileRDD and WholeTextFileRDD . === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[creating-instance]] Creating NewHadoopRDD Instance NewHadoopRDD takes the following when created: [[sc]] SparkContext.md[] [[inputFormatClass]] HDFS' InputFormat[K, V] [[keyClass]] K class name [[valueClass]] V class name [[_conf]] transient HDFS' Configuration NewHadoopRDD initializes the < >.","title":"NewHadoopRDD"},{"location":"rdd/OrderedRDDFunctions/","text":"OrderedRDDFunctions \u00b6 class OrderedRDDFunctions [ K : Ordering : ClassTag , V : ClassTag , P <: Product2 [ K , V ] : ClassTag ] OrderedRDDFunctions adds extra operators to RDDs of (key, value) pairs ( RDD[(K, V)] ) where the K key is sortable (i.e. any key type K that has an implicit Ordering[K] in scope). Tip Learn more about Ordering in the Scala Standard Library documentation. Creating Instance \u00b6 OrderedRDDFunctions takes the following to be created: RDD of P s OrderedRDDFunctions is created using RDD.rddToOrderedRDDFunctions implicit method. filterByRange \u00b6 filterByRange ( lower : K , upper : K ): RDD [ P ] filterByRange ...FIXME repartitionAndSortWithinPartitions \u00b6 repartitionAndSortWithinPartitions ( partitioner : Partitioner ): RDD [( K , V )] repartitionAndSortWithinPartitions creates a ShuffledRDD with the given Partitioner . Note repartitionAndSortWithinPartitions is a generalization of sortByKey operator. repartitionAndSortWithinPartitions is used when...FIXME sortByKey \u00b6 sortByKey ( ascending : Boolean = true , numPartitions : Int = self . partitions . length ): RDD [( K , V )] sortByKey creates a ShuffledRDD (with the RDD and a RangePartitioner ). Note sortByKey is a specialization of repartitionAndSortWithinPartitions operator. Note Spark uses sortByKey for RDD.sortBy operator.","title":"OrderedRDDFunctions"},{"location":"rdd/OrderedRDDFunctions/#orderedrddfunctions","text":"class OrderedRDDFunctions [ K : Ordering : ClassTag , V : ClassTag , P <: Product2 [ K , V ] : ClassTag ] OrderedRDDFunctions adds extra operators to RDDs of (key, value) pairs ( RDD[(K, V)] ) where the K key is sortable (i.e. any key type K that has an implicit Ordering[K] in scope). Tip Learn more about Ordering in the Scala Standard Library documentation.","title":"OrderedRDDFunctions"},{"location":"rdd/OrderedRDDFunctions/#creating-instance","text":"OrderedRDDFunctions takes the following to be created: RDD of P s OrderedRDDFunctions is created using RDD.rddToOrderedRDDFunctions implicit method.","title":"Creating Instance"},{"location":"rdd/OrderedRDDFunctions/#filterbyrange","text":"filterByRange ( lower : K , upper : K ): RDD [ P ] filterByRange ...FIXME","title":" filterByRange"},{"location":"rdd/OrderedRDDFunctions/#repartitionandsortwithinpartitions","text":"repartitionAndSortWithinPartitions ( partitioner : Partitioner ): RDD [( K , V )] repartitionAndSortWithinPartitions creates a ShuffledRDD with the given Partitioner . Note repartitionAndSortWithinPartitions is a generalization of sortByKey operator. repartitionAndSortWithinPartitions is used when...FIXME","title":" repartitionAndSortWithinPartitions"},{"location":"rdd/OrderedRDDFunctions/#sortbykey","text":"sortByKey ( ascending : Boolean = true , numPartitions : Int = self . partitions . length ): RDD [( K , V )] sortByKey creates a ShuffledRDD (with the RDD and a RangePartitioner ). Note sortByKey is a specialization of repartitionAndSortWithinPartitions operator. Note Spark uses sortByKey for RDD.sortBy operator.","title":" sortByKey"},{"location":"rdd/PairRDDFunctions/","text":"PairRDDFunctions \u00b6 PairRDDFunctions is an extension of RDD API for extra methods for key-value RDDs ( RDD[(K, V)] ). PairRDDFunctions is available in RDDs of key-value pairs via Scala implicit conversion. saveAsNewAPIHadoopFile \u00b6 saveAsNewAPIHadoopFile ( path : String , keyClass : Class [ _ ], valueClass : Class [ _ ], outputFormatClass : Class [ _ <: NewOutputFormat [ _ , _ ]], conf : Configuration = self . context . hadoopConfiguration ): Unit saveAsNewAPIHadoopFile [ F <: NewOutputFormat [ K , V ]]( path : String )( implicit fm : ClassTag [ F ]): Unit saveAsNewAPIHadoopFile creates a new Job ( Hadoop MapReduce ) for the given Configuration ( Hadoop ). saveAsNewAPIHadoopFile configures the Job (with the given keyClass , valueClass and outputFormatClass ). saveAsNewAPIHadoopFile sets mapreduce.output.fileoutputformat.outputdir configuration property to be the given path and saveAsNewAPIHadoopDataset . saveAsNewAPIHadoopDataset \u00b6 saveAsNewAPIHadoopDataset ( conf : Configuration ): Unit saveAsNewAPIHadoopDataset creates a new HadoopMapReduceWriteConfigUtil (with the given Configuration ) and writes the RDD out . Configuration should have all the relevant output params set (an output format , output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. groupByKey and reduceByKey \u00b6 reduceByKey ( func : ( V , V ) => V ): RDD [( K , V )] reduceByKey ( func : ( V , V ) => V , numPartitions : Int ): RDD [( K , V )] reduceByKey ( partitioner : Partitioner , func : ( V , V ) => V ): RDD [( K , V )] reduceByKey is sort of a particular case of aggregateByKey . You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon spark-data-sources.md[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. groupByKey (): RDD [( K , Iterable [ V ])] groupByKey ( numPartitions : Int ): RDD [( K , Iterable [ V ])] groupByKey ( partitioner : Partitioner ): RDD [( K , Iterable [ V ])] You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd . keyBy ( _ . kind ) . partitionBy ( new HashPartitioner ( PARTITIONS )) . foreachPartition (...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd . keyBy ( _ . kind ). reduceByKey (....) or mapValues or plenty of other solutions. FIXME combineByKeyWithClassTag \u00b6 combineByKeyWithClassTag [ C ]( createCombiner : V => C , mergeValue : ( C , V ) => C , mergeCombiners : ( C , C ) => C )( implicit ct : ClassTag [ C ]): RDD [( K , C )] // <1> combineByKeyWithClassTag [ C ]( createCombiner : V => C , mergeValue : ( C , V ) => C , mergeCombiners : ( C , C ) => C , numPartitions : Int )( implicit ct : ClassTag [ C ]): RDD [( K , C )] // <2> combineByKeyWithClassTag [ C ]( createCombiner : V => C , mergeValue : ( C , V ) => C , mergeCombiners : ( C , C ) => C , partitioner : Partitioner , mapSideCombine : Boolean = true , serializer : Serializer = null )( implicit ct : ClassTag [ C ]): RDD [( K , C )] combineByKeyWithClassTag creates an Aggregator for the given aggregation functions. combineByKeyWithClassTag branches off per the given Partitioner . If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply mapPartitions on the RDD with the following arguments: Iterator of the Aggregator preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a ShuffledRDD (with the Serializer , the Aggregator , and the mapSideCombine flag). Usage \u00b6 combineByKeyWithClassTag lays the foundation for the following transformations: aggregateByKey combineByKey countApproxDistinctByKey foldByKey groupByKey reduceByKey Requirements \u00b6 combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException : mergeCombiners must be defined combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled: Cannot use map-side combining with array keys. combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a HashPartitioner : HashPartitioner cannot partition array keys. Example \u00b6 val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner($n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue($n1, $n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners($c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at <console>:31 [] +-(4) MapPartitionsRDD[1] at keyBy at <console>:25 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] */","title":"PairRDDFunctions"},{"location":"rdd/PairRDDFunctions/#pairrddfunctions","text":"PairRDDFunctions is an extension of RDD API for extra methods for key-value RDDs ( RDD[(K, V)] ). PairRDDFunctions is available in RDDs of key-value pairs via Scala implicit conversion.","title":"PairRDDFunctions"},{"location":"rdd/PairRDDFunctions/#saveasnewapihadoopfile","text":"saveAsNewAPIHadoopFile ( path : String , keyClass : Class [ _ ], valueClass : Class [ _ ], outputFormatClass : Class [ _ <: NewOutputFormat [ _ , _ ]], conf : Configuration = self . context . hadoopConfiguration ): Unit saveAsNewAPIHadoopFile [ F <: NewOutputFormat [ K , V ]]( path : String )( implicit fm : ClassTag [ F ]): Unit saveAsNewAPIHadoopFile creates a new Job ( Hadoop MapReduce ) for the given Configuration ( Hadoop ). saveAsNewAPIHadoopFile configures the Job (with the given keyClass , valueClass and outputFormatClass ). saveAsNewAPIHadoopFile sets mapreduce.output.fileoutputformat.outputdir configuration property to be the given path and saveAsNewAPIHadoopDataset .","title":" saveAsNewAPIHadoopFile"},{"location":"rdd/PairRDDFunctions/#saveasnewapihadoopdataset","text":"saveAsNewAPIHadoopDataset ( conf : Configuration ): Unit saveAsNewAPIHadoopDataset creates a new HadoopMapReduceWriteConfigUtil (with the given Configuration ) and writes the RDD out . Configuration should have all the relevant output params set (an output format , output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job.","title":" saveAsNewAPIHadoopDataset"},{"location":"rdd/PairRDDFunctions/#groupbykey-and-reducebykey","text":"reduceByKey ( func : ( V , V ) => V ): RDD [( K , V )] reduceByKey ( func : ( V , V ) => V , numPartitions : Int ): RDD [( K , V )] reduceByKey ( partitioner : Partitioner , func : ( V , V ) => V ): RDD [( K , V )] reduceByKey is sort of a particular case of aggregateByKey . You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon spark-data-sources.md[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. groupByKey (): RDD [( K , Iterable [ V ])] groupByKey ( numPartitions : Int ): RDD [( K , Iterable [ V ])] groupByKey ( partitioner : Partitioner ): RDD [( K , Iterable [ V ])] You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd . keyBy ( _ . kind ) . partitionBy ( new HashPartitioner ( PARTITIONS )) . foreachPartition (...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd . keyBy ( _ . kind ). reduceByKey (....) or mapValues or plenty of other solutions. FIXME","title":" groupByKey and reduceByKey"},{"location":"rdd/PairRDDFunctions/#combinebykeywithclasstag","text":"combineByKeyWithClassTag [ C ]( createCombiner : V => C , mergeValue : ( C , V ) => C , mergeCombiners : ( C , C ) => C )( implicit ct : ClassTag [ C ]): RDD [( K , C )] // <1> combineByKeyWithClassTag [ C ]( createCombiner : V => C , mergeValue : ( C , V ) => C , mergeCombiners : ( C , C ) => C , numPartitions : Int )( implicit ct : ClassTag [ C ]): RDD [( K , C )] // <2> combineByKeyWithClassTag [ C ]( createCombiner : V => C , mergeValue : ( C , V ) => C , mergeCombiners : ( C , C ) => C , partitioner : Partitioner , mapSideCombine : Boolean = true , serializer : Serializer = null )( implicit ct : ClassTag [ C ]): RDD [( K , C )] combineByKeyWithClassTag creates an Aggregator for the given aggregation functions. combineByKeyWithClassTag branches off per the given Partitioner . If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply mapPartitions on the RDD with the following arguments: Iterator of the Aggregator preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a ShuffledRDD (with the Serializer , the Aggregator , and the mapSideCombine flag).","title":" combineByKeyWithClassTag"},{"location":"rdd/PairRDDFunctions/#usage","text":"combineByKeyWithClassTag lays the foundation for the following transformations: aggregateByKey combineByKey countApproxDistinctByKey foldByKey groupByKey reduceByKey","title":" Usage"},{"location":"rdd/PairRDDFunctions/#requirements","text":"combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException : mergeCombiners must be defined combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled: Cannot use map-side combining with array keys. combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a HashPartitioner : HashPartitioner cannot partition array keys.","title":" Requirements"},{"location":"rdd/PairRDDFunctions/#example","text":"val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner($n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue($n1, $n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners($c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at <console>:31 [] +-(4) MapPartitionsRDD[1] at keyBy at <console>:25 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] */","title":" Example"},{"location":"rdd/ParallelCollectionRDD/","text":"ParallelCollectionRDD \u00b6 ParallelCollectionRDD is an RDD of a collection of elements with numSlices partitions and optional locationPrefs . ParallelCollectionRDD is the result of SparkContext.parallelize and SparkContext.makeRDD methods. The data collection is split on to numSlices slices. It uses ParallelCollectionPartition .","title":"ParallelCollectionRDD"},{"location":"rdd/ParallelCollectionRDD/#parallelcollectionrdd","text":"ParallelCollectionRDD is an RDD of a collection of elements with numSlices partitions and optional locationPrefs . ParallelCollectionRDD is the result of SparkContext.parallelize and SparkContext.makeRDD methods. The data collection is split on to numSlices slices. It uses ParallelCollectionPartition .","title":"ParallelCollectionRDD"},{"location":"rdd/Partition/","text":"Partition \u00b6 Partition is a < > of a < > of a RDD. NOTE: A partition is missing when it has not be computed yet. [[contract]] [[index]] Partition is identified by an partition index that is a unique identifier of a partition of a RDD. [source, scala] \u00b6 index: Int \u00b6","title":"Partition"},{"location":"rdd/Partition/#partition","text":"Partition is a < > of a < > of a RDD. NOTE: A partition is missing when it has not be computed yet. [[contract]] [[index]] Partition is identified by an partition index that is a unique identifier of a partition of a RDD.","title":"Partition"},{"location":"rdd/Partition/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/Partition/#index-int","text":"","title":"index: Int"},{"location":"rdd/Partitioner/","text":"Partitioner \u00b6 Partitioner is an abstraction of partitioners that define how the elements in a key-value pair RDD are partitioned by key. Partitioner maps keys to partition IDs (from 0 to numPartitions exclusive). Partitioner ensures that records with the same key are in the same partition. Partitioner is a Java Serializable . Contract \u00b6 Partition for Key \u00b6 getPartition ( key : Any ): Int Partition ID for the given key Number of Partitions \u00b6 numPartitions : Int Implementations \u00b6 CoalescedPartitioner (Spark SQL) HashPartitioner RangePartitioner","title":"Partitioner"},{"location":"rdd/Partitioner/#partitioner","text":"Partitioner is an abstraction of partitioners that define how the elements in a key-value pair RDD are partitioned by key. Partitioner maps keys to partition IDs (from 0 to numPartitions exclusive). Partitioner ensures that records with the same key are in the same partition. Partitioner is a Java Serializable .","title":"Partitioner"},{"location":"rdd/Partitioner/#contract","text":"","title":"Contract"},{"location":"rdd/Partitioner/#partition-for-key","text":"getPartition ( key : Any ): Int Partition ID for the given key","title":" Partition for Key"},{"location":"rdd/Partitioner/#number-of-partitions","text":"numPartitions : Int","title":" Number of Partitions"},{"location":"rdd/Partitioner/#implementations","text":"CoalescedPartitioner (Spark SQL) HashPartitioner RangePartitioner","title":"Implementations"},{"location":"rdd/RDD/","text":"RDD \u2014 Description of Distributed Computation \u00b6 RDD[T] is an abstraction of fault-tolerant resilient distributed datasets that are mere descriptions of computations over a distributed collection of records (of type T ). Contract \u00b6 Computing Partition \u00b6 compute ( split : Partition , context : TaskContext ): Iterator [ T ] Computes the input Partition (with the TaskContext ) to produce values (of type T ). Used when: RDD is requested to computeOrReadCheckpoint Partitions \u00b6 getPartitions : Array [ Partition ] Used when: RDD is requested for the partitions Implementations \u00b6 CheckpointRDD CoalescedRDD CoGroupedRDD HadoopRDD MapPartitionsRDD NewHadoopRDD ParallelCollectionRDD ReliableCheckpointRDD ShuffledRDD SubtractedRDD others Creating Instance \u00b6 RDD takes the following to be created: SparkContext Dependencies ( Parent RDDs that should be computed successfully before this RDD) Abstract Class RDD is an abstract class and cannot be created directly. It is created indirectly for the concrete RDDs . partitions \u00b6 partitions : Array [ Partition ] partitions ...FIXME partitions is used when: DAGScheduler is requested to getPreferredLocsInternal SparkContext is requested to runJob Stage is created others Recursive Dependencies \u00b6 toDebugString : String toDebugString ...FIXME doCheckpoint \u00b6 doCheckpoint (): Unit doCheckpoint ...FIXME doCheckpoint is used when: SparkContext is requested to run a job synchronously iterator \u00b6 iterator ( split : Partition , context : TaskContext ): Iterator [ T ] iterator ...FIXME Final Method iterator is a final method and may not be overridden in subclasses. See 5.2.6 final in the Scala Language Specification . getOrCompute \u00b6 getOrCompute ( partition : Partition , context : TaskContext ): Iterator [ T ] getOrCompute ...FIXME computeOrReadCheckpoint \u00b6 computeOrReadCheckpoint ( split : Partition , context : TaskContext ): Iterator [ T ] computeOrReadCheckpoint ...FIXME Implicit Methods \u00b6 rddToOrderedRDDFunctions \u00b6 rddToOrderedRDDFunctions [ K : Ordering : ClassTag , V : ClassTag ]( rdd : RDD [( K , V )]): OrderedRDDFunctions [ K , V , ( K , V )] rddToOrderedRDDFunctions is an Scala implicit method that creates an OrderedRDDFunctions . rddToOrderedRDDFunctions is used (implicitly) when: RDD.sortBy PairRDDFunctions.combineByKey Review Me \u00b6 == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | CoGroupedRDD | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce] transformations | HadoopRDD.md[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | MapPartitionsRDD.md[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , spark-rdd-transformations.md#mapPartitions[mapPartitions]) | ParallelCollectionRDD.md[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | ShuffledRDD.md[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce]) |=== == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a storage:StorageLevel.md[StorageLevel] specified. The default StorageLevel is storage:StorageLevel.md#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method. [source, scala] \u00b6 getStorageLevel: StorageLevel \u00b6 == [[id]] Unique Identifier [source, scala] \u00b6 id: Int \u00b6 id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for SparkContext.md#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a spark-barrier-execution-mode.md#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no ShuffleDependencies among the < > . There is at least one parent RDD that has the flag enabled ShuffledRDD.md[ShuffledRDD] has the flag always disabled. MapPartitionsRDD.md[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition [source, scala] \u00b6 getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a storage:BlockId.md#RDDBlockId[RDDBlockId] for the < > and the partition index . getOrCompute requests the BlockManager to storage:BlockManager.md#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses core:SparkEnv.md#get[SparkEnv] to access the current core:SparkEnv.md#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the storage:BlockManager.md#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an spark-InterruptibleIterator.md[InterruptibleIterator]. NOTE: spark-InterruptibleIterator.md[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for task killing functionality . For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and storage:BlockManager.md#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: storage:BlockManager.md#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies [source, scala] \u00b6 dependencies: Seq[Dependency[_]] \u00b6 dependencies returns the dependencies of a RDD . NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is checkpointed and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a OneToOneDependency . For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[checkpointRDD]] Getting CheckpointRDD [source, scala] \u00b6 checkpoint Option[CheckpointRDD[T]] \u00b6 checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method [source, scala] \u00b6 isCheckpointedAndMaterialized: Boolean \u00b6 isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method [source, scala] \u00b6 getNarrowAncestors: Seq[RDD[_]] \u00b6 getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to fromStage . == [[persist]] Persisting RDD [source, scala] \u00b6 persist(): this.type persist( newLevel: StorageLevel): this.type Refer to spark-rdd-caching.md#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method [source, scala] \u00b6 persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint [source, scala] \u00b6 computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[preferredLocations]] Defining Placement Preferences of RDD Partition [source, scala] \u00b6 preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to scheduler:DAGScheduler.md#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions [source, scala] \u00b6 partitions: Array[Partition] \u00b6 partitions returns the spark-rdd-partitions.md[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[checkpoint]] Reliable Checkpointing -- checkpoint Method [source, scala] \u00b6 checkpoint(): Unit \u00b6 checkpoint...FIXME checkpoint is used when...FIXME","title":"RDD"},{"location":"rdd/RDD/#rdd-description-of-distributed-computation","text":"RDD[T] is an abstraction of fault-tolerant resilient distributed datasets that are mere descriptions of computations over a distributed collection of records (of type T ).","title":"RDD &mdash; Description of Distributed Computation"},{"location":"rdd/RDD/#contract","text":"","title":"Contract"},{"location":"rdd/RDD/#computing-partition","text":"compute ( split : Partition , context : TaskContext ): Iterator [ T ] Computes the input Partition (with the TaskContext ) to produce values (of type T ). Used when: RDD is requested to computeOrReadCheckpoint","title":" Computing Partition"},{"location":"rdd/RDD/#partitions","text":"getPartitions : Array [ Partition ] Used when: RDD is requested for the partitions","title":" Partitions"},{"location":"rdd/RDD/#implementations","text":"CheckpointRDD CoalescedRDD CoGroupedRDD HadoopRDD MapPartitionsRDD NewHadoopRDD ParallelCollectionRDD ReliableCheckpointRDD ShuffledRDD SubtractedRDD others","title":"Implementations"},{"location":"rdd/RDD/#creating-instance","text":"RDD takes the following to be created: SparkContext Dependencies ( Parent RDDs that should be computed successfully before this RDD) Abstract Class RDD is an abstract class and cannot be created directly. It is created indirectly for the concrete RDDs .","title":"Creating Instance"},{"location":"rdd/RDD/#partitions_1","text":"partitions : Array [ Partition ] partitions ...FIXME partitions is used when: DAGScheduler is requested to getPreferredLocsInternal SparkContext is requested to runJob Stage is created others","title":" partitions"},{"location":"rdd/RDD/#recursive-dependencies","text":"toDebugString : String toDebugString ...FIXME","title":" Recursive Dependencies"},{"location":"rdd/RDD/#docheckpoint","text":"doCheckpoint (): Unit doCheckpoint ...FIXME doCheckpoint is used when: SparkContext is requested to run a job synchronously","title":" doCheckpoint"},{"location":"rdd/RDD/#iterator","text":"iterator ( split : Partition , context : TaskContext ): Iterator [ T ] iterator ...FIXME Final Method iterator is a final method and may not be overridden in subclasses. See 5.2.6 final in the Scala Language Specification .","title":" iterator"},{"location":"rdd/RDD/#getorcompute","text":"getOrCompute ( partition : Partition , context : TaskContext ): Iterator [ T ] getOrCompute ...FIXME","title":" getOrCompute"},{"location":"rdd/RDD/#computeorreadcheckpoint","text":"computeOrReadCheckpoint ( split : Partition , context : TaskContext ): Iterator [ T ] computeOrReadCheckpoint ...FIXME","title":" computeOrReadCheckpoint"},{"location":"rdd/RDD/#implicit-methods","text":"","title":"Implicit Methods"},{"location":"rdd/RDD/#rddtoorderedrddfunctions","text":"rddToOrderedRDDFunctions [ K : Ordering : ClassTag , V : ClassTag ]( rdd : RDD [( K , V )]): OrderedRDDFunctions [ K , V , ( K , V )] rddToOrderedRDDFunctions is an Scala implicit method that creates an OrderedRDDFunctions . rddToOrderedRDDFunctions is used (implicitly) when: RDD.sortBy PairRDDFunctions.combineByKey","title":" rddToOrderedRDDFunctions"},{"location":"rdd/RDD/#review-me","text":"== [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | CoGroupedRDD | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce] transformations | HadoopRDD.md[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | MapPartitionsRDD.md[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , spark-rdd-transformations.md#mapPartitions[mapPartitions]) | ParallelCollectionRDD.md[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | ShuffledRDD.md[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce]) |=== == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a storage:StorageLevel.md[StorageLevel] specified. The default StorageLevel is storage:StorageLevel.md#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method.","title":"Review Me"},{"location":"rdd/RDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getstoragelevel-storagelevel","text":"== [[id]] Unique Identifier","title":"getStorageLevel: StorageLevel"},{"location":"rdd/RDD/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#id-int","text":"id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for SparkContext.md#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a spark-barrier-execution-mode.md#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no ShuffleDependencies among the < > . There is at least one parent RDD that has the flag enabled ShuffledRDD.md[ShuffledRDD] has the flag always disabled. MapPartitionsRDD.md[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition","title":"id: Int"},{"location":"rdd/RDD/#source-scala_2","text":"getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a storage:BlockId.md#RDDBlockId[RDDBlockId] for the < > and the partition index . getOrCompute requests the BlockManager to storage:BlockManager.md#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses core:SparkEnv.md#get[SparkEnv] to access the current core:SparkEnv.md#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the storage:BlockManager.md#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an spark-InterruptibleIterator.md[InterruptibleIterator]. NOTE: spark-InterruptibleIterator.md[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for task killing functionality . For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and storage:BlockManager.md#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: storage:BlockManager.md#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_3","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#dependencies-seqdependency_","text":"dependencies returns the dependencies of a RDD . NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is checkpointed and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a OneToOneDependency . For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[checkpointRDD]] Getting CheckpointRDD","title":"dependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpoint-optioncheckpointrddt","text":"checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method","title":"checkpoint Option[CheckpointRDD[T]]"},{"location":"rdd/RDD/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#ischeckpointedandmaterialized-boolean","text":"isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method","title":"isCheckpointedAndMaterialized: Boolean"},{"location":"rdd/RDD/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnarrowancestors-seqrdd_","text":"getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to fromStage . == [[persist]] Persisting RDD","title":"getNarrowAncestors: Seq[RDD[_]]"},{"location":"rdd/RDD/#source-scala_7","text":"persist(): this.type persist( newLevel: StorageLevel): this.type Refer to spark-rdd-caching.md#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_8","text":"persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_9","text":"computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[preferredLocations]] Defining Placement Preferences of RDD Partition","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_10","text":"preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to scheduler:DAGScheduler.md#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_11","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitions-arraypartition","text":"partitions returns the spark-rdd-partitions.md[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[checkpoint]] Reliable Checkpointing -- checkpoint Method","title":"partitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpoint-unit","text":"checkpoint...FIXME checkpoint is used when...FIXME","title":"checkpoint(): Unit"},{"location":"rdd/RDDCheckpointData/","text":"RDDCheckpointData \u00b6 RDDCheckpointData is an abstraction of information related to RDD checkpointing. == [[implementations]] Available RDDCheckpointDatas [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDDCheckpointData | Description | rdd:LocalRDDCheckpointData.md[LocalRDDCheckpointData] | [[LocalRDDCheckpointData]] | rdd:ReliableRDDCheckpointData.md[ReliableRDDCheckpointData] | [[ReliableRDDCheckpointData]] Reliable Checkpointing |=== == [[creating-instance]] Creating Instance RDDCheckpointData takes the following to be created: [[rdd]] rdd:RDD.md[RDD] == [[Serializable]] RDDCheckpointData as Serializable RDDCheckpointData is java.io.Serializable. == [[cpState]] States [[Initialized]] Initialized [[CheckpointingInProgress]] CheckpointingInProgress [[Checkpointed]] Checkpointed == [[checkpoint]] Checkpointing RDD [source, scala] \u00b6 checkpoint(): CheckpointRDD[T] \u00b6 checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to rdd:RDD.md#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to rdd:RDD.md#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"RDDCheckpointData"},{"location":"rdd/RDDCheckpointData/#rddcheckpointdata","text":"RDDCheckpointData is an abstraction of information related to RDD checkpointing. == [[implementations]] Available RDDCheckpointDatas [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDDCheckpointData | Description | rdd:LocalRDDCheckpointData.md[LocalRDDCheckpointData] | [[LocalRDDCheckpointData]] | rdd:ReliableRDDCheckpointData.md[ReliableRDDCheckpointData] | [[ReliableRDDCheckpointData]] Reliable Checkpointing |=== == [[creating-instance]] Creating Instance RDDCheckpointData takes the following to be created: [[rdd]] rdd:RDD.md[RDD] == [[Serializable]] RDDCheckpointData as Serializable RDDCheckpointData is java.io.Serializable. == [[cpState]] States [[Initialized]] Initialized [[CheckpointingInProgress]] CheckpointingInProgress [[Checkpointed]] Checkpointed == [[checkpoint]] Checkpointing RDD","title":"RDDCheckpointData"},{"location":"rdd/RDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#checkpoint-checkpointrddt","text":"checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to rdd:RDD.md#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to rdd:RDD.md#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method","title":"checkpoint(): CheckpointRDD[T]"},{"location":"rdd/RDDCheckpointData/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/RangePartitioner/","text":"RangePartitioner \u00b6 RangePartitioner is a Partitioner for bucketed partitioning . RangePartitioner is used for sortByKey operator (among other uses). Creating Instance \u00b6 RangePartitioner takes the following to be created: Number of Partitions Key-Value RDD ( RDD[_ <: Product2[K, V]] ) ascending flag (default: true ) samplePointsPerPartitionHint (default: 20 ) Number of Partitions \u00b6 numPartitions : Int numPartitions is the length of the rangeBounds array plus 1 . numPartitions is part of the Partitioner abstraction. Partition for Key \u00b6 getPartition ( key : Any ): Int getPartition ...FIXME getPartition is part of the Partitioner abstraction. Range Bounds \u00b6 rangeBounds : Array [ K ] rangeBounds is an Array[K] ...FIXME determineBounds Utility \u00b6 determineBounds [ K : Ordering : ClassTag ]( candidates : ArrayBuffer [( K , Float )], partitions : Int ): Array [ K ] determineBounds ...FIXME","title":"RangePartitioner"},{"location":"rdd/RangePartitioner/#rangepartitioner","text":"RangePartitioner is a Partitioner for bucketed partitioning . RangePartitioner is used for sortByKey operator (among other uses).","title":"RangePartitioner"},{"location":"rdd/RangePartitioner/#creating-instance","text":"RangePartitioner takes the following to be created: Number of Partitions Key-Value RDD ( RDD[_ <: Product2[K, V]] ) ascending flag (default: true ) samplePointsPerPartitionHint (default: 20 )","title":"Creating Instance"},{"location":"rdd/RangePartitioner/#number-of-partitions","text":"numPartitions : Int numPartitions is the length of the rangeBounds array plus 1 . numPartitions is part of the Partitioner abstraction.","title":" Number of Partitions"},{"location":"rdd/RangePartitioner/#partition-for-key","text":"getPartition ( key : Any ): Int getPartition ...FIXME getPartition is part of the Partitioner abstraction.","title":" Partition for Key"},{"location":"rdd/RangePartitioner/#range-bounds","text":"rangeBounds : Array [ K ] rangeBounds is an Array[K] ...FIXME","title":" Range Bounds"},{"location":"rdd/RangePartitioner/#determinebounds-utility","text":"determineBounds [ K : Ordering : ClassTag ]( candidates : ArrayBuffer [( K , Float )], partitions : Int ): Array [ K ] determineBounds ...FIXME","title":" determineBounds Utility"},{"location":"rdd/ReliableCheckpointRDD/","text":"ReliableCheckpointRDD \u00b6 ReliableCheckpointRDD is an CheckpointRDD . Creating Instance \u00b6 ReliableCheckpointRDD takes the following to be created: [[sc]] SparkContext.md[] [[checkpointPath]] Checkpoint Directory (on a Hadoop DFS-compatible file system) <<_partitioner, Partitioner>> ReliableCheckpointRDD is created when: ReliableCheckpointRDD utility is used to < >. SparkContext is requested to SparkContext.md#checkpointFile[checkpointFile] == [[checkpointPartitionerFileName]] Checkpointed Partitioner File ReliableCheckpointRDD uses _partitioner as the name of the file in the < > with the < > serialized to. == [[partitioner]] Partitioner ReliableCheckpointRDD can be given a rdd:Partitioner.md[Partitioner] to be created. When rdd:RDD.md#partitioner[requested for the Partitioner] (as an RDD), ReliableCheckpointRDD returns the one it was created with or < >. == [[writeRDDToCheckpointDirectory]] Writing RDD to Checkpoint Directory [source, scala] \u00b6 writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to rdd:ReliableRDDCheckpointData.md#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory [source,scala] \u00b6 writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on configuration-properties.md#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the core:SparkEnv.md#serializer[default Serializer] for a new serializer:Serializer.md#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to serializer:SerializerInstance.md#serializeStream[serialize the output stream] and serializer:DeserializationStream.md#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Written partitioner to [partitionerFilePath] \u00b6 In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Error writing partitioner [partitioner] to [checkpointDirPath] \u00b6 writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory [source,scala] \u00b6 readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on configuration-properties.md#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the core:SparkEnv.md#serializer[default Serializer] for a new serializer:Serializer.md#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to serializer:SerializerInstance.md#deserializeStream[deserialize the input stream] and serializer:DeserializationStream.md#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner. [source,plaintext] \u00b6 Read partitioner from [partitionerFilePath] \u00b6 In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL \u00b6 Refer to spark-logging.md[Logging].","title":"ReliableCheckpointRDD"},{"location":"rdd/ReliableCheckpointRDD/#reliablecheckpointrdd","text":"ReliableCheckpointRDD is an CheckpointRDD .","title":"ReliableCheckpointRDD"},{"location":"rdd/ReliableCheckpointRDD/#creating-instance","text":"ReliableCheckpointRDD takes the following to be created: [[sc]] SparkContext.md[] [[checkpointPath]] Checkpoint Directory (on a Hadoop DFS-compatible file system) <<_partitioner, Partitioner>> ReliableCheckpointRDD is created when: ReliableCheckpointRDD utility is used to < >. SparkContext is requested to SparkContext.md#checkpointFile[checkpointFile] == [[checkpointPartitionerFileName]] Checkpointed Partitioner File ReliableCheckpointRDD uses _partitioner as the name of the file in the < > with the < > serialized to. == [[partitioner]] Partitioner ReliableCheckpointRDD can be given a rdd:Partitioner.md[Partitioner] to be created. When rdd:RDD.md#partitioner[requested for the Partitioner] (as an RDD), ReliableCheckpointRDD returns the one it was created with or < >. == [[writeRDDToCheckpointDirectory]] Writing RDD to Checkpoint Directory","title":"Creating Instance"},{"location":"rdd/ReliableCheckpointRDD/#source-scala","text":"writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to rdd:ReliableRDDCheckpointData.md#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory","title":"[source, scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala","text":"writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on configuration-properties.md#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the core:SparkEnv.md#serializer[default Serializer] for a new serializer:Serializer.md#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to serializer:SerializerInstance.md#serializeStream[serialize the output stream] and serializer:DeserializationStream.md#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#written-partitioner-to-partitionerfilepath","text":"In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"Written partitioner to [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#error-writing-partitioner-partitioner-to-checkpointdirpath","text":"writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory","title":"Error writing partitioner [partitioner] to [checkpointDirPath]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala_1","text":"readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on configuration-properties.md#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the core:SparkEnv.md#serializer[default Serializer] for a new serializer:Serializer.md#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to serializer:SerializerInstance.md#deserializeStream[deserialize the input stream] and serializer:DeserializationStream.md#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner.","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#read-partitioner-from-partitionerfilepath","text":"In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Read partitioner from [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#log4jloggerorgapachesparkrddreliablecheckpointrddall","text":"Refer to spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL"},{"location":"rdd/ReliableRDDCheckpointData/","text":"ReliableRDDCheckpointData \u00b6 ReliableRDDCheckpointData is a RDDCheckpointData for Reliable Checkpointing . Creating Instance \u00b6 ReliableRDDCheckpointData takes the following to be created: [[rdd]] rdd:RDD.md[++RDD[T]++] ReliableRDDCheckpointData is created for rdd:RDD.md#checkpoint[RDD.checkpoint] operator. == [[cpDir]][[checkpointPath]] Checkpoint Directory ReliableRDDCheckpointData creates a subdirectory of the SparkContext.md#checkpointDir[application-wide checkpoint directory] for < > the given < >. The name of the subdirectory uses the rdd:RDD.md#id[unique identifier] of the < >: [source,plaintext] \u00b6 rdd-[id] \u00b6 == [[doCheckpoint]] Checkpointing RDD [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint rdd:ReliableCheckpointRDD.md#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With configuration-properties.md#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the SparkContext.md#cleaner[ContextCleaner] to core:ContextCleaner.md#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD. [source,plaintext] \u00b6 Done checkpointing RDD [id] to [cpDir], new parent is RDD [id] \u00b6 doCheckpoint is part of the rdd:RDDCheckpointData.md#doCheckpoint[RDDCheckpointData] abstraction.","title":"ReliableRDDCheckpointData"},{"location":"rdd/ReliableRDDCheckpointData/#reliablerddcheckpointdata","text":"ReliableRDDCheckpointData is a RDDCheckpointData for Reliable Checkpointing .","title":"ReliableRDDCheckpointData"},{"location":"rdd/ReliableRDDCheckpointData/#creating-instance","text":"ReliableRDDCheckpointData takes the following to be created: [[rdd]] rdd:RDD.md[++RDD[T]++] ReliableRDDCheckpointData is created for rdd:RDD.md#checkpoint[RDD.checkpoint] operator. == [[cpDir]][[checkpointPath]] Checkpoint Directory ReliableRDDCheckpointData creates a subdirectory of the SparkContext.md#checkpointDir[application-wide checkpoint directory] for < > the given < >. The name of the subdirectory uses the rdd:RDD.md#id[unique identifier] of the < >:","title":"Creating Instance"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#rdd-id","text":"== [[doCheckpoint]] Checkpointing RDD","title":"rdd-[id]"},{"location":"rdd/ReliableRDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ReliableRDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint rdd:ReliableCheckpointRDD.md#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With configuration-properties.md#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the SparkContext.md#cleaner[ContextCleaner] to core:ContextCleaner.md#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#done-checkpointing-rdd-id-to-cpdir-new-parent-is-rdd-id","text":"doCheckpoint is part of the rdd:RDDCheckpointData.md#doCheckpoint[RDDCheckpointData] abstraction.","title":"Done checkpointing RDD [id] to [cpDir], new parent is RDD [id]"},{"location":"rdd/ShuffleDependency/","text":"ShuffleDependency \u00b6 ShuffleDependency is a Dependency on the output of a ShuffleMapStage of a key-value RDD . ShuffleDependency uses the RDD to know the number of (map-side/pre-shuffle) partitions and the Partitioner for the number of (reduce-size/post-shuffle) partitions. ShuffleDependency [ K : ClassTag , V : ClassTag , C : ClassTag ] Creating Instance \u00b6 ShuffleDependency takes the following to be created: RDD of key-value pairs ( RDD[_ <: Product2[K, V]] ) Partitioner Serializer (default: SparkEnv.get.serializer ) Optional Key Ordering (default: undefined) Optional Aggregator mapSideCombine ShuffleWriteProcessor ShuffleDependency is created when: CoGroupedRDD is requested for the dependencies (for RDDs with different partitioners) ShuffledRDD is requested for the dependencies SubtractedRDD is requested for the dependencies (for an RDD with different partitioner) ShuffleExchangeExec ( Spark SQL ) physical operator is requested to prepare a ShuffleDependency When created, ShuffleDependency gets the shuffle id . ShuffleDependency registers itself with the ShuffleManager and gets a ShuffleHandle (available as shuffleHandle ). ShuffleDependency uses SparkEnv to access the ShuffleManager . In the end, ShuffleDependency registers itself with the ContextCleaner (if configured) and the ShuffleDriverComponents . Aggregator \u00b6 aggregator : Option [ Aggregator [ K , V , C ]] ShuffleDependency may be given a map/reduce-side Aggregator when created . ShuffleDependency asserts (when created ) that an Aggregator is defined when the mapSideCombine flag is enabled. aggregator is used when: SortShuffleWriter is requested to write records (for mapper tasks) BlockStoreShuffleReader is requested to read records (for reducer tasks) Shuffle ID \u00b6 shuffleId : Int ShuffleDependency is identified uniquely by an application-wide shuffle ID (that is requested from SparkContext when created ). Partitioner \u00b6 ShuffleDependency is given a Partitioner (when created ). ShuffleDependency uses the Partitioner to partition the shuffle output. The Partitioner is used when: SortShuffleWriter is requested to write records (and create an ExternalSorter ) others (FIXME) ShuffleHandle \u00b6 ShuffleDependency registers itself with the ShuffleManager when created . The ShuffleHandle is used when: CoGroupedRDDs , ShuffledRDD , SubtractedRDD , and ShuffledRowRDD ( Spark SQL ) are requested to compute a partition (to get a ShuffleReader for a ShuffleDependency ) ShuffleMapTask is requested to run (to get a ShuffleWriter for a ShuffleDependency). Map-Size Partial Aggregation Flag \u00b6 ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using the Aggregator . mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some uses of ShuffledRDD . ShuffleDependency requires that the optional Aggregator is actually defined for the flag enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to read combined records for a reduce task SortShuffleManager is requested to register a shuffle SortShuffleWriter is requested to write records","title":"ShuffleDependency"},{"location":"rdd/ShuffleDependency/#shuffledependency","text":"ShuffleDependency is a Dependency on the output of a ShuffleMapStage of a key-value RDD . ShuffleDependency uses the RDD to know the number of (map-side/pre-shuffle) partitions and the Partitioner for the number of (reduce-size/post-shuffle) partitions. ShuffleDependency [ K : ClassTag , V : ClassTag , C : ClassTag ]","title":"ShuffleDependency"},{"location":"rdd/ShuffleDependency/#creating-instance","text":"ShuffleDependency takes the following to be created: RDD of key-value pairs ( RDD[_ <: Product2[K, V]] ) Partitioner Serializer (default: SparkEnv.get.serializer ) Optional Key Ordering (default: undefined) Optional Aggregator mapSideCombine ShuffleWriteProcessor ShuffleDependency is created when: CoGroupedRDD is requested for the dependencies (for RDDs with different partitioners) ShuffledRDD is requested for the dependencies SubtractedRDD is requested for the dependencies (for an RDD with different partitioner) ShuffleExchangeExec ( Spark SQL ) physical operator is requested to prepare a ShuffleDependency When created, ShuffleDependency gets the shuffle id . ShuffleDependency registers itself with the ShuffleManager and gets a ShuffleHandle (available as shuffleHandle ). ShuffleDependency uses SparkEnv to access the ShuffleManager . In the end, ShuffleDependency registers itself with the ContextCleaner (if configured) and the ShuffleDriverComponents .","title":"Creating Instance"},{"location":"rdd/ShuffleDependency/#aggregator","text":"aggregator : Option [ Aggregator [ K , V , C ]] ShuffleDependency may be given a map/reduce-side Aggregator when created . ShuffleDependency asserts (when created ) that an Aggregator is defined when the mapSideCombine flag is enabled. aggregator is used when: SortShuffleWriter is requested to write records (for mapper tasks) BlockStoreShuffleReader is requested to read records (for reducer tasks)","title":" Aggregator"},{"location":"rdd/ShuffleDependency/#shuffle-id","text":"shuffleId : Int ShuffleDependency is identified uniquely by an application-wide shuffle ID (that is requested from SparkContext when created ).","title":" Shuffle ID"},{"location":"rdd/ShuffleDependency/#partitioner","text":"ShuffleDependency is given a Partitioner (when created ). ShuffleDependency uses the Partitioner to partition the shuffle output. The Partitioner is used when: SortShuffleWriter is requested to write records (and create an ExternalSorter ) others (FIXME)","title":" Partitioner"},{"location":"rdd/ShuffleDependency/#shufflehandle","text":"ShuffleDependency registers itself with the ShuffleManager when created . The ShuffleHandle is used when: CoGroupedRDDs , ShuffledRDD , SubtractedRDD , and ShuffledRowRDD ( Spark SQL ) are requested to compute a partition (to get a ShuffleReader for a ShuffleDependency ) ShuffleMapTask is requested to run (to get a ShuffleWriter for a ShuffleDependency).","title":" ShuffleHandle"},{"location":"rdd/ShuffleDependency/#map-size-partial-aggregation-flag","text":"ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using the Aggregator . mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some uses of ShuffledRDD . ShuffleDependency requires that the optional Aggregator is actually defined for the flag enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to read combined records for a reduce task SortShuffleManager is requested to register a shuffle SortShuffleWriter is requested to write records","title":" Map-Size Partial Aggregation Flag"},{"location":"rdd/ShuffledRDD/","text":"ShuffledRDD \u00b6 ShuffledRDD is an RDD of key-value pairs that represents a shuffle step in a RDD lineage . Creating Instance \u00b6 ShuffledRDD takes the following to be created: RDD (of K keys and V values) Partitioner ShuffledRDD is created for the following RDD operators: OrderedRDDFunctions.sortByKey and OrderedRDDFunctions.repartitionAndSortWithinPartitions PairRDDFunctions.combineByKeyWithClassTag and PairRDDFunctions.partitionBy RDD.coalesce (with shuffle flag enabled) Key, Value and Combiner Types \u00b6 class ShuffledRDD [ K : ClassTag , V : ClassTag , C : ClassTag ] ShuffledRDD is given an RDD of K keys and V values to be created . When computed , ShuffledRDD produces pairs of K keys and C values. isBarrier Flag \u00b6 ShuffledRDD has isBarrier flag always disabled ( false ). Map-Side Combine Flag \u00b6 ShuffledRDD uses a map-side combine flag to create a ShuffleDependency when requested for the dependencies (there is always only one). The flag is disabled ( false ) by default and can be changed using setMapSideCombine method. setMapSideCombine ( mapSideCombine : Boolean ): ShuffledRDD [ K , V , C ] setMapSideCombine is used for PairRDDFunctions.combineByKeyWithClassTag transformation (which defaults to the flag enabled). Computing Partition \u00b6 compute ( split : Partition , context : TaskContext ): Iterator [( K , C )] compute requests the only dependency (that is assumed a ShuffleDependency ) for the ShuffleHandle . compute uses the SparkEnv to access the ShuffleManager . compute requests the ShuffleManager for the ShuffleReader (for the ShuffleHandle and the partition ). In the end, compute requests the ShuffleReader to read the combined key-value pairs (of type (K, C) ). compute is part of the RDD abstraction. Placement Preferences of Partition \u00b6 getPreferredLocations ( partition : Partition ): Seq [ String ] getPreferredLocations requests MapOutputTrackerMaster for the preferred locations of the given partition ( BlockManagers with the most map outputs). getPreferredLocations uses SparkEnv to access the current MapOutputTrackerMaster . getPreferredLocations is part of the RDD abstraction. Dependencies \u00b6 getDependencies : Seq [ Dependency [ _ ]] getDependencies uses the user-specified Serializer if defined or requests the current SerializerManager for one . getDependencies uses the mapSideCombine internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single ShuffleDependency (with the previous RDD , the Partitioner , and the Serializer ). getDependencies is part of the RDD abstraction. ShuffledRDDPartition \u00b6 ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the Partitioner ). User-Specified Serializer \u00b6 User-specified Serializer for the single ShuffleDependency dependency userSpecifiedSerializer : Option [ Serializer ] = None userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for PairRDDFunctions.combineByKeyWithClassTag transformation). Demos \u00b6 ShuffledRDD and coalesce \u00b6 val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at <pastie>:75 [] | CoalescedRDD[8] at coalesce at <pastie>:75 [] | ShuffledRDD[7] at coalesce at <pastie>:75 [] +-(16) MapPartitionsRDD[6] at coalesce at <pastie>:75 [] | ParallelCollectionRDD[5] at parallelize at <pastie>:74 [] ShuffledRDD and sortByKey \u00b6 val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at <console>:74 [] +-(4) ShuffledRDD[12] at groupBy at <console>:74 [] +-(4) MapPartitionsRDD[11] at groupBy at <console>:74 [] | MapPartitionsRDD[9] at coalesce at <pastie>:75 [] | CoalescedRDD[8] at coalesce at <pastie>:75 [] | ShuffledRDD[7] at coalesce at <pastie>:75 [] +-(16) MapPartitionsRDD[6] at coalesce at <pastie>:75 [] | ParallelCollectionRDD[5] at parallelize at <pastie>:74 []","title":"ShuffledRDD"},{"location":"rdd/ShuffledRDD/#shuffledrdd","text":"ShuffledRDD is an RDD of key-value pairs that represents a shuffle step in a RDD lineage .","title":"ShuffledRDD"},{"location":"rdd/ShuffledRDD/#creating-instance","text":"ShuffledRDD takes the following to be created: RDD (of K keys and V values) Partitioner ShuffledRDD is created for the following RDD operators: OrderedRDDFunctions.sortByKey and OrderedRDDFunctions.repartitionAndSortWithinPartitions PairRDDFunctions.combineByKeyWithClassTag and PairRDDFunctions.partitionBy RDD.coalesce (with shuffle flag enabled)","title":"Creating Instance"},{"location":"rdd/ShuffledRDD/#key-value-and-combiner-types","text":"class ShuffledRDD [ K : ClassTag , V : ClassTag , C : ClassTag ] ShuffledRDD is given an RDD of K keys and V values to be created . When computed , ShuffledRDD produces pairs of K keys and C values.","title":"Key, Value and Combiner Types"},{"location":"rdd/ShuffledRDD/#isbarrier-flag","text":"ShuffledRDD has isBarrier flag always disabled ( false ).","title":" isBarrier Flag"},{"location":"rdd/ShuffledRDD/#map-side-combine-flag","text":"ShuffledRDD uses a map-side combine flag to create a ShuffleDependency when requested for the dependencies (there is always only one). The flag is disabled ( false ) by default and can be changed using setMapSideCombine method. setMapSideCombine ( mapSideCombine : Boolean ): ShuffledRDD [ K , V , C ] setMapSideCombine is used for PairRDDFunctions.combineByKeyWithClassTag transformation (which defaults to the flag enabled).","title":" Map-Side Combine Flag"},{"location":"rdd/ShuffledRDD/#computing-partition","text":"compute ( split : Partition , context : TaskContext ): Iterator [( K , C )] compute requests the only dependency (that is assumed a ShuffleDependency ) for the ShuffleHandle . compute uses the SparkEnv to access the ShuffleManager . compute requests the ShuffleManager for the ShuffleReader (for the ShuffleHandle and the partition ). In the end, compute requests the ShuffleReader to read the combined key-value pairs (of type (K, C) ). compute is part of the RDD abstraction.","title":" Computing Partition"},{"location":"rdd/ShuffledRDD/#placement-preferences-of-partition","text":"getPreferredLocations ( partition : Partition ): Seq [ String ] getPreferredLocations requests MapOutputTrackerMaster for the preferred locations of the given partition ( BlockManagers with the most map outputs). getPreferredLocations uses SparkEnv to access the current MapOutputTrackerMaster . getPreferredLocations is part of the RDD abstraction.","title":" Placement Preferences of Partition"},{"location":"rdd/ShuffledRDD/#dependencies","text":"getDependencies : Seq [ Dependency [ _ ]] getDependencies uses the user-specified Serializer if defined or requests the current SerializerManager for one . getDependencies uses the mapSideCombine internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single ShuffleDependency (with the previous RDD , the Partitioner , and the Serializer ). getDependencies is part of the RDD abstraction.","title":" Dependencies"},{"location":"rdd/ShuffledRDD/#shuffledrddpartition","text":"ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the Partitioner ).","title":" ShuffledRDDPartition"},{"location":"rdd/ShuffledRDD/#user-specified-serializer","text":"User-specified Serializer for the single ShuffleDependency dependency userSpecifiedSerializer : Option [ Serializer ] = None userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for PairRDDFunctions.combineByKeyWithClassTag transformation).","title":" User-Specified Serializer"},{"location":"rdd/ShuffledRDD/#demos","text":"","title":"Demos"},{"location":"rdd/ShuffledRDD/#shuffledrdd-and-coalesce","text":"val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at <pastie>:75 [] | CoalescedRDD[8] at coalesce at <pastie>:75 [] | ShuffledRDD[7] at coalesce at <pastie>:75 [] +-(16) MapPartitionsRDD[6] at coalesce at <pastie>:75 [] | ParallelCollectionRDD[5] at parallelize at <pastie>:74 []","title":"ShuffledRDD and coalesce"},{"location":"rdd/ShuffledRDD/#shuffledrdd-and-sortbykey","text":"val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at <console>:74 [] +-(4) ShuffledRDD[12] at groupBy at <console>:74 [] +-(4) MapPartitionsRDD[11] at groupBy at <console>:74 [] | MapPartitionsRDD[9] at coalesce at <pastie>:75 [] | CoalescedRDD[8] at coalesce at <pastie>:75 [] | ShuffledRDD[7] at coalesce at <pastie>:75 [] +-(16) MapPartitionsRDD[6] at coalesce at <pastie>:75 [] | ParallelCollectionRDD[5] at parallelize at <pastie>:74 []","title":"ShuffledRDD and sortByKey"},{"location":"rdd/SubtractedRDD/","text":"SubtractedRDD \u00b6 === [[compute]] Computing Partition (in TaskContext) -- compute Method [source, scala] \u00b6 compute(p: Partition, context: TaskContext): Iterator[(K, V)] \u00b6 compute is part of the RDD abstraction. compute ...FIXME","title":"SubtractedRDD"},{"location":"rdd/SubtractedRDD/#subtractedrdd","text":"=== [[compute]] Computing Partition (in TaskContext) -- compute Method","title":"SubtractedRDD"},{"location":"rdd/SubtractedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/SubtractedRDD/#computep-partition-context-taskcontext-iteratork-v","text":"compute is part of the RDD abstraction. compute ...FIXME","title":"compute(p: Partition, context: TaskContext): Iterator[(K, V)]"},{"location":"rdd/checkpointing/","text":"RDD Checkpointing \u00b6 RDD Checkpointing is a process of truncating rdd:spark-rdd-lineage.md[RDD lineage graph] and saving it to a reliable distributed (HDFS) or local file system. There are two types of checkpointing: < > - RDD checkpointing that saves the actual intermediate RDD data to a reliable distributed file system (e.g. Hadoop DFS) < > - RDD checkpointing that saves the data to a local file system It's up to a Spark application developer to decide when and how to checkpoint using RDD.checkpoint() method. Before checkpointing is used, a Spark developer has to set the checkpoint directory using SparkContext.setCheckpointDir(directory: String) method. == [[reliable-checkpointing]] Reliable Checkpointing You call SparkContext.setCheckpointDir(directory: String) to set the checkpoint directory - the directory where RDDs are checkpointed. The directory must be a HDFS path if running on a cluster. The reason is that the driver may attempt to reconstruct the checkpointed RDD from its own local file system, which is incorrect because the checkpoint files are actually on the executor machines. You mark an RDD for checkpointing by calling RDD.checkpoint() . The RDD will be saved to a file inside the checkpoint directory and all references to its parent RDDs will be removed. This function has to be called before any job has been executed on this RDD. NOTE: It is strongly recommended that a checkpointed RDD is persisted in memory, otherwise saving it on a file will require recomputation. When an action is called on a checkpointed RDD, the following INFO message is printed out in the logs: Done checkpointing RDD 5 to [path], new parent is RDD [id] == [[local-checkpointing]] Local Checkpointing rdd:RDD.md#localCheckpoint[localCheckpoint] allows to truncate rdd:spark-rdd-lineage.md[RDD lineage graph] while skipping the expensive step of replicating the materialized data to a reliable distributed file system. This is useful for RDDs with long lineages that need to be truncated periodically, e.g. GraphX. Local checkpointing trades fault-tolerance for performance. NOTE: The checkpoint directory set through SparkContext.setCheckpointDir is not used. == [[demo]] Demo [source,plaintext] \u00b6 val rdd = sc.parallelize(0 to 9) scala> rdd.checkpoint org.apache.spark.SparkException: Checkpoint directory has not been set in the SparkContext at org.apache.spark.rdd.RDD.checkpoint(RDD.scala:1599) ... 49 elided sc.setCheckpointDir(\"/tmp/rdd-checkpoint\") // Creates a subdirectory for this SparkContext $ ls /tmp/rdd-checkpoint/ fc21e1d1-3cd9-4d51-880f-58d1dd07f783 // Mark the RDD to checkpoint at the earliest action rdd.checkpoint scala> println(rdd.getCheckpointFile) Some(file:/tmp/rdd-checkpoint/fc21e1d1-3cd9-4d51-880f-58d1dd07f783/rdd-2) scala> println(ns.id) 2 scala> println(rdd.getNumPartitions) 16 rdd.count // Check out the checkpoint directory // You should find a directory for the checkpointed RDD, e.g. rdd-2 // The number of part-000* files is exactly the number of partitions $ ls -ltra /tmp/rdd-checkpoint/fc21e1d1-3cd9-4d51-880f-58d1dd07f783/rdd-2/part-000* | wc -l 16","title":"RDD Checkpointing"},{"location":"rdd/checkpointing/#rdd-checkpointing","text":"RDD Checkpointing is a process of truncating rdd:spark-rdd-lineage.md[RDD lineage graph] and saving it to a reliable distributed (HDFS) or local file system. There are two types of checkpointing: < > - RDD checkpointing that saves the actual intermediate RDD data to a reliable distributed file system (e.g. Hadoop DFS) < > - RDD checkpointing that saves the data to a local file system It's up to a Spark application developer to decide when and how to checkpoint using RDD.checkpoint() method. Before checkpointing is used, a Spark developer has to set the checkpoint directory using SparkContext.setCheckpointDir(directory: String) method. == [[reliable-checkpointing]] Reliable Checkpointing You call SparkContext.setCheckpointDir(directory: String) to set the checkpoint directory - the directory where RDDs are checkpointed. The directory must be a HDFS path if running on a cluster. The reason is that the driver may attempt to reconstruct the checkpointed RDD from its own local file system, which is incorrect because the checkpoint files are actually on the executor machines. You mark an RDD for checkpointing by calling RDD.checkpoint() . The RDD will be saved to a file inside the checkpoint directory and all references to its parent RDDs will be removed. This function has to be called before any job has been executed on this RDD. NOTE: It is strongly recommended that a checkpointed RDD is persisted in memory, otherwise saving it on a file will require recomputation. When an action is called on a checkpointed RDD, the following INFO message is printed out in the logs: Done checkpointing RDD 5 to [path], new parent is RDD [id] == [[local-checkpointing]] Local Checkpointing rdd:RDD.md#localCheckpoint[localCheckpoint] allows to truncate rdd:spark-rdd-lineage.md[RDD lineage graph] while skipping the expensive step of replicating the materialized data to a reliable distributed file system. This is useful for RDDs with long lineages that need to be truncated periodically, e.g. GraphX. Local checkpointing trades fault-tolerance for performance. NOTE: The checkpoint directory set through SparkContext.setCheckpointDir is not used. == [[demo]] Demo","title":"RDD Checkpointing"},{"location":"rdd/checkpointing/#sourceplaintext","text":"val rdd = sc.parallelize(0 to 9) scala> rdd.checkpoint org.apache.spark.SparkException: Checkpoint directory has not been set in the SparkContext at org.apache.spark.rdd.RDD.checkpoint(RDD.scala:1599) ... 49 elided sc.setCheckpointDir(\"/tmp/rdd-checkpoint\") // Creates a subdirectory for this SparkContext $ ls /tmp/rdd-checkpoint/ fc21e1d1-3cd9-4d51-880f-58d1dd07f783 // Mark the RDD to checkpoint at the earliest action rdd.checkpoint scala> println(rdd.getCheckpointFile) Some(file:/tmp/rdd-checkpoint/fc21e1d1-3cd9-4d51-880f-58d1dd07f783/rdd-2) scala> println(ns.id) 2 scala> println(rdd.getNumPartitions) 16 rdd.count // Check out the checkpoint directory // You should find a directory for the checkpointed RDD, e.g. rdd-2 // The number of part-000* files is exactly the number of partitions $ ls -ltra /tmp/rdd-checkpoint/fc21e1d1-3cd9-4d51-880f-58d1dd07f783/rdd-2/part-000* | wc -l 16","title":"[source,plaintext]"},{"location":"rdd/spark-rdd-actions/","text":"Actions \u00b6 RDD Actions are RDD operations that produce concrete non-RDD values. They materialize a value in a Spark program. In other words, a RDD operation that returns a value of any type but RDD[T] is an action. action: RDD => a value NOTE: Actions are synchronous. You can use < > to release a calling thread while calling actions. They trigger execution of < > to return values. Simply put, an action evaluates the spark-rdd-lineage.md[RDD lineage graph]. You can think of actions as a valve and until action is fired, the data to be processed is not even in the pipes, i.e. transformations. Only actions can materialize the entire processing pipeline with real data. aggregate collect count countApprox* countByValue* first fold foreach foreachPartition max min reduce saveAs* (e.g. saveAsTextFile , saveAsHadoopFile ) take takeOrdered takeSample toLocalIterator top treeAggregate treeReduce Actions run jobs using SparkContext.runJob or directly DAGScheduler.runJob . scala> :type words scala> words.count // <1> res0: Long = 502 TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to spark-rdd-caching.md[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync","title":"Actions"},{"location":"rdd/spark-rdd-actions/#actions","text":"RDD Actions are RDD operations that produce concrete non-RDD values. They materialize a value in a Spark program. In other words, a RDD operation that returns a value of any type but RDD[T] is an action. action: RDD => a value NOTE: Actions are synchronous. You can use < > to release a calling thread while calling actions. They trigger execution of < > to return values. Simply put, an action evaluates the spark-rdd-lineage.md[RDD lineage graph]. You can think of actions as a valve and until action is fired, the data to be processed is not even in the pipes, i.e. transformations. Only actions can materialize the entire processing pipeline with real data. aggregate collect count countApprox* countByValue* first fold foreach foreachPartition max min reduce saveAs* (e.g. saveAsTextFile , saveAsHadoopFile ) take takeOrdered takeSample toLocalIterator top treeAggregate treeReduce Actions run jobs using SparkContext.runJob or directly DAGScheduler.runJob . scala> :type words scala> words.count // <1> res0: Long = 502 TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to spark-rdd-caching.md[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync","title":"Actions"},{"location":"rdd/spark-rdd-caching/","text":"== RDD Caching and Persistence Caching or persistence are optimisation techniques for (iterative and interactive) Spark computations. They help saving interim partial results so they can be reused in subsequent stages. These interim results as RDDs are thus kept in memory (default) or more solid storages like disk and/or replicated. RDDs can be cached using < > operation. They can also be persisted using < > operation. The difference between cache and persist operations is purely syntactic. cache is a synonym of persist or persist(MEMORY_ONLY) , i.e. cache is merely persist with the default storage level MEMORY_ONLY . NOTE: Due to the very small and purely syntactic difference between caching and persistence of RDDs the two terms are often used interchangeably and I will follow the \"pattern\" here. RDDs can also be < > to remove RDD from a permanent storage like memory and/or disk. === [[cache]] Caching RDD -- cache Method [source, scala] \u00b6 cache(): this.type = persist() \u00b6 cache is a synonym of < > with storage:StorageLevel.md[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel storage:StorageLevel.md[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is core:ContextCleaner.md#registerRDDForCleanup[registered to ContextCleaner ] (if available) and SparkContext.md#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls SparkContext.md#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets storage:StorageLevel.md[ NONE storage level] as the current storage level.","title":"Caching and Persistence"},{"location":"rdd/spark-rdd-caching/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#cache-thistype-persist","text":"cache is a synonym of < > with storage:StorageLevel.md[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods","title":"cache(): this.type = persist()"},{"location":"rdd/spark-rdd-caching/#source-scala_1","text":"persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel storage:StorageLevel.md[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is core:ContextCleaner.md#registerRDDForCleanup[registered to ContextCleaner ] (if available) and SparkContext.md#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#unpersistblocking-boolean-true-thistype","text":"When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls SparkContext.md#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets storage:StorageLevel.md[ NONE storage level] as the current storage level.","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/spark-rdd-lineage/","text":"== RDD Lineage -- Logical Execution Plan RDD Lineage (aka RDD operator graph or RDD dependency graph ) is a graph of all the parent RDDs of a RDD. It is built as a result of applying transformations to the RDD and creates a < >. NOTE: The execution DAG or physical execution plan is the scheduler:DAGScheduler.md[DAG of stages]. NOTE: The following diagram uses cartesian or zip for learning purposes only. You may use other operators to build a RDD graph. .RDD lineage image::rdd-lineage.png[align=\"center\"] The above RDD graph could be the result of the following series of transformations: val r00 = sc.parallelize(0 to 9) val r01 = sc.parallelize(0 to 90 by 10) val r10 = r00 cartesian r01 val r11 = r00.map(n => (n, n)) val r12 = r00 zip r01 val r13 = r01.keyBy(_ / 20) val r20 = Seq(r11, r12, r13).foldLeft(r10)(_ union _) A RDD lineage graph is hence a graph of what transformations need to be executed after an action has been called. You can learn about a RDD lineage graph using < > method. === [[logical-execution-plan]] Logical Execution Plan Logical Execution Plan starts with the earliest RDDs (those with no dependencies on other RDDs or reference cached data) and ends with the RDD that produces the result of the action that has been called to execute. NOTE: A logical plan, i.e. a DAG, is materialized and executed when SparkContext.md#runJob[ SparkContext is requested to run a Spark job]. === [[toDebugString]] Getting RDD Lineage Graph -- toDebugString Method [source, scala] \u00b6 toDebugString: String \u00b6 You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence SparkContext.md#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"RDD Lineage"},{"location":"rdd/spark-rdd-lineage/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-lineage/#todebugstring-string","text":"You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence SparkContext.md#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"toDebugString: String"},{"location":"rdd/spark-rdd-operations/","text":"== Operators - Transformations and Actions RDDs have two types of operations: spark-rdd-transformations.md[transformations] and spark-rdd-actions.md[actions]. NOTE: Operators are also called operations . === Gotchas - things to watch for Even if you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors. See https://issues.apache.org/jira/browse/SPARK-5063","title":"Operators"},{"location":"rdd/spark-rdd-partitions/","text":"== Partitions and Partitioning === Introduction Depending on how you look at Spark (programmer, devop, admin), an RDD is about the content (developer's and data scientist's perspective) or how it gets spread out over a cluster (performance), i.e. how many partitions an RDD represents. A partition (aka split ) is a logical chunk of a large distributed data set. [CAUTION] \u00b6 FIXME How does the number of partitions map to the number of tasks? How to verify it? How does the mapping between partitions and tasks correspond to data locality if any? \u00b6 Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning [TIP] \u00b6 Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?] https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark) \u00b6 By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation [source, scala] \u00b6 repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL spark-data-locality.md[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL spark-data-locality.md[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation [source, scala] \u00b6 coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 The coalesce transformation is used to change the number of partitions. It can trigger spark-rdd-shuffle.md[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use spark-rdd-lineage.md#toDebugString[toDebugString] to check out the spark-rdd-lineage.md[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"Partitions and Partitioning"},{"location":"rdd/spark-rdd-partitions/#caution","text":"FIXME How does the number of partitions map to the number of tasks? How to verify it?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-partitions/#how-does-the-mapping-between-partitions-and-tasks-correspond-to-data-locality-if-any","text":"Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning","title":"How does the mapping between partitions and tasks correspond to data locality if any?"},{"location":"rdd/spark-rdd-partitions/#tip","text":"Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?]","title":"[TIP]"},{"location":"rdd/spark-rdd-partitions/#httpssparkapacheorgdocslatesttuninghtmltuning-spark-the-official-documentation-of-spark","text":"By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation","title":"https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark)"},{"location":"rdd/spark-rdd-partitions/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#repartitionnumpartitions-intimplicit-ord-orderingt-null-rddt","text":"repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL spark-data-locality.md[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL spark-data-locality.md[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation","title":"repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-partitions/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#coalescenumpartitions-int-shuffle-boolean-falseimplicit-ord-orderingt-null-rddt","text":"The coalesce transformation is used to change the number of partitions. It can trigger spark-rdd-shuffle.md[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use spark-rdd-lineage.md#toDebugString[toDebugString] to check out the spark-rdd-lineage.md[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-shuffle/","text":"= RDD shuffling :url-spark-docs: https://spark.apache.org/docs/{spark-version } TIP: Read the official documentation about the topic {url-spark-docs}/rdd-programming-guide.html#shuffle-operations[Shuffle operations]. It is still better than this page. Shuffling is a process of spark-rdd-partitions.md[redistributing data across partitions] (aka repartitioning ) that may or may not cause moving data across JVM processes or even over the wire (between executors on separate machines). Shuffling is the process of data transfer between stages. TIP: Avoid shuffling at all cost. Think about ways to leverage existing partitions. Leverage partial aggregation to reduce data transfer. By default, shuffling doesn't change the number of partitions, but their content. Avoid groupByKey and use reduceByKey or combineByKey instead. ** groupByKey shuffles all the data, which is slow. ** reduceByKey shuffles only the results of sub-aggregations in each partition of the data. == Example - join PairRDD offers http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/programming-guide.html#JoinLink[join ] transformation that (quoting the official documentation): When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Let's have a look at an example and see how it works under the covers: scala> val kv = (0 to 5) zip Stream.continually(5) kv: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,5), (1,5), (2,5), (3,5), (4,5), (5,5)) scala> val kw = (0 to 5) zip Stream.continually(10) kw: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,10), (1,10), (2,10), (3,10), (4,10), (5,10)) scala> val kvR = sc.parallelize(kv) kvR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:26 scala> val kwR = sc.parallelize(kw) kwR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:26 scala> val joined = kvR join kwR joined: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[10] at join at <console>:32 scala> joined.toDebugString res7: String = (8) MapPartitionsRDD[10] at join at <console>:32 [] | MapPartitionsRDD[9] at join at <console>:32 [] | CoGroupedRDD[8] at join at <console>:32 [] +-(8) ParallelCollectionRDD[3] at parallelize at <console>:26 [] +-(8) ParallelCollectionRDD[4] at parallelize at <console>:26 [] It doesn't look good when there is an \"angle\" between \"nodes\" in an operation graph. It appears before the join operation so shuffle is expected. Here is how the job of executing joined.count looks in Web UI. .Executing joined.count image::spark-shuffle-join-webui.png[align=\"center\"] The screenshot of Web UI shows 3 stages with two parallelize to Shuffle Write and count to Shuffle Read. It means shuffling has indeed happened. CAUTION: FIXME Just learnt about sc.range(0, 5) as a shorter version of sc.parallelize(0 to 5) join operation is one of the cogroup operations that uses defaultPartitioner , i.e. walks through spark-rdd-lineage.md[the RDD lineage graph] (sorted by the number of partitions decreasing) and picks the partitioner with positive number of output partitions. Otherwise, it checks configuration-properties.md#spark.default.parallelism[spark.default.parallelism] configuration and if defined picks rdd:HashPartitioner.md[HashPartitioner] with the default parallelism of the scheduler:SchedulerBackend.md[SchedulerBackend]. join is almost CoGroupedRDD.mapValues . CAUTION: FIXME the default parallelism of scheduler backend","title":"Shuffling"},{"location":"rdd/spark-rdd-transformations/","text":"Transformations -- Lazy Operations on RDD (to Create One or More RDDs) \u00b6 Transformations are lazy operations on an rdd:RDD.md[RDD] that create one or many new RDDs. // T and U are Scala types transformation : RDD [ T ] => RDD [ U ] transformation : RDD [ T ] => Seq [ RDD [ U ]] In other words, transformations are functions that take an RDD as the input and produce one or many RDDs as the output. Transformations do not change the input RDD (since rdd:index.md#introduction[RDDs are immutable] and hence cannot be modified), but produce one or more new RDDs by applying the computations they represent. [[methods]] .(Subset of) RDD Transformations (Public API) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregate a| [[aggregate]] [source, scala] \u00b6 aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]] [source, scala] \u00b6 barrier(): RDDBarrier[T] \u00b6 ( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]] [source, scala] \u00b6 cache(): this.type \u00b6 Persists the RDD with the storage:StorageLevel.md#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]] [source, scala] \u00b6 coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]] [source, scala] \u00b6 filter(f: T => Boolean): RDD[T] \u00b6 | flatMap a| [[flatMap]] [source, scala] \u00b6 flatMap U : RDD[U] \u00b6 | map a| [[map]] [source, scala] \u00b6 map U : RDD[U] \u00b6 | mapPartitions a| [[mapPartitions]] [source, scala] \u00b6 mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]] [source, scala] \u00b6 mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]] [source, scala] \u00b6 randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]] [source, scala] \u00b6 ++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]] [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a spark-rdd-lineage.md[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations. [source,scala] \u00b6 scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute spark-rdd-shuffle.md[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex [source, scala] \u00b6 zipWithIndex(): RDD[(T, Long)] \u00b6 zipWithIndex zips this RDD[T] with its element indices. [CAUTION] \u00b6 If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices. [source, scala] \u00b6 val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"Transformations"},{"location":"rdd/spark-rdd-transformations/#transformations-lazy-operations-on-rdd-to-create-one-or-more-rdds","text":"Transformations are lazy operations on an rdd:RDD.md[RDD] that create one or many new RDDs. // T and U are Scala types transformation : RDD [ T ] => RDD [ U ] transformation : RDD [ T ] => Seq [ RDD [ U ]] In other words, transformations are functions that take an RDD as the input and produce one or many RDDs as the output. Transformations do not change the input RDD (since rdd:index.md#introduction[RDDs are immutable] and hence cannot be modified), but produce one or more new RDDs by applying the computations they represent. [[methods]] .(Subset of) RDD Transformations (Public API) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregate a| [[aggregate]]","title":"Transformations -- Lazy Operations on RDD (to Create One or More RDDs)"},{"location":"rdd/spark-rdd-transformations/#source-scala","text":"aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#barrier-rddbarriert","text":"( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]]","title":"barrier(): RDDBarrier[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#cache-thistype","text":"Persists the RDD with the storage:StorageLevel.md#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]]","title":"cache(): this.type"},{"location":"rdd/spark-rdd-transformations/#source-scala_3","text":"coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#filterf-t-boolean-rddt","text":"| flatMap a| [[flatMap]]","title":"filter(f: T =&gt; Boolean): RDD[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#flatmapu-rddu","text":"| map a| [[map]]","title":"flatMapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#mapu-rddu","text":"| mapPartitions a| [[mapPartitions]]","title":"mapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_7","text":"mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_8","text":"mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_9","text":"randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_10","text":"++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_11","text":"persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a spark-rdd-lineage.md[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations.","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#sourcescala","text":"scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute spark-rdd-shuffle.md[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex","title":"[source,scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#zipwithindex-rddt-long","text":"zipWithIndex zips this RDD[T] with its element indices.","title":"zipWithIndex(): RDD[(T, Long)]"},{"location":"rdd/spark-rdd-transformations/#caution","text":"If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices.","title":"[CAUTION]"},{"location":"rdd/spark-rdd-transformations/#source-scala_13","text":"val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"[source, scala]"},{"location":"rest/","text":"= Status REST API -- Monitoring Spark Applications Using REST API Status REST API is a collection of REST endpoints under /api/v1 URI path in the spark-api-UIRoot.md[root containers for application UI information]: [[SparkUI]] spark-webui-SparkUI.md[SparkUI] - Application UI for an active Spark application (i.e. a Spark application that is still running) [[HistoryServer]] spark-history-server:HistoryServer.md[HistoryServer] - Application UI for active and completed Spark applications (i.e. Spark applications that are still running or have already finished) Status REST API uses spark-api-ApiRootResource.md[ApiRootResource] main resource class that registers /api/v1 URI < >. [[paths]] .URI Paths [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Path | Description | [[applications]] applications | [[ApplicationListResource]] Delegates to the spark-api-ApplicationListResource.md[ApplicationListResource] resource class | [[applications_appId]] applications/\\{appId} | [[OneApplicationResource]] Delegates to the spark-api-OneApplicationResource.md[OneApplicationResource] resource class | [[version]] version | Creates a VersionInfo with the current version of Spark |=== Status REST API uses the following components: https://jersey.github.io/[Jersey RESTful Web Services framework] with support for the https://github.com/jax-rs[Java API for RESTful Web Services] (JAX-RS API) https://www.eclipse.org/jetty/[Eclipse Jetty] as the lightweight HTTP server and the https://jcp.org/en/jsr/detail?id=369[Java Servlet] container","title":"Index"},{"location":"rest/AbstractApplicationResource/","text":"== [[AbstractApplicationResource]] AbstractApplicationResource AbstractApplicationResource is a spark-api-BaseAppResource.md[BaseAppResource] with a set of < > that are common across < >. // start spark-shell $ http http://localhost:4040/api/v1/applications HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 257 Content-Type: application/json Date: Tue, 05 Jun 2018 18:46:32 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [ { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-05T15:04:48.328GMT\", \"lastUpdatedEpoch\": 1528211088328, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-05T15:04:48.328GMT\", \"startTimeEpoch\": 1528211088328 } ], \"id\": \"local-1528211089216\", \"name\": \"Spark shell\" } ] $ http http://localhost:4040/api/v1/applications/local-1528211089216/storage/rdd HTTP/1.1 200 OK Content-Length: 3 Content-Type: application/json Date: Tue, 05 Jun 2018 18:48:00 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [] // Execute the following query in spark-shell spark.range(5).cache.count $ http http://localhost:4040/api/v1/applications/local-1528211089216/storage/rdd // output omitted for brevity [[implementations]] .AbstractApplicationResources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | AbstractApplicationResource | Description | spark-api-OneApplicationResource.md[OneApplicationResource] | [[OneApplicationResource]] Handles applications/appId requests | spark-api-OneApplicationAttemptResource.md[OneApplicationAttemptResource] | [[OneApplicationAttemptResource]] |=== [[paths]] .AbstractApplicationResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | allexecutors | GET | < > | environment | GET | < > | executors | GET | < > | jobs | GET | < > | jobs/{jobId: \\\\d+} | GET | < > | logs | GET | < > stages < > | storage/rdd/{rddId: \\\\d+} | GET | < > | [[storage_rdd]] storage/rdd | GET | < > |=== === [[rddList]] rddList Method [source, scala] \u00b6 rddList(): Seq[RDDStorageInfo] \u00b6 rddList ...FIXME NOTE: rddList is used when...FIXME === [[environmentInfo]] environmentInfo Method [source, scala] \u00b6 environmentInfo(): ApplicationEnvironmentInfo \u00b6 environmentInfo ...FIXME NOTE: environmentInfo is used when...FIXME === [[rddData]] rddData Method [source, scala] \u00b6 rddData(@PathParam(\"rddId\") rddId: Int): RDDStorageInfo \u00b6 rddData ...FIXME NOTE: rddData is used when...FIXME === [[allExecutorList]] allExecutorList Method [source, scala] \u00b6 allExecutorList(): Seq[ExecutorSummary] \u00b6 allExecutorList ...FIXME NOTE: allExecutorList is used when...FIXME === [[executorList]] executorList Method [source, scala] \u00b6 executorList(): Seq[ExecutorSummary] \u00b6 executorList ...FIXME NOTE: executorList is used when...FIXME === [[oneJob]] oneJob Method [source, scala] \u00b6 oneJob(@PathParam(\"jobId\") jobId: Int): JobData \u00b6 oneJob ...FIXME NOTE: oneJob is used when...FIXME === [[jobsList]] jobsList Method [source, scala] \u00b6 jobsList(@QueryParam(\"status\") statuses: JList[JobExecutionStatus]): Seq[JobData] \u00b6 jobsList ...FIXME NOTE: jobsList is used when...FIXME","title":"AbstractApplicationResource"},{"location":"rest/AbstractApplicationResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#rddlist-seqrddstorageinfo","text":"rddList ...FIXME NOTE: rddList is used when...FIXME === [[environmentInfo]] environmentInfo Method","title":"rddList(): Seq[RDDStorageInfo]"},{"location":"rest/AbstractApplicationResource/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#environmentinfo-applicationenvironmentinfo","text":"environmentInfo ...FIXME NOTE: environmentInfo is used when...FIXME === [[rddData]] rddData Method","title":"environmentInfo(): ApplicationEnvironmentInfo"},{"location":"rest/AbstractApplicationResource/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#rdddatapathparamrddid-rddid-int-rddstorageinfo","text":"rddData ...FIXME NOTE: rddData is used when...FIXME === [[allExecutorList]] allExecutorList Method","title":"rddData(@PathParam(\"rddId\") rddId: Int): RDDStorageInfo"},{"location":"rest/AbstractApplicationResource/#source-scala_3","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#allexecutorlist-seqexecutorsummary","text":"allExecutorList ...FIXME NOTE: allExecutorList is used when...FIXME === [[executorList]] executorList Method","title":"allExecutorList(): Seq[ExecutorSummary]"},{"location":"rest/AbstractApplicationResource/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#executorlist-seqexecutorsummary","text":"executorList ...FIXME NOTE: executorList is used when...FIXME === [[oneJob]] oneJob Method","title":"executorList(): Seq[ExecutorSummary]"},{"location":"rest/AbstractApplicationResource/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#onejobpathparamjobid-jobid-int-jobdata","text":"oneJob ...FIXME NOTE: oneJob is used when...FIXME === [[jobsList]] jobsList Method","title":"oneJob(@PathParam(\"jobId\") jobId: Int): JobData"},{"location":"rest/AbstractApplicationResource/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#jobslistqueryparamstatus-statuses-jlistjobexecutionstatus-seqjobdata","text":"jobsList ...FIXME NOTE: jobsList is used when...FIXME","title":"jobsList(@QueryParam(\"status\") statuses: JList[JobExecutionStatus]): Seq[JobData]"},{"location":"rest/ApiRequestContext/","text":"== [[ApiRequestContext]] ApiRequestContext ApiRequestContext is the < > of...FIXME [[contract]] [source, scala] package org.apache.spark.status.api.v1 trait ApiRequestContext { // only required methods that have no implementation // the others follow @Context var servletContext: ServletContext = _ @Context var httpRequest: HttpServletRequest = _ } NOTE: ApiRequestContext is a private[v1] contract. .ApiRequestContext Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | httpRequest | [[httpRequest]] Java Servlets' HttpServletRequest Used when...FIXME | servletContext | [[servletContext]] Java Servlets' ServletContext Used when...FIXME |=== [[implementations]] .ApiRequestContexts [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ApiRequestContext | Description | spark-api-ApiRootResource.md[ApiRootResource] | [[ApiRootResource]] | ApiStreamingApp | [[ApiStreamingApp]] | spark-api-ApplicationListResource.md[ApplicationListResource] | [[ApplicationListResource]] | spark-api-BaseAppResource.md[BaseAppResource] | [[BaseAppResource]] | SecurityFilter | [[SecurityFilter]] |=== === [[uiRoot]] Getting Current UIRoot -- uiRoot Method [source, scala] \u00b6 uiRoot: UIRoot \u00b6 uiRoot simply requests UIRootFromServletContext to spark-api-UIRootFromServletContext.md#getUiRoot[get the current UIRoot] (for the given < >). NOTE: uiRoot is used when...FIXME","title":"ApiRequestContext"},{"location":"rest/ApiRequestContext/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/ApiRequestContext/#uiroot-uiroot","text":"uiRoot simply requests UIRootFromServletContext to spark-api-UIRootFromServletContext.md#getUiRoot[get the current UIRoot] (for the given < >). NOTE: uiRoot is used when...FIXME","title":"uiRoot: UIRoot"},{"location":"rest/ApiRootResource/","text":"== [[ApiRootResource]] ApiRootResource -- /api/v1 URI Handler ApiRootResource is the spark-api-ApiRequestContext.md[ApiRequestContext] for the /v1 URI path. ApiRootResource uses @Path(\"/v1\") annotation at the class level. It is a partial URI path template relative to the base URI of the server on which the resource is deployed, the context root of the application, and the URL pattern to which the JAX-RS runtime responds. TIP: Learn more about @Path annotation in https://docs.oracle.com/cd/E19798-01/821-1841/6nmq2cp26/index.html[The @Path Annotation and URI Path Templates]. ApiRootResource < > the /api/* context handler (with the REST resources and providers in org.apache.spark.status.api.v1 package). With the @Path(\"/v1\") annotation and after < > the /api/* context handler, ApiRootResource serves HTTP requests for < > under the /api/v1 URI paths for spark-webui-SparkUI.md#initialize[SparkUI] and spark-history-server:HistoryServer.md#initialize[HistoryServer]. ApiRootResource gives the metrics of a Spark application in JSON format (using JAX-RS API). // start spark-shell $ http http://localhost:4040/api/v1/applications HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 257 Content-Type: application/json Date: Tue, 05 Jun 2018 18:36:16 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [ { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-05T15:04:48.328GMT\", \"lastUpdatedEpoch\": 1528211088328, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-05T15:04:48.328GMT\", \"startTimeEpoch\": 1528211088328 } ], \"id\": \"local-1528211089216\", \"name\": \"Spark shell\" } ] // Fixed in Spark 2.3.1 // https://issues.apache.org/jira/browse/SPARK-24188 $ http http://localhost:4040/api/v1/version HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 43 Content-Type: application/json Date: Thu, 14 Jun 2018 08:19:06 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent { \"spark\": \"2.3.1\" } [[paths]] .ApiRootResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description [[applications]] applications [[ApplicationListResource]] Delegates to the spark-api-ApplicationListResource.md[ApplicationListResource] resource class [[applications_appId]] applications/\\{appId} [[OneApplicationResource]] Delegates to the spark-api-OneApplicationResource.md[OneApplicationResource] resource class | [[version]] version | GET | Creates a VersionInfo with the current version of Spark |=== === [[getServletHandler]] Creating /api/* Context Handler -- getServletHandler Method [source, scala] \u00b6 getServletHandler(uiRoot: UIRoot): ServletContextHandler \u00b6 getServletHandler creates a Jetty ServletContextHandler for /api context path. NOTE: The Jetty ServletContextHandler created does not support HTTP sessions as REST API is stateless. getServletHandler creates a Jetty ServletHolder with the resources and providers in org.apache.spark.status.api.v1 package. It then registers the ServletHolder to serve /* context path (under the ServletContextHandler for /api ). getServletHandler requests UIRootFromServletContext to spark-api-UIRootFromServletContext.md#setUiRoot[setUiRoot] with the ServletContextHandler and the input spark-api-UIRoot.md[UIRoot]. NOTE: getServletHandler is used when spark-webui-SparkUI.md#initialize[SparkUI] and spark-history-server:HistoryServer.md#initialize[HistoryServer] are requested to initialize.","title":"ApiRootResource"},{"location":"rest/ApiRootResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/ApiRootResource/#getservlethandleruiroot-uiroot-servletcontexthandler","text":"getServletHandler creates a Jetty ServletContextHandler for /api context path. NOTE: The Jetty ServletContextHandler created does not support HTTP sessions as REST API is stateless. getServletHandler creates a Jetty ServletHolder with the resources and providers in org.apache.spark.status.api.v1 package. It then registers the ServletHolder to serve /* context path (under the ServletContextHandler for /api ). getServletHandler requests UIRootFromServletContext to spark-api-UIRootFromServletContext.md#setUiRoot[setUiRoot] with the ServletContextHandler and the input spark-api-UIRoot.md[UIRoot]. NOTE: getServletHandler is used when spark-webui-SparkUI.md#initialize[SparkUI] and spark-history-server:HistoryServer.md#initialize[HistoryServer] are requested to initialize.","title":"getServletHandler(uiRoot: UIRoot): ServletContextHandler"},{"location":"rest/ApplicationListResource/","text":"== [[ApplicationListResource]] ApplicationListResource -- applications URI Handler ApplicationListResource is a spark-api-ApiRequestContext.md[ApiRequestContext] that spark-api-ApiRootResource.md#applications[ApiRootResource] uses to handle < > URI path. [[paths]] .ApplicationListResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | [[root]] / | GET | < > |=== // start spark-shell // there should be a single Spark application -- the spark-shell itself $ http http://localhost:4040/api/v1/applications HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 255 Content-Type: application/json Date: Wed, 06 Jun 2018 12:40:33 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [ { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-06T12:30:19.220GMT\", \"lastUpdatedEpoch\": 1528288219220, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-06T12:30:19.220GMT\", \"startTimeEpoch\": 1528288219220 } ], \"id\": \"local-1528288219790\", \"name\": \"Spark shell\" } ] === [[isAttemptInRange]] isAttemptInRange Internal Method [source, scala] \u00b6 isAttemptInRange( attempt: ApplicationAttemptInfo, minStartDate: SimpleDateParam, maxStartDate: SimpleDateParam, minEndDate: SimpleDateParam, maxEndDate: SimpleDateParam, anyRunning: Boolean): Boolean isAttemptInRange ...FIXME NOTE: isAttemptInRange is used exclusively when ApplicationListResource is requested to handle a < > HTTP request. === [[appList]] appList Method [source, scala] \u00b6 appList( @QueryParam(\"status\") status: JList[ApplicationStatus], @DefaultValue(\"2010-01-01\") @QueryParam(\"minDate\") minDate: SimpleDateParam, @DefaultValue(\"3000-01-01\") @QueryParam(\"maxDate\") maxDate: SimpleDateParam, @DefaultValue(\"2010-01-01\") @QueryParam(\"minEndDate\") minEndDate: SimpleDateParam, @DefaultValue(\"3000-01-01\") @QueryParam(\"maxEndDate\") maxEndDate: SimpleDateParam, @QueryParam(\"limit\") limit: Integer) : Iterator[ApplicationInfo] appList ...FIXME NOTE: appList is used when...FIXME","title":"ApplicationListResource"},{"location":"rest/ApplicationListResource/#source-scala","text":"isAttemptInRange( attempt: ApplicationAttemptInfo, minStartDate: SimpleDateParam, maxStartDate: SimpleDateParam, minEndDate: SimpleDateParam, maxEndDate: SimpleDateParam, anyRunning: Boolean): Boolean isAttemptInRange ...FIXME NOTE: isAttemptInRange is used exclusively when ApplicationListResource is requested to handle a < > HTTP request. === [[appList]] appList Method","title":"[source, scala]"},{"location":"rest/ApplicationListResource/#source-scala_1","text":"appList( @QueryParam(\"status\") status: JList[ApplicationStatus], @DefaultValue(\"2010-01-01\") @QueryParam(\"minDate\") minDate: SimpleDateParam, @DefaultValue(\"3000-01-01\") @QueryParam(\"maxDate\") maxDate: SimpleDateParam, @DefaultValue(\"2010-01-01\") @QueryParam(\"minEndDate\") minEndDate: SimpleDateParam, @DefaultValue(\"3000-01-01\") @QueryParam(\"maxEndDate\") maxEndDate: SimpleDateParam, @QueryParam(\"limit\") limit: Integer) : Iterator[ApplicationInfo] appList ...FIXME NOTE: appList is used when...FIXME","title":"[source, scala]"},{"location":"rest/BaseAppResource/","text":"== [[BaseAppResource]] BaseAppResource BaseAppResource is the contract of spark-api-ApiRequestContext.md[ApiRequestContexts] that can < > and use < > and < > path parameters in URI paths. [[path-params]] .BaseAppResource's Path Parameters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appId | [[appId]] @PathParam(\"appId\") Used when...FIXME | attemptId | [[attemptId]] @PathParam(\"attemptId\") Used when...FIXME |=== [[implementations]] .BaseAppResources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | BaseAppResource | Description | spark-api-AbstractApplicationResource.md[AbstractApplicationResource] | [[AbstractApplicationResource]] | BaseStreamingAppResource | [[BaseStreamingAppResource]] | spark-api-StagesResource.md[StagesResource] | [[StagesResource]] |=== NOTE: BaseAppResource is a private[v1] contract. === [[withUI]] withUI Method [source, scala] \u00b6 withUI T : T \u00b6 withUI ...FIXME NOTE: withUI is used when...FIXME","title":"BaseAppResource"},{"location":"rest/BaseAppResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/BaseAppResource/#withuit-t","text":"withUI ...FIXME NOTE: withUI is used when...FIXME","title":"withUIT: T"},{"location":"rest/OneApplicationAttemptResource/","text":"== [[OneApplicationAttemptResource]] OneApplicationAttemptResource OneApplicationAttemptResource is a spark-api-AbstractApplicationResource.md[AbstractApplicationResource] (and so a spark-api-ApiRequestContext.md[ApiRequestContext] indirectly). OneApplicationAttemptResource is used when AbstractApplicationResource is requested to spark-api-AbstractApplicationResource.md#applicationAttempt[applicationAttempt]. [[paths]] .OneApplicationAttemptResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | [[root]] / | GET | < > |=== // start spark-shell // there should be a single Spark application -- the spark-shell itself // CAUTION: FIXME Demo of OneApplicationAttemptResource in Action === [[getAttempt]] getAttempt Method [source, scala] \u00b6 getAttempt(): ApplicationAttemptInfo \u00b6 getAttempt requests the spark-api-ApiRequestContext.md#uiRoot[UIRoot] for the spark-api-UIRoot.md#getApplicationInfo[application info] (given the spark-api-BaseAppResource.md#appId[appId]) and finds the spark-api-BaseAppResource.md#attemptId[attemptId] among the available attempts. NOTE: spark-api-BaseAppResource.md#appId[appId] and spark-api-BaseAppResource.md#attemptId[attemptId] are path parameters. In the end, getAttempt returns the ApplicationAttemptInfo if available or reports a NotFoundException : unknown app [appId], attempt [attemptId]","title":"OneApplicationAttemptResource"},{"location":"rest/OneApplicationAttemptResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/OneApplicationAttemptResource/#getattempt-applicationattemptinfo","text":"getAttempt requests the spark-api-ApiRequestContext.md#uiRoot[UIRoot] for the spark-api-UIRoot.md#getApplicationInfo[application info] (given the spark-api-BaseAppResource.md#appId[appId]) and finds the spark-api-BaseAppResource.md#attemptId[attemptId] among the available attempts. NOTE: spark-api-BaseAppResource.md#appId[appId] and spark-api-BaseAppResource.md#attemptId[attemptId] are path parameters. In the end, getAttempt returns the ApplicationAttemptInfo if available or reports a NotFoundException : unknown app [appId], attempt [attemptId]","title":"getAttempt(): ApplicationAttemptInfo"},{"location":"rest/OneApplicationResource/","text":"== [[OneApplicationResource]] OneApplicationResource -- applications/appId URI Handler OneApplicationResource is a spark-api-AbstractApplicationResource.md[AbstractApplicationResource] (and so a spark-api-ApiRequestContext.md[ApiRequestContext] indirectly) that spark-api-ApiRootResource.md#applications_appId[ApiRootResource] uses to handle < > URI path. [[paths]] .OneApplicationResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | [[root]] / | GET | < > |=== // start spark-shell // there should be a single Spark application -- the spark-shell itself $ http http://localhost:4040/api/v1/applications HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 255 Content-Type: application/json Date: Wed, 06 Jun 2018 12:40:33 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [ { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-06T12:30:19.220GMT\", \"lastUpdatedEpoch\": 1528288219220, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-06T12:30:19.220GMT\", \"startTimeEpoch\": 1528288219220 } ], \"id\": \"local-1528288219790\", \"name\": \"Spark shell\" } ] $ http http://localhost:4040/api/v1/applications/local-1528288219790 HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 255 Content-Type: application/json Date: Wed, 06 Jun 2018 12:41:43 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-06T12:30:19.220GMT\", \"lastUpdatedEpoch\": 1528288219220, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-06T12:30:19.220GMT\", \"startTimeEpoch\": 1528288219220 } ], \"id\": \"local-1528288219790\", \"name\": \"Spark shell\" } === [[getApp]] getApp Method [source, scala] \u00b6 getApp(): ApplicationInfo \u00b6 getApp requests the spark-api-ApiRequestContext.md#uiRoot[UIRoot] for the spark-api-UIRoot.md#getApplicationInfo[application info] (given the spark-api-BaseAppResource.md#appId[appId]). In the end, getApp returns the ApplicationInfo if available or reports a NotFoundException : unknown app: [appId]","title":"OneApplicationResource"},{"location":"rest/OneApplicationResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/OneApplicationResource/#getapp-applicationinfo","text":"getApp requests the spark-api-ApiRequestContext.md#uiRoot[UIRoot] for the spark-api-UIRoot.md#getApplicationInfo[application info] (given the spark-api-BaseAppResource.md#appId[appId]). In the end, getApp returns the ApplicationInfo if available or reports a NotFoundException : unknown app: [appId]","title":"getApp(): ApplicationInfo"},{"location":"rest/StagesResource/","text":"== [[StagesResource]] StagesResource StagesResource is...FIXME [[paths]] .StagesResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | | GET | < > | {stageId: \\d+} | GET | < > | {stageId: \\d+}/{stageAttemptId: \\d+} | GET | < > | {stageId: \\d+}/{stageAttemptId: \\d+}/taskSummary | GET | < > | {stageId: \\d+}/{stageAttemptId: \\d+}/taskList | GET | < > |=== === [[stageList]] stageList Method [source, scala] \u00b6 stageList(@QueryParam(\"status\") statuses: JList[StageStatus]): Seq[StageData] \u00b6 stageList ...FIXME NOTE: stageList is used when...FIXME === [[stageData]] stageData Method [source, scala] \u00b6 stageData( @PathParam(\"stageId\") stageId: Int, @QueryParam(\"details\") @DefaultValue(\"true\") details: Boolean): Seq[StageData] stageData ...FIXME NOTE: stageData is used when...FIXME === [[oneAttemptData]] oneAttemptData Method [source, scala] \u00b6 oneAttemptData( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @QueryParam(\"details\") @DefaultValue(\"true\") details: Boolean): StageData oneAttemptData ...FIXME NOTE: oneAttemptData is used when...FIXME === [[taskSummary]] taskSummary Method [source, scala] \u00b6 taskSummary( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @DefaultValue(\"0.05,0.25,0.5,0.75,0.95\") @QueryParam(\"quantiles\") quantileString: String) : TaskMetricDistributions taskSummary ...FIXME NOTE: taskSummary is used when...FIXME === [[taskList]] taskList Method [source, scala] \u00b6 taskList( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @DefaultValue(\"0\") @QueryParam(\"offset\") offset: Int, @DefaultValue(\"20\") @QueryParam(\"length\") length: Int, @DefaultValue(\"ID\") @QueryParam(\"sortBy\") sortBy: TaskSorting): Seq[TaskData] taskList ...FIXME NOTE: taskList is used when...FIXME","title":"StagesResource"},{"location":"rest/StagesResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/StagesResource/#stagelistqueryparamstatus-statuses-jliststagestatus-seqstagedata","text":"stageList ...FIXME NOTE: stageList is used when...FIXME === [[stageData]] stageData Method","title":"stageList(@QueryParam(\"status\") statuses: JList[StageStatus]): Seq[StageData]"},{"location":"rest/StagesResource/#source-scala_1","text":"stageData( @PathParam(\"stageId\") stageId: Int, @QueryParam(\"details\") @DefaultValue(\"true\") details: Boolean): Seq[StageData] stageData ...FIXME NOTE: stageData is used when...FIXME === [[oneAttemptData]] oneAttemptData Method","title":"[source, scala]"},{"location":"rest/StagesResource/#source-scala_2","text":"oneAttemptData( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @QueryParam(\"details\") @DefaultValue(\"true\") details: Boolean): StageData oneAttemptData ...FIXME NOTE: oneAttemptData is used when...FIXME === [[taskSummary]] taskSummary Method","title":"[source, scala]"},{"location":"rest/StagesResource/#source-scala_3","text":"taskSummary( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @DefaultValue(\"0.05,0.25,0.5,0.75,0.95\") @QueryParam(\"quantiles\") quantileString: String) : TaskMetricDistributions taskSummary ...FIXME NOTE: taskSummary is used when...FIXME === [[taskList]] taskList Method","title":"[source, scala]"},{"location":"rest/StagesResource/#source-scala_4","text":"taskList( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @DefaultValue(\"0\") @QueryParam(\"offset\") offset: Int, @DefaultValue(\"20\") @QueryParam(\"length\") length: Int, @DefaultValue(\"ID\") @QueryParam(\"sortBy\") sortBy: TaskSorting): Seq[TaskData] taskList ...FIXME NOTE: taskList is used when...FIXME","title":"[source, scala]"},{"location":"rest/UIRoot/","text":"== [[UIRoot]] UIRoot -- Contract for Root Contrainers of Application UI Information UIRoot is the < > of the < >. [[contract]] [source, scala] package org.apache.spark.status.api.v1 trait UIRoot { // only required methods that have no implementation // the others follow def withSparkUI T (fn: SparkUI => T): T def getApplicationInfoList: Iterator[ApplicationInfo] def getApplicationInfo(appId: String): Option[ApplicationInfo] def securityManager: SecurityManager } NOTE: UIRoot is a private[spark] contract. .UIRoot Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | getApplicationInfo | [[getApplicationInfo]] Used when...FIXME | getApplicationInfoList | [[getApplicationInfoList]] Used when...FIXME | securityManager | [[securityManager]] Used when...FIXME | withSparkUI | [[withSparkUI]] Used exclusively when BaseAppResource is requested spark-api-BaseAppResource.md#withUI[withUI] |=== [[implementations]] .UIRoots [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | UIRoot | Description | spark-history-server:HistoryServer.md[HistoryServer] | [[HistoryServer]] Application UI for active and completed Spark applications (i.e. Spark applications that are still running or have already finished) | spark-webui-SparkUI.md[SparkUI] | [[SparkUI]] Application UI for an active Spark application (i.e. a Spark application that is still running) |=== === [[writeEventLogs]] writeEventLogs Method [source, scala] \u00b6 writeEventLogs(appId: String, attemptId: Option[String], zipStream: ZipOutputStream): Unit \u00b6 writeEventLogs ...FIXME NOTE: writeEventLogs is used when...FIXME","title":"UIRoot"},{"location":"rest/UIRoot/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/UIRoot/#writeeventlogsappid-string-attemptid-optionstring-zipstream-zipoutputstream-unit","text":"writeEventLogs ...FIXME NOTE: writeEventLogs is used when...FIXME","title":"writeEventLogs(appId: String, attemptId: Option[String], zipStream: ZipOutputStream): Unit"},{"location":"rest/UIRootFromServletContext/","text":"== [[UIRootFromServletContext]] UIRootFromServletContext UIRootFromServletContext manages the current < > object in a Jetty ContextHandler . [[attribute]] UIRootFromServletContext uses its canonical name for the context attribute that is used to < > or < > the current spark-api-UIRoot.md[UIRoot] object (in Jetty's ContextHandler ). NOTE: https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/server/handler/ContextHandler.html[ContextHandler ] is the environment for multiple Jetty Handlers , e.g. URI context path, class loader, static resource base. In essence, UIRootFromServletContext is simply a \"bridge\" between two worlds, Spark's spark-api-UIRoot.md[UIRoot] and Jetty's ContextHandler . === [[setUiRoot]] setUiRoot Method [source, scala] \u00b6 setUiRoot(contextHandler: ContextHandler, uiRoot: UIRoot): Unit \u00b6 setUiRoot ...FIXME NOTE: setUiRoot is used exclusively when ApiRootResource is requested to spark-api-ApiRootResource.md#getServletHandler[register /api/* context handler]. === [[getUiRoot]] getUiRoot Method [source, scala] \u00b6 getUiRoot(context: ServletContext): UIRoot \u00b6 getUiRoot ...FIXME NOTE: getUiRoot is used exclusively when ApiRequestContext is requested for the current spark-api-ApiRequestContext.md#uiRoot[UIRoot].","title":"UIRootFromServletContext"},{"location":"rest/UIRootFromServletContext/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/UIRootFromServletContext/#setuirootcontexthandler-contexthandler-uiroot-uiroot-unit","text":"setUiRoot ...FIXME NOTE: setUiRoot is used exclusively when ApiRootResource is requested to spark-api-ApiRootResource.md#getServletHandler[register /api/* context handler]. === [[getUiRoot]] getUiRoot Method","title":"setUiRoot(contextHandler: ContextHandler, uiRoot: UIRoot): Unit"},{"location":"rest/UIRootFromServletContext/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rest/UIRootFromServletContext/#getuirootcontext-servletcontext-uiroot","text":"getUiRoot ...FIXME NOTE: getUiRoot is used exclusively when ApiRequestContext is requested for the current spark-api-ApiRequestContext.md#uiRoot[UIRoot].","title":"getUiRoot(context: ServletContext): UIRoot"},{"location":"rpc/","text":"RPC System \u00b6 RPC System is a communication system of Spark services. The main abstractions are RpcEnv and RpcEndpoint .","title":"RPC System"},{"location":"rpc/#rpc-system","text":"RPC System is a communication system of Spark services. The main abstractions are RpcEnv and RpcEndpoint .","title":"RPC System"},{"location":"rpc/NettyRpcEnv/","text":"NettyRpcEnv \u00b6 NettyRpcEnv is an RpcEnv that uses Netty ( \"an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients\" ). Creating Instance \u00b6 NettyRpcEnv takes the following to be created: SparkConf JavaSerializerInstance Host Name SecurityManager Number of CPU Cores NettyRpcEnv is created when: NettyRpcEnvFactory is requested to create an RpcEnv","title":"NettyRpcEnv"},{"location":"rpc/NettyRpcEnv/#nettyrpcenv","text":"NettyRpcEnv is an RpcEnv that uses Netty ( \"an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients\" ).","title":"NettyRpcEnv"},{"location":"rpc/NettyRpcEnv/#creating-instance","text":"NettyRpcEnv takes the following to be created: SparkConf JavaSerializerInstance Host Name SecurityManager Number of CPU Cores NettyRpcEnv is created when: NettyRpcEnvFactory is requested to create an RpcEnv","title":"Creating Instance"},{"location":"rpc/NettyRpcEnvFactory/","text":"NettyRpcEnvFactory \u00b6 NettyRpcEnvFactory is an RpcEnvFactory for a Netty-based RpcEnv . Creating RpcEnv \u00b6 create ( config : RpcEnvConfig ): RpcEnv create creates a JavaSerializerInstance (using a JavaSerializer). Note KryoSerializer is not supported. create creates a rpc:NettyRpcEnv.md[] with the JavaSerializerInstance. create uses the given rpc:RpcEnvConfig.md[] for the rpc:RpcEnvConfig.md#advertiseAddress[advertised address], rpc:RpcEnvConfig.md#securityManager[SecurityManager] and rpc:RpcEnvConfig.md#numUsableCores[number of CPU cores]. create returns the NettyRpcEnv unless the rpc:RpcEnvConfig.md#clientMode[clientMode] is turned off ( server mode ). In server mode, create attempts to start the NettyRpcEnv on a given port. create uses the given rpc:RpcEnvConfig.md[] for the rpc:RpcEnvConfig.md#port[port], rpc:RpcEnvConfig.md#bindAddress[bind address], and rpc:RpcEnvConfig.md#name[name]. With the port, the NettyRpcEnv is requested to rpc:NettyRpcEnv.md#startServer[start a server]. create is part of the rpc:RpcEnvFactory.md#create[RpcEnvFactory] abstraction.","title":"NettyRpcEnvFactory"},{"location":"rpc/NettyRpcEnvFactory/#nettyrpcenvfactory","text":"NettyRpcEnvFactory is an RpcEnvFactory for a Netty-based RpcEnv .","title":"NettyRpcEnvFactory"},{"location":"rpc/NettyRpcEnvFactory/#creating-rpcenv","text":"create ( config : RpcEnvConfig ): RpcEnv create creates a JavaSerializerInstance (using a JavaSerializer). Note KryoSerializer is not supported. create creates a rpc:NettyRpcEnv.md[] with the JavaSerializerInstance. create uses the given rpc:RpcEnvConfig.md[] for the rpc:RpcEnvConfig.md#advertiseAddress[advertised address], rpc:RpcEnvConfig.md#securityManager[SecurityManager] and rpc:RpcEnvConfig.md#numUsableCores[number of CPU cores]. create returns the NettyRpcEnv unless the rpc:RpcEnvConfig.md#clientMode[clientMode] is turned off ( server mode ). In server mode, create attempts to start the NettyRpcEnv on a given port. create uses the given rpc:RpcEnvConfig.md[] for the rpc:RpcEnvConfig.md#port[port], rpc:RpcEnvConfig.md#bindAddress[bind address], and rpc:RpcEnvConfig.md#name[name]. With the port, the NettyRpcEnv is requested to rpc:NettyRpcEnv.md#startServer[start a server]. create is part of the rpc:RpcEnvFactory.md#create[RpcEnvFactory] abstraction.","title":" Creating RpcEnv"},{"location":"rpc/NettyStreamManager/","text":"= NettyStreamManager NettyStreamManager is a network:StreamManager.md[]. == [[creating-instance]] Creating Instance NettyStreamManager takes the following to be created: [[rpcEnv]] rpc:NettyRpcEnv.md[] NettyStreamManager is created for rpc:NettyRpcEnv.md#streamManager[NettyRpcEnv]. == [[registerStream]] registerStream Method [source,java] \u00b6 long registerStream( String appId, Iterator buffers) registerStream...FIXME registerStream is used when...FIXME","title":"NettyStreamManager"},{"location":"rpc/NettyStreamManager/#sourcejava","text":"long registerStream( String appId, Iterator buffers) registerStream...FIXME registerStream is used when...FIXME","title":"[source,java]"},{"location":"rpc/RpcAddress/","text":"RpcAddress \u00b6 RpcAddress is a logical address of an RPC system, with hostname and port. RpcAddress can be encoded as a Spark URL in the format of spark://host:port . Creating Instance \u00b6 RpcAddress takes the following to be created: Host Port Creating RpcAddress based on Spark URL \u00b6 fromSparkURL ( sparkUrl : String ): RpcAddress fromSparkURL extract a host and a port from the input Spark URL and creates an RpcAddress . fromSparkURL is used when: StandaloneAppClient is created ClientApp is requested to start Worker is requested to startRpcEnvAndEndpoint","title":"RpcAddress"},{"location":"rpc/RpcAddress/#rpcaddress","text":"RpcAddress is a logical address of an RPC system, with hostname and port. RpcAddress can be encoded as a Spark URL in the format of spark://host:port .","title":"RpcAddress"},{"location":"rpc/RpcAddress/#creating-instance","text":"RpcAddress takes the following to be created: Host Port","title":"Creating Instance"},{"location":"rpc/RpcAddress/#creating-rpcaddress-based-on-spark-url","text":"fromSparkURL ( sparkUrl : String ): RpcAddress fromSparkURL extract a host and a port from the input Spark URL and creates an RpcAddress . fromSparkURL is used when: StandaloneAppClient is created ClientApp is requested to start Worker is requested to startRpcEnvAndEndpoint","title":" Creating RpcAddress based on Spark URL"},{"location":"rpc/RpcEndpoint/","text":"RpcEndpoint \u00b6 RpcEndpoint is an abstraction of RPC endpoints that are registered to an RpcEnv to process one- ( fire-and-forget ) or two-way messages. Contract \u00b6 onConnected \u00b6 onConnected ( remoteAddress : RpcAddress ): Unit Invoked when RpcAddress is connected to the current node Used when: Inbox is requested to process a RemoteProcessConnected message onDisconnected \u00b6 onDisconnected ( remoteAddress : RpcAddress ): Unit Used when: Inbox is requested to process a RemoteProcessDisconnected message onError \u00b6 onError ( cause : Throwable ): Unit Used when: Inbox is requested to process a message that threw a NonFatal exception onNetworkError \u00b6 onNetworkError ( cause : Throwable , remoteAddress : RpcAddress ): Unit Used when: Inbox is requested to process a RemoteProcessConnectionError message onStart \u00b6 onStart (): Unit Used when: Inbox is requested to process an OnStart message onStop \u00b6 onStop (): Unit Used when: Inbox is requested to process an OnStop message Processing One-Way Messages \u00b6 receive : PartialFunction [ Any , Unit ] Used when: Inbox is requested to process an OneWayMessage message Processing Two-Way Messages \u00b6 receiveAndReply ( context : RpcCallContext ): PartialFunction [ Any , Unit ] Used when: Inbox is requested to process a RpcMessage message RpcEnv \u00b6 rpcEnv : RpcEnv RpcEnv this RpcEndpoint is registered to Implementations \u00b6 AMEndpoint IsolatedRpcEndpoint MapOutputTrackerMasterEndpoint OutputCommitCoordinatorEndpoint RpcEndpointVerifier ThreadSafeRpcEndpoint WorkerWatcher self \u00b6 self : RpcEndpointRef self requests the RpcEnv for the RpcEndpointRef of this RpcEndpoint . self throws an IllegalArgumentException when the RpcEnv has not been initialized: rpcEnv has not been initialized Stopping RpcEndpoint \u00b6 stop (): Unit stop requests the RpcEnv to stop this RpcEndpoint","title":"RpcEndpoint"},{"location":"rpc/RpcEndpoint/#rpcendpoint","text":"RpcEndpoint is an abstraction of RPC endpoints that are registered to an RpcEnv to process one- ( fire-and-forget ) or two-way messages.","title":"RpcEndpoint"},{"location":"rpc/RpcEndpoint/#contract","text":"","title":"Contract"},{"location":"rpc/RpcEndpoint/#onconnected","text":"onConnected ( remoteAddress : RpcAddress ): Unit Invoked when RpcAddress is connected to the current node Used when: Inbox is requested to process a RemoteProcessConnected message","title":" onConnected"},{"location":"rpc/RpcEndpoint/#ondisconnected","text":"onDisconnected ( remoteAddress : RpcAddress ): Unit Used when: Inbox is requested to process a RemoteProcessDisconnected message","title":" onDisconnected"},{"location":"rpc/RpcEndpoint/#onerror","text":"onError ( cause : Throwable ): Unit Used when: Inbox is requested to process a message that threw a NonFatal exception","title":" onError"},{"location":"rpc/RpcEndpoint/#onnetworkerror","text":"onNetworkError ( cause : Throwable , remoteAddress : RpcAddress ): Unit Used when: Inbox is requested to process a RemoteProcessConnectionError message","title":" onNetworkError"},{"location":"rpc/RpcEndpoint/#onstart","text":"onStart (): Unit Used when: Inbox is requested to process an OnStart message","title":" onStart"},{"location":"rpc/RpcEndpoint/#onstop","text":"onStop (): Unit Used when: Inbox is requested to process an OnStop message","title":" onStop"},{"location":"rpc/RpcEndpoint/#processing-one-way-messages","text":"receive : PartialFunction [ Any , Unit ] Used when: Inbox is requested to process an OneWayMessage message","title":" Processing One-Way Messages"},{"location":"rpc/RpcEndpoint/#processing-two-way-messages","text":"receiveAndReply ( context : RpcCallContext ): PartialFunction [ Any , Unit ] Used when: Inbox is requested to process a RpcMessage message","title":" Processing Two-Way Messages"},{"location":"rpc/RpcEndpoint/#rpcenv","text":"rpcEnv : RpcEnv RpcEnv this RpcEndpoint is registered to","title":" RpcEnv"},{"location":"rpc/RpcEndpoint/#implementations","text":"AMEndpoint IsolatedRpcEndpoint MapOutputTrackerMasterEndpoint OutputCommitCoordinatorEndpoint RpcEndpointVerifier ThreadSafeRpcEndpoint WorkerWatcher","title":"Implementations"},{"location":"rpc/RpcEndpoint/#self","text":"self : RpcEndpointRef self requests the RpcEnv for the RpcEndpointRef of this RpcEndpoint . self throws an IllegalArgumentException when the RpcEnv has not been initialized: rpcEnv has not been initialized","title":" self"},{"location":"rpc/RpcEndpoint/#stopping-rpcendpoint","text":"stop (): Unit stop requests the RpcEnv to stop this RpcEndpoint","title":" Stopping RpcEndpoint"},{"location":"rpc/RpcEndpointAddress/","text":"= RpcEndpointAddress RpcEndpointAddress is a logical address of an endpoint in an RPC system, with < > and name . RpcEndpointAddress is in the format of spark://[name]@[rpcAddress.host]:[rpcAddress.port] .","title":"RpcEndpointAddress"},{"location":"rpc/RpcEndpointRef/","text":"RpcEndpointRef \u00b6 RpcEndpointRef is a reference to a rpc:RpcEndpoint.md[RpcEndpoint] in a rpc:index.md[RpcEnv]. RpcEndpointRef is a serializable entity and so you can send it over a network or save it for later use (it can however be deserialized using the owning RpcEnv only). A RpcEndpointRef has < > (a Spark URL), and a name. You can send asynchronous one-way messages to the corresponding RpcEndpoint using < > method. You can send a semi-synchronous message, i.e. \"subscribe\" to be notified when a response arrives, using ask method. You can also block the current calling thread for a response using askWithRetry method. spark.rpc.numRetries (default: 3 ) - the number of times to retry connection attempts. spark.rpc.retry.wait (default: 3s ) - the number of milliseconds to wait on each retry. It also uses rpc:index.md#endpoint-lookup-timeout[lookup timeouts]. == [[send]] send Method CAUTION: FIXME == [[askWithRetry]] askWithRetry Method CAUTION: FIXME","title":"RpcEndpointRef"},{"location":"rpc/RpcEndpointRef/#rpcendpointref","text":"RpcEndpointRef is a reference to a rpc:RpcEndpoint.md[RpcEndpoint] in a rpc:index.md[RpcEnv]. RpcEndpointRef is a serializable entity and so you can send it over a network or save it for later use (it can however be deserialized using the owning RpcEnv only). A RpcEndpointRef has < > (a Spark URL), and a name. You can send asynchronous one-way messages to the corresponding RpcEndpoint using < > method. You can send a semi-synchronous message, i.e. \"subscribe\" to be notified when a response arrives, using ask method. You can also block the current calling thread for a response using askWithRetry method. spark.rpc.numRetries (default: 3 ) - the number of times to retry connection attempts. spark.rpc.retry.wait (default: 3s ) - the number of milliseconds to wait on each retry. It also uses rpc:index.md#endpoint-lookup-timeout[lookup timeouts]. == [[send]] send Method CAUTION: FIXME == [[askWithRetry]] askWithRetry Method CAUTION: FIXME","title":"RpcEndpointRef"},{"location":"rpc/RpcEnv/","text":"RpcEnv \u00b6 RpcEnv is an abstraction of RPC environments . Contract \u00b6 address \u00b6 address : RpcAddress RpcAddress of this RPC environments asyncSetupEndpointRefByURI \u00b6 asyncSetupEndpointRefByURI ( uri : String ): Future [ RpcEndpointRef ] Looking up a RpcEndpointRef of the RPC endpoint by URI (asynchronously) Used when: WorkerWatcher is created CoarseGrainedExecutorBackend is requested to onStart RpcEnv is requested to setupEndpointRefByURI awaitTermination \u00b6 awaitTermination (): Unit Blocks the current thread till the RPC environment terminates Used when: SparkEnv is requested to stop ClientApp is requested to start LocalSparkCluster is requested to stop Master and Worker are launched CoarseGrainedExecutorBackend is requested to run deserialize \u00b6 deserialize [ T ]( deserializationAction : () => T ): T Used when: PersistenceEngine is requested to readPersistedData NettyRpcEnv is requested to deserialize endpointRef \u00b6 endpointRef ( endpoint : RpcEndpoint ): RpcEndpointRef Used when: RpcEndpoint is requested for the RpcEndpointRef to itself RpcEnvFileServer \u00b6 fileServer : RpcEnvFileServer RpcEnvFileServer of this RPC environment Used when: SparkContext is requested to addFile , addJar and is created (and registers the REPL's output directory) openChannel \u00b6 openChannel ( uri : String ): ReadableByteChannel Opens a channel to download a file at the given URI Used when: Utils utility is used to doFetchFile ExecutorClassLoader is requested to getClassFileInputStreamFromSparkRPC setupEndpoint \u00b6 setupEndpoint ( name : String , endpoint : RpcEndpoint ): RpcEndpointRef shutdown \u00b6 shutdown (): Unit Shuts down this RPC environment asynchronously (and to make sure this RpcEnv exits successfully, use awaitTermination ) Used when: SparkEnv is requested to stop LocalSparkCluster is requested to stop DriverWrapper is launched CoarseGrainedExecutorBackend is launched NettyRpcEnvFactory is requested to create an RpcEnv (in server mode and failed to assign a port) Stopping RpcEndpointRef \u00b6 stop ( endpoint : RpcEndpointRef ): Unit Used when: SparkContext is requested to stop RpcEndpoint is requested to stop BlockManager is requested to stop in Spark SQL Implementations \u00b6 NettyRpcEnv Creating Instance \u00b6 RpcEnv takes the following to be created: SparkConf RpcEnv is created using RpcEnv.create utility. Abstract Class RpcEnv is an abstract class and cannot be created directly. It is created indirectly for the concrete RpcEnvs . Creating RpcEnv \u00b6 create ( name : String , host : String , port : Int , conf : SparkConf , securityManager : SecurityManager , clientMode : Boolean = false ): RpcEnv // (1) create ( name : String , bindAddress : String , advertiseAddress : String , port : Int , conf : SparkConf , securityManager : SecurityManager , numUsableCores : Int , clientMode : Boolean ): RpcEnv Uses 0 for numUsableCores create creates a NettyRpcEnvFactory and requests it to create an RpcEnv (with a new RpcEnvConfig with all the given arguments). create is used when: SparkEnv utility is requested to create a SparkEnv ( clientMode flag is turned on for executors and off for the driver) With clientMode flag true : CoarseGrainedExecutorBackend is requested to run ClientApp is requested to start Spark Standalone's Master is requested to startRpcEnvAndEndpoint Spark Standalone's Worker is requested to startRpcEnvAndEndpoint DriverWrapper is launched ApplicationMaster (Spark on YARN) is requested to runExecutorLauncher (in client deploy mode) Default Endpoint Lookup Timeout \u00b6 RpcEnv uses the default lookup timeout for...FIXME When a remote endpoint is resolved, a local RPC environment connects to the remote one ( endpoint lookup ). To configure the time needed for the endpoint lookup you can use the following settings. It is a prioritized list of lookup timeout properties (the higher on the list, the more important): spark.rpc.lookupTimeout spark.network.timeout","title":"RpcEnv"},{"location":"rpc/RpcEnv/#rpcenv","text":"RpcEnv is an abstraction of RPC environments .","title":"RpcEnv"},{"location":"rpc/RpcEnv/#contract","text":"","title":"Contract"},{"location":"rpc/RpcEnv/#address","text":"address : RpcAddress RpcAddress of this RPC environments","title":" address"},{"location":"rpc/RpcEnv/#asyncsetupendpointrefbyuri","text":"asyncSetupEndpointRefByURI ( uri : String ): Future [ RpcEndpointRef ] Looking up a RpcEndpointRef of the RPC endpoint by URI (asynchronously) Used when: WorkerWatcher is created CoarseGrainedExecutorBackend is requested to onStart RpcEnv is requested to setupEndpointRefByURI","title":" asyncSetupEndpointRefByURI"},{"location":"rpc/RpcEnv/#awaittermination","text":"awaitTermination (): Unit Blocks the current thread till the RPC environment terminates Used when: SparkEnv is requested to stop ClientApp is requested to start LocalSparkCluster is requested to stop Master and Worker are launched CoarseGrainedExecutorBackend is requested to run","title":" awaitTermination"},{"location":"rpc/RpcEnv/#deserialize","text":"deserialize [ T ]( deserializationAction : () => T ): T Used when: PersistenceEngine is requested to readPersistedData NettyRpcEnv is requested to deserialize","title":" deserialize"},{"location":"rpc/RpcEnv/#endpointref","text":"endpointRef ( endpoint : RpcEndpoint ): RpcEndpointRef Used when: RpcEndpoint is requested for the RpcEndpointRef to itself","title":" endpointRef"},{"location":"rpc/RpcEnv/#rpcenvfileserver","text":"fileServer : RpcEnvFileServer RpcEnvFileServer of this RPC environment Used when: SparkContext is requested to addFile , addJar and is created (and registers the REPL's output directory)","title":" RpcEnvFileServer"},{"location":"rpc/RpcEnv/#openchannel","text":"openChannel ( uri : String ): ReadableByteChannel Opens a channel to download a file at the given URI Used when: Utils utility is used to doFetchFile ExecutorClassLoader is requested to getClassFileInputStreamFromSparkRPC","title":" openChannel"},{"location":"rpc/RpcEnv/#setupendpoint","text":"setupEndpoint ( name : String , endpoint : RpcEndpoint ): RpcEndpointRef","title":" setupEndpoint"},{"location":"rpc/RpcEnv/#shutdown","text":"shutdown (): Unit Shuts down this RPC environment asynchronously (and to make sure this RpcEnv exits successfully, use awaitTermination ) Used when: SparkEnv is requested to stop LocalSparkCluster is requested to stop DriverWrapper is launched CoarseGrainedExecutorBackend is launched NettyRpcEnvFactory is requested to create an RpcEnv (in server mode and failed to assign a port)","title":" shutdown"},{"location":"rpc/RpcEnv/#stopping-rpcendpointref","text":"stop ( endpoint : RpcEndpointRef ): Unit Used when: SparkContext is requested to stop RpcEndpoint is requested to stop BlockManager is requested to stop in Spark SQL","title":" Stopping RpcEndpointRef"},{"location":"rpc/RpcEnv/#implementations","text":"NettyRpcEnv","title":"Implementations"},{"location":"rpc/RpcEnv/#creating-instance","text":"RpcEnv takes the following to be created: SparkConf RpcEnv is created using RpcEnv.create utility. Abstract Class RpcEnv is an abstract class and cannot be created directly. It is created indirectly for the concrete RpcEnvs .","title":"Creating Instance"},{"location":"rpc/RpcEnv/#creating-rpcenv","text":"create ( name : String , host : String , port : Int , conf : SparkConf , securityManager : SecurityManager , clientMode : Boolean = false ): RpcEnv // (1) create ( name : String , bindAddress : String , advertiseAddress : String , port : Int , conf : SparkConf , securityManager : SecurityManager , numUsableCores : Int , clientMode : Boolean ): RpcEnv Uses 0 for numUsableCores create creates a NettyRpcEnvFactory and requests it to create an RpcEnv (with a new RpcEnvConfig with all the given arguments). create is used when: SparkEnv utility is requested to create a SparkEnv ( clientMode flag is turned on for executors and off for the driver) With clientMode flag true : CoarseGrainedExecutorBackend is requested to run ClientApp is requested to start Spark Standalone's Master is requested to startRpcEnvAndEndpoint Spark Standalone's Worker is requested to startRpcEnvAndEndpoint DriverWrapper is launched ApplicationMaster (Spark on YARN) is requested to runExecutorLauncher (in client deploy mode)","title":" Creating RpcEnv"},{"location":"rpc/RpcEnv/#default-endpoint-lookup-timeout","text":"RpcEnv uses the default lookup timeout for...FIXME When a remote endpoint is resolved, a local RPC environment connects to the remote one ( endpoint lookup ). To configure the time needed for the endpoint lookup you can use the following settings. It is a prioritized list of lookup timeout properties (the higher on the list, the more important): spark.rpc.lookupTimeout spark.network.timeout","title":" Default Endpoint Lookup Timeout"},{"location":"rpc/RpcEnvConfig/","text":"= RpcEnvConfig :page-toclevels: -1 [[creating-instance]] RpcEnvConfig is a configuration of an rpc:RpcEnv.md[]: [[conf]] SparkConf.md[] [[name]] System Name [[bindAddress]] Bind Address [[advertiseAddress]] Advertised Address [[port]] Port [[securityManager]] SecurityManager [[numUsableCores]] Number of CPU cores < > RpcEnvConfig is created when RpcEnv utility is used to rpc:RpcEnv.md#create[create an RpcEnv] (using rpc:RpcEnvFactory.md[]). == [[clientMode]] Client Mode When an RPC Environment is initialized core:SparkEnv.md#createDriverEnv[as part of the initialization of the driver] or core:SparkEnv.md#createExecutorEnv[executors] (using RpcEnv.create ), clientMode is false for the driver and true for executors. Copied (almost verbatim) from https://issues.apache.org/jira/browse/SPARK-10997[SPARK-10997 Netty-based RPC env should support a \"client-only\" mode] and the https://github.com/apache/spark/commit/71d1c907dec446db566b19f912159fd8f46deb7d[commit ]: \"Client mode\" means the RPC env will not listen for incoming connections. This allows certain processes in the Spark stack (such as Executors or tha YARN client-mode AM) to act as pure clients when using the netty-based RPC backend, reducing the number of sockets Spark apps need to use and also the number of open ports. The AM connects to the driver in \"client mode\", and that connection is used for all driver -- AM communication, and so the AM is properly notified when the connection goes down. In \"general\", non-YARN case, clientMode flag is therefore enabled for executors and disabled for the driver. In Spark on YARN in spark-deploy-mode.md#client[ client deploy mode], clientMode flag is however enabled explicitly when Spark on YARN's spark-yarn-applicationmaster.md#runExecutorLauncher-sparkYarnAM[ApplicationMaster] creates the sparkYarnAM RPC Environment.","title":"RpcEnvConfig"},{"location":"rpc/RpcEnvFactory/","text":"= RpcEnvFactory RpcEnvFactory is an abstraction of < > to < >. == [[implementations]] Available RpcEnvFactories rpc:NettyRpcEnvFactory.md[] is the default and only known RpcEnvFactory in Apache Spark (as of https://github.com/apache/spark/commit/4f5a24d7e73104771f233af041eeba4f41675974[this commit]). == [[create]] Creating RpcEnv [source,scala] \u00b6 create( config: RpcEnvConfig): RpcEnv create is used when RpcEnv utility is requested to rpc:RpcEnv.md#create[create an RpcEnv].","title":"RpcEnvFactory"},{"location":"rpc/RpcEnvFactory/#sourcescala","text":"create( config: RpcEnvConfig): RpcEnv create is used when RpcEnv utility is requested to rpc:RpcEnv.md#create[create an RpcEnv].","title":"[source,scala]"},{"location":"rpc/RpcEnvFileServer/","text":"= RpcEnvFileServer RpcEnvFileServer is...FIXME","title":"RpcEnvFileServer"},{"location":"rpc/RpcUtils/","text":"RpcUtils \u00b6 Maximum Message Size \u00b6 maxMessageSizeBytes ( conf : SparkConf ): Int maxMessageSizeBytes is the value of spark.rpc.message.maxSize configuration property in bytes (by multiplying the value by 1024 * 1024 ). maxMessageSizeBytes throws an IllegalArgumentException when the value is above 2047 MB: spark.rpc.message.maxSize should not be greater than 2047 MB maxMessageSizeBytes is used when: MapOutputTrackerMaster is requested for the maxRpcMessageSize Executor is requested for the maxDirectResultSize CoarseGrainedSchedulerBackend is requested for the maxRpcMessageSize makeDriverRef \u00b6 makeDriverRef ( name : String , conf : SparkConf , rpcEnv : RpcEnv ): RpcEndpointRef makeDriverRef ...FIXME makeDriverRef is used when: BarrierTaskContext is created SparkEnv utility is used to create a SparkEnv (on executors) Executor is created PluginContextImpl is requested for driverEndpoint","title":"RpcUtils"},{"location":"rpc/RpcUtils/#rpcutils","text":"","title":"RpcUtils"},{"location":"rpc/RpcUtils/#maximum-message-size","text":"maxMessageSizeBytes ( conf : SparkConf ): Int maxMessageSizeBytes is the value of spark.rpc.message.maxSize configuration property in bytes (by multiplying the value by 1024 * 1024 ). maxMessageSizeBytes throws an IllegalArgumentException when the value is above 2047 MB: spark.rpc.message.maxSize should not be greater than 2047 MB maxMessageSizeBytes is used when: MapOutputTrackerMaster is requested for the maxRpcMessageSize Executor is requested for the maxDirectResultSize CoarseGrainedSchedulerBackend is requested for the maxRpcMessageSize","title":" Maximum Message Size"},{"location":"rpc/RpcUtils/#makedriverref","text":"makeDriverRef ( name : String , conf : SparkConf , rpcEnv : RpcEnv ): RpcEndpointRef makeDriverRef ...FIXME makeDriverRef is used when: BarrierTaskContext is created SparkEnv utility is used to create a SparkEnv (on executors) Executor is created PluginContextImpl is requested for driverEndpoint","title":" makeDriverRef"},{"location":"rpc/spark-rpc-netty/","text":"Netty-Based RpcEnv \u00b6 Netty-based RPC Environment is created by NettyRpcEnvFactory when rpc:index.md#settings[spark.rpc] is netty or org.apache.spark.rpc.netty.NettyRpcEnvFactory . NettyRpcEnv is only started on spark-driver.md[the driver]. See < >. The default port to listen to is 7077 . When NettyRpcEnv starts, the following INFO message is printed out in the logs: Successfully started service 'NettyRpcEnv' on port 0. == [[thread-pools]] Thread Pools === shuffle-server-ID EventLoopGroup uses a daemon thread pool called shuffle-server-ID , where ID is a unique integer for NioEventLoopGroup ( NIO ) or EpollEventLoopGroup ( EPOLL ) for the Shuffle server. CAUTION: FIXME Review Netty's NioEventLoopGroup . CAUTION: FIXME Where are SO_BACKLOG , SO_RCVBUF , SO_SNDBUF channel options used? === dispatcher-event-loop-ID NettyRpcEnv's Dispatcher uses the daemon fixed thread pool with < > threads. Thread names are formatted as dispatcher-event-loop-ID , where ID is a unique, sequentially assigned integer. It starts the message processing loop on all of the threads. === netty-rpc-env-timeout NettyRpcEnv uses the daemon single-thread scheduled thread pool netty-rpc-env-timeout . \"netty-rpc-env-timeout\" #87 daemon prio=5 os_prio=31 tid=0x00007f887775a000 nid=0xc503 waiting on condition [0x0000000123397000] === netty-rpc-connection-ID NettyRpcEnv uses the daemon cached thread pool with up to < > threads. Thread names are formatted as netty-rpc-connection-ID , where ID is a unique, sequentially assigned integer. == [[settings]] Settings The Netty-based implementation uses the following properties: spark.rpc.io.mode (default: NIO ) - NIO or EPOLL for low-level IO. NIO is always available, while EPOLL is only available on Linux. NIO uses io.netty.channel.nio.NioEventLoopGroup while EPOLL io.netty.channel.epoll.EpollEventLoopGroup . spark.shuffle.io.numConnectionsPerPeer always equals 1 spark.rpc.io.threads (default: 0 ; maximum: 8 ) - the number of threads to use for the Netty client and server thread pools. ** spark.shuffle.io.serverThreads (default: the value of spark.rpc.io.threads ) ** spark.shuffle.io.clientThreads (default: the value of spark.rpc.io.threads ) spark.rpc.netty.dispatcher.numThreads (default: the number of processors available to JVM) spark.rpc.connect.threads (default: 64 ) - used in cluster mode to communicate with a remote RPC endpoint spark.port.maxRetries (default: 16 or 100 for testing when spark.testing is set) controls the maximum number of binding attempts/retries to a port before giving up. == [[endpoints]] Endpoints endpoint-verifier ( RpcEndpointVerifier ) - a rpc:RpcEndpoint.md[RpcEndpoint] for remote RpcEnvs to query whether an RpcEndpoint exists or not. It uses Dispatcher that keeps track of registered endpoints and responds true / false to CheckExistence message. endpoint-verifier is used to check out whether a given endpoint exists or not before the endpoint's reference is given back to clients. One use case is when an spark-standalone.md#AppClient[AppClient connects to standalone Masters] before it registers the application it acts for. CAUTION: FIXME Who'd like to use endpoint-verifier and how? == Message Dispatcher A message dispatcher is responsible for routing RPC messages to the appropriate endpoint(s). It uses the daemon fixed thread pool dispatcher-event-loop with spark.rpc.netty.dispatcher.numThreads threads for dispatching messages. \"dispatcher-event-loop-0\" #26 daemon prio=5 os_prio=31 tid=0x00007f8877153800 nid=0x7103 waiting on condition [0x000000011f78b000]","title":"spark-rpc-netty"},{"location":"rpc/spark-rpc-netty/#netty-based-rpcenv","text":"Netty-based RPC Environment is created by NettyRpcEnvFactory when rpc:index.md#settings[spark.rpc] is netty or org.apache.spark.rpc.netty.NettyRpcEnvFactory . NettyRpcEnv is only started on spark-driver.md[the driver]. See < >. The default port to listen to is 7077 . When NettyRpcEnv starts, the following INFO message is printed out in the logs: Successfully started service 'NettyRpcEnv' on port 0. == [[thread-pools]] Thread Pools === shuffle-server-ID EventLoopGroup uses a daemon thread pool called shuffle-server-ID , where ID is a unique integer for NioEventLoopGroup ( NIO ) or EpollEventLoopGroup ( EPOLL ) for the Shuffle server. CAUTION: FIXME Review Netty's NioEventLoopGroup . CAUTION: FIXME Where are SO_BACKLOG , SO_RCVBUF , SO_SNDBUF channel options used? === dispatcher-event-loop-ID NettyRpcEnv's Dispatcher uses the daemon fixed thread pool with < > threads. Thread names are formatted as dispatcher-event-loop-ID , where ID is a unique, sequentially assigned integer. It starts the message processing loop on all of the threads. === netty-rpc-env-timeout NettyRpcEnv uses the daemon single-thread scheduled thread pool netty-rpc-env-timeout . \"netty-rpc-env-timeout\" #87 daemon prio=5 os_prio=31 tid=0x00007f887775a000 nid=0xc503 waiting on condition [0x0000000123397000] === netty-rpc-connection-ID NettyRpcEnv uses the daemon cached thread pool with up to < > threads. Thread names are formatted as netty-rpc-connection-ID , where ID is a unique, sequentially assigned integer. == [[settings]] Settings The Netty-based implementation uses the following properties: spark.rpc.io.mode (default: NIO ) - NIO or EPOLL for low-level IO. NIO is always available, while EPOLL is only available on Linux. NIO uses io.netty.channel.nio.NioEventLoopGroup while EPOLL io.netty.channel.epoll.EpollEventLoopGroup . spark.shuffle.io.numConnectionsPerPeer always equals 1 spark.rpc.io.threads (default: 0 ; maximum: 8 ) - the number of threads to use for the Netty client and server thread pools. ** spark.shuffle.io.serverThreads (default: the value of spark.rpc.io.threads ) ** spark.shuffle.io.clientThreads (default: the value of spark.rpc.io.threads ) spark.rpc.netty.dispatcher.numThreads (default: the number of processors available to JVM) spark.rpc.connect.threads (default: 64 ) - used in cluster mode to communicate with a remote RPC endpoint spark.port.maxRetries (default: 16 or 100 for testing when spark.testing is set) controls the maximum number of binding attempts/retries to a port before giving up. == [[endpoints]] Endpoints endpoint-verifier ( RpcEndpointVerifier ) - a rpc:RpcEndpoint.md[RpcEndpoint] for remote RpcEnvs to query whether an RpcEndpoint exists or not. It uses Dispatcher that keeps track of registered endpoints and responds true / false to CheckExistence message. endpoint-verifier is used to check out whether a given endpoint exists or not before the endpoint's reference is given back to clients. One use case is when an spark-standalone.md#AppClient[AppClient connects to standalone Masters] before it registers the application it acts for. CAUTION: FIXME Who'd like to use endpoint-verifier and how? == Message Dispatcher A message dispatcher is responsible for routing RPC messages to the appropriate endpoint(s). It uses the daemon fixed thread pool dispatcher-event-loop with spark.rpc.netty.dispatcher.numThreads threads for dispatching messages. \"dispatcher-event-loop-0\" #26 daemon prio=5 os_prio=31 tid=0x00007f8877153800 nid=0x7103 waiting on condition [0x000000011f78b000]","title":"Netty-Based RpcEnv"},{"location":"scheduler/","text":"Spark Scheduler \u00b6 Spark Scheduler is a core component of Apache Spark that is responsible for scheduling tasks for execution. Spark Scheduler uses the high-level stage-oriented DAGScheduler and the low-level task-oriented TaskScheduler . Resources \u00b6 Deep Dive into the Apache Spark Scheduler by Xingbo Jiang (Databricks)","title":"Spark Scheduler"},{"location":"scheduler/#spark-scheduler","text":"Spark Scheduler is a core component of Apache Spark that is responsible for scheduling tasks for execution. Spark Scheduler uses the high-level stage-oriented DAGScheduler and the low-level task-oriented TaskScheduler .","title":"Spark Scheduler"},{"location":"scheduler/#resources","text":"Deep Dive into the Apache Spark Scheduler by Xingbo Jiang (Databricks)","title":"Resources"},{"location":"scheduler/ActiveJob/","text":"ActiveJob \u00b6 A job (aka action job or active job ) is a top-level work item (computation) submitted to scheduler:DAGScheduler.md[DAGScheduler] to rdd:spark-rdd-actions.md[compute the result of an action] (or for scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]). .RDD actions submit jobs to DAGScheduler image::action-job.png[align=\"center\"] Computing a job is equivalent to computing the partitions of the RDD the action has been executed upon. The number of partitions in a job depends on the type of a stage - scheduler:ResultStage.md[ResultStage] or scheduler:ShuffleMapStage.md[ShuffleMapStage]. A job starts with a single target RDD, but can ultimately include other RDDs that are all part of rdd:spark-rdd-lineage.md[RDD lineage]. The parent stages are the instances of scheduler:ShuffleMapStage.md[ShuffleMapStage]. .Computing a job is computing the partitions of an RDD image::rdd-job-partitions.png[align=\"center\"] NOTE: Note that not all partitions have always to be computed for scheduler:ResultStage.md[ResultStages] for actions like first() and lookup() . Internally, a job is represented by an instance of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala[private[spark ] class org.apache.spark.scheduler.ActiveJob]. [CAUTION] \u00b6 FIXME * Where are instances of ActiveJob used? \u00b6 A job can be one of two logical types (that are only distinguished by an internal finalStage field of ActiveJob ): Map-stage job that computes the map output files for a scheduler:ShuffleMapStage.md[ShuffleMapStage] (for submitMapStage ) before any downstream stages are submitted. + It is also used for scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling], to look at map output statistics before submitting later stages. Result job that computes a scheduler:ResultStage.md[ResultStage] to execute an action. Jobs track how many partitions have already been computed (using finished array of Boolean elements).","title":"ActiveJob"},{"location":"scheduler/ActiveJob/#activejob","text":"A job (aka action job or active job ) is a top-level work item (computation) submitted to scheduler:DAGScheduler.md[DAGScheduler] to rdd:spark-rdd-actions.md[compute the result of an action] (or for scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]). .RDD actions submit jobs to DAGScheduler image::action-job.png[align=\"center\"] Computing a job is equivalent to computing the partitions of the RDD the action has been executed upon. The number of partitions in a job depends on the type of a stage - scheduler:ResultStage.md[ResultStage] or scheduler:ShuffleMapStage.md[ShuffleMapStage]. A job starts with a single target RDD, but can ultimately include other RDDs that are all part of rdd:spark-rdd-lineage.md[RDD lineage]. The parent stages are the instances of scheduler:ShuffleMapStage.md[ShuffleMapStage]. .Computing a job is computing the partitions of an RDD image::rdd-job-partitions.png[align=\"center\"] NOTE: Note that not all partitions have always to be computed for scheduler:ResultStage.md[ResultStages] for actions like first() and lookup() . Internally, a job is represented by an instance of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala[private[spark ] class org.apache.spark.scheduler.ActiveJob].","title":"ActiveJob"},{"location":"scheduler/ActiveJob/#caution","text":"FIXME","title":"[CAUTION]"},{"location":"scheduler/ActiveJob/#where-are-instances-of-activejob-used","text":"A job can be one of two logical types (that are only distinguished by an internal finalStage field of ActiveJob ): Map-stage job that computes the map output files for a scheduler:ShuffleMapStage.md[ShuffleMapStage] (for submitMapStage ) before any downstream stages are submitted. + It is also used for scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling], to look at map output statistics before submitting later stages. Result job that computes a scheduler:ResultStage.md[ResultStage] to execute an action. Jobs track how many partitions have already been computed (using finished array of Boolean elements).","title":"* Where are instances of ActiveJob used?"},{"location":"scheduler/BarrierTaskContext/","text":"BarrierTaskContext -- TaskContext for Barrier Tasks \u00b6 BarrierTaskContext is a concrete TaskContext that is < > exclusively when Task is requested to scheduler:Task.md#run[run] and the task is scheduler:Task.md#isBarrier[isBarrier] (when Executor is requested to executor:Executor.md#launchTask[launch a task (on \"Executor task launch worker\" thread pool) sometime in the future]). [[creating-instance]] [[taskContext]] BarrierTaskContext takes a single TaskContext to be created. === [[barrierCoordinator]] RpcEndpointRef BarrierTaskContext creates an RpcEndpointRef for...FIXME","title":"BarrierTaskContext"},{"location":"scheduler/BarrierTaskContext/#barriertaskcontext-taskcontext-for-barrier-tasks","text":"BarrierTaskContext is a concrete TaskContext that is < > exclusively when Task is requested to scheduler:Task.md#run[run] and the task is scheduler:Task.md#isBarrier[isBarrier] (when Executor is requested to executor:Executor.md#launchTask[launch a task (on \"Executor task launch worker\" thread pool) sometime in the future]). [[creating-instance]] [[taskContext]] BarrierTaskContext takes a single TaskContext to be created. === [[barrierCoordinator]] RpcEndpointRef BarrierTaskContext creates an RpcEndpointRef for...FIXME","title":"BarrierTaskContext -- TaskContext for Barrier Tasks"},{"location":"scheduler/BlacklistTracker/","text":"BlacklistTracker \u00b6 BlacklistTracker is...FIXME","title":"BlacklistTracker"},{"location":"scheduler/BlacklistTracker/#blacklisttracker","text":"BlacklistTracker is...FIXME","title":"BlacklistTracker"},{"location":"scheduler/CoarseGrainedSchedulerBackend/","text":"CoarseGrainedSchedulerBackend \u00b6 CoarseGrainedSchedulerBackend is a base SchedulerBackend for coarse-grained schedulers . CoarseGrainedSchedulerBackend is an ExecutorAllocationClient . CoarseGrainedSchedulerBackend is responsible for requesting resources from a cluster manager for executors that it in turn uses to launch tasks (on CoarseGrainedExecutorBackend ). CoarseGrainedSchedulerBackend holds executors for the duration of the Spark job rather than relinquishing executors whenever a task is done and asking the scheduler to launch a new executor for each new task. CoarseGrainedSchedulerBackend registers CoarseGrainedScheduler RPC Endpoint that executors use for RPC communication. Note Active executors are executors that are not pending to be removed or lost . Implementations \u00b6 KubernetesClusterSchedulerBackend ( Spark on Kubernetes ) MesosCoarseGrainedSchedulerBackend StandaloneSchedulerBackend YarnSchedulerBackend Creating Instance \u00b6 CoarseGrainedSchedulerBackend takes the following to be created: TaskSchedulerImpl RpcEnv Creating DriverEndpoint \u00b6 createDriverEndpoint ( properties : Seq [( String , String )]): DriverEndpoint createDriverEndpoint creates a DriverEndpoint . Note The purpose of createDriverEndpoint is to let CoarseGrainedSchedulerBackends to provide custom implementations (e.g. KubernetesClusterSchedulerBackend ). createDriverEndpoint is used when CoarseGrainedSchedulerBackend is created (and initializes the driverEndpoint internal reference). decommissionExecutors \u00b6 decommissionExecutors ( executorsAndDecomInfo : Array [( String , ExecutorDecommissionInfo )], adjustTargetNumExecutors : Boolean , triggeredByExecutor : Boolean ): Seq [ String ] decommissionExecutors is part of the ExecutorAllocationClient abstraction. decommissionExecutors ...FIXME totalRegisteredExecutors Registry \u00b6 totalRegisteredExecutors : AtomicInteger totalRegisteredExecutors is an internal registry of the number of registered executors (a Java AtomicInteger ). totalRegisteredExecutors starts from 0 . totalRegisteredExecutors is incremented when: DriverEndpoint is requested to handle a RegisterExecutor message totalRegisteredExecutors is decremented when: DriverEndpoint is requested to remove an executor isReady \u00b6 isReady (): Boolean isReady is part of the SchedulerBackend abstraction. isReady ...FIXME Sufficient Resources Registered \u00b6 sufficientResourcesRegistered (): Boolean sufficientResourcesRegistered is true (and is supposed to be overriden by custom CoarseGrainedSchedulerBackends ). Minimum Resources Available Ratio \u00b6 minRegisteredRatio : Double minRegisteredRatio is a ratio of the minimum resources available to the total expected resources for the CoarseGrainedSchedulerBackend to be ready for scheduling tasks (for execution). minRegisteredRatio uses spark.scheduler.minRegisteredResourcesRatio configuration property if defined or defaults to 0.0 . minRegisteredRatio can be between 0.0 and 1.0 (inclusive). minRegisteredRatio is used when: CoarseGrainedSchedulerBackend is requested to isReady StandaloneSchedulerBackend is requested to sufficientResourcesRegistered KubernetesClusterSchedulerBackend is requested to sufficientResourcesRegistered MesosCoarseGrainedSchedulerBackend is requested to sufficientResourcesRegistered YarnSchedulerBackend is requested to sufficientResourcesRegistered DriverEndpoint \u00b6 driverEndpoint : RpcEndpointRef CoarseGrainedSchedulerBackend creates a DriverEndpoint when created . The DriverEndpoint is used to communicate with the driver (by sending RPC messages). Available Executors Registry \u00b6 executorDataMap : HashMap [ String , ExecutorData ] CoarseGrainedSchedulerBackend tracks available executors using executorDataMap registry (of ExecutorData s by executor id). A new entry is added when DriverEndpoint is requested to handle RegisterExecutor message. An entry is removed when DriverEndpoint is requested to handle RemoveExecutor message or a remote host (with one or many executors) disconnects . Revive Messages Scheduler Service \u00b6 reviveThread : ScheduledExecutorService CoarseGrainedSchedulerBackend creates a Java ScheduledExecutorService when created . The ScheduledExecutorService is used by DriverEndpoint RPC Endpoint to post ReviveOffers messages regularly . Maximum Size of RPC Message \u00b6 maxRpcMessageSize is the value of spark.rpc.message.maxSize configuration property. Making Fake Resource Offers on Executors \u00b6 makeOffers (): Unit makeOffers ( executorId : String ): Unit makeOffers takes the active executors (out of the < > internal registry) and creates WorkerOffer resource offers for each (one per executor with the executor's id, host and free cores). CAUTION: Only free cores are considered in making offers. Memory is not! Why?! It then requests TaskSchedulerImpl.md#resourceOffers[ TaskSchedulerImpl to process the resource offers] to create a collection of TaskDescription collections that it in turn uses to launch tasks . Getting Executor Ids \u00b6 When called, getExecutorIds simply returns executor ids from the internal < > registry. NOTE: It is called when SparkContext.md#getExecutorIds[SparkContext calculates executor ids]. Requesting Executors \u00b6 requestExecutors ( numAdditionalExecutors : Int ): Boolean requestExecutors is a \"decorator\" method that ultimately calls a cluster-specific doRequestTotalExecutors method and returns whether the request was acknowledged or not (it is assumed false by default). requestExecutors method is part of the ExecutorAllocationClient abstraction. When called, you should see the following INFO message followed by DEBUG message in the logs: Requesting [numAdditionalExecutors] additional executor(s) from the cluster manager Number of pending executors is now [numPendingExecutors] < > is increased by the input numAdditionalExecutors . requestExecutors requests executors from a cluster manager (that reflects the current computation needs). The \"new executor total\" is a sum of the internal < > and < > decreased by the < >. If numAdditionalExecutors is negative, a IllegalArgumentException is thrown: Attempted to request a negative number of additional executor(s) [numAdditionalExecutors] from the cluster manager. Please specify a positive number! NOTE: It is a final method that no other scheduler backends could customize further. NOTE: The method is a synchronized block that makes multiple concurrent requests be handled in a serial fashion, i.e. one by one. Requesting Exact Number of Executors \u00b6 requestTotalExecutors ( numExecutors : Int , localityAwareTasks : Int , hostToLocalTaskCount : Map [ String , Int ]): Boolean requestTotalExecutors is a \"decorator\" method that ultimately calls a cluster-specific doRequestTotalExecutors method and returns whether the request was acknowledged or not (it is assumed false by default). requestTotalExecutors is part of the ExecutorAllocationClient abstraction. It sets the internal < > and < > registries. It then calculates the exact number of executors which is the input numExecutors and the < > decreased by the number of < >. If numExecutors is negative, a IllegalArgumentException is thrown: Attempted to request a negative number of executor(s) [numExecutors] from the cluster manager. Please specify a positive number! NOTE: It is a final method that no other scheduler backends could customize further. NOTE: The method is a synchronized block that makes multiple concurrent requests be handled in a serial fashion, i.e. one by one. Finding Default Level of Parallelism \u00b6 defaultParallelism (): Int defaultParallelism is part of the SchedulerBackend abstraction. defaultParallelism is spark.default.parallelism configuration property if defined. Otherwise, defaultParallelism is the maximum of totalCoreCount or 2 . Killing Task \u00b6 killTask ( taskId : Long , executorId : String , interruptThread : Boolean ): Unit killTask is part of the SchedulerBackend abstraction. killTask simply sends a KillTask message to < >. Stopping All Executors \u00b6 stopExecutors sends a blocking < > message to < > (if already initialized). NOTE: It is called exclusively while CoarseGrainedSchedulerBackend is < >. You should see the following INFO message in the logs: Shutting down all executors Reset State \u00b6 reset resets the internal state: Sets < > to 0 Clears executorsPendingToRemove Sends a blocking < > message to < > for every executor (in the internal executorDataMap ) to inform it about SlaveLost with the message: + Stale executor after cluster manager re-registered. reset is a method that is defined in CoarseGrainedSchedulerBackend , but used and overriden exclusively by yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend]. Remove Executor \u00b6 removeExecutor ( executorId : String , reason : ExecutorLossReason ) removeExecutor sends a blocking < > message to < >. NOTE: It is called by subclasses spark-standalone.md#SparkDeploySchedulerBackend[SparkDeploySchedulerBackend], spark-mesos/spark-mesos.md#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend], and yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend]. CoarseGrainedScheduler RPC Endpoint \u00b6 When < >, it registers CoarseGrainedScheduler RPC endpoint to be the driver's communication endpoint. driverEndpoint is a DriverEndpoint . Note CoarseGrainedSchedulerBackend is created while SparkContext is being created that in turn lives inside a Spark driver . That explains the name driverEndpoint (at least partially). It is called standalone scheduler's driver endpoint internally. It tracks: It uses driver-revive-thread daemon single-thread thread pool for ...FIXME CAUTION: FIXME A potential issue with driverEndpoint.asInstanceOf[NettyRpcEndpointRef].toURI - doubles spark:// prefix. Starting CoarseGrainedSchedulerBackend \u00b6 start (): Unit start is part of the SchedulerBackend abstraction. start takes all spark. -prefixed properties and registers the < CoarseGrainedScheduler RPC endpoint>> (backed by DriverEndpoint ThreadSafeRpcEndpoint ). NOTE: start uses < > to access the current SparkContext.md[SparkContext] and in turn SparkConf.md[SparkConf]. NOTE: start uses < > that was given when < CoarseGrainedSchedulerBackend was created>>. Checking If Sufficient Compute Resources Available Or Waiting Time PassedMethod \u00b6 isReady (): Boolean isReady is part of the SchedulerBackend abstraction. isReady allows to delay task launching until < > or < > passes. Internally, isReady < >. NOTE: < > by default responds that sufficient resources are available. If the < >, you should see the following INFO message in the logs and isReady is positive. SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: [minRegisteredRatio] If there are no sufficient resources available yet (the above requirement does not hold), isReady checks whether the time since < > passed < > to give a way to launch tasks (even when < > not being reached yet). You should see the following INFO message in the logs and isReady is positive. SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: [maxRegisteredWaitingTimeMs](ms) Otherwise, when < > and < > has not elapsed, isReady is negative. Reviving Resource Offers \u00b6 reviveOffers (): Unit reviveOffers is part of the SchedulerBackend abstraction. reviveOffers simply sends a ReviveOffers message to CoarseGrainedSchedulerBackend RPC endpoint . Stopping SchedulerBackend \u00b6 stop (): Unit stop is part of the SchedulerBackend abstraction. stop < > and < CoarseGrainedScheduler RPC endpoint>> (by sending a blocking StopDriver message). In case of any Exception , stop reports a SparkException with the message: Error stopping standalone scheduler's driver endpoint createDriverEndpointRef \u00b6 createDriverEndpointRef ( properties : ArrayBuffer [( String , String )]): RpcEndpointRef createDriverEndpointRef < > and rpc:index.md#setupEndpoint[registers it] as CoarseGrainedScheduler . createDriverEndpointRef is used when CoarseGrainedSchedulerBackend is requested to < >. Checking Whether Executor is Active \u00b6 isExecutorActive ( id : String ): Boolean isExecutorActive is part of the ExecutorAllocationClient abstraction. isExecutorActive ...FIXME Requesting Executors from Cluster Manager \u00b6 doRequestTotalExecutors ( requestedTotal : Int ): Future [ Boolean ] doRequestTotalExecutors returns a completed Future with false value. doRequestTotalExecutors is used when: CoarseGrainedSchedulerBackend is requested to requestExecutors , requestTotalExecutors and killExecutors Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend=ALL Refer to Logging . Internal Properties \u00b6 [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description [[currentExecutorIdCounter]] currentExecutorIdCounter The last (highest) identifier of all < >. Used exclusively in yarn/spark-yarn-cluster-YarnSchedulerEndpoint.md#RetrieveLastAllocatedExecutorId[ YarnSchedulerEndpoint to respond to RetrieveLastAllocatedExecutorId message]. | [[createTime]] createTime | Current time | The time < CoarseGrainedSchedulerBackend was created>>. | [[defaultAskTimeout]] defaultAskTimeout | rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout] or rpc:index.md#spark.network.timeout[spark.network.timeout] or 120s | Default timeout for blocking RPC messages ( aka ask messages). | [[driverEndpoint]] driverEndpoint | (uninitialized) a| rpc:RpcEndpointRef.md[RPC endpoint reference] to CoarseGrainedScheduler RPC endpoint (with DriverEndpoint as the message handler). Initialized when CoarseGrainedSchedulerBackend < >. Used when CoarseGrainedSchedulerBackend executes the following (asynchronously, i.e. on a separate thread): < > < > < > < > < > < > | [[executorsPendingToRemove]] executorsPendingToRemove | empty | Executors marked as removed but the confirmation from a cluster manager has not arrived yet. | [[hostToLocalTaskCount]] hostToLocalTaskCount | empty | Registry of hostnames and possible number of task running on them. | [[localityAwareTasks]] localityAwareTasks | 0 | Number of pending tasks...FIXME | [[maxRegisteredWaitingTimeMs]] maxRegisteredWaitingTimeMs | < > | | [[numPendingExecutors]] numPendingExecutors | 0 | | [[totalCoreCount]] totalCoreCount | 0 | Total number of CPU cores, i.e. the sum of all the cores on all executors. |===","title":"CoarseGrainedSchedulerBackend"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#coarsegrainedschedulerbackend","text":"CoarseGrainedSchedulerBackend is a base SchedulerBackend for coarse-grained schedulers . CoarseGrainedSchedulerBackend is an ExecutorAllocationClient . CoarseGrainedSchedulerBackend is responsible for requesting resources from a cluster manager for executors that it in turn uses to launch tasks (on CoarseGrainedExecutorBackend ). CoarseGrainedSchedulerBackend holds executors for the duration of the Spark job rather than relinquishing executors whenever a task is done and asking the scheduler to launch a new executor for each new task. CoarseGrainedSchedulerBackend registers CoarseGrainedScheduler RPC Endpoint that executors use for RPC communication. Note Active executors are executors that are not pending to be removed or lost .","title":"CoarseGrainedSchedulerBackend"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#implementations","text":"KubernetesClusterSchedulerBackend ( Spark on Kubernetes ) MesosCoarseGrainedSchedulerBackend StandaloneSchedulerBackend YarnSchedulerBackend","title":"Implementations"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#creating-instance","text":"CoarseGrainedSchedulerBackend takes the following to be created: TaskSchedulerImpl RpcEnv","title":"Creating Instance"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#creating-driverendpoint","text":"createDriverEndpoint ( properties : Seq [( String , String )]): DriverEndpoint createDriverEndpoint creates a DriverEndpoint . Note The purpose of createDriverEndpoint is to let CoarseGrainedSchedulerBackends to provide custom implementations (e.g. KubernetesClusterSchedulerBackend ). createDriverEndpoint is used when CoarseGrainedSchedulerBackend is created (and initializes the driverEndpoint internal reference).","title":" Creating DriverEndpoint"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#decommissionexecutors","text":"decommissionExecutors ( executorsAndDecomInfo : Array [( String , ExecutorDecommissionInfo )], adjustTargetNumExecutors : Boolean , triggeredByExecutor : Boolean ): Seq [ String ] decommissionExecutors is part of the ExecutorAllocationClient abstraction. decommissionExecutors ...FIXME","title":" decommissionExecutors"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#totalregisteredexecutors-registry","text":"totalRegisteredExecutors : AtomicInteger totalRegisteredExecutors is an internal registry of the number of registered executors (a Java AtomicInteger ). totalRegisteredExecutors starts from 0 . totalRegisteredExecutors is incremented when: DriverEndpoint is requested to handle a RegisterExecutor message totalRegisteredExecutors is decremented when: DriverEndpoint is requested to remove an executor","title":" totalRegisteredExecutors Registry"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#isready","text":"isReady (): Boolean isReady is part of the SchedulerBackend abstraction. isReady ...FIXME","title":" isReady"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#sufficient-resources-registered","text":"sufficientResourcesRegistered (): Boolean sufficientResourcesRegistered is true (and is supposed to be overriden by custom CoarseGrainedSchedulerBackends ).","title":" Sufficient Resources Registered"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#minimum-resources-available-ratio","text":"minRegisteredRatio : Double minRegisteredRatio is a ratio of the minimum resources available to the total expected resources for the CoarseGrainedSchedulerBackend to be ready for scheduling tasks (for execution). minRegisteredRatio uses spark.scheduler.minRegisteredResourcesRatio configuration property if defined or defaults to 0.0 . minRegisteredRatio can be between 0.0 and 1.0 (inclusive). minRegisteredRatio is used when: CoarseGrainedSchedulerBackend is requested to isReady StandaloneSchedulerBackend is requested to sufficientResourcesRegistered KubernetesClusterSchedulerBackend is requested to sufficientResourcesRegistered MesosCoarseGrainedSchedulerBackend is requested to sufficientResourcesRegistered YarnSchedulerBackend is requested to sufficientResourcesRegistered","title":" Minimum Resources Available Ratio"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#driverendpoint","text":"driverEndpoint : RpcEndpointRef CoarseGrainedSchedulerBackend creates a DriverEndpoint when created . The DriverEndpoint is used to communicate with the driver (by sending RPC messages).","title":" DriverEndpoint"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#available-executors-registry","text":"executorDataMap : HashMap [ String , ExecutorData ] CoarseGrainedSchedulerBackend tracks available executors using executorDataMap registry (of ExecutorData s by executor id). A new entry is added when DriverEndpoint is requested to handle RegisterExecutor message. An entry is removed when DriverEndpoint is requested to handle RemoveExecutor message or a remote host (with one or many executors) disconnects .","title":" Available Executors Registry"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#revive-messages-scheduler-service","text":"reviveThread : ScheduledExecutorService CoarseGrainedSchedulerBackend creates a Java ScheduledExecutorService when created . The ScheduledExecutorService is used by DriverEndpoint RPC Endpoint to post ReviveOffers messages regularly .","title":" Revive Messages Scheduler Service"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#maximum-size-of-rpc-message","text":"maxRpcMessageSize is the value of spark.rpc.message.maxSize configuration property.","title":" Maximum Size of RPC Message"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#making-fake-resource-offers-on-executors","text":"makeOffers (): Unit makeOffers ( executorId : String ): Unit makeOffers takes the active executors (out of the < > internal registry) and creates WorkerOffer resource offers for each (one per executor with the executor's id, host and free cores). CAUTION: Only free cores are considered in making offers. Memory is not! Why?! It then requests TaskSchedulerImpl.md#resourceOffers[ TaskSchedulerImpl to process the resource offers] to create a collection of TaskDescription collections that it in turn uses to launch tasks .","title":" Making Fake Resource Offers on Executors"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#getting-executor-ids","text":"When called, getExecutorIds simply returns executor ids from the internal < > registry. NOTE: It is called when SparkContext.md#getExecutorIds[SparkContext calculates executor ids].","title":" Getting Executor Ids"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#requesting-executors","text":"requestExecutors ( numAdditionalExecutors : Int ): Boolean requestExecutors is a \"decorator\" method that ultimately calls a cluster-specific doRequestTotalExecutors method and returns whether the request was acknowledged or not (it is assumed false by default). requestExecutors method is part of the ExecutorAllocationClient abstraction. When called, you should see the following INFO message followed by DEBUG message in the logs: Requesting [numAdditionalExecutors] additional executor(s) from the cluster manager Number of pending executors is now [numPendingExecutors] < > is increased by the input numAdditionalExecutors . requestExecutors requests executors from a cluster manager (that reflects the current computation needs). The \"new executor total\" is a sum of the internal < > and < > decreased by the < >. If numAdditionalExecutors is negative, a IllegalArgumentException is thrown: Attempted to request a negative number of additional executor(s) [numAdditionalExecutors] from the cluster manager. Please specify a positive number! NOTE: It is a final method that no other scheduler backends could customize further. NOTE: The method is a synchronized block that makes multiple concurrent requests be handled in a serial fashion, i.e. one by one.","title":" Requesting Executors"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#requesting-exact-number-of-executors","text":"requestTotalExecutors ( numExecutors : Int , localityAwareTasks : Int , hostToLocalTaskCount : Map [ String , Int ]): Boolean requestTotalExecutors is a \"decorator\" method that ultimately calls a cluster-specific doRequestTotalExecutors method and returns whether the request was acknowledged or not (it is assumed false by default). requestTotalExecutors is part of the ExecutorAllocationClient abstraction. It sets the internal < > and < > registries. It then calculates the exact number of executors which is the input numExecutors and the < > decreased by the number of < >. If numExecutors is negative, a IllegalArgumentException is thrown: Attempted to request a negative number of executor(s) [numExecutors] from the cluster manager. Please specify a positive number! NOTE: It is a final method that no other scheduler backends could customize further. NOTE: The method is a synchronized block that makes multiple concurrent requests be handled in a serial fashion, i.e. one by one.","title":" Requesting Exact Number of Executors"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#finding-default-level-of-parallelism","text":"defaultParallelism (): Int defaultParallelism is part of the SchedulerBackend abstraction. defaultParallelism is spark.default.parallelism configuration property if defined. Otherwise, defaultParallelism is the maximum of totalCoreCount or 2 .","title":" Finding Default Level of Parallelism"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#killing-task","text":"killTask ( taskId : Long , executorId : String , interruptThread : Boolean ): Unit killTask is part of the SchedulerBackend abstraction. killTask simply sends a KillTask message to < >.","title":" Killing Task"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#stopping-all-executors","text":"stopExecutors sends a blocking < > message to < > (if already initialized). NOTE: It is called exclusively while CoarseGrainedSchedulerBackend is < >. You should see the following INFO message in the logs: Shutting down all executors","title":" Stopping All Executors"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#reset-state","text":"reset resets the internal state: Sets < > to 0 Clears executorsPendingToRemove Sends a blocking < > message to < > for every executor (in the internal executorDataMap ) to inform it about SlaveLost with the message: + Stale executor after cluster manager re-registered. reset is a method that is defined in CoarseGrainedSchedulerBackend , but used and overriden exclusively by yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend].","title":" Reset State"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#remove-executor","text":"removeExecutor ( executorId : String , reason : ExecutorLossReason ) removeExecutor sends a blocking < > message to < >. NOTE: It is called by subclasses spark-standalone.md#SparkDeploySchedulerBackend[SparkDeploySchedulerBackend], spark-mesos/spark-mesos.md#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend], and yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend].","title":" Remove Executor"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#coarsegrainedscheduler-rpc-endpoint","text":"When < >, it registers CoarseGrainedScheduler RPC endpoint to be the driver's communication endpoint. driverEndpoint is a DriverEndpoint . Note CoarseGrainedSchedulerBackend is created while SparkContext is being created that in turn lives inside a Spark driver . That explains the name driverEndpoint (at least partially). It is called standalone scheduler's driver endpoint internally. It tracks: It uses driver-revive-thread daemon single-thread thread pool for ...FIXME CAUTION: FIXME A potential issue with driverEndpoint.asInstanceOf[NettyRpcEndpointRef].toURI - doubles spark:// prefix.","title":" CoarseGrainedScheduler RPC Endpoint"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#starting-coarsegrainedschedulerbackend","text":"start (): Unit start is part of the SchedulerBackend abstraction. start takes all spark. -prefixed properties and registers the < CoarseGrainedScheduler RPC endpoint>> (backed by DriverEndpoint ThreadSafeRpcEndpoint ). NOTE: start uses < > to access the current SparkContext.md[SparkContext] and in turn SparkConf.md[SparkConf]. NOTE: start uses < > that was given when < CoarseGrainedSchedulerBackend was created>>.","title":" Starting CoarseGrainedSchedulerBackend"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#checking-if-sufficient-compute-resources-available-or-waiting-time-passedmethod","text":"isReady (): Boolean isReady is part of the SchedulerBackend abstraction. isReady allows to delay task launching until < > or < > passes. Internally, isReady < >. NOTE: < > by default responds that sufficient resources are available. If the < >, you should see the following INFO message in the logs and isReady is positive. SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: [minRegisteredRatio] If there are no sufficient resources available yet (the above requirement does not hold), isReady checks whether the time since < > passed < > to give a way to launch tasks (even when < > not being reached yet). You should see the following INFO message in the logs and isReady is positive. SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: [maxRegisteredWaitingTimeMs](ms) Otherwise, when < > and < > has not elapsed, isReady is negative.","title":" Checking If Sufficient Compute Resources Available Or Waiting Time PassedMethod"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#reviving-resource-offers","text":"reviveOffers (): Unit reviveOffers is part of the SchedulerBackend abstraction. reviveOffers simply sends a ReviveOffers message to CoarseGrainedSchedulerBackend RPC endpoint .","title":" Reviving Resource Offers"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#stopping-schedulerbackend","text":"stop (): Unit stop is part of the SchedulerBackend abstraction. stop < > and < CoarseGrainedScheduler RPC endpoint>> (by sending a blocking StopDriver message). In case of any Exception , stop reports a SparkException with the message: Error stopping standalone scheduler's driver endpoint","title":" Stopping SchedulerBackend"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#createdriverendpointref","text":"createDriverEndpointRef ( properties : ArrayBuffer [( String , String )]): RpcEndpointRef createDriverEndpointRef < > and rpc:index.md#setupEndpoint[registers it] as CoarseGrainedScheduler . createDriverEndpointRef is used when CoarseGrainedSchedulerBackend is requested to < >.","title":" createDriverEndpointRef"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#checking-whether-executor-is-active","text":"isExecutorActive ( id : String ): Boolean isExecutorActive is part of the ExecutorAllocationClient abstraction. isExecutorActive ...FIXME","title":" Checking Whether Executor is Active"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#requesting-executors-from-cluster-manager","text":"doRequestTotalExecutors ( requestedTotal : Int ): Future [ Boolean ] doRequestTotalExecutors returns a completed Future with false value. doRequestTotalExecutors is used when: CoarseGrainedSchedulerBackend is requested to requestExecutors , requestTotalExecutors and killExecutors","title":" Requesting Executors from Cluster Manager"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#internal-properties","text":"[cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description [[currentExecutorIdCounter]] currentExecutorIdCounter The last (highest) identifier of all < >. Used exclusively in yarn/spark-yarn-cluster-YarnSchedulerEndpoint.md#RetrieveLastAllocatedExecutorId[ YarnSchedulerEndpoint to respond to RetrieveLastAllocatedExecutorId message]. | [[createTime]] createTime | Current time | The time < CoarseGrainedSchedulerBackend was created>>. | [[defaultAskTimeout]] defaultAskTimeout | rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout] or rpc:index.md#spark.network.timeout[spark.network.timeout] or 120s | Default timeout for blocking RPC messages ( aka ask messages). | [[driverEndpoint]] driverEndpoint | (uninitialized) a| rpc:RpcEndpointRef.md[RPC endpoint reference] to CoarseGrainedScheduler RPC endpoint (with DriverEndpoint as the message handler). Initialized when CoarseGrainedSchedulerBackend < >. Used when CoarseGrainedSchedulerBackend executes the following (asynchronously, i.e. on a separate thread): < > < > < > < > < > < > | [[executorsPendingToRemove]] executorsPendingToRemove | empty | Executors marked as removed but the confirmation from a cluster manager has not arrived yet. | [[hostToLocalTaskCount]] hostToLocalTaskCount | empty | Registry of hostnames and possible number of task running on them. | [[localityAwareTasks]] localityAwareTasks | 0 | Number of pending tasks...FIXME | [[maxRegisteredWaitingTimeMs]] maxRegisteredWaitingTimeMs | < > | | [[numPendingExecutors]] numPendingExecutors | 0 | | [[totalCoreCount]] totalCoreCount | 0 | Total number of CPU cores, i.e. the sum of all the cores on all executors. |===","title":"Internal Properties"},{"location":"scheduler/CompressedMapStatus/","text":"CompressedMapStatus \u00b6 CompressedMapStatus is...FIXME","title":"CompressedMapStatus"},{"location":"scheduler/CompressedMapStatus/#compressedmapstatus","text":"CompressedMapStatus is...FIXME","title":"CompressedMapStatus"},{"location":"scheduler/DAGScheduler/","text":"DAGScheduler \u00b6 Note The introduction that follows was highly influenced by the scaladoc of org.apache.spark.scheduler.DAGScheduler . As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read the sources and only then read this and the related pages afterwards. Introduction \u00b6 DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling using Jobs and Stages . DAGScheduler transforms a logical execution plan ( RDD lineage of dependencies built using RDD transformations ) to a physical execution plan (using stages ). After an action has been called on an RDD , SparkContext hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as TaskSets for execution. DAGScheduler works solely on the driver and is created as part of SparkContext's initialization (right after TaskScheduler and SchedulerBackend are ready). DAGScheduler does three things in Spark: Computes an execution DAG (DAG of stages) for a job Determines the preferred locations to run each task on Handles failures due to shuffle output files being lost DAGScheduler computes a directed acyclic graph (DAG) of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to TaskScheduler . In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to TaskScheduler . DAGScheduler tracks which rdd/spark-rdd-caching.md[RDDs are cached (or persisted)] to avoid \"recomputing\" them, i.e. redoing the map side of a shuffle. DAGScheduler remembers what ShuffleMapStage.md[ShuffleMapStage]s have already produced output files (that are stored in BlockManager s). DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of a RDD. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage. DAGScheduler uses an event queue architecture in which a thread can post DAGSchedulerEvent events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section < >. DAGScheduler runs stages in topological order. DAGScheduler uses SparkContext , TaskScheduler , LiveListenerBus.md[], MapOutputTracker.md[MapOutputTracker] and storage:BlockManager.md[BlockManager] for its services. However, at the very minimum, DAGScheduler takes a SparkContext only (and requests SparkContext for the other services). When DAGScheduler schedules a job as a result of rdd/index.md#actions[executing an action on a RDD] or calling SparkContext.runJob() method directly , it spawns parallel tasks to compute (partial) results per partition. Creating Instance \u00b6 DAGScheduler takes the following to be created: SparkContext TaskScheduler LiveListenerBus MapOutputTrackerMaster BlockManagerMaster SparkEnv Clock DAGScheduler is created when SparkContext is created. While being created, DAGScheduler requests the TaskScheduler to associate itself with and requests DAGScheduler Event Bus to start accepting events. DAGSchedulerSource \u00b6 DAGScheduler uses DAGSchedulerSource for performance metrics. DAGScheduler Event Bus \u00b6 DAGScheduler uses an event bus to process scheduling events on a separate thread (one by one and asynchronously). DAGScheduler requests the event bus to start right when created and stops it when requested to stop . DAGScheduler defines event-posting methods for posting DAGSchedulerEvent events to the event bus. TaskScheduler \u00b6 DAGScheduler is given a TaskScheduler when created . TaskScheduler is used for the following: Submitting missing tasks of a stage Handling task completion (CompletionEvent) Killing a task Failing a job and all other independent single-job stages Stopping itself Running Job \u00b6 runJob [ T , U ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], callSite : CallSite , resultHandler : ( Int , U ) => Unit , properties : Properties ): Unit runJob submits a job and waits until a result is available. runJob prints out the following INFO message to the logs when the job has finished successfully: Job [jobId] finished: [callSite], took [time] s runJob prints out the following INFO message to the logs when the job has failed: Job [jobId] failed: [callSite], took [time] s runJob is used when: SparkContext is requested to run a job Submitting Job \u00b6 submitJob [ T , U ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], callSite : CallSite , resultHandler : ( Int , U ) => Unit , properties : Properties ): JobWaiter [ U ] submitJob increments the nextJobId internal counter. submitJob creates a JobWaiter for the (number of) partitions and the given resultHandler function. submitJob requests the DAGSchedulerEventProcessLoop to post a JobSubmitted . In the end, submitJob returns the JobWaiter . For empty partitions (no partitions to compute), submitJob requests the LiveListenerBus to post a SparkListenerJobStart and SparkListenerJobEnd (with JobSucceeded result marker) events and returns a JobWaiter with no tasks to wait for. submitJob throws an IllegalArgumentException when the partitions indices are not among the partitions of the given RDD : Attempting to access a non-existent partition: [p]. Total number of partitions: [maxPartitions] submitJob is used when: SparkContext is requested to submit a job DAGScheduler is requested to run a job Partition Placement Preferences \u00b6 DAGScheduler keeps track of block locations per RDD and partition. DAGScheduler uses TaskLocation that includes a host name and an executor id on that host (as ExecutorCacheTaskLocation ). The keys are RDDs (their ids) and the values are arrays indexed by partition numbers. Each entry is a set of block locations where a RDD partition is cached, i.e. the BlockManager s of the blocks. Initialized empty when DAGScheduler is created . Used when DAGScheduler is requested for the locations of the cache blocks of a RDD . ActiveJobs \u00b6 DAGScheduler tracks ActiveJob s: Adds a new ActiveJob when requested to handle JobSubmitted or MapStageSubmitted events Removes an ActiveJob when requested to clean up after an ActiveJob and independent stages . Removes all ActiveJobs when requested to doCancelAllJobs . DAGScheduler uses ActiveJobs registry when requested to handle JobGroupCancelled or TaskCompletion events, to cleanUpAfterSchedulerStop and to abort a stage . The number of ActiveJobs is available using job.activeJobs performance metric. Creating ResultStage for RDD \u00b6 createResultStage ( rdd : RDD [ _ ], func : ( TaskContext , Iterator [ _ ]) => _ , partitions : Array [ Int ], jobId : Int , callSite : CallSite ): ResultStage createResultStage ...FIXME createResultStage is used when DAGScheduler is requested to handle a JobSubmitted event . Creating ShuffleMapStage for ShuffleDependency \u00b6 createShuffleMapStage ( shuffleDep : ShuffleDependency [ _ , _ , _ ], jobId : Int ): ShuffleMapStage createShuffleMapStage creates a ShuffleMapStage for the given ShuffleDependency as follows: Stage ID is generated based on nextStageId internal counter RDD is taken from the given ShuffleDependency Number of tasks is the number of partitions of the RDD Parent RDDs MapOutputTrackerMaster createShuffleMapStage registers the ShuffleMapStage in the stageIdToStage and shuffleIdToMapStage internal registries. createShuffleMapStage updateJobIdStageIdMaps . createShuffleMapStage requests the MapOutputTrackerMaster to check whether it contains the shuffle ID or not . If not, createShuffleMapStage prints out the following INFO message to the logs and requests the MapOutputTrackerMaster to register the shuffle . Registering RDD [id] ([creationSite]) as input to shuffle [shuffleId] createShuffleMapStage is used when: DAGScheduler is requested to find or create a ShuffleMapStage for a given ShuffleDependency Cleaning Up After Job and Independent Stages \u00b6 cleanupStateForJobAndIndependentStages ( job : ActiveJob ): Unit cleanupStateForJobAndIndependentStages cleans up the state for job and any stages that are not part of any other job. cleanupStateForJobAndIndependentStages looks the job up in the internal < > registry. If no stages are found, the following ERROR is printed out to the logs: No stages registered for job [jobId] Oterwise, cleanupStateForJobAndIndependentStages uses < > registry to find the stages (the real objects not ids!). For each stage, cleanupStateForJobAndIndependentStages reads the jobs the stage belongs to. If the job does not belong to the jobs of the stage, the following ERROR is printed out to the logs: Job [jobId] not registered for stage [stageId] even though that stage was registered for the job If the job was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. < >, < >, < >, < > and < >. While removing from < >, you should see the following DEBUG message in the logs: Removing running stage [stageId] While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from waiting set. While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from failed set. After all cleaning (using < > as the source registry), if the stage belonged to the one and only job , you should see the following DEBUG message in the logs: After removal of stage [stageId], remaining stages = [stageIdToStage.size] The job is removed from < >, < >, < > registries. The final stage of the job is removed, i.e. ResultStage or ShuffleMapStage . cleanupStateForJobAndIndependentStages is used in handleTaskCompletion when a ResultTask has completed successfully , failJobAndIndependentStages and markMapStageJobAsFinished . Marking ShuffleMapStage Job Finished \u00b6 markMapStageJobAsFinished ( job : ActiveJob , stats : MapOutputStatistics ): Unit markMapStageJobAsFinished marks the active job finished and notifies Spark listeners. Internally, markMapStageJobAsFinished marks the zeroth partition finished and increases the number of tasks finished in job . The job listener is notified about the 0 th task succeeded . The < job and independent stages are cleaned up>>. Ultimately, SparkListenerJobEnd is posted to LiveListenerBus (as < >) for the job , the current time (in millis) and JobSucceeded job result. markMapStageJobAsFinished is used in handleMapStageSubmitted and handleTaskCompletion . Finding Or Creating Missing Direct Parent ShuffleMapStages (For ShuffleDependencies) of RDD \u00b6 getOrCreateParentStages ( rdd : RDD [ _ ], firstJobId : Int ): List [ Stage ] getOrCreateParentStages < ShuffleDependencies >> of the input rdd and then < ShuffleMapStage stages>> for each ShuffleDependency . getOrCreateParentStages is used when DAGScheduler is requested to create a ShuffleMapStage or a ResultStage . Marking Stage Finished \u00b6 markStageAsFinished ( stage : Stage , errorMessage : Option [ String ] = None , willRetry : Boolean = false ): Unit markStageAsFinished ...FIXME markStageAsFinished is used when...FIXME Finding or Creating ShuffleMapStage for ShuffleDependency \u00b6 getOrCreateShuffleMapStage ( shuffleDep : ShuffleDependency [ _ , _ , _ ], firstJobId : Int ): ShuffleMapStage getOrCreateShuffleMapStage finds a ShuffleMapStage by the shuffleId of the given ShuffleDependency in the shuffleIdToMapStage internal registry and returns it if available. If not found, getOrCreateShuffleMapStage finds all the missing ancestor shuffle dependencies and creates the missing ShuffleMapStage stages (including one for the input ShuffleDependency ). getOrCreateShuffleMapStage is used when: DAGScheduler is requested to find or create missing direct parent ShuffleMapStages of an RDD , find missing parent ShuffleMapStages for a stage , handle a MapStageSubmitted event , and check out stage dependency on a stage Finding Missing ShuffleDependencies For RDD \u00b6 getMissingAncestorShuffleDependencies ( rdd : RDD [ _ ]): Stack [ ShuffleDependency [ _ , _ , _ ]] getMissingAncestorShuffleDependencies finds all missing shuffle dependencies for the given RDD traversing its rdd/spark-rdd-lineage.md[RDD lineage]. NOTE: A missing shuffle dependency of a RDD is a dependency not registered in < shuffleIdToMapStage internal registry>>. Internally, getMissingAncestorShuffleDependencies < >\u2009of the input RDD and collects the ones that are not registered in < shuffleIdToMapStage internal registry>>. It repeats the process for the RDDs of the parent shuffle dependencies. getMissingAncestorShuffleDependencies is used when DAGScheduler is requested to find all ShuffleMapStage stages for a ShuffleDependency . Finding Direct Parent Shuffle Dependencies of RDD \u00b6 getShuffleDependencies ( rdd : RDD [ _ ]): HashSet [ ShuffleDependency [ _ , _ , _ ]] getShuffleDependencies finds direct parent shuffle dependencies for the given RDD . Internally, getShuffleDependencies takes the direct rdd/index.md#dependencies[shuffle dependencies of the input RDD] and direct shuffle dependencies of all the parent non- ShuffleDependencies in the dependency chain (aka RDD lineage ). getShuffleDependencies is used when DAGScheduler is requested to find or create missing direct parent ShuffleMapStages (for ShuffleDependencies of a RDD) and find all missing shuffle dependencies for a given RDD . Failing Job and Independent Single-Job Stages \u00b6 failJobAndIndependentStages ( job : ActiveJob , failureReason : String , exception : Option [ Throwable ] = None ): Unit failJobAndIndependentStages fails the input job and all the stages that are only used by the job. Internally, failJobAndIndependentStages uses < jobIdToStageIds internal registry>> to look up the stages registered for the job. If no stages could be found, you should see the following ERROR message in the logs: No stages registered for job [id] Otherwise, for every stage, failJobAndIndependentStages finds the job ids the stage belongs to. If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs: Job [id] not registered for stage [id] even though that stage was registered for the job Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in runningStages internal registry), TaskScheduler.md#contract[ TaskScheduler is requested to cancel the stage's tasks] and < >. NOTE: failJobAndIndependentStages uses < >, < >, and < > internal registries. failJobAndIndependentStages is used when...FIXME Aborting Stage \u00b6 abortStage ( failedStage : Stage , reason : String , exception : Option [ Throwable ]): Unit abortStage is an internal method that finds all the active jobs that depend on the failedStage stage and fails them. Internally, abortStage looks the failedStage stage up in the internal < > registry and exits if there the stage was not registered earlier. If it was, abortStage finds all the active jobs (in the internal < > registry) with the < failedStage stage>>. At this time, the completionTime property (of the failed stage's StageInfo ) is assigned to the current time (millis). All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka independent stages ) are < > (with the failure reason being \"Job aborted due to stage failure: [reason]\" and the input exception ). If there are no jobs depending on the failed stage, you should see the following INFO message in the logs: Ignoring failure of [failedStage] because all jobs depending on it are done abortStage is used when DAGScheduler is requested to handle a TaskSetFailed event , submit a stage , submit missing tasks of a stage , handle a TaskCompletion event . Checking Out Stage Dependency on Given Stage \u00b6 stageDependsOn ( stage : Stage , target : Stage ): Boolean stageDependsOn compares two stages and returns whether the stage depends on target stage (i.e. true ) or not (i.e. false ). NOTE: A stage A depends on stage B if B is among the ancestors of A . Internally, stageDependsOn walks through the graph of RDDs of the input stage . For every RDD in the RDD's dependencies (using RDD.dependencies ) stageDependsOn adds the RDD of a NarrowDependency to a stack of RDDs to visit while for a ShuffleDependency it < ShuffleMapStage stages for a ShuffleDependency >> for the dependency and the stage 's first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs. After all the RDDs of the input stage are visited, stageDependsOn checks if the target 's RDD is among the RDDs of the stage , i.e. whether the stage depends on target stage. stageDependsOn is used when DAGScheduler is requested to abort a stage . Submitting Waiting Child Stages for Execution \u00b6 submitWaitingChildStages ( parent : Stage ): Unit submitWaitingChildStages submits for execution all waiting stages for which the input parent Stage.md[Stage] is the direct parent. NOTE: Waiting stages are the stages registered in < waitingStages internal registry>>. When executed, you should see the following TRACE messages in the logs: Checking if any dependencies of [parent] are now runnable running: [runningStages] waiting: [waitingStages] failed: [failedStages] submitWaitingChildStages finds child stages of the input parent stage, removes them from waitingStages internal registry, and < > one by one sorted by their job ids. submitWaitingChildStages is used when DAGScheduler is requested to submits missing tasks for a stage and handles a successful ShuffleMapTask completion . Submitting Stage (with Missing Parents) for Execution \u00b6 submitStage ( stage : Stage ): Unit submitStage submits the input stage or its missing parents (if there any stages not computed yet before the input stage could). NOTE: submitStage is also used to DAGSchedulerEventProcessLoop.md#resubmitFailedStages[resubmit failed stages]. submitStage recursively submits any missing parents of the stage . Internally, submitStage first finds the earliest-created job id that needs the stage . NOTE: A stage itself tracks the jobs (their ids) it belongs to (using the internal jobIds registry). The following steps depend on whether there is a job or not. If there are no jobs that require the stage , submitStage < > with the reason: No active job for stage [id] If however there is a job for the stage , you should see the following DEBUG message in the logs: submitStage([stage]) submitStage checks the status of the stage and continues when it was not recorded in < >, < > or < > internal registries. It simply exits otherwise. With the stage ready for submission, submitStage calculates the < stage >> (sorted by their job ids). You should see the following DEBUG message in the logs: missing: [missing] When the stage has no parent stages missing, you should see the following INFO message in the logs: Submitting [stage] ([stage.rdd]), which has no missing parents submitStage < stage >> (with the earliest-created job id) and finishes. If however there are missing parent stages for the stage , submitStage < >, and the stage is recorded in the internal < > registry. submitStage is used recursively for missing parents of the given stage and when DAGScheduler is requested for the following: resubmitFailedStages (ResubmitFailedStages event) submitWaitingChildStages (CompletionEvent event) Handle JobSubmitted , MapStageSubmitted and TaskCompletion events Stage Attempts \u00b6 A single stage can be re-executed in multiple attempts due to fault recovery. The number of attempts is configured (FIXME). If TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits the lost stage. This is detected through a DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[ CompletionEvent with FetchFailed ], or an < > event. DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. Please note that tasks from the old attempts of a stage could still be running. A stage object tracks multiple StageInfo objects to pass to Spark listeners or the web UI. The latest StageInfo for the most recent attempt for a stage is accessible through latestInfo . Preferred Locations \u00b6 DAGScheduler computes where to run each task in a stage based on the rdd/index.md#getPreferredLocations[preferred locations of its underlying RDDs], or < >. Adaptive Query Planning / Adaptive Scheduling \u00b6 See SPARK-9850 Adaptive execution in Spark for the design document. The work is currently in progress. DAGScheduler.submitMapStage method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages. ScheduledExecutorService daemon services \u00b6 DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation): dag-scheduler-message - a daemon thread pool using j.u.c.ScheduledThreadPoolExecutor with core pool size 1 . It is used to post a DAGSchedulerEventProcessLoop.md#ResubmitFailedStages[ResubmitFailedStages] event when DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[ FetchFailed is reported]. They are created using ThreadUtils.newDaemonSingleThreadScheduledExecutor method that uses Guava DSL to instantiate a ThreadFactory. Finding Missing Parent ShuffleMapStages For Stage \u00b6 getMissingParentStages ( stage : Stage ): List [ Stage ] getMissingParentStages finds missing parent ShuffleMapStage s in the dependency graph of the input stage (using the breadth-first search algorithm ). Internally, getMissingParentStages starts with the stage 's RDD and walks up the tree of all parent RDDs to find < >. NOTE: A Stage tracks the associated RDD using Stage.md#rdd[ rdd property]. NOTE: An uncached partition of a RDD is a partition that has Nil in the < > (which results in no RDD blocks in any of the active storage:BlockManager.md[BlockManager]s on executors). getMissingParentStages traverses the rdd/index.md#dependencies[parent dependencies of the RDD] and acts according to their type, i.e. ShuffleDependency or NarrowDependency . NOTE: ShuffleDependency and NarrowDependency are the main top-level Dependencies . For each NarrowDependency , getMissingParentStages simply marks the corresponding RDD to visit and moves on to a next dependency of a RDD or works on another unvisited parent RDD. NOTE: NarrowDependency is a RDD dependency that allows for pipelined execution. getMissingParentStages focuses on ShuffleDependency dependencies. NOTE: ShuffleDependency is a RDD dependency that represents a dependency on the output of a ShuffleMapStage , i.e. shuffle map stage . For each ShuffleDependency , getMissingParentStages < ShuffleMapStage stages>>. If the ShuffleMapStage is not available , it is added to the set of missing (map) stages. NOTE: A ShuffleMapStage is available when all its partitions are computed, i.e. results are available (as blocks). CAUTION: FIXME...IMAGE with ShuffleDependencies queried getMissingParentStages is used when DAGScheduler is requested to submit a stage and handle JobSubmitted and MapStageSubmitted events. Submitting Missing Tasks of Stage \u00b6 submitMissingTasks ( stage : Stage , jobId : Int ): Unit submitMissingTasks prints out the following DEBUG message to the logs: submitMissingTasks([stage]) submitMissingTasks requests the given Stage for the missing partitions (partitions that need to be computed). submitMissingTasks adds the stage to the runningStages internal registry. submitMissingTasks notifies the OutputCommitCoordinator that stage execution started . submitMissingTasks determines preferred locations ( task locality preferences ) of the missing partitions. submitMissingTasks requests the stage for a new stage attempt . submitMissingTasks requests the LiveListenerBus to post a SparkListenerStageSubmitted event. submitMissingTasks uses the closure Serializer to serialize the stage and create a so-called task binary. submitMissingTasks serializes the RDD (of the stage) and either the ShuffleDependency or the compute function based on the type of the stage ( ShuffleMapStage or ResultStage , respectively). submitMissingTasks creates a broadcast variable for the task binary. Note That shows how important Broadcast s are for Spark itself to distribute data among executors in a Spark application in the most efficient way. submitMissingTasks creates tasks for every missing partition: ShuffleMapTasks for a ShuffleMapStage ResultTasks for a ResultStage If there are tasks to submit for execution (i.e. there are missing partitions in the stage), submitMissingTasks prints out the following INFO message to the logs: Submitting [size] missing tasks from [stage] ([rdd]) (first 15 tasks are for partitions [partitionIds]) submitMissingTasks requests the < > to TaskScheduler.md#submitTasks[submit the tasks for execution] (as a new TaskSet.md[TaskSet]). With no tasks to submit for execution, submitMissingTasks < >. submitMissingTasks prints out the following DEBUG messages based on the type of the stage: Stage [stage] is actually done; (available: [isAvailable],available outputs: [numAvailableOutputs],partitions: [numPartitions]) or Stage [stage] is actually done; (partitions: [numPartitions]) for ShuffleMapStage and ResultStage , respectively. In the end, with no tasks to submit for execution, submitMissingTasks < > and exits. submitMissingTasks is used when DAGScheduler is requested to submit a stage for execution . Finding Preferred Locations for Missing Partitions \u00b6 getPreferredLocs ( rdd : RDD [ _ ], partition : Int ): Seq [ TaskLocation ] getPreferredLocs is simply an alias for the internal (recursive) < >. getPreferredLocs is used when...FIXME Finding BlockManagers (Executors) for Cached RDD Partitions (aka Block Location Discovery) \u00b6 getCacheLocs ( rdd : RDD [ _ ]): IndexedSeq [ Seq [ TaskLocation ]] getCacheLocs gives TaskLocations (block locations) for the partitions of the input rdd . getCacheLocs caches lookup results in < > internal registry. NOTE: The size of the collection from getCacheLocs is exactly the number of partitions in rdd RDD. NOTE: The size of every TaskLocation collection (i.e. every entry in the result of getCacheLocs ) is exactly the number of blocks managed using storage:BlockManager.md[BlockManagers] on executors. Internally, getCacheLocs finds rdd in the < > internal registry (of partition locations per RDD). If rdd is not in < > internal registry, getCacheLocs branches per its storage:StorageLevel.md[storage level]. For NONE storage level (i.e. no caching), the result is an empty locations (i.e. no location preference). For other non- NONE storage levels, getCacheLocs storage:BlockManagerMaster.md#getLocations-block-array[requests BlockManagerMaster for block locations] that are then mapped to TaskLocations with the hostname of the owning BlockManager for a block (of a partition) and the executor id. NOTE: getCacheLocs uses < > that was defined when < >. getCacheLocs records the computed block locations per partition (as TaskLocation ) in < > internal registry. NOTE: getCacheLocs requests locations from BlockManagerMaster using storage:BlockId.md#RDDBlockId[RDDBlockId] with the RDD id and the partition indices (which implies that the order of the partitions matters to request proper blocks). NOTE: DAGScheduler uses TaskLocation.md[TaskLocations] (with host and executor) while storage:BlockManagerMaster.md[BlockManagerMaster] uses storage:BlockManagerId.md[] (to track similar information, i.e. block locations). getCacheLocs is used when DAGScheduler is requested to find missing parent MapStages and getPreferredLocsInternal . Finding Placement Preferences for RDD Partition (recursively) \u00b6 getPreferredLocsInternal ( rdd : RDD [ _ ], partition : Int , visited : HashSet [( RDD [ _ ], Int )]): Seq [ TaskLocation ] getPreferredLocsInternal first < TaskLocations for the partition of the rdd >> (using < > internal cache) and returns them. Otherwise, if not found, getPreferredLocsInternal rdd/index.md#preferredLocations[requests rdd for the preferred locations of partition ] and returns them. NOTE: Preferred locations of the partitions of a RDD are also called placement preferences or locality preferences . Otherwise, if not found, getPreferredLocsInternal finds the first parent NarrowDependency and (recursively) finds TaskLocations . If all the attempts fail to yield any non-empty result, getPreferredLocsInternal returns an empty collection of TaskLocation.md[TaskLocations]. getPreferredLocsInternal is used when DAGScheduler is requested for the preferred locations for missing partitions . Stopping DAGScheduler \u00b6 stop (): Unit stop stops the internal dag-scheduler-message thread pool, dag-scheduler-event-loop , and TaskScheduler . stop is used when SparkContext is requested to stop . checkBarrierStageWithNumSlots \u00b6 checkBarrierStageWithNumSlots ( rdd : RDD [ _ ]): Unit checkBarrierStageWithNumSlots...FIXME checkBarrierStageWithNumSlots is used when DAGScheduler is requested to create < > and < > stages. Killing Task \u00b6 killTaskAttempt ( taskId : Long , interruptThread : Boolean , reason : String ): Boolean killTaskAttempt requests the TaskScheduler to kill a task . killTaskAttempt is used when SparkContext is requested to kill a task . cleanUpAfterSchedulerStop \u00b6 cleanUpAfterSchedulerStop (): Unit cleanUpAfterSchedulerStop ...FIXME cleanUpAfterSchedulerStop is used when DAGSchedulerEventProcessLoop is requested to onStop . removeExecutorAndUnregisterOutputs \u00b6 removeExecutorAndUnregisterOutputs ( execId : String , fileLost : Boolean , hostToUnregisterOutputs : Option [ String ], maybeEpoch : Option [ Long ] = None ): Unit removeExecutorAndUnregisterOutputs...FIXME removeExecutorAndUnregisterOutputs is used when DAGScheduler is requested to handle < > (due to a fetch failure) and < > events. markMapStageJobsAsFinished \u00b6 markMapStageJobsAsFinished ( shuffleStage : ShuffleMapStage ): Unit markMapStageJobsAsFinished ...FIXME markMapStageJobsAsFinished is used when DAGScheduler is requested to submit missing tasks (of a ShuffleMapStage that has just been computed) and handle a task completion (of a ShuffleMapStage ). updateJobIdStageIdMaps \u00b6 updateJobIdStageIdMaps ( jobId : Int , stage : Stage ): Unit updateJobIdStageIdMaps ...FIXME updateJobIdStageIdMaps is used when DAGScheduler is requested to create ShuffleMapStage and ResultStage stages. executorHeartbeatReceived \u00b6 executorHeartbeatReceived ( execId : String , // (taskId, stageId, stageAttemptId, accumUpdates) accumUpdates : Array [( Long , Int , Int , Seq [ AccumulableInfo ])], blockManagerId : BlockManagerId , // (stageId, stageAttemptId) -> metrics executorUpdates : mutable . Map [( Int , Int ), ExecutorMetrics ]): Boolean executorHeartbeatReceived posts a SparkListenerExecutorMetricsUpdate (to listenerBus ) and informs BlockManagerMaster that blockManagerId block manager is alive (by posting BlockManagerHeartbeat ). executorHeartbeatReceived is used when TaskSchedulerImpl is requested to handle an executor heartbeat . Event Handlers \u00b6 AllJobsCancelled Event Handler \u00b6 doCancelAllJobs (): Unit doCancelAllJobs ...FIXME doCancelAllJobs is used when DAGSchedulerEventProcessLoop is requested to handle an AllJobsCancelled event and onError . BeginEvent Event Handler \u00b6 handleBeginEvent ( task : Task [ _ ], taskInfo : TaskInfo ): Unit handleBeginEvent ...FIXME handleBeginEvent is used when DAGSchedulerEventProcessLoop is requested to handle a BeginEvent event. Handling Task Completion Event \u00b6 handleTaskCompletion ( event : CompletionEvent ): Unit handleTaskCompletion handles a CompletionEvent . handleTaskCompletion notifies the OutputCommitCoordinator that a task completed . handleTaskCompletion finds the stage in the stageIdToStage registry. If not found, handleTaskCompletion postTaskEnd and quits. handleTaskCompletion updateAccumulators . handleTaskCompletion announces task completion application-wide . handleTaskCompletion branches off per TaskEndReason (as event.reason ). TaskEndReason Description Success Acts according to the type of the task that completed, i.e. ShuffleMapTask and ResultTask Resubmitted others Handling Successful Task Completion \u00b6 When a task has finished successfully (i.e. Success end reason), handleTaskCompletion marks the partition as no longer pending (i.e. the partition the task worked on is removed from pendingPartitions of the stage). NOTE: A Stage tracks its own pending partitions using scheduler:Stage.md#pendingPartitions[ pendingPartitions property]. handleTaskCompletion branches off given the type of the task that completed, i.e. < > and < >. Handling Successful ResultTask Completion \u00b6 For scheduler:ResultTask.md[ResultTask], the stage is assumed a scheduler:ResultStage.md[ResultStage]. handleTaskCompletion finds the ActiveJob associated with the ResultStage . NOTE: scheduler:ResultStage.md[ResultStage] tracks the optional ActiveJob as scheduler:ResultStage.md#activeJob[ activeJob property]. There could only be one active job for a ResultStage . If there is no job for the ResultStage , you should see the following INFO message in the logs: Ignoring result from [task] because its job has finished Otherwise, when the ResultStage has a ActiveJob , handleTaskCompletion checks the status of the partition output for the partition the ResultTask ran for. NOTE: ActiveJob tracks task completions in finished property with flags for every partition in a stage. When the flag for a partition is enabled (i.e. true ), it is assumed that the partition has been computed (and no results from any ResultTask are expected and hence simply ignored). CAUTION: FIXME Describe why could a partition has more ResultTask running. handleTaskCompletion ignores the CompletionEvent when the partition has already been marked as completed for the stage and simply exits. handleTaskCompletion scheduler:DAGScheduler.md#updateAccumulators[updates accumulators]. The partition for the ActiveJob (of the ResultStage ) is marked as computed and the number of partitions calculated increased. NOTE: ActiveJob tracks what partitions have already been computed and their number. If the ActiveJob has finished (when the number of partitions computed is exactly the number of partitions in a stage) handleTaskCompletion does the following (in order): scheduler:DAGScheduler.md#markStageAsFinished[Marks ResultStage computed]. scheduler:DAGScheduler.md#cleanupStateForJobAndIndependentStages[Cleans up after ActiveJob and independent stages]. Announces the job completion application-wide (by posting a SparkListener.md#SparkListenerJobEnd[SparkListenerJobEnd] to scheduler:LiveListenerBus.md[]). In the end, handleTaskCompletion notifies JobListener of the ActiveJob that the task succeeded . NOTE: A task succeeded notification holds the output index and the result. When the notification throws an exception (because it runs user code), handleTaskCompletion notifies JobListener about the failure (wrapping it inside a SparkDriverExecutionException exception). Handling Successful ShuffleMapTask Completion \u00b6 For scheduler:ShuffleMapTask.md[ShuffleMapTask], the stage is assumed a scheduler:ShuffleMapStage.md[ShuffleMapStage]. handleTaskCompletion scheduler:DAGScheduler.md#updateAccumulators[updates accumulators]. The task's result is assumed scheduler:MapStatus.md[MapStatus] that knows the executor where the task has finished. You should see the following DEBUG message in the logs: ShuffleMapTask finished on [execId] If the executor is registered in scheduler:DAGScheduler.md#failedEpoch[ failedEpoch internal registry] and the epoch of the completed task is not greater than that of the executor (as in failedEpoch registry), you should see the following INFO message in the logs: Ignoring possibly bogus [task] completion from executor [executorId] Otherwise, handleTaskCompletion scheduler:ShuffleMapStage.md#addOutputLoc[registers the MapStatus result for the partition with the stage] (of the completed task). handleTaskCompletion does more processing only if the ShuffleMapStage is registered as still running (in scheduler:DAGScheduler.md#runningStages[ runningStages internal registry]) and the scheduler:Stage.md#pendingPartitions[ ShuffleMapStage stage has no pending partitions to compute]. The ShuffleMapStage is < >. You should see the following INFO messages in the logs: looking for newly runnable stages running: [runningStages] waiting: [waitingStages] failed: [failedStages] handleTaskCompletion scheduler:MapOutputTrackerMaster.md#registerMapOutputs[registers the shuffle map outputs of the ShuffleDependency with MapOutputTrackerMaster ] (with the epoch incremented) and scheduler:DAGScheduler.md#clearCacheLocs[clears internal cache of the stage's RDD block locations]. NOTE: scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] is given when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. If the scheduler:ShuffleMapStage.md#isAvailable[ ShuffleMapStage stage is ready], all scheduler:ShuffleMapStage.md#mapStageJobs[active jobs of the stage] (aka map-stage jobs ) are scheduler:DAGScheduler.md#markMapStageJobAsFinished[marked as finished] (with scheduler:MapOutputTrackerMaster.md#getStatistics[ MapOutputStatistics from MapOutputTrackerMaster for the ShuffleDependency ]). NOTE: A ShuffleMapStage stage is ready (aka available ) when all partitions have shuffle outputs, i.e. when their tasks have completed. Eventually, handleTaskCompletion scheduler:DAGScheduler.md#submitWaitingChildStages[submits waiting child stages (of the ready ShuffleMapStage )]. If however the ShuffleMapStage is not ready, you should see the following INFO message in the logs: Resubmitting [shuffleStage] ([shuffleStage.name]) because some of its tasks had failed: [missingPartitions] In the end, handleTaskCompletion scheduler:DAGScheduler.md#submitStage[submits the ShuffleMapStage for execution]. TaskEndReason: Resubmitted \u00b6 For Resubmitted case, you should see the following INFO message in the logs: Resubmitted [task], so marking it as still running The task (by task.partitionId ) is added to the collection of pending partitions of the stage (using stage.pendingPartitions ). TIP: A stage knows how many partitions are yet to be calculated. A task knows about the partition id for which it was launched. Task Failed with FetchFailed Exception \u00b6 FetchFailed ( bmAddress : BlockManagerId , shuffleId : Int , mapId : Int , reduceId : Int , message : String ) extends TaskFailedReason When FetchFailed happens, stageIdToStage is used to access the failed stage (using task.stageId and the task is available in event in handleTaskCompletion(event: CompletionEvent) ). shuffleToMapStage is used to access the map stage (using shuffleId ). If failedStage.latestInfo.attemptId != task.stageAttemptId , you should see the following INFO in the logs: Ignoring fetch failure from [task] as it's from [failedStage] attempt [task.stageAttemptId] and there is a more recent attempt for that stage (attempt ID [failedStage.latestInfo.attemptId]) running CAUTION: FIXME What does failedStage.latestInfo.attemptId != task.stageAttemptId mean? And the case finishes. Otherwise, the case continues. If the failed stage is in runningStages , the following INFO message shows in the logs: Marking [failedStage] ([failedStage.name]) as failed due to a fetch failure from [mapStage] ([mapStage.name]) markStageAsFinished(failedStage, Some(failureMessage)) is called. CAUTION: FIXME What does markStageAsFinished do? If the failed stage is not in runningStages , the following DEBUG message shows in the logs: Received fetch failure from [task], but its from [failedStage] which is no longer running When disallowStageRetryForTest is set, abortStage(failedStage, \"Fetch failure will not retry stage due to testing config\", None) is called. CAUTION: FIXME Describe disallowStageRetryForTest and abortStage . If the scheduler:Stage.md#failedOnFetchAndShouldAbort[number of fetch failed attempts for the stage exceeds the allowed number], the scheduler:DAGScheduler.md#abortStage[failed stage is aborted] with the reason: [failedStage] ([name]) has failed the maximum allowable number of times: 4. Most recent failure reason: [failureMessage] If there are no failed stages reported (scheduler:DAGScheduler.md#failedStages[DAGScheduler.failedStages] is empty), the following INFO shows in the logs: Resubmitting [mapStage] ([mapStage.name]) and [failedStage] ([failedStage.name]) due to fetch failure And the following code is executed: messageScheduler.schedule( new Runnable { override def run(): Unit = eventProcessLoop.post(ResubmitFailedStages) }, DAGScheduler.RESUBMIT_TIMEOUT, TimeUnit.MILLISECONDS) CAUTION: FIXME What does the above code do? For all the cases, the failed stage and map stages are both added to the internal scheduler:DAGScheduler.md#failedStages[registry of failed stages]. If mapId (in the FetchFailed object for the case) is provided, the map stage output is cleaned up (as it is broken) using mapStage.removeOutputLoc(mapId, bmAddress) and scheduler:MapOutputTracker.md#unregisterMapOutput[MapOutputTrackerMaster.unregisterMapOutput(shuffleId, mapId, bmAddress)] methods. CAUTION: FIXME What does mapStage.removeOutputLoc do? If BlockManagerId (as bmAddress in the FetchFailed object) is defined, handleTaskCompletion < > (with filesLost enabled and maybeEpoch from the scheduler:Task.md#epoch[Task] that completed). handleTaskCompletion is used when: DAGSchedulerEventProcessLoop is requested to handle a CompletionEvent event. ExecutorAdded Event Handler \u00b6 handleExecutorAdded ( execId : String , host : String ): Unit handleExecutorAdded ...FIXME handleExecutorAdded is used when DAGSchedulerEventProcessLoop is requested to handle an ExecutorAdded event. ExecutorLost Event Handler \u00b6 handleExecutorLost ( execId : String , workerLost : Boolean ): Unit handleExecutorLost checks whether the input optional maybeEpoch is defined and if not requests the scheduler:MapOutputTracker.md#getEpoch[current epoch from MapOutputTrackerMaster ]. NOTE: MapOutputTrackerMaster is passed in (as mapOutputTracker ) when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. CAUTION: FIXME When is maybeEpoch passed in? .DAGScheduler.handleExecutorLost image::dagscheduler-handleExecutorLost.png[align=\"center\"] Recurring ExecutorLost events lead to the following repeating DEBUG message in the logs: DEBUG Additional executor lost message for [execId] (epoch [currentEpoch]) NOTE: handleExecutorLost handler uses DAGScheduler 's failedEpoch and FIXME internal registries. Otherwise, when the executor execId is not in the scheduler:DAGScheduler.md#failedEpoch[list of executor lost] or the executor failure's epoch is smaller than the input maybeEpoch , the executor's lost event is recorded in scheduler:DAGScheduler.md#failedEpoch[ failedEpoch internal registry]. CAUTION: FIXME Describe the case above in simpler non-technical words. Perhaps change the order, too. You should see the following INFO message in the logs: INFO Executor lost: [execId] (epoch [epoch]) storage:BlockManagerMaster.md#removeExecutor[ BlockManagerMaster is requested to remove the lost executor execId ]. CAUTION: FIXME Review what's filesLost . handleExecutorLost exits unless the ExecutorLost event was for a map output fetch operation (and the input filesLost is true ) or external shuffle service is not used. In such a case, you should see the following INFO message in the logs: Shuffle files lost for executor: [execId] (epoch [epoch]) handleExecutorLost walks over all scheduler:ShuffleMapStage.md[ShuffleMapStage]s in scheduler:DAGScheduler.md#shuffleToMapStage[DAGScheduler's shuffleToMapStage internal registry] and do the following (in order): ShuffleMapStage.removeOutputsOnExecutor(execId) is called scheduler:MapOutputTrackerMaster.md#registerMapOutputs[MapOutputTrackerMaster.registerMapOutputs(shuffleId, stage.outputLocInMapOutputTrackerFormat(), changeEpoch = true)] is called. In case scheduler:DAGScheduler.md#shuffleToMapStage[DAGScheduler's shuffleToMapStage internal registry] has no shuffles registered, scheduler:MapOutputTrackerMaster.md#incrementEpoch[ MapOutputTrackerMaster is requested to increment epoch]. Ultimatelly, DAGScheduler scheduler:DAGScheduler.md#clearCacheLocs[clears the internal cache of RDD partition locations]. handleExecutorLost is used when DAGSchedulerEventProcessLoop is requested to handle an ExecutorLost event. GettingResultEvent Event Handler \u00b6 handleGetTaskResult ( taskInfo : TaskInfo ): Unit handleGetTaskResult ...FIXME handleGetTaskResult is used when DAGSchedulerEventProcessLoop is requested to handle a GettingResultEvent event. JobCancelled Event Handler \u00b6 handleJobCancellation ( jobId : Int , reason : Option [ String ]): Unit handleJobCancellation looks up the active job for the input job ID (in jobIdToActiveJob internal registry) and fails it and all associated independent stages with failure reason: Job [jobId] cancelled [reason] When the input job ID is not found, handleJobCancellation prints out the following DEBUG message to the logs: Trying to cancel unregistered job [jobId] handleJobCancellation is used when DAGScheduler is requested to handle a JobCancelled event, doCancelAllJobs , handleJobGroupCancelled , handleStageCancellation . JobGroupCancelled Event Handler \u00b6 handleJobGroupCancelled ( groupId : String ): Unit handleJobGroupCancelled finds active jobs in a group and cancels them. Internally, handleJobGroupCancelled computes all the active jobs (registered in the internal collection of active jobs ) that have spark.jobGroup.id scheduling property set to groupId . handleJobGroupCancelled then cancels every active job in the group one by one and the cancellation reason: part of cancelled job group [groupId] handleJobGroupCancelled is used when DAGScheduler is requested to handle JobGroupCancelled event. Handling JobSubmitted Event \u00b6 handleJobSubmitted ( jobId : Int , finalRDD : RDD [ _ ], func : ( TaskContext , Iterator [ _ ]) => _ , partitions : Array [ Int ], callSite : CallSite , listener : JobListener , properties : Properties ): Unit handleJobSubmitted creates a ResultStage (as finalStage in the picture below) for the given RDD, func , partitions , jobId and callSite . handleJobSubmitted creates an ActiveJob for the ResultStage . handleJobSubmitted clears the internal cache of RDD partition locations . Important FIXME Why is this clearing here so important? handleJobSubmitted prints out the following INFO messages to the logs (with missingParentStages ): Got job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingParentStages] handleJobSubmitted registers the new ActiveJob in jobIdToActiveJob and activeJobs internal registries. handleJobSubmitted requests the ResultStage to associate itself with the ActiveJob . handleJobSubmitted uses the jobIdToStageIds internal registry to find all registered stages for the given jobId . handleJobSubmitted uses the stageIdToStage internal registry to request the Stages for the latestInfo . In the end, handleJobSubmitted posts a SparkListenerJobStart message to the LiveListenerBus and submits the ResultStage . handleJobSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a JobSubmitted event. MapStageSubmitted Event Handler \u00b6 handleMapStageSubmitted ( jobId : Int , dependency : ShuffleDependency [ _ , _ , _ ], callSite : CallSite , listener : JobListener , properties : Properties ): Unit Note MapStageSubmitted event processing is very similar to < > events. handleMapStageSubmitted finds or creates a new ShuffleMapStage for the input ShuffleDependency and jobId . handleMapStageSubmitted creates an ActiveJob . handleMapStageSubmitted clears the internal cache of RDD partition locations . Important FIXME Why is this clearing here so important? handleMapStageSubmitted prints out the following INFO messages to the logs: Got map stage job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingParentStages] handleMapStageSubmitted registers the new job in jobIdToActiveJob and activeJobs internal registries, and with the final ShuffleMapStage . Note ShuffleMapStage can have multiple ActiveJob s registered. handleMapStageSubmitted finds all the registered stages for the input jobId and collects their latest StageInfo . In the end, handleMapStageSubmitted posts SparkListenerJobStart message to LiveListenerBus and submits the ShuffleMapStage . When the ShuffleMapStage is available already, handleMapStageSubmitted marks the job finished . When handleMapStageSubmitted could not find or create a ShuffleMapStage , handleMapStageSubmitted prints out the following WARN message to the logs. Creating new stage failed due to exception - job: [id] handleMapStageSubmitted notifies listener about the job failure and exits. handleMapStageSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a MapStageSubmitted event. ResubmitFailedStages Event Handler \u00b6 resubmitFailedStages (): Unit resubmitFailedStages iterates over the internal collection of failed stages and submits them. Note resubmitFailedStages does nothing when there are no failed stages reported . resubmitFailedStages prints out the following INFO message to the logs: Resubmitting failed stages resubmitFailedStages clears the internal cache of RDD partition locations and makes a copy of the collection of failed stages to track failed stages afresh. Note At this point DAGScheduler has no failed stages reported. The previously-reported failed stages are sorted by the corresponding job ids in incremental order and resubmitted . resubmitFailedStages is used when DAGSchedulerEventProcessLoop is requested to handle a ResubmitFailedStages event. SpeculativeTaskSubmitted Event Handler \u00b6 handleSpeculativeTaskSubmitted (): Unit handleSpeculativeTaskSubmitted ...FIXME handleSpeculativeTaskSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a SpeculativeTaskSubmitted event. StageCancelled Event Handler \u00b6 handleStageCancellation (): Unit handleStageCancellation ...FIXME handleStageCancellation is used when DAGSchedulerEventProcessLoop is requested to handle a StageCancelled event. TaskSetFailed Event Handler \u00b6 handleTaskSetFailed (): Unit handleTaskSetFailed ...FIXME handleTaskSetFailed is used when DAGSchedulerEventProcessLoop is requested to handle a TaskSetFailed event. WorkerRemoved Event Handler \u00b6 handleWorkerRemoved ( workerId : String , host : String , message : String ): Unit handleWorkerRemoved ...FIXME handleWorkerRemoved is used when DAGSchedulerEventProcessLoop is requested to handle a WorkerRemoved event. Internal Properties \u00b6 failedEpoch \u00b6 The lookup table of lost executors and the epoch of the event. failedStages \u00b6 Stages that failed due to fetch failures (when a DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[task fails with FetchFailed exception]). jobIdToActiveJob \u00b6 The lookup table of ActiveJob s per job id. jobIdToStageIds \u00b6 The lookup table of all stages per ActiveJob id nextJobId Counter \u00b6 nextJobId : AtomicInteger nextJobId is a Java AtomicInteger for job IDs. nextJobId starts at 0 . Used when DAGScheduler is requested for numTotalJobs , to submitJob , runApproximateJob and submitMapStage . nextStageId \u00b6 The next stage id counting from 0 . Used when DAGScheduler creates a < > and a < >. It is the key in < >. runningStages \u00b6 The set of stages that are currently \"running\". A stage is added when < > gets executed (without first checking if the stage has not already been added). shuffleIdToMapStage \u00b6 A lookup table of ShuffleMapStage s by ShuffleDependency stageIdToStage \u00b6 A lookup table of stages by stage ID Used when DAGScheduler creates a shuffle map stage , creates a result stage , cleans up job state and independent stages , is informed that a task is started , a taskset has failed , a job is submitted (to compute a ResultStage ) , a map stage was submitted , a task has completed or a stage was cancelled , updates accumulators , aborts a stage and fails a job and independent stages . waitingStages \u00b6 Stages with parents to be computed Event Posting Methods \u00b6 Posting AllJobsCancelled \u00b6 Posts an AllJobsCancelled Used when SparkContext is requested to cancel all running or scheduled Spark jobs Posting JobCancelled \u00b6 Posts a JobCancelled Used when SparkContext or JobWaiter are requested to cancel a Spark job Posting JobGroupCancelled \u00b6 Posts a JobGroupCancelled Used when SparkContext is requested to cancel a job group Posting StageCancelled \u00b6 Posts a StageCancelled Used when SparkContext is requested to cancel a stage Posting ExecutorAdded \u00b6 Posts an ExecutorAdded Used when TaskSchedulerImpl is requested to handle resource offers (and a new executor is found in the resource offers) Posting ExecutorLost \u00b6 Posts a ExecutorLost Used when TaskSchedulerImpl is requested to handle a task status update (and a task gets lost which is used to indicate that the executor got broken and hence should be considered lost) or executorLost Posting JobSubmitted \u00b6 Posts a JobSubmitted Used when SparkContext is requested to run an approximate job Posting SpeculativeTaskSubmitted \u00b6 Posts a SpeculativeTaskSubmitted Used when TaskSetManager is requested to checkAndSubmitSpeculatableTask Posting MapStageSubmitted \u00b6 Posts a MapStageSubmitted Used when SparkContext is requested to submit a MapStage for execution Posting CompletionEvent \u00b6 Posts a CompletionEvent Used when TaskSetManager is requested to handleSuccessfulTask , handleFailedTask , and executorLost Posting GettingResultEvent \u00b6 Posts a GettingResultEvent Used when TaskSetManager is requested to handle a task fetching result Posting TaskSetFailed \u00b6 Posts a TaskSetFailed Used when TaskSetManager is requested to abort Posting BeginEvent \u00b6 Posts a BeginEvent Used when TaskSetManager is requested to start a task Posting WorkerRemoved \u00b6 Posts a WorkerRemoved Used when TaskSchedulerImpl is requested to handle a removed worker event Updating Accumulators of Completed Tasks \u00b6 updateAccumulators ( event : CompletionEvent ): Unit updateAccumulators merges the partial values of accumulators from a completed task (based on the given CompletionEvent ) into their \"source\" accumulators on the driver. For every AccumulatorV2 update (in the given CompletionEvent ), updateAccumulators finds the corresponding accumulator on the driver and requests the AccumulatorV2 to merge the updates . updateAccumulators ...FIXME For named accumulators with the update value being a non-zero value, i.e. not Accumulable.zero : stage.latestInfo.accumulables for the AccumulableInfo.id is set CompletionEvent.taskInfo.accumulables has a new AccumulableInfo added. CAUTION: FIXME Where are Stage.latestInfo.accumulables and CompletionEvent.taskInfo.accumulables used? updateAccumulators is used when DAGScheduler is requested to handle a task completion . Posting SparkListenerTaskEnd (at Task Completion) \u00b6 postTaskEnd ( event : CompletionEvent ): Unit postTaskEnd reconstructs task metrics (from the accumulator updates in the CompletionEvent ). In the end, postTaskEnd creates a SparkListenerTaskEnd and requests the LiveListenerBus to post it . postTaskEnd is used when: DAGScheduler is requested to handle a task completion Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.DAGScheduler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.DAGScheduler=ALL Refer to Logging .","title":"DAGScheduler"},{"location":"scheduler/DAGScheduler/#dagscheduler","text":"Note The introduction that follows was highly influenced by the scaladoc of org.apache.spark.scheduler.DAGScheduler . As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read the sources and only then read this and the related pages afterwards.","title":"DAGScheduler"},{"location":"scheduler/DAGScheduler/#introduction","text":"DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling using Jobs and Stages . DAGScheduler transforms a logical execution plan ( RDD lineage of dependencies built using RDD transformations ) to a physical execution plan (using stages ). After an action has been called on an RDD , SparkContext hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as TaskSets for execution. DAGScheduler works solely on the driver and is created as part of SparkContext's initialization (right after TaskScheduler and SchedulerBackend are ready). DAGScheduler does three things in Spark: Computes an execution DAG (DAG of stages) for a job Determines the preferred locations to run each task on Handles failures due to shuffle output files being lost DAGScheduler computes a directed acyclic graph (DAG) of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to TaskScheduler . In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to TaskScheduler . DAGScheduler tracks which rdd/spark-rdd-caching.md[RDDs are cached (or persisted)] to avoid \"recomputing\" them, i.e. redoing the map side of a shuffle. DAGScheduler remembers what ShuffleMapStage.md[ShuffleMapStage]s have already produced output files (that are stored in BlockManager s). DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of a RDD. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage. DAGScheduler uses an event queue architecture in which a thread can post DAGSchedulerEvent events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section < >. DAGScheduler runs stages in topological order. DAGScheduler uses SparkContext , TaskScheduler , LiveListenerBus.md[], MapOutputTracker.md[MapOutputTracker] and storage:BlockManager.md[BlockManager] for its services. However, at the very minimum, DAGScheduler takes a SparkContext only (and requests SparkContext for the other services). When DAGScheduler schedules a job as a result of rdd/index.md#actions[executing an action on a RDD] or calling SparkContext.runJob() method directly , it spawns parallel tasks to compute (partial) results per partition.","title":"Introduction"},{"location":"scheduler/DAGScheduler/#creating-instance","text":"DAGScheduler takes the following to be created: SparkContext TaskScheduler LiveListenerBus MapOutputTrackerMaster BlockManagerMaster SparkEnv Clock DAGScheduler is created when SparkContext is created. While being created, DAGScheduler requests the TaskScheduler to associate itself with and requests DAGScheduler Event Bus to start accepting events.","title":"Creating Instance"},{"location":"scheduler/DAGScheduler/#dagschedulersource","text":"DAGScheduler uses DAGSchedulerSource for performance metrics.","title":" DAGSchedulerSource"},{"location":"scheduler/DAGScheduler/#dagscheduler-event-bus","text":"DAGScheduler uses an event bus to process scheduling events on a separate thread (one by one and asynchronously). DAGScheduler requests the event bus to start right when created and stops it when requested to stop . DAGScheduler defines event-posting methods for posting DAGSchedulerEvent events to the event bus.","title":" DAGScheduler Event Bus"},{"location":"scheduler/DAGScheduler/#taskscheduler","text":"DAGScheduler is given a TaskScheduler when created . TaskScheduler is used for the following: Submitting missing tasks of a stage Handling task completion (CompletionEvent) Killing a task Failing a job and all other independent single-job stages Stopping itself","title":" TaskScheduler"},{"location":"scheduler/DAGScheduler/#running-job","text":"runJob [ T , U ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], callSite : CallSite , resultHandler : ( Int , U ) => Unit , properties : Properties ): Unit runJob submits a job and waits until a result is available. runJob prints out the following INFO message to the logs when the job has finished successfully: Job [jobId] finished: [callSite], took [time] s runJob prints out the following INFO message to the logs when the job has failed: Job [jobId] failed: [callSite], took [time] s runJob is used when: SparkContext is requested to run a job","title":" Running Job"},{"location":"scheduler/DAGScheduler/#submitting-job","text":"submitJob [ T , U ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], callSite : CallSite , resultHandler : ( Int , U ) => Unit , properties : Properties ): JobWaiter [ U ] submitJob increments the nextJobId internal counter. submitJob creates a JobWaiter for the (number of) partitions and the given resultHandler function. submitJob requests the DAGSchedulerEventProcessLoop to post a JobSubmitted . In the end, submitJob returns the JobWaiter . For empty partitions (no partitions to compute), submitJob requests the LiveListenerBus to post a SparkListenerJobStart and SparkListenerJobEnd (with JobSucceeded result marker) events and returns a JobWaiter with no tasks to wait for. submitJob throws an IllegalArgumentException when the partitions indices are not among the partitions of the given RDD : Attempting to access a non-existent partition: [p]. Total number of partitions: [maxPartitions] submitJob is used when: SparkContext is requested to submit a job DAGScheduler is requested to run a job","title":" Submitting Job"},{"location":"scheduler/DAGScheduler/#partition-placement-preferences","text":"DAGScheduler keeps track of block locations per RDD and partition. DAGScheduler uses TaskLocation that includes a host name and an executor id on that host (as ExecutorCacheTaskLocation ). The keys are RDDs (their ids) and the values are arrays indexed by partition numbers. Each entry is a set of block locations where a RDD partition is cached, i.e. the BlockManager s of the blocks. Initialized empty when DAGScheduler is created . Used when DAGScheduler is requested for the locations of the cache blocks of a RDD .","title":" Partition Placement Preferences"},{"location":"scheduler/DAGScheduler/#activejobs","text":"DAGScheduler tracks ActiveJob s: Adds a new ActiveJob when requested to handle JobSubmitted or MapStageSubmitted events Removes an ActiveJob when requested to clean up after an ActiveJob and independent stages . Removes all ActiveJobs when requested to doCancelAllJobs . DAGScheduler uses ActiveJobs registry when requested to handle JobGroupCancelled or TaskCompletion events, to cleanUpAfterSchedulerStop and to abort a stage . The number of ActiveJobs is available using job.activeJobs performance metric.","title":" ActiveJobs"},{"location":"scheduler/DAGScheduler/#creating-resultstage-for-rdd","text":"createResultStage ( rdd : RDD [ _ ], func : ( TaskContext , Iterator [ _ ]) => _ , partitions : Array [ Int ], jobId : Int , callSite : CallSite ): ResultStage createResultStage ...FIXME createResultStage is used when DAGScheduler is requested to handle a JobSubmitted event .","title":" Creating ResultStage for RDD"},{"location":"scheduler/DAGScheduler/#creating-shufflemapstage-for-shuffledependency","text":"createShuffleMapStage ( shuffleDep : ShuffleDependency [ _ , _ , _ ], jobId : Int ): ShuffleMapStage createShuffleMapStage creates a ShuffleMapStage for the given ShuffleDependency as follows: Stage ID is generated based on nextStageId internal counter RDD is taken from the given ShuffleDependency Number of tasks is the number of partitions of the RDD Parent RDDs MapOutputTrackerMaster createShuffleMapStage registers the ShuffleMapStage in the stageIdToStage and shuffleIdToMapStage internal registries. createShuffleMapStage updateJobIdStageIdMaps . createShuffleMapStage requests the MapOutputTrackerMaster to check whether it contains the shuffle ID or not . If not, createShuffleMapStage prints out the following INFO message to the logs and requests the MapOutputTrackerMaster to register the shuffle . Registering RDD [id] ([creationSite]) as input to shuffle [shuffleId] createShuffleMapStage is used when: DAGScheduler is requested to find or create a ShuffleMapStage for a given ShuffleDependency","title":" Creating ShuffleMapStage for ShuffleDependency"},{"location":"scheduler/DAGScheduler/#cleaning-up-after-job-and-independent-stages","text":"cleanupStateForJobAndIndependentStages ( job : ActiveJob ): Unit cleanupStateForJobAndIndependentStages cleans up the state for job and any stages that are not part of any other job. cleanupStateForJobAndIndependentStages looks the job up in the internal < > registry. If no stages are found, the following ERROR is printed out to the logs: No stages registered for job [jobId] Oterwise, cleanupStateForJobAndIndependentStages uses < > registry to find the stages (the real objects not ids!). For each stage, cleanupStateForJobAndIndependentStages reads the jobs the stage belongs to. If the job does not belong to the jobs of the stage, the following ERROR is printed out to the logs: Job [jobId] not registered for stage [stageId] even though that stage was registered for the job If the job was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. < >, < >, < >, < > and < >. While removing from < >, you should see the following DEBUG message in the logs: Removing running stage [stageId] While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from waiting set. While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from failed set. After all cleaning (using < > as the source registry), if the stage belonged to the one and only job , you should see the following DEBUG message in the logs: After removal of stage [stageId], remaining stages = [stageIdToStage.size] The job is removed from < >, < >, < > registries. The final stage of the job is removed, i.e. ResultStage or ShuffleMapStage . cleanupStateForJobAndIndependentStages is used in handleTaskCompletion when a ResultTask has completed successfully , failJobAndIndependentStages and markMapStageJobAsFinished .","title":" Cleaning Up After Job and Independent Stages"},{"location":"scheduler/DAGScheduler/#marking-shufflemapstage-job-finished","text":"markMapStageJobAsFinished ( job : ActiveJob , stats : MapOutputStatistics ): Unit markMapStageJobAsFinished marks the active job finished and notifies Spark listeners. Internally, markMapStageJobAsFinished marks the zeroth partition finished and increases the number of tasks finished in job . The job listener is notified about the 0 th task succeeded . The < job and independent stages are cleaned up>>. Ultimately, SparkListenerJobEnd is posted to LiveListenerBus (as < >) for the job , the current time (in millis) and JobSucceeded job result. markMapStageJobAsFinished is used in handleMapStageSubmitted and handleTaskCompletion .","title":" Marking ShuffleMapStage Job Finished"},{"location":"scheduler/DAGScheduler/#finding-or-creating-missing-direct-parent-shufflemapstages-for-shuffledependencies-of-rdd","text":"getOrCreateParentStages ( rdd : RDD [ _ ], firstJobId : Int ): List [ Stage ] getOrCreateParentStages < ShuffleDependencies >> of the input rdd and then < ShuffleMapStage stages>> for each ShuffleDependency . getOrCreateParentStages is used when DAGScheduler is requested to create a ShuffleMapStage or a ResultStage .","title":" Finding Or Creating Missing Direct Parent ShuffleMapStages (For ShuffleDependencies) of RDD"},{"location":"scheduler/DAGScheduler/#marking-stage-finished","text":"markStageAsFinished ( stage : Stage , errorMessage : Option [ String ] = None , willRetry : Boolean = false ): Unit markStageAsFinished ...FIXME markStageAsFinished is used when...FIXME","title":" Marking Stage Finished"},{"location":"scheduler/DAGScheduler/#finding-or-creating-shufflemapstage-for-shuffledependency","text":"getOrCreateShuffleMapStage ( shuffleDep : ShuffleDependency [ _ , _ , _ ], firstJobId : Int ): ShuffleMapStage getOrCreateShuffleMapStage finds a ShuffleMapStage by the shuffleId of the given ShuffleDependency in the shuffleIdToMapStage internal registry and returns it if available. If not found, getOrCreateShuffleMapStage finds all the missing ancestor shuffle dependencies and creates the missing ShuffleMapStage stages (including one for the input ShuffleDependency ). getOrCreateShuffleMapStage is used when: DAGScheduler is requested to find or create missing direct parent ShuffleMapStages of an RDD , find missing parent ShuffleMapStages for a stage , handle a MapStageSubmitted event , and check out stage dependency on a stage","title":" Finding or Creating ShuffleMapStage for ShuffleDependency"},{"location":"scheduler/DAGScheduler/#finding-missing-shuffledependencies-for-rdd","text":"getMissingAncestorShuffleDependencies ( rdd : RDD [ _ ]): Stack [ ShuffleDependency [ _ , _ , _ ]] getMissingAncestorShuffleDependencies finds all missing shuffle dependencies for the given RDD traversing its rdd/spark-rdd-lineage.md[RDD lineage]. NOTE: A missing shuffle dependency of a RDD is a dependency not registered in < shuffleIdToMapStage internal registry>>. Internally, getMissingAncestorShuffleDependencies < >\u2009of the input RDD and collects the ones that are not registered in < shuffleIdToMapStage internal registry>>. It repeats the process for the RDDs of the parent shuffle dependencies. getMissingAncestorShuffleDependencies is used when DAGScheduler is requested to find all ShuffleMapStage stages for a ShuffleDependency .","title":" Finding Missing ShuffleDependencies For RDD"},{"location":"scheduler/DAGScheduler/#finding-direct-parent-shuffle-dependencies-of-rdd","text":"getShuffleDependencies ( rdd : RDD [ _ ]): HashSet [ ShuffleDependency [ _ , _ , _ ]] getShuffleDependencies finds direct parent shuffle dependencies for the given RDD . Internally, getShuffleDependencies takes the direct rdd/index.md#dependencies[shuffle dependencies of the input RDD] and direct shuffle dependencies of all the parent non- ShuffleDependencies in the dependency chain (aka RDD lineage ). getShuffleDependencies is used when DAGScheduler is requested to find or create missing direct parent ShuffleMapStages (for ShuffleDependencies of a RDD) and find all missing shuffle dependencies for a given RDD .","title":" Finding Direct Parent Shuffle Dependencies of RDD"},{"location":"scheduler/DAGScheduler/#failing-job-and-independent-single-job-stages","text":"failJobAndIndependentStages ( job : ActiveJob , failureReason : String , exception : Option [ Throwable ] = None ): Unit failJobAndIndependentStages fails the input job and all the stages that are only used by the job. Internally, failJobAndIndependentStages uses < jobIdToStageIds internal registry>> to look up the stages registered for the job. If no stages could be found, you should see the following ERROR message in the logs: No stages registered for job [id] Otherwise, for every stage, failJobAndIndependentStages finds the job ids the stage belongs to. If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs: Job [id] not registered for stage [id] even though that stage was registered for the job Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in runningStages internal registry), TaskScheduler.md#contract[ TaskScheduler is requested to cancel the stage's tasks] and < >. NOTE: failJobAndIndependentStages uses < >, < >, and < > internal registries. failJobAndIndependentStages is used when...FIXME","title":" Failing Job and Independent Single-Job Stages"},{"location":"scheduler/DAGScheduler/#aborting-stage","text":"abortStage ( failedStage : Stage , reason : String , exception : Option [ Throwable ]): Unit abortStage is an internal method that finds all the active jobs that depend on the failedStage stage and fails them. Internally, abortStage looks the failedStage stage up in the internal < > registry and exits if there the stage was not registered earlier. If it was, abortStage finds all the active jobs (in the internal < > registry) with the < failedStage stage>>. At this time, the completionTime property (of the failed stage's StageInfo ) is assigned to the current time (millis). All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka independent stages ) are < > (with the failure reason being \"Job aborted due to stage failure: [reason]\" and the input exception ). If there are no jobs depending on the failed stage, you should see the following INFO message in the logs: Ignoring failure of [failedStage] because all jobs depending on it are done abortStage is used when DAGScheduler is requested to handle a TaskSetFailed event , submit a stage , submit missing tasks of a stage , handle a TaskCompletion event .","title":" Aborting Stage"},{"location":"scheduler/DAGScheduler/#checking-out-stage-dependency-on-given-stage","text":"stageDependsOn ( stage : Stage , target : Stage ): Boolean stageDependsOn compares two stages and returns whether the stage depends on target stage (i.e. true ) or not (i.e. false ). NOTE: A stage A depends on stage B if B is among the ancestors of A . Internally, stageDependsOn walks through the graph of RDDs of the input stage . For every RDD in the RDD's dependencies (using RDD.dependencies ) stageDependsOn adds the RDD of a NarrowDependency to a stack of RDDs to visit while for a ShuffleDependency it < ShuffleMapStage stages for a ShuffleDependency >> for the dependency and the stage 's first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs. After all the RDDs of the input stage are visited, stageDependsOn checks if the target 's RDD is among the RDDs of the stage , i.e. whether the stage depends on target stage. stageDependsOn is used when DAGScheduler is requested to abort a stage .","title":" Checking Out Stage Dependency on Given Stage"},{"location":"scheduler/DAGScheduler/#submitting-waiting-child-stages-for-execution","text":"submitWaitingChildStages ( parent : Stage ): Unit submitWaitingChildStages submits for execution all waiting stages for which the input parent Stage.md[Stage] is the direct parent. NOTE: Waiting stages are the stages registered in < waitingStages internal registry>>. When executed, you should see the following TRACE messages in the logs: Checking if any dependencies of [parent] are now runnable running: [runningStages] waiting: [waitingStages] failed: [failedStages] submitWaitingChildStages finds child stages of the input parent stage, removes them from waitingStages internal registry, and < > one by one sorted by their job ids. submitWaitingChildStages is used when DAGScheduler is requested to submits missing tasks for a stage and handles a successful ShuffleMapTask completion .","title":" Submitting Waiting Child Stages for Execution"},{"location":"scheduler/DAGScheduler/#submitting-stage-with-missing-parents-for-execution","text":"submitStage ( stage : Stage ): Unit submitStage submits the input stage or its missing parents (if there any stages not computed yet before the input stage could). NOTE: submitStage is also used to DAGSchedulerEventProcessLoop.md#resubmitFailedStages[resubmit failed stages]. submitStage recursively submits any missing parents of the stage . Internally, submitStage first finds the earliest-created job id that needs the stage . NOTE: A stage itself tracks the jobs (their ids) it belongs to (using the internal jobIds registry). The following steps depend on whether there is a job or not. If there are no jobs that require the stage , submitStage < > with the reason: No active job for stage [id] If however there is a job for the stage , you should see the following DEBUG message in the logs: submitStage([stage]) submitStage checks the status of the stage and continues when it was not recorded in < >, < > or < > internal registries. It simply exits otherwise. With the stage ready for submission, submitStage calculates the < stage >> (sorted by their job ids). You should see the following DEBUG message in the logs: missing: [missing] When the stage has no parent stages missing, you should see the following INFO message in the logs: Submitting [stage] ([stage.rdd]), which has no missing parents submitStage < stage >> (with the earliest-created job id) and finishes. If however there are missing parent stages for the stage , submitStage < >, and the stage is recorded in the internal < > registry. submitStage is used recursively for missing parents of the given stage and when DAGScheduler is requested for the following: resubmitFailedStages (ResubmitFailedStages event) submitWaitingChildStages (CompletionEvent event) Handle JobSubmitted , MapStageSubmitted and TaskCompletion events","title":" Submitting Stage (with Missing Parents) for Execution"},{"location":"scheduler/DAGScheduler/#stage-attempts","text":"A single stage can be re-executed in multiple attempts due to fault recovery. The number of attempts is configured (FIXME). If TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits the lost stage. This is detected through a DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[ CompletionEvent with FetchFailed ], or an < > event. DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. Please note that tasks from the old attempts of a stage could still be running. A stage object tracks multiple StageInfo objects to pass to Spark listeners or the web UI. The latest StageInfo for the most recent attempt for a stage is accessible through latestInfo .","title":" Stage Attempts"},{"location":"scheduler/DAGScheduler/#preferred-locations","text":"DAGScheduler computes where to run each task in a stage based on the rdd/index.md#getPreferredLocations[preferred locations of its underlying RDDs], or < >.","title":" Preferred Locations"},{"location":"scheduler/DAGScheduler/#adaptive-query-planning-adaptive-scheduling","text":"See SPARK-9850 Adaptive execution in Spark for the design document. The work is currently in progress. DAGScheduler.submitMapStage method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages.","title":" Adaptive Query Planning / Adaptive Scheduling"},{"location":"scheduler/DAGScheduler/#scheduledexecutorservice-daemon-services","text":"DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation): dag-scheduler-message - a daemon thread pool using j.u.c.ScheduledThreadPoolExecutor with core pool size 1 . It is used to post a DAGSchedulerEventProcessLoop.md#ResubmitFailedStages[ResubmitFailedStages] event when DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[ FetchFailed is reported]. They are created using ThreadUtils.newDaemonSingleThreadScheduledExecutor method that uses Guava DSL to instantiate a ThreadFactory.","title":"ScheduledExecutorService daemon services"},{"location":"scheduler/DAGScheduler/#finding-missing-parent-shufflemapstages-for-stage","text":"getMissingParentStages ( stage : Stage ): List [ Stage ] getMissingParentStages finds missing parent ShuffleMapStage s in the dependency graph of the input stage (using the breadth-first search algorithm ). Internally, getMissingParentStages starts with the stage 's RDD and walks up the tree of all parent RDDs to find < >. NOTE: A Stage tracks the associated RDD using Stage.md#rdd[ rdd property]. NOTE: An uncached partition of a RDD is a partition that has Nil in the < > (which results in no RDD blocks in any of the active storage:BlockManager.md[BlockManager]s on executors). getMissingParentStages traverses the rdd/index.md#dependencies[parent dependencies of the RDD] and acts according to their type, i.e. ShuffleDependency or NarrowDependency . NOTE: ShuffleDependency and NarrowDependency are the main top-level Dependencies . For each NarrowDependency , getMissingParentStages simply marks the corresponding RDD to visit and moves on to a next dependency of a RDD or works on another unvisited parent RDD. NOTE: NarrowDependency is a RDD dependency that allows for pipelined execution. getMissingParentStages focuses on ShuffleDependency dependencies. NOTE: ShuffleDependency is a RDD dependency that represents a dependency on the output of a ShuffleMapStage , i.e. shuffle map stage . For each ShuffleDependency , getMissingParentStages < ShuffleMapStage stages>>. If the ShuffleMapStage is not available , it is added to the set of missing (map) stages. NOTE: A ShuffleMapStage is available when all its partitions are computed, i.e. results are available (as blocks). CAUTION: FIXME...IMAGE with ShuffleDependencies queried getMissingParentStages is used when DAGScheduler is requested to submit a stage and handle JobSubmitted and MapStageSubmitted events.","title":" Finding Missing Parent ShuffleMapStages For Stage"},{"location":"scheduler/DAGScheduler/#submitting-missing-tasks-of-stage","text":"submitMissingTasks ( stage : Stage , jobId : Int ): Unit submitMissingTasks prints out the following DEBUG message to the logs: submitMissingTasks([stage]) submitMissingTasks requests the given Stage for the missing partitions (partitions that need to be computed). submitMissingTasks adds the stage to the runningStages internal registry. submitMissingTasks notifies the OutputCommitCoordinator that stage execution started . submitMissingTasks determines preferred locations ( task locality preferences ) of the missing partitions. submitMissingTasks requests the stage for a new stage attempt . submitMissingTasks requests the LiveListenerBus to post a SparkListenerStageSubmitted event. submitMissingTasks uses the closure Serializer to serialize the stage and create a so-called task binary. submitMissingTasks serializes the RDD (of the stage) and either the ShuffleDependency or the compute function based on the type of the stage ( ShuffleMapStage or ResultStage , respectively). submitMissingTasks creates a broadcast variable for the task binary. Note That shows how important Broadcast s are for Spark itself to distribute data among executors in a Spark application in the most efficient way. submitMissingTasks creates tasks for every missing partition: ShuffleMapTasks for a ShuffleMapStage ResultTasks for a ResultStage If there are tasks to submit for execution (i.e. there are missing partitions in the stage), submitMissingTasks prints out the following INFO message to the logs: Submitting [size] missing tasks from [stage] ([rdd]) (first 15 tasks are for partitions [partitionIds]) submitMissingTasks requests the < > to TaskScheduler.md#submitTasks[submit the tasks for execution] (as a new TaskSet.md[TaskSet]). With no tasks to submit for execution, submitMissingTasks < >. submitMissingTasks prints out the following DEBUG messages based on the type of the stage: Stage [stage] is actually done; (available: [isAvailable],available outputs: [numAvailableOutputs],partitions: [numPartitions]) or Stage [stage] is actually done; (partitions: [numPartitions]) for ShuffleMapStage and ResultStage , respectively. In the end, with no tasks to submit for execution, submitMissingTasks < > and exits. submitMissingTasks is used when DAGScheduler is requested to submit a stage for execution .","title":" Submitting Missing Tasks of Stage"},{"location":"scheduler/DAGScheduler/#finding-preferred-locations-for-missing-partitions","text":"getPreferredLocs ( rdd : RDD [ _ ], partition : Int ): Seq [ TaskLocation ] getPreferredLocs is simply an alias for the internal (recursive) < >. getPreferredLocs is used when...FIXME","title":" Finding Preferred Locations for Missing Partitions"},{"location":"scheduler/DAGScheduler/#finding-blockmanagers-executors-for-cached-rdd-partitions-aka-block-location-discovery","text":"getCacheLocs ( rdd : RDD [ _ ]): IndexedSeq [ Seq [ TaskLocation ]] getCacheLocs gives TaskLocations (block locations) for the partitions of the input rdd . getCacheLocs caches lookup results in < > internal registry. NOTE: The size of the collection from getCacheLocs is exactly the number of partitions in rdd RDD. NOTE: The size of every TaskLocation collection (i.e. every entry in the result of getCacheLocs ) is exactly the number of blocks managed using storage:BlockManager.md[BlockManagers] on executors. Internally, getCacheLocs finds rdd in the < > internal registry (of partition locations per RDD). If rdd is not in < > internal registry, getCacheLocs branches per its storage:StorageLevel.md[storage level]. For NONE storage level (i.e. no caching), the result is an empty locations (i.e. no location preference). For other non- NONE storage levels, getCacheLocs storage:BlockManagerMaster.md#getLocations-block-array[requests BlockManagerMaster for block locations] that are then mapped to TaskLocations with the hostname of the owning BlockManager for a block (of a partition) and the executor id. NOTE: getCacheLocs uses < > that was defined when < >. getCacheLocs records the computed block locations per partition (as TaskLocation ) in < > internal registry. NOTE: getCacheLocs requests locations from BlockManagerMaster using storage:BlockId.md#RDDBlockId[RDDBlockId] with the RDD id and the partition indices (which implies that the order of the partitions matters to request proper blocks). NOTE: DAGScheduler uses TaskLocation.md[TaskLocations] (with host and executor) while storage:BlockManagerMaster.md[BlockManagerMaster] uses storage:BlockManagerId.md[] (to track similar information, i.e. block locations). getCacheLocs is used when DAGScheduler is requested to find missing parent MapStages and getPreferredLocsInternal .","title":" Finding BlockManagers (Executors) for Cached RDD Partitions (aka Block Location Discovery)"},{"location":"scheduler/DAGScheduler/#finding-placement-preferences-for-rdd-partition-recursively","text":"getPreferredLocsInternal ( rdd : RDD [ _ ], partition : Int , visited : HashSet [( RDD [ _ ], Int )]): Seq [ TaskLocation ] getPreferredLocsInternal first < TaskLocations for the partition of the rdd >> (using < > internal cache) and returns them. Otherwise, if not found, getPreferredLocsInternal rdd/index.md#preferredLocations[requests rdd for the preferred locations of partition ] and returns them. NOTE: Preferred locations of the partitions of a RDD are also called placement preferences or locality preferences . Otherwise, if not found, getPreferredLocsInternal finds the first parent NarrowDependency and (recursively) finds TaskLocations . If all the attempts fail to yield any non-empty result, getPreferredLocsInternal returns an empty collection of TaskLocation.md[TaskLocations]. getPreferredLocsInternal is used when DAGScheduler is requested for the preferred locations for missing partitions .","title":" Finding Placement Preferences for RDD Partition (recursively)"},{"location":"scheduler/DAGScheduler/#stopping-dagscheduler","text":"stop (): Unit stop stops the internal dag-scheduler-message thread pool, dag-scheduler-event-loop , and TaskScheduler . stop is used when SparkContext is requested to stop .","title":" Stopping DAGScheduler"},{"location":"scheduler/DAGScheduler/#checkbarrierstagewithnumslots","text":"checkBarrierStageWithNumSlots ( rdd : RDD [ _ ]): Unit checkBarrierStageWithNumSlots...FIXME checkBarrierStageWithNumSlots is used when DAGScheduler is requested to create < > and < > stages.","title":" checkBarrierStageWithNumSlots"},{"location":"scheduler/DAGScheduler/#killing-task","text":"killTaskAttempt ( taskId : Long , interruptThread : Boolean , reason : String ): Boolean killTaskAttempt requests the TaskScheduler to kill a task . killTaskAttempt is used when SparkContext is requested to kill a task .","title":" Killing Task"},{"location":"scheduler/DAGScheduler/#cleanupafterschedulerstop","text":"cleanUpAfterSchedulerStop (): Unit cleanUpAfterSchedulerStop ...FIXME cleanUpAfterSchedulerStop is used when DAGSchedulerEventProcessLoop is requested to onStop .","title":" cleanUpAfterSchedulerStop"},{"location":"scheduler/DAGScheduler/#removeexecutorandunregisteroutputs","text":"removeExecutorAndUnregisterOutputs ( execId : String , fileLost : Boolean , hostToUnregisterOutputs : Option [ String ], maybeEpoch : Option [ Long ] = None ): Unit removeExecutorAndUnregisterOutputs...FIXME removeExecutorAndUnregisterOutputs is used when DAGScheduler is requested to handle < > (due to a fetch failure) and < > events.","title":" removeExecutorAndUnregisterOutputs"},{"location":"scheduler/DAGScheduler/#markmapstagejobsasfinished","text":"markMapStageJobsAsFinished ( shuffleStage : ShuffleMapStage ): Unit markMapStageJobsAsFinished ...FIXME markMapStageJobsAsFinished is used when DAGScheduler is requested to submit missing tasks (of a ShuffleMapStage that has just been computed) and handle a task completion (of a ShuffleMapStage ).","title":" markMapStageJobsAsFinished"},{"location":"scheduler/DAGScheduler/#updatejobidstageidmaps","text":"updateJobIdStageIdMaps ( jobId : Int , stage : Stage ): Unit updateJobIdStageIdMaps ...FIXME updateJobIdStageIdMaps is used when DAGScheduler is requested to create ShuffleMapStage and ResultStage stages.","title":" updateJobIdStageIdMaps"},{"location":"scheduler/DAGScheduler/#executorheartbeatreceived","text":"executorHeartbeatReceived ( execId : String , // (taskId, stageId, stageAttemptId, accumUpdates) accumUpdates : Array [( Long , Int , Int , Seq [ AccumulableInfo ])], blockManagerId : BlockManagerId , // (stageId, stageAttemptId) -> metrics executorUpdates : mutable . Map [( Int , Int ), ExecutorMetrics ]): Boolean executorHeartbeatReceived posts a SparkListenerExecutorMetricsUpdate (to listenerBus ) and informs BlockManagerMaster that blockManagerId block manager is alive (by posting BlockManagerHeartbeat ). executorHeartbeatReceived is used when TaskSchedulerImpl is requested to handle an executor heartbeat .","title":" executorHeartbeatReceived"},{"location":"scheduler/DAGScheduler/#event-handlers","text":"","title":"Event Handlers"},{"location":"scheduler/DAGScheduler/#alljobscancelled-event-handler","text":"doCancelAllJobs (): Unit doCancelAllJobs ...FIXME doCancelAllJobs is used when DAGSchedulerEventProcessLoop is requested to handle an AllJobsCancelled event and onError .","title":" AllJobsCancelled Event Handler"},{"location":"scheduler/DAGScheduler/#beginevent-event-handler","text":"handleBeginEvent ( task : Task [ _ ], taskInfo : TaskInfo ): Unit handleBeginEvent ...FIXME handleBeginEvent is used when DAGSchedulerEventProcessLoop is requested to handle a BeginEvent event.","title":" BeginEvent Event Handler"},{"location":"scheduler/DAGScheduler/#handling-task-completion-event","text":"handleTaskCompletion ( event : CompletionEvent ): Unit handleTaskCompletion handles a CompletionEvent . handleTaskCompletion notifies the OutputCommitCoordinator that a task completed . handleTaskCompletion finds the stage in the stageIdToStage registry. If not found, handleTaskCompletion postTaskEnd and quits. handleTaskCompletion updateAccumulators . handleTaskCompletion announces task completion application-wide . handleTaskCompletion branches off per TaskEndReason (as event.reason ). TaskEndReason Description Success Acts according to the type of the task that completed, i.e. ShuffleMapTask and ResultTask Resubmitted others","title":" Handling Task Completion Event"},{"location":"scheduler/DAGScheduler/#handling-successful-task-completion","text":"When a task has finished successfully (i.e. Success end reason), handleTaskCompletion marks the partition as no longer pending (i.e. the partition the task worked on is removed from pendingPartitions of the stage). NOTE: A Stage tracks its own pending partitions using scheduler:Stage.md#pendingPartitions[ pendingPartitions property]. handleTaskCompletion branches off given the type of the task that completed, i.e. < > and < >.","title":" Handling Successful Task Completion"},{"location":"scheduler/DAGScheduler/#handling-successful-resulttask-completion","text":"For scheduler:ResultTask.md[ResultTask], the stage is assumed a scheduler:ResultStage.md[ResultStage]. handleTaskCompletion finds the ActiveJob associated with the ResultStage . NOTE: scheduler:ResultStage.md[ResultStage] tracks the optional ActiveJob as scheduler:ResultStage.md#activeJob[ activeJob property]. There could only be one active job for a ResultStage . If there is no job for the ResultStage , you should see the following INFO message in the logs: Ignoring result from [task] because its job has finished Otherwise, when the ResultStage has a ActiveJob , handleTaskCompletion checks the status of the partition output for the partition the ResultTask ran for. NOTE: ActiveJob tracks task completions in finished property with flags for every partition in a stage. When the flag for a partition is enabled (i.e. true ), it is assumed that the partition has been computed (and no results from any ResultTask are expected and hence simply ignored). CAUTION: FIXME Describe why could a partition has more ResultTask running. handleTaskCompletion ignores the CompletionEvent when the partition has already been marked as completed for the stage and simply exits. handleTaskCompletion scheduler:DAGScheduler.md#updateAccumulators[updates accumulators]. The partition for the ActiveJob (of the ResultStage ) is marked as computed and the number of partitions calculated increased. NOTE: ActiveJob tracks what partitions have already been computed and their number. If the ActiveJob has finished (when the number of partitions computed is exactly the number of partitions in a stage) handleTaskCompletion does the following (in order): scheduler:DAGScheduler.md#markStageAsFinished[Marks ResultStage computed]. scheduler:DAGScheduler.md#cleanupStateForJobAndIndependentStages[Cleans up after ActiveJob and independent stages]. Announces the job completion application-wide (by posting a SparkListener.md#SparkListenerJobEnd[SparkListenerJobEnd] to scheduler:LiveListenerBus.md[]). In the end, handleTaskCompletion notifies JobListener of the ActiveJob that the task succeeded . NOTE: A task succeeded notification holds the output index and the result. When the notification throws an exception (because it runs user code), handleTaskCompletion notifies JobListener about the failure (wrapping it inside a SparkDriverExecutionException exception).","title":" Handling Successful ResultTask Completion"},{"location":"scheduler/DAGScheduler/#handling-successful-shufflemaptask-completion","text":"For scheduler:ShuffleMapTask.md[ShuffleMapTask], the stage is assumed a scheduler:ShuffleMapStage.md[ShuffleMapStage]. handleTaskCompletion scheduler:DAGScheduler.md#updateAccumulators[updates accumulators]. The task's result is assumed scheduler:MapStatus.md[MapStatus] that knows the executor where the task has finished. You should see the following DEBUG message in the logs: ShuffleMapTask finished on [execId] If the executor is registered in scheduler:DAGScheduler.md#failedEpoch[ failedEpoch internal registry] and the epoch of the completed task is not greater than that of the executor (as in failedEpoch registry), you should see the following INFO message in the logs: Ignoring possibly bogus [task] completion from executor [executorId] Otherwise, handleTaskCompletion scheduler:ShuffleMapStage.md#addOutputLoc[registers the MapStatus result for the partition with the stage] (of the completed task). handleTaskCompletion does more processing only if the ShuffleMapStage is registered as still running (in scheduler:DAGScheduler.md#runningStages[ runningStages internal registry]) and the scheduler:Stage.md#pendingPartitions[ ShuffleMapStage stage has no pending partitions to compute]. The ShuffleMapStage is < >. You should see the following INFO messages in the logs: looking for newly runnable stages running: [runningStages] waiting: [waitingStages] failed: [failedStages] handleTaskCompletion scheduler:MapOutputTrackerMaster.md#registerMapOutputs[registers the shuffle map outputs of the ShuffleDependency with MapOutputTrackerMaster ] (with the epoch incremented) and scheduler:DAGScheduler.md#clearCacheLocs[clears internal cache of the stage's RDD block locations]. NOTE: scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] is given when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. If the scheduler:ShuffleMapStage.md#isAvailable[ ShuffleMapStage stage is ready], all scheduler:ShuffleMapStage.md#mapStageJobs[active jobs of the stage] (aka map-stage jobs ) are scheduler:DAGScheduler.md#markMapStageJobAsFinished[marked as finished] (with scheduler:MapOutputTrackerMaster.md#getStatistics[ MapOutputStatistics from MapOutputTrackerMaster for the ShuffleDependency ]). NOTE: A ShuffleMapStage stage is ready (aka available ) when all partitions have shuffle outputs, i.e. when their tasks have completed. Eventually, handleTaskCompletion scheduler:DAGScheduler.md#submitWaitingChildStages[submits waiting child stages (of the ready ShuffleMapStage )]. If however the ShuffleMapStage is not ready, you should see the following INFO message in the logs: Resubmitting [shuffleStage] ([shuffleStage.name]) because some of its tasks had failed: [missingPartitions] In the end, handleTaskCompletion scheduler:DAGScheduler.md#submitStage[submits the ShuffleMapStage for execution].","title":" Handling Successful ShuffleMapTask Completion"},{"location":"scheduler/DAGScheduler/#taskendreason-resubmitted","text":"For Resubmitted case, you should see the following INFO message in the logs: Resubmitted [task], so marking it as still running The task (by task.partitionId ) is added to the collection of pending partitions of the stage (using stage.pendingPartitions ). TIP: A stage knows how many partitions are yet to be calculated. A task knows about the partition id for which it was launched.","title":" TaskEndReason: Resubmitted"},{"location":"scheduler/DAGScheduler/#task-failed-with-fetchfailed-exception","text":"FetchFailed ( bmAddress : BlockManagerId , shuffleId : Int , mapId : Int , reduceId : Int , message : String ) extends TaskFailedReason When FetchFailed happens, stageIdToStage is used to access the failed stage (using task.stageId and the task is available in event in handleTaskCompletion(event: CompletionEvent) ). shuffleToMapStage is used to access the map stage (using shuffleId ). If failedStage.latestInfo.attemptId != task.stageAttemptId , you should see the following INFO in the logs: Ignoring fetch failure from [task] as it's from [failedStage] attempt [task.stageAttemptId] and there is a more recent attempt for that stage (attempt ID [failedStage.latestInfo.attemptId]) running CAUTION: FIXME What does failedStage.latestInfo.attemptId != task.stageAttemptId mean? And the case finishes. Otherwise, the case continues. If the failed stage is in runningStages , the following INFO message shows in the logs: Marking [failedStage] ([failedStage.name]) as failed due to a fetch failure from [mapStage] ([mapStage.name]) markStageAsFinished(failedStage, Some(failureMessage)) is called. CAUTION: FIXME What does markStageAsFinished do? If the failed stage is not in runningStages , the following DEBUG message shows in the logs: Received fetch failure from [task], but its from [failedStage] which is no longer running When disallowStageRetryForTest is set, abortStage(failedStage, \"Fetch failure will not retry stage due to testing config\", None) is called. CAUTION: FIXME Describe disallowStageRetryForTest and abortStage . If the scheduler:Stage.md#failedOnFetchAndShouldAbort[number of fetch failed attempts for the stage exceeds the allowed number], the scheduler:DAGScheduler.md#abortStage[failed stage is aborted] with the reason: [failedStage] ([name]) has failed the maximum allowable number of times: 4. Most recent failure reason: [failureMessage] If there are no failed stages reported (scheduler:DAGScheduler.md#failedStages[DAGScheduler.failedStages] is empty), the following INFO shows in the logs: Resubmitting [mapStage] ([mapStage.name]) and [failedStage] ([failedStage.name]) due to fetch failure And the following code is executed: messageScheduler.schedule( new Runnable { override def run(): Unit = eventProcessLoop.post(ResubmitFailedStages) }, DAGScheduler.RESUBMIT_TIMEOUT, TimeUnit.MILLISECONDS) CAUTION: FIXME What does the above code do? For all the cases, the failed stage and map stages are both added to the internal scheduler:DAGScheduler.md#failedStages[registry of failed stages]. If mapId (in the FetchFailed object for the case) is provided, the map stage output is cleaned up (as it is broken) using mapStage.removeOutputLoc(mapId, bmAddress) and scheduler:MapOutputTracker.md#unregisterMapOutput[MapOutputTrackerMaster.unregisterMapOutput(shuffleId, mapId, bmAddress)] methods. CAUTION: FIXME What does mapStage.removeOutputLoc do? If BlockManagerId (as bmAddress in the FetchFailed object) is defined, handleTaskCompletion < > (with filesLost enabled and maybeEpoch from the scheduler:Task.md#epoch[Task] that completed). handleTaskCompletion is used when: DAGSchedulerEventProcessLoop is requested to handle a CompletionEvent event.","title":" Task Failed with FetchFailed Exception"},{"location":"scheduler/DAGScheduler/#executoradded-event-handler","text":"handleExecutorAdded ( execId : String , host : String ): Unit handleExecutorAdded ...FIXME handleExecutorAdded is used when DAGSchedulerEventProcessLoop is requested to handle an ExecutorAdded event.","title":" ExecutorAdded Event Handler"},{"location":"scheduler/DAGScheduler/#executorlost-event-handler","text":"handleExecutorLost ( execId : String , workerLost : Boolean ): Unit handleExecutorLost checks whether the input optional maybeEpoch is defined and if not requests the scheduler:MapOutputTracker.md#getEpoch[current epoch from MapOutputTrackerMaster ]. NOTE: MapOutputTrackerMaster is passed in (as mapOutputTracker ) when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. CAUTION: FIXME When is maybeEpoch passed in? .DAGScheduler.handleExecutorLost image::dagscheduler-handleExecutorLost.png[align=\"center\"] Recurring ExecutorLost events lead to the following repeating DEBUG message in the logs: DEBUG Additional executor lost message for [execId] (epoch [currentEpoch]) NOTE: handleExecutorLost handler uses DAGScheduler 's failedEpoch and FIXME internal registries. Otherwise, when the executor execId is not in the scheduler:DAGScheduler.md#failedEpoch[list of executor lost] or the executor failure's epoch is smaller than the input maybeEpoch , the executor's lost event is recorded in scheduler:DAGScheduler.md#failedEpoch[ failedEpoch internal registry]. CAUTION: FIXME Describe the case above in simpler non-technical words. Perhaps change the order, too. You should see the following INFO message in the logs: INFO Executor lost: [execId] (epoch [epoch]) storage:BlockManagerMaster.md#removeExecutor[ BlockManagerMaster is requested to remove the lost executor execId ]. CAUTION: FIXME Review what's filesLost . handleExecutorLost exits unless the ExecutorLost event was for a map output fetch operation (and the input filesLost is true ) or external shuffle service is not used. In such a case, you should see the following INFO message in the logs: Shuffle files lost for executor: [execId] (epoch [epoch]) handleExecutorLost walks over all scheduler:ShuffleMapStage.md[ShuffleMapStage]s in scheduler:DAGScheduler.md#shuffleToMapStage[DAGScheduler's shuffleToMapStage internal registry] and do the following (in order): ShuffleMapStage.removeOutputsOnExecutor(execId) is called scheduler:MapOutputTrackerMaster.md#registerMapOutputs[MapOutputTrackerMaster.registerMapOutputs(shuffleId, stage.outputLocInMapOutputTrackerFormat(), changeEpoch = true)] is called. In case scheduler:DAGScheduler.md#shuffleToMapStage[DAGScheduler's shuffleToMapStage internal registry] has no shuffles registered, scheduler:MapOutputTrackerMaster.md#incrementEpoch[ MapOutputTrackerMaster is requested to increment epoch]. Ultimatelly, DAGScheduler scheduler:DAGScheduler.md#clearCacheLocs[clears the internal cache of RDD partition locations]. handleExecutorLost is used when DAGSchedulerEventProcessLoop is requested to handle an ExecutorLost event.","title":" ExecutorLost Event Handler"},{"location":"scheduler/DAGScheduler/#gettingresultevent-event-handler","text":"handleGetTaskResult ( taskInfo : TaskInfo ): Unit handleGetTaskResult ...FIXME handleGetTaskResult is used when DAGSchedulerEventProcessLoop is requested to handle a GettingResultEvent event.","title":" GettingResultEvent Event Handler"},{"location":"scheduler/DAGScheduler/#jobcancelled-event-handler","text":"handleJobCancellation ( jobId : Int , reason : Option [ String ]): Unit handleJobCancellation looks up the active job for the input job ID (in jobIdToActiveJob internal registry) and fails it and all associated independent stages with failure reason: Job [jobId] cancelled [reason] When the input job ID is not found, handleJobCancellation prints out the following DEBUG message to the logs: Trying to cancel unregistered job [jobId] handleJobCancellation is used when DAGScheduler is requested to handle a JobCancelled event, doCancelAllJobs , handleJobGroupCancelled , handleStageCancellation .","title":" JobCancelled Event Handler"},{"location":"scheduler/DAGScheduler/#jobgroupcancelled-event-handler","text":"handleJobGroupCancelled ( groupId : String ): Unit handleJobGroupCancelled finds active jobs in a group and cancels them. Internally, handleJobGroupCancelled computes all the active jobs (registered in the internal collection of active jobs ) that have spark.jobGroup.id scheduling property set to groupId . handleJobGroupCancelled then cancels every active job in the group one by one and the cancellation reason: part of cancelled job group [groupId] handleJobGroupCancelled is used when DAGScheduler is requested to handle JobGroupCancelled event.","title":" JobGroupCancelled Event Handler"},{"location":"scheduler/DAGScheduler/#handling-jobsubmitted-event","text":"handleJobSubmitted ( jobId : Int , finalRDD : RDD [ _ ], func : ( TaskContext , Iterator [ _ ]) => _ , partitions : Array [ Int ], callSite : CallSite , listener : JobListener , properties : Properties ): Unit handleJobSubmitted creates a ResultStage (as finalStage in the picture below) for the given RDD, func , partitions , jobId and callSite . handleJobSubmitted creates an ActiveJob for the ResultStage . handleJobSubmitted clears the internal cache of RDD partition locations . Important FIXME Why is this clearing here so important? handleJobSubmitted prints out the following INFO messages to the logs (with missingParentStages ): Got job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingParentStages] handleJobSubmitted registers the new ActiveJob in jobIdToActiveJob and activeJobs internal registries. handleJobSubmitted requests the ResultStage to associate itself with the ActiveJob . handleJobSubmitted uses the jobIdToStageIds internal registry to find all registered stages for the given jobId . handleJobSubmitted uses the stageIdToStage internal registry to request the Stages for the latestInfo . In the end, handleJobSubmitted posts a SparkListenerJobStart message to the LiveListenerBus and submits the ResultStage . handleJobSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a JobSubmitted event.","title":" Handling JobSubmitted Event"},{"location":"scheduler/DAGScheduler/#mapstagesubmitted-event-handler","text":"handleMapStageSubmitted ( jobId : Int , dependency : ShuffleDependency [ _ , _ , _ ], callSite : CallSite , listener : JobListener , properties : Properties ): Unit Note MapStageSubmitted event processing is very similar to < > events. handleMapStageSubmitted finds or creates a new ShuffleMapStage for the input ShuffleDependency and jobId . handleMapStageSubmitted creates an ActiveJob . handleMapStageSubmitted clears the internal cache of RDD partition locations . Important FIXME Why is this clearing here so important? handleMapStageSubmitted prints out the following INFO messages to the logs: Got map stage job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingParentStages] handleMapStageSubmitted registers the new job in jobIdToActiveJob and activeJobs internal registries, and with the final ShuffleMapStage . Note ShuffleMapStage can have multiple ActiveJob s registered. handleMapStageSubmitted finds all the registered stages for the input jobId and collects their latest StageInfo . In the end, handleMapStageSubmitted posts SparkListenerJobStart message to LiveListenerBus and submits the ShuffleMapStage . When the ShuffleMapStage is available already, handleMapStageSubmitted marks the job finished . When handleMapStageSubmitted could not find or create a ShuffleMapStage , handleMapStageSubmitted prints out the following WARN message to the logs. Creating new stage failed due to exception - job: [id] handleMapStageSubmitted notifies listener about the job failure and exits. handleMapStageSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a MapStageSubmitted event.","title":" MapStageSubmitted Event Handler"},{"location":"scheduler/DAGScheduler/#resubmitfailedstages-event-handler","text":"resubmitFailedStages (): Unit resubmitFailedStages iterates over the internal collection of failed stages and submits them. Note resubmitFailedStages does nothing when there are no failed stages reported . resubmitFailedStages prints out the following INFO message to the logs: Resubmitting failed stages resubmitFailedStages clears the internal cache of RDD partition locations and makes a copy of the collection of failed stages to track failed stages afresh. Note At this point DAGScheduler has no failed stages reported. The previously-reported failed stages are sorted by the corresponding job ids in incremental order and resubmitted . resubmitFailedStages is used when DAGSchedulerEventProcessLoop is requested to handle a ResubmitFailedStages event.","title":" ResubmitFailedStages Event Handler"},{"location":"scheduler/DAGScheduler/#speculativetasksubmitted-event-handler","text":"handleSpeculativeTaskSubmitted (): Unit handleSpeculativeTaskSubmitted ...FIXME handleSpeculativeTaskSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a SpeculativeTaskSubmitted event.","title":" SpeculativeTaskSubmitted Event Handler"},{"location":"scheduler/DAGScheduler/#stagecancelled-event-handler","text":"handleStageCancellation (): Unit handleStageCancellation ...FIXME handleStageCancellation is used when DAGSchedulerEventProcessLoop is requested to handle a StageCancelled event.","title":" StageCancelled Event Handler"},{"location":"scheduler/DAGScheduler/#tasksetfailed-event-handler","text":"handleTaskSetFailed (): Unit handleTaskSetFailed ...FIXME handleTaskSetFailed is used when DAGSchedulerEventProcessLoop is requested to handle a TaskSetFailed event.","title":" TaskSetFailed Event Handler"},{"location":"scheduler/DAGScheduler/#workerremoved-event-handler","text":"handleWorkerRemoved ( workerId : String , host : String , message : String ): Unit handleWorkerRemoved ...FIXME handleWorkerRemoved is used when DAGSchedulerEventProcessLoop is requested to handle a WorkerRemoved event.","title":" WorkerRemoved Event Handler"},{"location":"scheduler/DAGScheduler/#internal-properties","text":"","title":"Internal Properties"},{"location":"scheduler/DAGScheduler/#failedepoch","text":"The lookup table of lost executors and the epoch of the event.","title":" failedEpoch"},{"location":"scheduler/DAGScheduler/#failedstages","text":"Stages that failed due to fetch failures (when a DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[task fails with FetchFailed exception]).","title":" failedStages"},{"location":"scheduler/DAGScheduler/#jobidtoactivejob","text":"The lookup table of ActiveJob s per job id.","title":" jobIdToActiveJob"},{"location":"scheduler/DAGScheduler/#jobidtostageids","text":"The lookup table of all stages per ActiveJob id","title":" jobIdToStageIds"},{"location":"scheduler/DAGScheduler/#nextjobid-counter","text":"nextJobId : AtomicInteger nextJobId is a Java AtomicInteger for job IDs. nextJobId starts at 0 . Used when DAGScheduler is requested for numTotalJobs , to submitJob , runApproximateJob and submitMapStage .","title":" nextJobId Counter"},{"location":"scheduler/DAGScheduler/#nextstageid","text":"The next stage id counting from 0 . Used when DAGScheduler creates a < > and a < >. It is the key in < >.","title":" nextStageId"},{"location":"scheduler/DAGScheduler/#runningstages","text":"The set of stages that are currently \"running\". A stage is added when < > gets executed (without first checking if the stage has not already been added).","title":" runningStages"},{"location":"scheduler/DAGScheduler/#shuffleidtomapstage","text":"A lookup table of ShuffleMapStage s by ShuffleDependency","title":" shuffleIdToMapStage"},{"location":"scheduler/DAGScheduler/#stageidtostage","text":"A lookup table of stages by stage ID Used when DAGScheduler creates a shuffle map stage , creates a result stage , cleans up job state and independent stages , is informed that a task is started , a taskset has failed , a job is submitted (to compute a ResultStage ) , a map stage was submitted , a task has completed or a stage was cancelled , updates accumulators , aborts a stage and fails a job and independent stages .","title":" stageIdToStage"},{"location":"scheduler/DAGScheduler/#waitingstages","text":"Stages with parents to be computed","title":" waitingStages"},{"location":"scheduler/DAGScheduler/#event-posting-methods","text":"","title":"Event Posting Methods"},{"location":"scheduler/DAGScheduler/#posting-alljobscancelled","text":"Posts an AllJobsCancelled Used when SparkContext is requested to cancel all running or scheduled Spark jobs","title":" Posting AllJobsCancelled"},{"location":"scheduler/DAGScheduler/#posting-jobcancelled","text":"Posts a JobCancelled Used when SparkContext or JobWaiter are requested to cancel a Spark job","title":" Posting JobCancelled"},{"location":"scheduler/DAGScheduler/#posting-jobgroupcancelled","text":"Posts a JobGroupCancelled Used when SparkContext is requested to cancel a job group","title":" Posting JobGroupCancelled"},{"location":"scheduler/DAGScheduler/#posting-stagecancelled","text":"Posts a StageCancelled Used when SparkContext is requested to cancel a stage","title":" Posting StageCancelled"},{"location":"scheduler/DAGScheduler/#posting-executoradded","text":"Posts an ExecutorAdded Used when TaskSchedulerImpl is requested to handle resource offers (and a new executor is found in the resource offers)","title":" Posting ExecutorAdded"},{"location":"scheduler/DAGScheduler/#posting-executorlost","text":"Posts a ExecutorLost Used when TaskSchedulerImpl is requested to handle a task status update (and a task gets lost which is used to indicate that the executor got broken and hence should be considered lost) or executorLost","title":" Posting ExecutorLost"},{"location":"scheduler/DAGScheduler/#posting-jobsubmitted","text":"Posts a JobSubmitted Used when SparkContext is requested to run an approximate job","title":" Posting JobSubmitted"},{"location":"scheduler/DAGScheduler/#posting-speculativetasksubmitted","text":"Posts a SpeculativeTaskSubmitted Used when TaskSetManager is requested to checkAndSubmitSpeculatableTask","title":" Posting SpeculativeTaskSubmitted"},{"location":"scheduler/DAGScheduler/#posting-mapstagesubmitted","text":"Posts a MapStageSubmitted Used when SparkContext is requested to submit a MapStage for execution","title":" Posting MapStageSubmitted"},{"location":"scheduler/DAGScheduler/#posting-completionevent","text":"Posts a CompletionEvent Used when TaskSetManager is requested to handleSuccessfulTask , handleFailedTask , and executorLost","title":" Posting CompletionEvent"},{"location":"scheduler/DAGScheduler/#posting-gettingresultevent","text":"Posts a GettingResultEvent Used when TaskSetManager is requested to handle a task fetching result","title":" Posting GettingResultEvent"},{"location":"scheduler/DAGScheduler/#posting-tasksetfailed","text":"Posts a TaskSetFailed Used when TaskSetManager is requested to abort","title":" Posting TaskSetFailed"},{"location":"scheduler/DAGScheduler/#posting-beginevent","text":"Posts a BeginEvent Used when TaskSetManager is requested to start a task","title":" Posting BeginEvent"},{"location":"scheduler/DAGScheduler/#posting-workerremoved","text":"Posts a WorkerRemoved Used when TaskSchedulerImpl is requested to handle a removed worker event","title":" Posting WorkerRemoved"},{"location":"scheduler/DAGScheduler/#updating-accumulators-of-completed-tasks","text":"updateAccumulators ( event : CompletionEvent ): Unit updateAccumulators merges the partial values of accumulators from a completed task (based on the given CompletionEvent ) into their \"source\" accumulators on the driver. For every AccumulatorV2 update (in the given CompletionEvent ), updateAccumulators finds the corresponding accumulator on the driver and requests the AccumulatorV2 to merge the updates . updateAccumulators ...FIXME For named accumulators with the update value being a non-zero value, i.e. not Accumulable.zero : stage.latestInfo.accumulables for the AccumulableInfo.id is set CompletionEvent.taskInfo.accumulables has a new AccumulableInfo added. CAUTION: FIXME Where are Stage.latestInfo.accumulables and CompletionEvent.taskInfo.accumulables used? updateAccumulators is used when DAGScheduler is requested to handle a task completion .","title":" Updating Accumulators of Completed Tasks"},{"location":"scheduler/DAGScheduler/#posting-sparklistenertaskend-at-task-completion","text":"postTaskEnd ( event : CompletionEvent ): Unit postTaskEnd reconstructs task metrics (from the accumulator updates in the CompletionEvent ). In the end, postTaskEnd creates a SparkListenerTaskEnd and requests the LiveListenerBus to post it . postTaskEnd is used when: DAGScheduler is requested to handle a task completion","title":" Posting SparkListenerTaskEnd (at Task Completion)"},{"location":"scheduler/DAGScheduler/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.DAGScheduler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.DAGScheduler=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/DAGSchedulerEvent/","text":"DAGSchedulerEvent \u00b6 DAGSchedulerEvent is an abstraction of events that are handled by the DAGScheduler (on dag-scheduler-event-loop daemon thread ). AllJobsCancelled \u00b6 Carries no extra information Posted when DAGScheduler is requested to cancelAllJobs Event handler: doCancelAllJobs BeginEvent \u00b6 Carries the following: Task TaskInfo Posted when DAGScheduler is requested to taskStarted Event handler: handleBeginEvent CompletionEvent \u00b6 Carries the following: Completed Task TaskEndReason Result (value computed) AccumulatorV2 Updates Metric Peaks TaskInfo Posted when DAGScheduler is requested to taskEnded Event handler: handleTaskCompletion ExecutorAdded \u00b6 Carries the following: Executor ID Host name Posted when DAGScheduler is requested to executorAdded Event handler: handleExecutorAdded ExecutorLost \u00b6 Carries the following: Executor ID Reason Posted when DAGScheduler is requested to executorLost Event handler: handleExecutorLost GettingResultEvent \u00b6 Carries the following: TaskInfo Posted when DAGScheduler is requested to taskGettingResult Event handler: handleGetTaskResult JobCancelled \u00b6 JobCancelled event carries the following: Job ID Reason (optional) Posted when DAGScheduler is requested to cancelJob Event handler: handleJobCancellation JobGroupCancelled \u00b6 Carries the following: Group ID Posted when DAGScheduler is requested to cancelJobGroup Event handler: handleJobGroupCancelled JobSubmitted \u00b6 Carries the following: Job ID RDD Partition processing function (with a TaskContext and the partition data, i.e. (TaskContext, Iterator[_]) => _ ) Partition IDs to compute CallSite JobListener to keep updated about the status of the stage execution Execution properties Posted when: DAGScheduler is requested to submit a job , run an approximate job and handleJobSubmitted Event handler: handleJobSubmitted MapStageSubmitted \u00b6 Carries the following: Job ID ShuffleDependency CallSite JobListener Execution properties Posted when DAGScheduler is requested to submitMapStage Event handler: handleMapStageSubmitted ResubmitFailedStages \u00b6 Carries no extra information. Posted when DAGScheduler is requested to handleTaskCompletion Event handler: resubmitFailedStages SpeculativeTaskSubmitted \u00b6 Carries the following: Task Posted when DAGScheduler is requested to speculativeTaskSubmitted Event handler: handleSpeculativeTaskSubmitted StageCancelled \u00b6 Carries the following: Stage ID Reason (optional) Posted when DAGScheduler is requested to cancelStage Event handler: handleStageCancellation TaskSetFailed \u00b6 Carries the following: TaskSet Reason Exception (optional) Posted when DAGScheduler is requested to taskSetFailed Event handler: handleTaskSetFailed WorkerRemoved \u00b6 Carries the following: Worked ID Host name Reason Posted when DAGScheduler is requested to workerRemoved Event handler: handleWorkerRemoved","title":"DAGSchedulerEvent"},{"location":"scheduler/DAGSchedulerEvent/#dagschedulerevent","text":"DAGSchedulerEvent is an abstraction of events that are handled by the DAGScheduler (on dag-scheduler-event-loop daemon thread ).","title":"DAGSchedulerEvent"},{"location":"scheduler/DAGSchedulerEvent/#alljobscancelled","text":"Carries no extra information Posted when DAGScheduler is requested to cancelAllJobs Event handler: doCancelAllJobs","title":" AllJobsCancelled"},{"location":"scheduler/DAGSchedulerEvent/#beginevent","text":"Carries the following: Task TaskInfo Posted when DAGScheduler is requested to taskStarted Event handler: handleBeginEvent","title":" BeginEvent"},{"location":"scheduler/DAGSchedulerEvent/#completionevent","text":"Carries the following: Completed Task TaskEndReason Result (value computed) AccumulatorV2 Updates Metric Peaks TaskInfo Posted when DAGScheduler is requested to taskEnded Event handler: handleTaskCompletion","title":" CompletionEvent"},{"location":"scheduler/DAGSchedulerEvent/#executoradded","text":"Carries the following: Executor ID Host name Posted when DAGScheduler is requested to executorAdded Event handler: handleExecutorAdded","title":" ExecutorAdded"},{"location":"scheduler/DAGSchedulerEvent/#executorlost","text":"Carries the following: Executor ID Reason Posted when DAGScheduler is requested to executorLost Event handler: handleExecutorLost","title":" ExecutorLost"},{"location":"scheduler/DAGSchedulerEvent/#gettingresultevent","text":"Carries the following: TaskInfo Posted when DAGScheduler is requested to taskGettingResult Event handler: handleGetTaskResult","title":" GettingResultEvent"},{"location":"scheduler/DAGSchedulerEvent/#jobcancelled","text":"JobCancelled event carries the following: Job ID Reason (optional) Posted when DAGScheduler is requested to cancelJob Event handler: handleJobCancellation","title":" JobCancelled"},{"location":"scheduler/DAGSchedulerEvent/#jobgroupcancelled","text":"Carries the following: Group ID Posted when DAGScheduler is requested to cancelJobGroup Event handler: handleJobGroupCancelled","title":" JobGroupCancelled"},{"location":"scheduler/DAGSchedulerEvent/#jobsubmitted","text":"Carries the following: Job ID RDD Partition processing function (with a TaskContext and the partition data, i.e. (TaskContext, Iterator[_]) => _ ) Partition IDs to compute CallSite JobListener to keep updated about the status of the stage execution Execution properties Posted when: DAGScheduler is requested to submit a job , run an approximate job and handleJobSubmitted Event handler: handleJobSubmitted","title":" JobSubmitted"},{"location":"scheduler/DAGSchedulerEvent/#mapstagesubmitted","text":"Carries the following: Job ID ShuffleDependency CallSite JobListener Execution properties Posted when DAGScheduler is requested to submitMapStage Event handler: handleMapStageSubmitted","title":" MapStageSubmitted"},{"location":"scheduler/DAGSchedulerEvent/#resubmitfailedstages","text":"Carries no extra information. Posted when DAGScheduler is requested to handleTaskCompletion Event handler: resubmitFailedStages","title":" ResubmitFailedStages"},{"location":"scheduler/DAGSchedulerEvent/#speculativetasksubmitted","text":"Carries the following: Task Posted when DAGScheduler is requested to speculativeTaskSubmitted Event handler: handleSpeculativeTaskSubmitted","title":" SpeculativeTaskSubmitted"},{"location":"scheduler/DAGSchedulerEvent/#stagecancelled","text":"Carries the following: Stage ID Reason (optional) Posted when DAGScheduler is requested to cancelStage Event handler: handleStageCancellation","title":" StageCancelled"},{"location":"scheduler/DAGSchedulerEvent/#tasksetfailed","text":"Carries the following: TaskSet Reason Exception (optional) Posted when DAGScheduler is requested to taskSetFailed Event handler: handleTaskSetFailed","title":" TaskSetFailed"},{"location":"scheduler/DAGSchedulerEvent/#workerremoved","text":"Carries the following: Worked ID Host name Reason Posted when DAGScheduler is requested to workerRemoved Event handler: handleWorkerRemoved","title":" WorkerRemoved"},{"location":"scheduler/DAGSchedulerEventProcessLoop/","text":"DAGSchedulerEventProcessLoop \u00b6 DAGSchedulerEventProcessLoop is an event processing daemon thread to handle DAGSchedulerEvents (on a separate thread from the parent DAGScheduler 's). DAGSchedulerEventProcessLoop is registered under the name of dag-scheduler-event-loop . DAGSchedulerEventProcessLoop uses java.util.concurrent.LinkedBlockingDeque blocking deque that can grow indefinitely. Creating Instance \u00b6 DAGSchedulerEventProcessLoop takes the following to be created: DAGScheduler DAGSchedulerEventProcessLoop is created when DAGScheduler is created . Processing Event \u00b6 DAGSchedulerEvent Event Handler AllJobsCancelled doCancelAllJobs BeginEvent handleBeginEvent CompletionEvent handleTaskCompletion ExecutorAdded handleExecutorAdded ExecutorLost handleExecutorLost GettingResultEvent handleGetTaskResult JobCancelled handleJobCancellation JobGroupCancelled handleJobGroupCancelled JobSubmitted handleJobSubmitted MapStageSubmitted handleMapStageSubmitted ResubmitFailedStages resubmitFailedStages SpeculativeTaskSubmitted handleSpeculativeTaskSubmitted StageCancelled handleStageCancellation TaskSetFailed handleTaskSetFailed WorkerRemoved handleWorkerRemoved messageProcessingTime Timer \u00b6 DAGSchedulerEventProcessLoop uses messageProcessingTime timer to measure time of processing events .","title":"DAGSchedulerEventProcessLoop"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#dagschedulereventprocessloop","text":"DAGSchedulerEventProcessLoop is an event processing daemon thread to handle DAGSchedulerEvents (on a separate thread from the parent DAGScheduler 's). DAGSchedulerEventProcessLoop is registered under the name of dag-scheduler-event-loop . DAGSchedulerEventProcessLoop uses java.util.concurrent.LinkedBlockingDeque blocking deque that can grow indefinitely.","title":"DAGSchedulerEventProcessLoop"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#creating-instance","text":"DAGSchedulerEventProcessLoop takes the following to be created: DAGScheduler DAGSchedulerEventProcessLoop is created when DAGScheduler is created .","title":"Creating Instance"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#processing-event","text":"DAGSchedulerEvent Event Handler AllJobsCancelled doCancelAllJobs BeginEvent handleBeginEvent CompletionEvent handleTaskCompletion ExecutorAdded handleExecutorAdded ExecutorLost handleExecutorLost GettingResultEvent handleGetTaskResult JobCancelled handleJobCancellation JobGroupCancelled handleJobGroupCancelled JobSubmitted handleJobSubmitted MapStageSubmitted handleMapStageSubmitted ResubmitFailedStages resubmitFailedStages SpeculativeTaskSubmitted handleSpeculativeTaskSubmitted StageCancelled handleStageCancellation TaskSetFailed handleTaskSetFailed WorkerRemoved handleWorkerRemoved","title":" Processing Event"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#messageprocessingtime-timer","text":"DAGSchedulerEventProcessLoop uses messageProcessingTime timer to measure time of processing events .","title":" messageProcessingTime Timer"},{"location":"scheduler/DAGSchedulerSource/","text":"DAGSchedulerSource \u00b6 DAGSchedulerSource is the metrics source of DAGScheduler . The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"scheduler/DAGSchedulerSource/#dagschedulersource","text":"DAGSchedulerSource is the metrics source of DAGScheduler . The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"scheduler/DriverEndpoint/","text":"DriverEndpoint \u00b6 DriverEndpoint is a ThreadSafeRpcEndpoint that is a message handler for CoarseGrainedSchedulerBackend to communicate with CoarseGrainedExecutorBackend . DriverEndpoint uses executorDataMap internal registry of all the executors that registered with the driver . An executor sends a RegisterExecutor message to inform that it wants to register. Creating Instance \u00b6 DriverEndpoint takes no arguments to be created. DriverEndpoint is created when CoarseGrainedSchedulerBackend is requested for one . ExecutorLogUrlHandler \u00b6 logUrlHandler : ExecutorLogUrlHandler DriverEndpoint creates an ExecutorLogUrlHandler (based on spark.ui.custom.executor.log.url configuration property) when created . DriverEndpoint uses the ExecutorLogUrlHandler to create an ExecutorData when requested to handle a RegisterExecutor message. Starting DriverEndpoint \u00b6 onStart (): Unit onStart is part of the RpcEndpoint abstraction. onStart requests the Revive Messages Scheduler Service to schedule a periodic action that sends ReviveOffers messages every revive interval (based on spark.scheduler.revive.interval configuration property). Messages \u00b6 KillExecutorsOnHost \u00b6 CoarseGrainedSchedulerBackend is requested to kill all executors on a node KillTask \u00b6 CoarseGrainedSchedulerBackend is requested to kill a task . KillTask ( taskId : Long , executor : String , interruptThread : Boolean ) KillTask is sent when CoarseGrainedSchedulerBackend kills a task . When KillTask is received, DriverEndpoint finds executor (in executorDataMap registry). If found, DriverEndpoint passes the message on to the executor (using its registered RPC endpoint for CoarseGrainedExecutorBackend ). Otherwise, you should see the following WARN in the logs: Attempted to kill task [taskId] for unknown executor [executor]. LaunchedExecutor \u00b6 RegisterExecutor \u00b6 CoarseGrainedExecutorBackend registers with the driver RegisterExecutor ( executorId : String , executorRef : RpcEndpointRef , hostname : String , cores : Int , logUrls : Map [ String , String ]) RegisterExecutor is sent when CoarseGrainedExecutorBackend RPC Endpoint is requested to start . When received, DriverEndpoint makes sure that no other executors were registered under the input executorId and that the input hostname is not blacklisted . If the requirements hold, you should see the following INFO message in the logs: Registered executor [executorRef] ([address]) with ID [executorId] DriverEndpoint does the bookkeeping: Registers executorId (in addressToExecutorId ) Adds cores (in totalCoreCount ) Increments totalRegisteredExecutors Creates and registers ExecutorData for executorId (in executorDataMap ) Updates currentExecutorIdCounter if the input executorId is greater than the current value. If numPendingExecutors is greater than 0 , you should see the following DEBUG message in the logs and DriverEndpoint decrements numPendingExecutors . Decremented number of pending executors ([numPendingExecutors] left) DriverEndpoint sends RegisteredExecutor message back (that is to confirm that the executor was registered successfully). DriverEndpoint replies true (to acknowledge the message). DriverEndpoint then announces the new executor by posting SparkListenerExecutorAdded to LiveListenerBus . In the end, DriverEndpoint makes executor resource offers (for launching tasks) . If however there was already another executor registered under the input executorId , DriverEndpoint sends RegisterExecutorFailed message back with the reason: Duplicate executor ID: [executorId] If however the input hostname is blacklisted , you should see the following INFO message in the logs: Rejecting [executorId] as it has been blacklisted. DriverEndpoint sends RegisterExecutorFailed message back with the reason: Executor is blacklisted: [executorId] RemoveExecutor \u00b6 RemoveWorker \u00b6 RetrieveSparkAppConfig \u00b6 RetrieveSparkAppConfig ( resourceProfileId : Int ) Posted when: CoarseGrainedExecutorBackend standalone application is started When received , DriverEndpoint replies with a SparkAppConfig message with the following: spark -prefixed configuration properties IO Encryption Key Delegation tokens Default profile ReviveOffers \u00b6 Posted when: Periodically (every spark.scheduler.revive.interval ) right after DriverEndpoint is requested to start CoarseGrainedSchedulerBackend is requested to revive resource offers When received , DriverEndpoint makes executor resource offers . StatusUpdate \u00b6 CoarseGrainedExecutorBackend sends task status updates to the driver StatusUpdate ( executorId : String , taskId : Long , state : TaskState , data : SerializableBuffer ) StatusUpdate is sent when CoarseGrainedExecutorBackend sends task status updates to the driver . When StatusUpdate is received, DriverEndpoint requests the TaskSchedulerImpl to handle the task status update . If the task has finished , DriverEndpoint updates the number of cores available for work on the corresponding executor (registered in executorDataMap ). DriverEndpoint makes an executor resource offer on the single executor . When DriverEndpoint found no executor (in executorDataMap ), you should see the following WARN message in the logs: Ignored task status update ([taskId] state [state]) from unknown executor with ID [executorId] StopDriver \u00b6 StopExecutors \u00b6 StopExecutors message is receive-reply and blocking. When received, the following INFO message appears in the logs: Asking each executor to shut down It then sends a StopExecutor message to every registered executor (from executorDataMap ). UpdateDelegationTokens \u00b6 Making Executor Resource Offers (for Launching Tasks) \u00b6 makeOffers (): Unit makeOffers creates WorkerOffer s for all active executors . makeOffers requests TaskSchedulerImpl to generate tasks for the available worker offers . When there are tasks to be launched (from TaskSchedulerImpl ) makeOffers does so . makeOffers is used when DriverEndpoint handles ReviveOffers or RegisterExecutor messages. Making Executor Resource Offer on Single Executor (for Launching Tasks) \u00b6 makeOffers ( executorId : String ): Unit makeOffers makes sure that the input executorId is alive . NOTE: makeOffers does nothing when the input executorId is registered as pending to be removed or got lost. makeOffers finds the executor data (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] registry) and creates a scheduler:TaskSchedulerImpl.md#WorkerOffer[WorkerOffer]. NOTE: WorkerOffer represents a resource offer with CPU cores available on an executor. makeOffers then scheduler:TaskSchedulerImpl.md#resourceOffers[requests TaskSchedulerImpl to generate tasks for the WorkerOffer ] followed by launching the tasks (on the executor). makeOffers is used when CoarseGrainedSchedulerBackend RPC endpoint (DriverEndpoint) handles a StatusUpdate message. Launching Tasks \u00b6 launchTasks ( tasks : Seq [ Seq [ TaskDescription ]]): Unit Note The input tasks collection contains one or more TaskDescription s per executor (and the \"task partitioning\" per executor is of no use in launchTasks so it simply flattens the input data structure). For every TaskDescription (in the given tasks collection), launchTasks encodes it and makes sure that the encoded task size is below the allowed message size . launchTasks looks up the ExecutorData of the executor that has been assigned to execute the task (in executorDataMap internal registry) and decreases the executor's free cores (based on spark.task.cpus configuration property). Note Scheduling in Spark relies on cores only (not memory), i.e. the number of tasks Spark can run on an executor is limited by the number of cores available only. When submitting a Spark application for execution both executor resources -- memory and cores -- can however be specified explicitly. It is the job of a cluster manager to monitor the memory and take action when its use exceeds what was assigned. launchTasks prints out the following DEBUG message to the logs: Launching task [taskId] on executor id: [executorId] hostname: [executorHost]. In the end, launchTasks sends the (serialized) task to the executor (by sending a LaunchTask message to the executor's RPC endpoint with the serialized task insize SerializableBuffer ). Note This is the moment in a task's lifecycle when the driver sends the serialized task to an assigned executor. launchTasks is used when CoarseGrainedSchedulerBackend is requested to make resource offers on single or all executors. Task Exceeds Allowed Size \u00b6 In case the size of a serialized TaskDescription equals or exceeds the maximum allowed RPC message size , launchTasks looks up the TaskSetManager for the TaskDescription (in taskIdToTaskSetManager registry) and aborts it with the following message: Serialized task [id]:[index] was [limit] bytes, which exceeds max allowed: spark.rpc.message.maxSize ([maxRpcMessageSize] bytes). Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values. Removing Executor \u00b6 removeExecutor ( executorId : String , reason : ExecutorLossReason ): Unit When removeExecutor is executed, you should see the following DEBUG message in the logs: Asked to remove executor [executorId] with reason [reason] removeExecutor then tries to find the executorId executor (in executorDataMap internal registry). If the executorId executor was found, removeExecutor removes the executor from the following registries: addressToExecutorId executorDataMap < > executorsPendingToRemove removeExecutor decrements: totalCoreCount by the executor's totalCores totalRegisteredExecutors In the end, removeExecutor notifies TaskSchedulerImpl that an executor was lost . removeExecutor posts SparkListenerExecutorRemoved to LiveListenerBus (with the executorId executor). If however the executorId executor could not be found, removeExecutor requests BlockManagerMaster to remove the executor asynchronously . Note removeExecutor uses SparkEnv to access the current BlockManager and then BlockManagerMaster . You should see the following INFO message in the logs: Asked to remove non-existent executor [executorId] removeExecutor is used when DriverEndpoint handles RemoveExecutor message and gets disassociated with a remote RPC endpoint of an executor . Removing Worker \u00b6 removeWorker ( workerId : String , host : String , message : String ): Unit removeWorker prints out the following DEBUG message to the logs: Asked to remove worker [workerId] with reason [message] In the end, removeWorker simply requests the TaskSchedulerImpl to workerRemoved . removeWorker is used when DriverEndpoint is requested to handle a RemoveWorker event. Processing One-Way Messages \u00b6 receive : PartialFunction [ Any , Unit ] receive is part of the RpcEndpoint abstraction. receive ...FIXME Processing Two-Way Messages \u00b6 receiveAndReply ( context : RpcCallContext ): PartialFunction [ Any , Unit ] receiveAndReply is part of the RpcEndpoint abstraction. receiveAndReply ...FIXME onDisconnected Callback \u00b6 onDisconnected removes the worker from the internal addressToExecutorId registry (that effectively removes the worker from a cluster). onDisconnected removes the executor with the reason being SlaveLost and message: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages. Executors by RpcAddress Registry \u00b6 addressToExecutorId : Map [ RpcAddress , String ] Executor addresses (host and port) for executors. Set when an executor connects to register itself. Disabling Executor \u00b6 disableExecutor ( executorId : String ): Boolean disableExecutor checks whether the executor is active : If so, disableExecutor adds the executor to the executorsPendingLossReason registry Otherwise, disableExecutor checks whether added to executorsPendingToRemove registry disableExecutor determines whether the executor should really be disabled (as active or registered in executorsPendingToRemove registry). If the executor should be disabled, disableExecutor prints out the following INFO message to the logs and notifies the TaskSchedulerImpl that the executor is lost . Disabling executor [executorId]. disableExecutor returns the indication whether the executor should have been disabled or not. disableExecutor is used when: KubernetesDriverEndpoint is requested to handle onDisconnected event YarnDriverEndpoint is requested to handle onDisconnected event Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint=ALL Refer to Logging .","title":"DriverEndpoint"},{"location":"scheduler/DriverEndpoint/#driverendpoint","text":"DriverEndpoint is a ThreadSafeRpcEndpoint that is a message handler for CoarseGrainedSchedulerBackend to communicate with CoarseGrainedExecutorBackend . DriverEndpoint uses executorDataMap internal registry of all the executors that registered with the driver . An executor sends a RegisterExecutor message to inform that it wants to register.","title":"DriverEndpoint"},{"location":"scheduler/DriverEndpoint/#creating-instance","text":"DriverEndpoint takes no arguments to be created. DriverEndpoint is created when CoarseGrainedSchedulerBackend is requested for one .","title":"Creating Instance"},{"location":"scheduler/DriverEndpoint/#executorlogurlhandler","text":"logUrlHandler : ExecutorLogUrlHandler DriverEndpoint creates an ExecutorLogUrlHandler (based on spark.ui.custom.executor.log.url configuration property) when created . DriverEndpoint uses the ExecutorLogUrlHandler to create an ExecutorData when requested to handle a RegisterExecutor message.","title":" ExecutorLogUrlHandler"},{"location":"scheduler/DriverEndpoint/#starting-driverendpoint","text":"onStart (): Unit onStart is part of the RpcEndpoint abstraction. onStart requests the Revive Messages Scheduler Service to schedule a periodic action that sends ReviveOffers messages every revive interval (based on spark.scheduler.revive.interval configuration property).","title":" Starting DriverEndpoint"},{"location":"scheduler/DriverEndpoint/#messages","text":"","title":"Messages"},{"location":"scheduler/DriverEndpoint/#killexecutorsonhost","text":"CoarseGrainedSchedulerBackend is requested to kill all executors on a node","title":" KillExecutorsOnHost"},{"location":"scheduler/DriverEndpoint/#killtask","text":"CoarseGrainedSchedulerBackend is requested to kill a task . KillTask ( taskId : Long , executor : String , interruptThread : Boolean ) KillTask is sent when CoarseGrainedSchedulerBackend kills a task . When KillTask is received, DriverEndpoint finds executor (in executorDataMap registry). If found, DriverEndpoint passes the message on to the executor (using its registered RPC endpoint for CoarseGrainedExecutorBackend ). Otherwise, you should see the following WARN in the logs: Attempted to kill task [taskId] for unknown executor [executor].","title":" KillTask"},{"location":"scheduler/DriverEndpoint/#launchedexecutor","text":"","title":" LaunchedExecutor"},{"location":"scheduler/DriverEndpoint/#registerexecutor","text":"CoarseGrainedExecutorBackend registers with the driver RegisterExecutor ( executorId : String , executorRef : RpcEndpointRef , hostname : String , cores : Int , logUrls : Map [ String , String ]) RegisterExecutor is sent when CoarseGrainedExecutorBackend RPC Endpoint is requested to start . When received, DriverEndpoint makes sure that no other executors were registered under the input executorId and that the input hostname is not blacklisted . If the requirements hold, you should see the following INFO message in the logs: Registered executor [executorRef] ([address]) with ID [executorId] DriverEndpoint does the bookkeeping: Registers executorId (in addressToExecutorId ) Adds cores (in totalCoreCount ) Increments totalRegisteredExecutors Creates and registers ExecutorData for executorId (in executorDataMap ) Updates currentExecutorIdCounter if the input executorId is greater than the current value. If numPendingExecutors is greater than 0 , you should see the following DEBUG message in the logs and DriverEndpoint decrements numPendingExecutors . Decremented number of pending executors ([numPendingExecutors] left) DriverEndpoint sends RegisteredExecutor message back (that is to confirm that the executor was registered successfully). DriverEndpoint replies true (to acknowledge the message). DriverEndpoint then announces the new executor by posting SparkListenerExecutorAdded to LiveListenerBus . In the end, DriverEndpoint makes executor resource offers (for launching tasks) . If however there was already another executor registered under the input executorId , DriverEndpoint sends RegisterExecutorFailed message back with the reason: Duplicate executor ID: [executorId] If however the input hostname is blacklisted , you should see the following INFO message in the logs: Rejecting [executorId] as it has been blacklisted. DriverEndpoint sends RegisterExecutorFailed message back with the reason: Executor is blacklisted: [executorId]","title":" RegisterExecutor"},{"location":"scheduler/DriverEndpoint/#removeexecutor","text":"","title":" RemoveExecutor"},{"location":"scheduler/DriverEndpoint/#removeworker","text":"","title":" RemoveWorker"},{"location":"scheduler/DriverEndpoint/#retrievesparkappconfig","text":"RetrieveSparkAppConfig ( resourceProfileId : Int ) Posted when: CoarseGrainedExecutorBackend standalone application is started When received , DriverEndpoint replies with a SparkAppConfig message with the following: spark -prefixed configuration properties IO Encryption Key Delegation tokens Default profile","title":" RetrieveSparkAppConfig"},{"location":"scheduler/DriverEndpoint/#reviveoffers","text":"Posted when: Periodically (every spark.scheduler.revive.interval ) right after DriverEndpoint is requested to start CoarseGrainedSchedulerBackend is requested to revive resource offers When received , DriverEndpoint makes executor resource offers .","title":" ReviveOffers"},{"location":"scheduler/DriverEndpoint/#statusupdate","text":"CoarseGrainedExecutorBackend sends task status updates to the driver StatusUpdate ( executorId : String , taskId : Long , state : TaskState , data : SerializableBuffer ) StatusUpdate is sent when CoarseGrainedExecutorBackend sends task status updates to the driver . When StatusUpdate is received, DriverEndpoint requests the TaskSchedulerImpl to handle the task status update . If the task has finished , DriverEndpoint updates the number of cores available for work on the corresponding executor (registered in executorDataMap ). DriverEndpoint makes an executor resource offer on the single executor . When DriverEndpoint found no executor (in executorDataMap ), you should see the following WARN message in the logs: Ignored task status update ([taskId] state [state]) from unknown executor with ID [executorId]","title":" StatusUpdate"},{"location":"scheduler/DriverEndpoint/#stopdriver","text":"","title":" StopDriver"},{"location":"scheduler/DriverEndpoint/#stopexecutors","text":"StopExecutors message is receive-reply and blocking. When received, the following INFO message appears in the logs: Asking each executor to shut down It then sends a StopExecutor message to every registered executor (from executorDataMap ).","title":" StopExecutors"},{"location":"scheduler/DriverEndpoint/#updatedelegationtokens","text":"","title":" UpdateDelegationTokens"},{"location":"scheduler/DriverEndpoint/#making-executor-resource-offers-for-launching-tasks","text":"makeOffers (): Unit makeOffers creates WorkerOffer s for all active executors . makeOffers requests TaskSchedulerImpl to generate tasks for the available worker offers . When there are tasks to be launched (from TaskSchedulerImpl ) makeOffers does so . makeOffers is used when DriverEndpoint handles ReviveOffers or RegisterExecutor messages.","title":" Making Executor Resource Offers (for Launching Tasks)"},{"location":"scheduler/DriverEndpoint/#making-executor-resource-offer-on-single-executor-for-launching-tasks","text":"makeOffers ( executorId : String ): Unit makeOffers makes sure that the input executorId is alive . NOTE: makeOffers does nothing when the input executorId is registered as pending to be removed or got lost. makeOffers finds the executor data (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] registry) and creates a scheduler:TaskSchedulerImpl.md#WorkerOffer[WorkerOffer]. NOTE: WorkerOffer represents a resource offer with CPU cores available on an executor. makeOffers then scheduler:TaskSchedulerImpl.md#resourceOffers[requests TaskSchedulerImpl to generate tasks for the WorkerOffer ] followed by launching the tasks (on the executor). makeOffers is used when CoarseGrainedSchedulerBackend RPC endpoint (DriverEndpoint) handles a StatusUpdate message.","title":" Making Executor Resource Offer on Single Executor (for Launching Tasks)"},{"location":"scheduler/DriverEndpoint/#launching-tasks","text":"launchTasks ( tasks : Seq [ Seq [ TaskDescription ]]): Unit Note The input tasks collection contains one or more TaskDescription s per executor (and the \"task partitioning\" per executor is of no use in launchTasks so it simply flattens the input data structure). For every TaskDescription (in the given tasks collection), launchTasks encodes it and makes sure that the encoded task size is below the allowed message size . launchTasks looks up the ExecutorData of the executor that has been assigned to execute the task (in executorDataMap internal registry) and decreases the executor's free cores (based on spark.task.cpus configuration property). Note Scheduling in Spark relies on cores only (not memory), i.e. the number of tasks Spark can run on an executor is limited by the number of cores available only. When submitting a Spark application for execution both executor resources -- memory and cores -- can however be specified explicitly. It is the job of a cluster manager to monitor the memory and take action when its use exceeds what was assigned. launchTasks prints out the following DEBUG message to the logs: Launching task [taskId] on executor id: [executorId] hostname: [executorHost]. In the end, launchTasks sends the (serialized) task to the executor (by sending a LaunchTask message to the executor's RPC endpoint with the serialized task insize SerializableBuffer ). Note This is the moment in a task's lifecycle when the driver sends the serialized task to an assigned executor. launchTasks is used when CoarseGrainedSchedulerBackend is requested to make resource offers on single or all executors.","title":" Launching Tasks"},{"location":"scheduler/DriverEndpoint/#task-exceeds-allowed-size","text":"In case the size of a serialized TaskDescription equals or exceeds the maximum allowed RPC message size , launchTasks looks up the TaskSetManager for the TaskDescription (in taskIdToTaskSetManager registry) and aborts it with the following message: Serialized task [id]:[index] was [limit] bytes, which exceeds max allowed: spark.rpc.message.maxSize ([maxRpcMessageSize] bytes). Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values.","title":" Task Exceeds Allowed Size"},{"location":"scheduler/DriverEndpoint/#removing-executor","text":"removeExecutor ( executorId : String , reason : ExecutorLossReason ): Unit When removeExecutor is executed, you should see the following DEBUG message in the logs: Asked to remove executor [executorId] with reason [reason] removeExecutor then tries to find the executorId executor (in executorDataMap internal registry). If the executorId executor was found, removeExecutor removes the executor from the following registries: addressToExecutorId executorDataMap < > executorsPendingToRemove removeExecutor decrements: totalCoreCount by the executor's totalCores totalRegisteredExecutors In the end, removeExecutor notifies TaskSchedulerImpl that an executor was lost . removeExecutor posts SparkListenerExecutorRemoved to LiveListenerBus (with the executorId executor). If however the executorId executor could not be found, removeExecutor requests BlockManagerMaster to remove the executor asynchronously . Note removeExecutor uses SparkEnv to access the current BlockManager and then BlockManagerMaster . You should see the following INFO message in the logs: Asked to remove non-existent executor [executorId] removeExecutor is used when DriverEndpoint handles RemoveExecutor message and gets disassociated with a remote RPC endpoint of an executor .","title":" Removing Executor"},{"location":"scheduler/DriverEndpoint/#removing-worker","text":"removeWorker ( workerId : String , host : String , message : String ): Unit removeWorker prints out the following DEBUG message to the logs: Asked to remove worker [workerId] with reason [message] In the end, removeWorker simply requests the TaskSchedulerImpl to workerRemoved . removeWorker is used when DriverEndpoint is requested to handle a RemoveWorker event.","title":" Removing Worker"},{"location":"scheduler/DriverEndpoint/#processing-one-way-messages","text":"receive : PartialFunction [ Any , Unit ] receive is part of the RpcEndpoint abstraction. receive ...FIXME","title":" Processing One-Way Messages"},{"location":"scheduler/DriverEndpoint/#processing-two-way-messages","text":"receiveAndReply ( context : RpcCallContext ): PartialFunction [ Any , Unit ] receiveAndReply is part of the RpcEndpoint abstraction. receiveAndReply ...FIXME","title":" Processing Two-Way Messages"},{"location":"scheduler/DriverEndpoint/#ondisconnected-callback","text":"onDisconnected removes the worker from the internal addressToExecutorId registry (that effectively removes the worker from a cluster). onDisconnected removes the executor with the reason being SlaveLost and message: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.","title":" onDisconnected Callback"},{"location":"scheduler/DriverEndpoint/#executors-by-rpcaddress-registry","text":"addressToExecutorId : Map [ RpcAddress , String ] Executor addresses (host and port) for executors. Set when an executor connects to register itself.","title":" Executors by RpcAddress Registry"},{"location":"scheduler/DriverEndpoint/#disabling-executor","text":"disableExecutor ( executorId : String ): Boolean disableExecutor checks whether the executor is active : If so, disableExecutor adds the executor to the executorsPendingLossReason registry Otherwise, disableExecutor checks whether added to executorsPendingToRemove registry disableExecutor determines whether the executor should really be disabled (as active or registered in executorsPendingToRemove registry). If the executor should be disabled, disableExecutor prints out the following INFO message to the logs and notifies the TaskSchedulerImpl that the executor is lost . Disabling executor [executorId]. disableExecutor returns the indication whether the executor should have been disabled or not. disableExecutor is used when: KubernetesDriverEndpoint is requested to handle onDisconnected event YarnDriverEndpoint is requested to handle onDisconnected event","title":" Disabling Executor"},{"location":"scheduler/DriverEndpoint/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/ExecutorData/","text":"ExecutorData \u00b6 ExecutorData is a metadata of an executor: Executor's RPC Endpoint Executor's RpcAddress Executor's Host Executor's Free Cores Executor's Total Cores Executor's Log URLs ( Map[String, String] ) Executor's Attributes ( Map[String, String] ) Executor's Resources Info ( Map[String, ExecutorResourceInfo] ) Executor's ResourceProfile ID ExecutorData is created for every executor registered (when DriverEndpoint is requested to handle a RegisterExecutor message). ExecutorData is used by CoarseGrainedSchedulerBackend to track registered executors . Note ExecutorData is posted as part of SparkListenerExecutorAdded event by DriverEndpoint every time an executor is registered.","title":"ExecutorData"},{"location":"scheduler/ExecutorData/#executordata","text":"ExecutorData is a metadata of an executor: Executor's RPC Endpoint Executor's RpcAddress Executor's Host Executor's Free Cores Executor's Total Cores Executor's Log URLs ( Map[String, String] ) Executor's Attributes ( Map[String, String] ) Executor's Resources Info ( Map[String, ExecutorResourceInfo] ) Executor's ResourceProfile ID ExecutorData is created for every executor registered (when DriverEndpoint is requested to handle a RegisterExecutor message). ExecutorData is used by CoarseGrainedSchedulerBackend to track registered executors . Note ExecutorData is posted as part of SparkListenerExecutorAdded event by DriverEndpoint every time an executor is registered.","title":"ExecutorData"},{"location":"scheduler/ExternalClusterManager/","text":"ExternalClusterManager \u00b6 ExternalClusterManager is an abstraction of pluggable cluster managers that can create a SchedulerBackend and TaskScheduler for a given master URL (when SparkContext is created). Note The support for pluggable cluster managers was introduced in SPARK-13904 Add support for pluggable cluster manager . ExternalClusterManager can be registered using the java.util.ServiceLoader mechanism (with service markers under META-INF/services directory). Contract \u00b6 Checking Support for Master URL \u00b6 canCreate ( masterURL : String ): Boolean Checks whether this cluster manager instance can create scheduler components for a given master URL Used when SparkContext is created (and requested for a cluster manager ) Creating SchedulerBackend \u00b6 createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ): SchedulerBackend Creates a SchedulerBackend for a given SparkContext , master URL, and TaskScheduler . Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler ) Creating TaskScheduler \u00b6 createTaskScheduler ( sc : SparkContext , masterURL : String ): TaskScheduler Creates a TaskScheduler for a given SparkContext and master URL Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler ) Initializing Scheduling Components \u00b6 initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ): Unit Initializes the TaskScheduler and SchedulerBackend Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler ) Implementations \u00b6 KubernetesClusterManager ( Spark on Kubernetes ) MesosClusterManager YarnClusterManager","title":"ExternalClusterManager"},{"location":"scheduler/ExternalClusterManager/#externalclustermanager","text":"ExternalClusterManager is an abstraction of pluggable cluster managers that can create a SchedulerBackend and TaskScheduler for a given master URL (when SparkContext is created). Note The support for pluggable cluster managers was introduced in SPARK-13904 Add support for pluggable cluster manager . ExternalClusterManager can be registered using the java.util.ServiceLoader mechanism (with service markers under META-INF/services directory).","title":"ExternalClusterManager"},{"location":"scheduler/ExternalClusterManager/#contract","text":"","title":"Contract"},{"location":"scheduler/ExternalClusterManager/#checking-support-for-master-url","text":"canCreate ( masterURL : String ): Boolean Checks whether this cluster manager instance can create scheduler components for a given master URL Used when SparkContext is created (and requested for a cluster manager )","title":" Checking Support for Master URL"},{"location":"scheduler/ExternalClusterManager/#creating-schedulerbackend","text":"createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ): SchedulerBackend Creates a SchedulerBackend for a given SparkContext , master URL, and TaskScheduler . Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler )","title":" Creating SchedulerBackend"},{"location":"scheduler/ExternalClusterManager/#creating-taskscheduler","text":"createTaskScheduler ( sc : SparkContext , masterURL : String ): TaskScheduler Creates a TaskScheduler for a given SparkContext and master URL Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler )","title":" Creating TaskScheduler"},{"location":"scheduler/ExternalClusterManager/#initializing-scheduling-components","text":"initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ): Unit Initializes the TaskScheduler and SchedulerBackend Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler )","title":" Initializing Scheduling Components"},{"location":"scheduler/ExternalClusterManager/#implementations","text":"KubernetesClusterManager ( Spark on Kubernetes ) MesosClusterManager YarnClusterManager","title":"Implementations"},{"location":"scheduler/FIFOSchedulableBuilder/","text":"== FIFOSchedulableBuilder - SchedulableBuilder for FIFO Scheduling Mode FIFOSchedulableBuilder is a < > that holds a single spark-scheduler-Pool.md[Pool] (that is given when < FIFOSchedulableBuilder is created>>). NOTE: FIFOSchedulableBuilder is the scheduler:TaskSchedulerImpl.md#creating-instance[default SchedulableBuilder for TaskSchedulerImpl ]. NOTE: When FIFOSchedulableBuilder is created, the TaskSchedulerImpl passes its own rootPool (a part of scheduler:TaskScheduler.md#contract[TaskScheduler Contract]). FIFOSchedulableBuilder obeys the < > as follows: < > does nothing. addTaskSetManager spark-scheduler-Pool.md#addSchedulable[passes the input Schedulable to the one and only rootPool Pool (using addSchedulable )] and completely disregards the properties of the Schedulable. === [[creating-instance]] Creating FIFOSchedulableBuilder Instance FIFOSchedulableBuilder takes the following when created: [[rootPool]] rootPool spark-scheduler-Pool.md[Pool]","title":"FIFOSchedulableBuilder"},{"location":"scheduler/FairSchedulableBuilder/","text":"FairSchedulableBuilder \u00b6 FairSchedulableBuilder is a < > that is < > exclusively for scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] for FAIR scheduling mode (when configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FAIR ). [[creating-instance]] FairSchedulableBuilder takes the following to be created: [[rootPool]] < > [[conf]] SparkConf.md[] Once < >, TaskSchedulerImpl requests the FairSchedulableBuilder to < >. [[DEFAULT_SCHEDULER_FILE]] FairSchedulableBuilder uses the pools defined in an < > that is assumed to be the value of the configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property or the default fairscheduler.xml (that is < >). TIP: Use conf/fairscheduler.xml.template as a template for the < >. [[DEFAULT_POOL_NAME]] FairSchedulableBuilder always has the default pool defined (and < > unless done in the < >). [[FAIR_SCHEDULER_PROPERTIES]] [[spark.scheduler.pool]] FairSchedulableBuilder uses spark.scheduler.pool local property for the name of the pool to use when requested to < > (default: < >). Note SparkContext.setLocalProperty lets you set local properties per thread to group jobs in logical groups, e.g. to allow FairSchedulableBuilder to use spark.scheduler.pool property and to group jobs from different threads to be submitted for execution on a non-< > pool. [source, scala] \u00b6 scala> :type sc org.apache.spark.SparkContext sc.setLocalProperty(\"spark.scheduler.pool\", \"production\") // whatever is executed afterwards is submitted to production pool \u00b6 [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.scheduler.FairSchedulableBuilder logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.FairSchedulableBuilder=ALL Refer to < >. \u00b6 === [[allocations-file]] Allocation Pools Configuration File The allocation pools configuration file is an XML file. The default conf/fairscheduler.xml.template is as follows: [source, xml] \u00b6 FAIR 1 2 FIFO 2 3 TIP: The top-level element's name allocations can be anything. Spark does not insist on allocations and accepts any name. === [[buildPools]] Building (Tree of) Pools of Schedulables -- buildPools Method [source, scala] \u00b6 buildPools(): Unit \u00b6 NOTE: buildPools is part of the < > to build a tree of < >. buildPools < > if available and then < >. buildPools prints out the following INFO message to the logs when the configuration file (per the configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property) could be read: Creating Fair Scheduler pools from [file] buildPools prints out the following INFO message to the logs when the configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property was not used to define the configuration file and the < > is used instead: Creating Fair Scheduler pools from default file: [DEFAULT_SCHEDULER_FILE] When neither configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property nor the < > could be used, buildPools prints out the following WARN message to the logs: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in [DEFAULT_SCHEDULER_FILE] or set spark.scheduler.allocation.file to a file that contains the configuration. === [[addTaskSetManager]] addTaskSetManager Method [source, scala] \u00b6 addTaskSetManager(manager: Schedulable, properties: Properties): Unit \u00b6 NOTE: addTaskSetManager is part of the < > to register a new < > with the < > addTaskSetManager finds the pool by name (in the given Properties ) under the < > property or defaults to the < > pool if undefined. addTaskSetManager then requests the < > to < >. Unless found, addTaskSetManager creates a new < > with the < > (as if the < > pool were used) and requests the < > to < >. In the end, addTaskSetManager prints out the following WARN message to the logs: A job was submitted with scheduler pool [poolName], which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain [poolName]. Created [poolName] with default configuration (schedulingMode: [mode], minShare: [minShare], weight: [weight]) addTaskSetManager then requests the pool (found or newly-created) to < > the given < >. In the end, addTaskSetManager prints out the following INFO message to the logs: Added task set [name] tasks to pool [poolName] === [[buildDefaultPool]] Registering Default Pool -- buildDefaultPool Method [source, scala] \u00b6 buildDefaultPool(): Unit \u00b6 buildDefaultPool requests the < > to < > (one with the < > name). Unless already available, buildDefaultPool creates a < > with the following: < > pool name FIFO scheduling mode 0 for the initial minimum share 1 for the initial weight In the end, buildDefaultPool requests the < > to < > followed by the INFO message in the logs: Created default pool: [name], schedulingMode: [mode], minShare: [minShare], weight: [weight] NOTE: buildDefaultPool is used exclusively when FairSchedulableBuilder is requested to < >. === [[buildFairSchedulerPool]] Building Pools from XML Allocations File -- buildFairSchedulerPool Internal Method [source, scala] \u00b6 buildFairSchedulerPool( is: InputStream, fileName: String): Unit buildFairSchedulerPool starts by loading the XML file from the given InputStream . For every pool element, buildFairSchedulerPool creates a < > with the following: Pool name per name attribute Scheduling mode per schedulingMode element (case-insensitive with FIFO as the default) Initial minimum share per minShare element (default: 0 ) Initial weight per weight element (default: 1 ) In the end, buildFairSchedulerPool requests the < > to < > followed by the INFO message in the logs: Created pool: [name], schedulingMode: [mode], minShare: [minShare], weight: [weight] NOTE: buildFairSchedulerPool is used exclusively when FairSchedulableBuilder is requested to < >.","title":"FairSchedulableBuilder"},{"location":"scheduler/FairSchedulableBuilder/#fairschedulablebuilder","text":"FairSchedulableBuilder is a < > that is < > exclusively for scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] for FAIR scheduling mode (when configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FAIR ). [[creating-instance]] FairSchedulableBuilder takes the following to be created: [[rootPool]] < > [[conf]] SparkConf.md[] Once < >, TaskSchedulerImpl requests the FairSchedulableBuilder to < >. [[DEFAULT_SCHEDULER_FILE]] FairSchedulableBuilder uses the pools defined in an < > that is assumed to be the value of the configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property or the default fairscheduler.xml (that is < >). TIP: Use conf/fairscheduler.xml.template as a template for the < >. [[DEFAULT_POOL_NAME]] FairSchedulableBuilder always has the default pool defined (and < > unless done in the < >). [[FAIR_SCHEDULER_PROPERTIES]] [[spark.scheduler.pool]] FairSchedulableBuilder uses spark.scheduler.pool local property for the name of the pool to use when requested to < > (default: < >). Note SparkContext.setLocalProperty lets you set local properties per thread to group jobs in logical groups, e.g. to allow FairSchedulableBuilder to use spark.scheduler.pool property and to group jobs from different threads to be submitted for execution on a non-< > pool.","title":"FairSchedulableBuilder"},{"location":"scheduler/FairSchedulableBuilder/#source-scala","text":"scala> :type sc org.apache.spark.SparkContext sc.setLocalProperty(\"spark.scheduler.pool\", \"production\")","title":"[source, scala]"},{"location":"scheduler/FairSchedulableBuilder/#whatever-is-executed-afterwards-is-submitted-to-production-pool","text":"[[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.scheduler.FairSchedulableBuilder logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.FairSchedulableBuilder=ALL","title":"// whatever is executed afterwards is submitted to production pool"},{"location":"scheduler/FairSchedulableBuilder/#refer-to","text":"=== [[allocations-file]] Allocation Pools Configuration File The allocation pools configuration file is an XML file. The default conf/fairscheduler.xml.template is as follows:","title":"Refer to &lt;&gt;."},{"location":"scheduler/FairSchedulableBuilder/#source-xml","text":"FAIR 1 2 FIFO 2 3 TIP: The top-level element's name allocations can be anything. Spark does not insist on allocations and accepts any name. === [[buildPools]] Building (Tree of) Pools of Schedulables -- buildPools Method","title":"[source, xml]"},{"location":"scheduler/FairSchedulableBuilder/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/FairSchedulableBuilder/#buildpools-unit","text":"NOTE: buildPools is part of the < > to build a tree of < >. buildPools < > if available and then < >. buildPools prints out the following INFO message to the logs when the configuration file (per the configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property) could be read: Creating Fair Scheduler pools from [file] buildPools prints out the following INFO message to the logs when the configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property was not used to define the configuration file and the < > is used instead: Creating Fair Scheduler pools from default file: [DEFAULT_SCHEDULER_FILE] When neither configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property nor the < > could be used, buildPools prints out the following WARN message to the logs: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in [DEFAULT_SCHEDULER_FILE] or set spark.scheduler.allocation.file to a file that contains the configuration. === [[addTaskSetManager]] addTaskSetManager Method","title":"buildPools(): Unit"},{"location":"scheduler/FairSchedulableBuilder/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/FairSchedulableBuilder/#addtasksetmanagermanager-schedulable-properties-properties-unit","text":"NOTE: addTaskSetManager is part of the < > to register a new < > with the < > addTaskSetManager finds the pool by name (in the given Properties ) under the < > property or defaults to the < > pool if undefined. addTaskSetManager then requests the < > to < >. Unless found, addTaskSetManager creates a new < > with the < > (as if the < > pool were used) and requests the < > to < >. In the end, addTaskSetManager prints out the following WARN message to the logs: A job was submitted with scheduler pool [poolName], which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain [poolName]. Created [poolName] with default configuration (schedulingMode: [mode], minShare: [minShare], weight: [weight]) addTaskSetManager then requests the pool (found or newly-created) to < > the given < >. In the end, addTaskSetManager prints out the following INFO message to the logs: Added task set [name] tasks to pool [poolName] === [[buildDefaultPool]] Registering Default Pool -- buildDefaultPool Method","title":"addTaskSetManager(manager: Schedulable, properties: Properties): Unit"},{"location":"scheduler/FairSchedulableBuilder/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/FairSchedulableBuilder/#builddefaultpool-unit","text":"buildDefaultPool requests the < > to < > (one with the < > name). Unless already available, buildDefaultPool creates a < > with the following: < > pool name FIFO scheduling mode 0 for the initial minimum share 1 for the initial weight In the end, buildDefaultPool requests the < > to < > followed by the INFO message in the logs: Created default pool: [name], schedulingMode: [mode], minShare: [minShare], weight: [weight] NOTE: buildDefaultPool is used exclusively when FairSchedulableBuilder is requested to < >. === [[buildFairSchedulerPool]] Building Pools from XML Allocations File -- buildFairSchedulerPool Internal Method","title":"buildDefaultPool(): Unit"},{"location":"scheduler/FairSchedulableBuilder/#source-scala_4","text":"buildFairSchedulerPool( is: InputStream, fileName: String): Unit buildFairSchedulerPool starts by loading the XML file from the given InputStream . For every pool element, buildFairSchedulerPool creates a < > with the following: Pool name per name attribute Scheduling mode per schedulingMode element (case-insensitive with FIFO as the default) Initial minimum share per minShare element (default: 0 ) Initial weight per weight element (default: 1 ) In the end, buildFairSchedulerPool requests the < > to < > followed by the INFO message in the logs: Created pool: [name], schedulingMode: [mode], minShare: [minShare], weight: [weight] NOTE: buildFairSchedulerPool is used exclusively when FairSchedulableBuilder is requested to < >.","title":"[source, scala]"},{"location":"scheduler/HighlyCompressedMapStatus/","text":"HighlyCompressedMapStatus \u00b6 HighlyCompressedMapStatus is...FIXME","title":"HighlyCompressedMapStatus"},{"location":"scheduler/HighlyCompressedMapStatus/#highlycompressedmapstatus","text":"HighlyCompressedMapStatus is...FIXME","title":"HighlyCompressedMapStatus"},{"location":"scheduler/JobListener/","text":"JobListener \u00b6 JobListener is an abstraction of listeners that listen for job completion or failure events (after submitting a job to the DAGScheduler ). Contract \u00b6 taskSucceeded \u00b6 taskSucceeded ( index : Int , result : Any ): Unit Used when DAGScheduler is requested to handleTaskCompletion or markMapStageJobAsFinished jobFailed \u00b6 jobFailed ( exception : Exception ): Unit Used when DAGScheduler is requested to cleanUpAfterSchedulerStop , handleJobSubmitted , handleMapStageSubmitted , handleTaskCompletion or failJobAndIndependentStages Implementations \u00b6 ApproximateActionListener JobWaiter","title":"JobListener"},{"location":"scheduler/JobListener/#joblistener","text":"JobListener is an abstraction of listeners that listen for job completion or failure events (after submitting a job to the DAGScheduler ).","title":"JobListener"},{"location":"scheduler/JobListener/#contract","text":"","title":"Contract"},{"location":"scheduler/JobListener/#tasksucceeded","text":"taskSucceeded ( index : Int , result : Any ): Unit Used when DAGScheduler is requested to handleTaskCompletion or markMapStageJobAsFinished","title":" taskSucceeded"},{"location":"scheduler/JobListener/#jobfailed","text":"jobFailed ( exception : Exception ): Unit Used when DAGScheduler is requested to cleanUpAfterSchedulerStop , handleJobSubmitted , handleMapStageSubmitted , handleTaskCompletion or failJobAndIndependentStages","title":" jobFailed"},{"location":"scheduler/JobListener/#implementations","text":"ApproximateActionListener JobWaiter","title":"Implementations"},{"location":"scheduler/JobWaiter/","text":"JobWaiter \u00b6 JobWaiter is a JobListener to listen to task events and to know when all have finished successfully or not . Creating Instance \u00b6 JobWaiter takes the following to be created: DAGScheduler Job ID Total number of tasks Result Handler Function ( (Int, T) => Unit ) JobWaiter is created when DAGScheduler is requested to submit a job or a map stage . Scala Promise \u00b6 jobPromise : Promise [ Unit ] jobPromise is a Scala Promise that is completed when all tasks have finished successfully or failed with an exception . taskSucceeded \u00b6 taskSucceeded ( index : Int , result : Any ): Unit taskSucceeded executes the Result Handler Function with the given index and result . taskSucceeded marks the waiter finished successfully when all tasks have finished. taskSucceeded is part of the JobListener abstraction. jobFailed \u00b6 jobFailed ( exception : Exception ): Unit jobFailed marks the waiter failed. jobFailed is part of the JobListener abstraction.","title":"JobWaiter"},{"location":"scheduler/JobWaiter/#jobwaiter","text":"JobWaiter is a JobListener to listen to task events and to know when all have finished successfully or not .","title":"JobWaiter"},{"location":"scheduler/JobWaiter/#creating-instance","text":"JobWaiter takes the following to be created: DAGScheduler Job ID Total number of tasks Result Handler Function ( (Int, T) => Unit ) JobWaiter is created when DAGScheduler is requested to submit a job or a map stage .","title":"Creating Instance"},{"location":"scheduler/JobWaiter/#scala-promise","text":"jobPromise : Promise [ Unit ] jobPromise is a Scala Promise that is completed when all tasks have finished successfully or failed with an exception .","title":" Scala Promise"},{"location":"scheduler/JobWaiter/#tasksucceeded","text":"taskSucceeded ( index : Int , result : Any ): Unit taskSucceeded executes the Result Handler Function with the given index and result . taskSucceeded marks the waiter finished successfully when all tasks have finished. taskSucceeded is part of the JobListener abstraction.","title":" taskSucceeded"},{"location":"scheduler/JobWaiter/#jobfailed","text":"jobFailed ( exception : Exception ): Unit jobFailed marks the waiter failed. jobFailed is part of the JobListener abstraction.","title":" jobFailed"},{"location":"scheduler/LiveListenerBus/","text":"LiveListenerBus \u00b6 LiveListenerBus is an event bus to dispatch Spark events to registered SparkListener s. LiveListenerBus is a single-JVM SparkListenerBus that uses listenerThread to poll events . Note The event queue is java.util.concurrent.LinkedBlockingQueue with capacity of 10000 SparkListenerEvent events. Creating Instance \u00b6 LiveListenerBus takes the following to be created: SparkConf LiveListenerBus is created (and started ) when SparkContext is requested to initialize . Event Queues \u00b6 queues : CopyOnWriteArrayList [ AsyncEventQueue ] LiveListenerBus manages AsyncEventQueue s. queues is initialized empty when LiveListenerBus is created . queues is used when: Registering Listener with Queue Posting Event to All Queues Deregistering Listener Starting LiveListenerBus LiveListenerBusMetrics \u00b6 metrics : LiveListenerBusMetrics LiveListenerBus creates a LiveListenerBusMetrics when created . metrics is registered (with a MetricsSystem ) when LiveListenerBus is started . metrics is used to: Increment events posted every event posting Create a AsyncEventQueue when adding a listener to a queue Starting LiveListenerBus \u00b6 start ( sc : SparkContext , metricsSystem : MetricsSystem ): Unit start starts AsyncEventQueue s (from the queues internal registry). In the end, start requests the given MetricsSystem to register the LiveListenerBusMetrics . start is used when: SparkContext is created Posting Event to All Queues \u00b6 post ( event : SparkListenerEvent ): Unit post puts the input event onto the internal eventQueue queue and releases the internal eventLock semaphore. If the event placement was not successful (and it could happen since it is tapped at 10000 events) onDropEvent method is called. The event publishing is only possible when stopped flag has been enabled. post is used when...FIXME postToQueues \u00b6 postToQueues ( event : SparkListenerEvent ): Unit postToQueues ...FIXME Event Dropped Callback \u00b6 onDropEvent ( event : SparkListenerEvent ): Unit onDropEvent is called when no further events can be added to the internal eventQueue queue (while posting a SparkListenerEvent event ). It simply prints out the following ERROR message to the logs and ensures that it happens only once. Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler. Stopping LiveListenerBus \u00b6 stop (): Unit stop releases the internal eventLock semaphore and waits until listenerThread dies. It can only happen after all events were posted (and polling eventQueue gives nothing). stopped flag is enabled. listenerThread for Event Polling \u00b6 LiveListenerBus uses a SparkListenerBus single-daemon thread that ensures that the polling events from the event queue is only after the listener was started and only one event at a time. Registering Listener with Status Queue \u00b6 addToStatusQueue ( listener : SparkListenerInterface ): Unit addToStatusQueue adds the given SparkListenerInterface to appStatus queue. addToStatusQueue is used when: BarrierCoordinator is requested to onStart SparkContext is created HiveThriftServer2 utility is used to createListenerAndUI SharedState ( Spark SQL ) is requested to create a SQLAppStatusStore Registering Listener with Shared Queue \u00b6 addToSharedQueue ( listener : SparkListenerInterface ): Unit addToSharedQueue adds the given SparkListenerInterface to shared queue. addToSharedQueue is used when: SparkContext is requested to register a SparkListener and register extra SparkListeners ExecutionListenerBus (Spark Structured Streaming) is created Registering Listener with executorManagement Queue \u00b6 addToManagementQueue ( listener : SparkListenerInterface ): Unit addToManagementQueue adds the given SparkListenerInterface to executorManagement queue. addToManagementQueue is used when: ExecutorAllocationManager is requested to start HeartbeatReceiver is created Registering Listener with eventLog Queue \u00b6 addToEventLogQueue ( listener : SparkListenerInterface ): Unit addToEventLogQueue adds the given SparkListenerInterface to eventLog queue. addToEventLogQueue is used when: SparkContext is created (with event logging enabled) Registering Listener with Queue \u00b6 addToQueue ( listener : SparkListenerInterface , queue : String ): Unit addToQueue finds the queue in the queues internal registry. If found, addToQueue requests it to add the given listener If not found, addToQueue creates a AsyncEventQueue (with the given name, the LiveListenerBusMetrics , and this LiveListenerBus ) and requests it to add the given listener . The AsyncEventQueue is started and added to the queues internal registry. addToQueue is used when: LiveListenerBus is requested to addToSharedQueue , addToManagementQueue , addToStatusQueue , addToEventLogQueue StreamingQueryListenerBus ( Spark Structured Streaming ) is created Deregistering Listener \u00b6 removeListener ( listener : SparkListenerInterface ): Unit removeListener ...FIXME removeListener is used when: BarrierCoordinator is requested to onStop SparkContext is requested to deregister a SparkListener AsyncEventQueue is requested to deregister a listener on error","title":"LiveListenerBus"},{"location":"scheduler/LiveListenerBus/#livelistenerbus","text":"LiveListenerBus is an event bus to dispatch Spark events to registered SparkListener s. LiveListenerBus is a single-JVM SparkListenerBus that uses listenerThread to poll events . Note The event queue is java.util.concurrent.LinkedBlockingQueue with capacity of 10000 SparkListenerEvent events.","title":"LiveListenerBus"},{"location":"scheduler/LiveListenerBus/#creating-instance","text":"LiveListenerBus takes the following to be created: SparkConf LiveListenerBus is created (and started ) when SparkContext is requested to initialize .","title":"Creating Instance"},{"location":"scheduler/LiveListenerBus/#event-queues","text":"queues : CopyOnWriteArrayList [ AsyncEventQueue ] LiveListenerBus manages AsyncEventQueue s. queues is initialized empty when LiveListenerBus is created . queues is used when: Registering Listener with Queue Posting Event to All Queues Deregistering Listener Starting LiveListenerBus","title":" Event Queues"},{"location":"scheduler/LiveListenerBus/#livelistenerbusmetrics","text":"metrics : LiveListenerBusMetrics LiveListenerBus creates a LiveListenerBusMetrics when created . metrics is registered (with a MetricsSystem ) when LiveListenerBus is started . metrics is used to: Increment events posted every event posting Create a AsyncEventQueue when adding a listener to a queue","title":" LiveListenerBusMetrics"},{"location":"scheduler/LiveListenerBus/#starting-livelistenerbus","text":"start ( sc : SparkContext , metricsSystem : MetricsSystem ): Unit start starts AsyncEventQueue s (from the queues internal registry). In the end, start requests the given MetricsSystem to register the LiveListenerBusMetrics . start is used when: SparkContext is created","title":" Starting LiveListenerBus"},{"location":"scheduler/LiveListenerBus/#posting-event-to-all-queues","text":"post ( event : SparkListenerEvent ): Unit post puts the input event onto the internal eventQueue queue and releases the internal eventLock semaphore. If the event placement was not successful (and it could happen since it is tapped at 10000 events) onDropEvent method is called. The event publishing is only possible when stopped flag has been enabled. post is used when...FIXME","title":" Posting Event to All Queues"},{"location":"scheduler/LiveListenerBus/#posttoqueues","text":"postToQueues ( event : SparkListenerEvent ): Unit postToQueues ...FIXME","title":" postToQueues"},{"location":"scheduler/LiveListenerBus/#event-dropped-callback","text":"onDropEvent ( event : SparkListenerEvent ): Unit onDropEvent is called when no further events can be added to the internal eventQueue queue (while posting a SparkListenerEvent event ). It simply prints out the following ERROR message to the logs and ensures that it happens only once. Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.","title":" Event Dropped Callback"},{"location":"scheduler/LiveListenerBus/#stopping-livelistenerbus","text":"stop (): Unit stop releases the internal eventLock semaphore and waits until listenerThread dies. It can only happen after all events were posted (and polling eventQueue gives nothing). stopped flag is enabled.","title":" Stopping LiveListenerBus"},{"location":"scheduler/LiveListenerBus/#listenerthread-for-event-polling","text":"LiveListenerBus uses a SparkListenerBus single-daemon thread that ensures that the polling events from the event queue is only after the listener was started and only one event at a time.","title":" listenerThread for Event Polling"},{"location":"scheduler/LiveListenerBus/#registering-listener-with-status-queue","text":"addToStatusQueue ( listener : SparkListenerInterface ): Unit addToStatusQueue adds the given SparkListenerInterface to appStatus queue. addToStatusQueue is used when: BarrierCoordinator is requested to onStart SparkContext is created HiveThriftServer2 utility is used to createListenerAndUI SharedState ( Spark SQL ) is requested to create a SQLAppStatusStore","title":" Registering Listener with Status Queue"},{"location":"scheduler/LiveListenerBus/#registering-listener-with-shared-queue","text":"addToSharedQueue ( listener : SparkListenerInterface ): Unit addToSharedQueue adds the given SparkListenerInterface to shared queue. addToSharedQueue is used when: SparkContext is requested to register a SparkListener and register extra SparkListeners ExecutionListenerBus (Spark Structured Streaming) is created","title":" Registering Listener with Shared Queue"},{"location":"scheduler/LiveListenerBus/#registering-listener-with-executormanagement-queue","text":"addToManagementQueue ( listener : SparkListenerInterface ): Unit addToManagementQueue adds the given SparkListenerInterface to executorManagement queue. addToManagementQueue is used when: ExecutorAllocationManager is requested to start HeartbeatReceiver is created","title":" Registering Listener with executorManagement Queue"},{"location":"scheduler/LiveListenerBus/#registering-listener-with-eventlog-queue","text":"addToEventLogQueue ( listener : SparkListenerInterface ): Unit addToEventLogQueue adds the given SparkListenerInterface to eventLog queue. addToEventLogQueue is used when: SparkContext is created (with event logging enabled)","title":" Registering Listener with eventLog Queue"},{"location":"scheduler/LiveListenerBus/#registering-listener-with-queue","text":"addToQueue ( listener : SparkListenerInterface , queue : String ): Unit addToQueue finds the queue in the queues internal registry. If found, addToQueue requests it to add the given listener If not found, addToQueue creates a AsyncEventQueue (with the given name, the LiveListenerBusMetrics , and this LiveListenerBus ) and requests it to add the given listener . The AsyncEventQueue is started and added to the queues internal registry. addToQueue is used when: LiveListenerBus is requested to addToSharedQueue , addToManagementQueue , addToStatusQueue , addToEventLogQueue StreamingQueryListenerBus ( Spark Structured Streaming ) is created","title":" Registering Listener with Queue"},{"location":"scheduler/LiveListenerBus/#deregistering-listener","text":"removeListener ( listener : SparkListenerInterface ): Unit removeListener ...FIXME removeListener is used when: BarrierCoordinator is requested to onStop SparkContext is requested to deregister a SparkListener AsyncEventQueue is requested to deregister a listener on error","title":" Deregistering Listener"},{"location":"scheduler/MapOutputTracker/","text":"MapOutputTracker \u00b6 MapOutputTracker is an base abstraction of shuffle map output location registries . Contract \u00b6 getMapSizesByExecutorId \u00b6 getMapSizesByExecutorId ( shuffleId : Int , startPartition : Int , endPartition : Int ): Iterator [( BlockManagerId , Seq [( BlockId , Long , Int )])] Used when: SortShuffleManager is requested for a ShuffleReader getMapSizesByRange \u00b6 getMapSizesByRange ( shuffleId : Int , startMapIndex : Int , endMapIndex : Int , startPartition : Int , endPartition : Int ): Iterator [( BlockManagerId , Seq [( BlockId , Long , Int )])] Used when: SortShuffleManager is requested for a ShuffleReader unregisterShuffle \u00b6 unregisterShuffle ( shuffleId : Int ): Unit Deletes map output status information for the specified shuffle stage Used when: ContextCleaner is requested to doCleanupShuffle BlockManagerSlaveEndpoint is requested to handle a RemoveShuffle message Implementations \u00b6 MapOutputTrackerMaster MapOutputTrackerWorker Creating Instance \u00b6 MapOutputTracker takes the following to be created: SparkConf Abstract Class MapOutputTracker is an abstract class and cannot be created directly. It is created indirectly for the concrete MapOutputTrackers . Accessing MapOutputTracker \u00b6 MapOutputTracker is available using SparkEnv (on the driver and executors). SparkEnv . get . mapOutputTracker MapOutputTracker RPC Endpoint \u00b6 trackerEndpoint is a RpcEndpointRef of the MapOutputTracker RPC endpoint. trackerEndpoint is initialized (registered or looked up) when SparkEnv is created for the driver and executors. trackerEndpoint is used to communicate (synchronously) . trackerEndpoint is cleared ( null ) when MapOutputTrackerMaster is requested to stop . Deregistering Map Output Status Information of Shuffle Stage \u00b6 unregisterShuffle ( shuffleId : Int ): Unit Deregisters map output status information for the given shuffle stage Used when: ContextCleaner is requested for shuffle cleanup BlockManagerSlaveEndpoint is requested to remove a shuffle Stopping MapOutputTracker \u00b6 stop (): Unit stop does nothing at all. stop is used when SparkEnv is requested to stop (and stops all the services, incl. MapOutputTracker ). Converting MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes \u00b6 convertMapStatuses ( shuffleId : Int , startPartition : Int , endPartition : Int , statuses : Array [ MapStatus ]): Seq [( BlockManagerId , Seq [( BlockId , Long )])] convertMapStatuses iterates over the input statuses array (of MapStatus entries indexed by map id) and creates a collection of BlockManagerId s (for each MapStatus entry) with a ShuffleBlockId (with the input shuffleId , a mapId , and partition ranging from the input startPartition and endPartition ) and estimated size for the reduce block for every status and partitions. For any empty MapStatus , convertMapStatuses prints out the following ERROR message to the logs: Missing an output location for shuffle [id] And convertMapStatuses throws a MetadataFetchFailedException (with shuffleId , startPartition , and the above error message). convertMapStatuses is used when: MapOutputTrackerMaster is requested for the sizes of shuffle map outputs by executor and range MapOutputTrackerWorker is requested to sizes of shuffle map outputs by executor and range Sending Blocking Messages To trackerEndpoint RpcEndpointRef \u00b6 askTracker [ T ]( message : Any ): T askTracker sends the input message to trackerEndpoint RpcEndpointRef and waits for a result. When an exception happens, askTracker prints out the following ERROR message to the logs and throws a SparkException . Error communicating with MapOutputTracker askTracker is used when MapOutputTracker is requested to fetches map outputs for ShuffleDependency remotely and sends a one-way message . Epoch \u00b6 Starts from 0 when MapOutputTracker is created . Can be updated (on MapOutputTrackerWorkers ) or incremented (on the driver's MapOutputTrackerMaster ). sendTracker \u00b6 sendTracker ( message : Any ): Unit sendTracker ...FIXME sendTracker is used when: MapOutputTrackerMaster is requested to stop Utilities \u00b6 serializeMapStatuses \u00b6 serializeMapStatuses ( statuses : Array [ MapStatus ], broadcastManager : BroadcastManager , isLocal : Boolean , minBroadcastSize : Int , conf : SparkConf ): ( Array [ Byte ], Broadcast [ Array [ Byte ]]) serializeMapStatuses serializes the given array of map output locations into an efficient byte format (to send to reduce tasks). serializeMapStatuses compresses the serialized bytes using GZIP. They are supposed to be pretty compressible because many map outputs will be on the same hostname. Internally, serializeMapStatuses creates a Java ByteArrayOutputStream . serializeMapStatuses writes out 0 (direct) first. serializeMapStatuses creates a Java GZIPOutputStream (with the ByteArrayOutputStream created) and writes out the given statuses array. serializeMapStatuses decides whether to return the output array (of the output stream) or use a broadcast variable based on the size of the byte array. If the size of the result byte array is the given minBroadcastSize threshold or bigger, serializeMapStatuses requests the input BroadcastManager to create a broadcast variable . serializeMapStatuses resets the ByteArrayOutputStream and starts over. serializeMapStatuses writes out 1 (broadcast) first. serializeMapStatuses creates a new Java GZIPOutputStream (with the ByteArrayOutputStream created) and writes out the broadcast variable. serializeMapStatuses prints out the following INFO message to the logs: Broadcast mapstatuses size = [ length ], actual size = [ length ] serializeMapStatuses is used when ShuffleStatus is requested to serialize shuffle map output statuses . deserializeMapStatuses \u00b6 deserializeMapStatuses ( bytes : Array [ Byte ], conf : SparkConf ): Array [ MapStatus ] deserializeMapStatuses ...FIXME deserializeMapStatuses is used when: MapOutputTrackerWorker is requested to getStatuses","title":"MapOutputTracker"},{"location":"scheduler/MapOutputTracker/#mapoutputtracker","text":"MapOutputTracker is an base abstraction of shuffle map output location registries .","title":"MapOutputTracker"},{"location":"scheduler/MapOutputTracker/#contract","text":"","title":"Contract"},{"location":"scheduler/MapOutputTracker/#getmapsizesbyexecutorid","text":"getMapSizesByExecutorId ( shuffleId : Int , startPartition : Int , endPartition : Int ): Iterator [( BlockManagerId , Seq [( BlockId , Long , Int )])] Used when: SortShuffleManager is requested for a ShuffleReader","title":" getMapSizesByExecutorId"},{"location":"scheduler/MapOutputTracker/#getmapsizesbyrange","text":"getMapSizesByRange ( shuffleId : Int , startMapIndex : Int , endMapIndex : Int , startPartition : Int , endPartition : Int ): Iterator [( BlockManagerId , Seq [( BlockId , Long , Int )])] Used when: SortShuffleManager is requested for a ShuffleReader","title":" getMapSizesByRange"},{"location":"scheduler/MapOutputTracker/#unregistershuffle","text":"unregisterShuffle ( shuffleId : Int ): Unit Deletes map output status information for the specified shuffle stage Used when: ContextCleaner is requested to doCleanupShuffle BlockManagerSlaveEndpoint is requested to handle a RemoveShuffle message","title":" unregisterShuffle"},{"location":"scheduler/MapOutputTracker/#implementations","text":"MapOutputTrackerMaster MapOutputTrackerWorker","title":"Implementations"},{"location":"scheduler/MapOutputTracker/#creating-instance","text":"MapOutputTracker takes the following to be created: SparkConf Abstract Class MapOutputTracker is an abstract class and cannot be created directly. It is created indirectly for the concrete MapOutputTrackers .","title":"Creating Instance"},{"location":"scheduler/MapOutputTracker/#accessing-mapoutputtracker","text":"MapOutputTracker is available using SparkEnv (on the driver and executors). SparkEnv . get . mapOutputTracker","title":"Accessing MapOutputTracker"},{"location":"scheduler/MapOutputTracker/#mapoutputtracker-rpc-endpoint","text":"trackerEndpoint is a RpcEndpointRef of the MapOutputTracker RPC endpoint. trackerEndpoint is initialized (registered or looked up) when SparkEnv is created for the driver and executors. trackerEndpoint is used to communicate (synchronously) . trackerEndpoint is cleared ( null ) when MapOutputTrackerMaster is requested to stop .","title":" MapOutputTracker RPC Endpoint"},{"location":"scheduler/MapOutputTracker/#deregistering-map-output-status-information-of-shuffle-stage","text":"unregisterShuffle ( shuffleId : Int ): Unit Deregisters map output status information for the given shuffle stage Used when: ContextCleaner is requested for shuffle cleanup BlockManagerSlaveEndpoint is requested to remove a shuffle","title":" Deregistering Map Output Status Information of Shuffle Stage"},{"location":"scheduler/MapOutputTracker/#stopping-mapoutputtracker","text":"stop (): Unit stop does nothing at all. stop is used when SparkEnv is requested to stop (and stops all the services, incl. MapOutputTracker ).","title":" Stopping MapOutputTracker"},{"location":"scheduler/MapOutputTracker/#converting-mapstatuses-to-blockmanagerids-with-shuffleblockids-and-their-sizes","text":"convertMapStatuses ( shuffleId : Int , startPartition : Int , endPartition : Int , statuses : Array [ MapStatus ]): Seq [( BlockManagerId , Seq [( BlockId , Long )])] convertMapStatuses iterates over the input statuses array (of MapStatus entries indexed by map id) and creates a collection of BlockManagerId s (for each MapStatus entry) with a ShuffleBlockId (with the input shuffleId , a mapId , and partition ranging from the input startPartition and endPartition ) and estimated size for the reduce block for every status and partitions. For any empty MapStatus , convertMapStatuses prints out the following ERROR message to the logs: Missing an output location for shuffle [id] And convertMapStatuses throws a MetadataFetchFailedException (with shuffleId , startPartition , and the above error message). convertMapStatuses is used when: MapOutputTrackerMaster is requested for the sizes of shuffle map outputs by executor and range MapOutputTrackerWorker is requested to sizes of shuffle map outputs by executor and range","title":" Converting MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes"},{"location":"scheduler/MapOutputTracker/#sending-blocking-messages-to-trackerendpoint-rpcendpointref","text":"askTracker [ T ]( message : Any ): T askTracker sends the input message to trackerEndpoint RpcEndpointRef and waits for a result. When an exception happens, askTracker prints out the following ERROR message to the logs and throws a SparkException . Error communicating with MapOutputTracker askTracker is used when MapOutputTracker is requested to fetches map outputs for ShuffleDependency remotely and sends a one-way message .","title":" Sending Blocking Messages To trackerEndpoint RpcEndpointRef"},{"location":"scheduler/MapOutputTracker/#epoch","text":"Starts from 0 when MapOutputTracker is created . Can be updated (on MapOutputTrackerWorkers ) or incremented (on the driver's MapOutputTrackerMaster ).","title":" Epoch"},{"location":"scheduler/MapOutputTracker/#sendtracker","text":"sendTracker ( message : Any ): Unit sendTracker ...FIXME sendTracker is used when: MapOutputTrackerMaster is requested to stop","title":" sendTracker"},{"location":"scheduler/MapOutputTracker/#utilities","text":"","title":"Utilities"},{"location":"scheduler/MapOutputTracker/#serializemapstatuses","text":"serializeMapStatuses ( statuses : Array [ MapStatus ], broadcastManager : BroadcastManager , isLocal : Boolean , minBroadcastSize : Int , conf : SparkConf ): ( Array [ Byte ], Broadcast [ Array [ Byte ]]) serializeMapStatuses serializes the given array of map output locations into an efficient byte format (to send to reduce tasks). serializeMapStatuses compresses the serialized bytes using GZIP. They are supposed to be pretty compressible because many map outputs will be on the same hostname. Internally, serializeMapStatuses creates a Java ByteArrayOutputStream . serializeMapStatuses writes out 0 (direct) first. serializeMapStatuses creates a Java GZIPOutputStream (with the ByteArrayOutputStream created) and writes out the given statuses array. serializeMapStatuses decides whether to return the output array (of the output stream) or use a broadcast variable based on the size of the byte array. If the size of the result byte array is the given minBroadcastSize threshold or bigger, serializeMapStatuses requests the input BroadcastManager to create a broadcast variable . serializeMapStatuses resets the ByteArrayOutputStream and starts over. serializeMapStatuses writes out 1 (broadcast) first. serializeMapStatuses creates a new Java GZIPOutputStream (with the ByteArrayOutputStream created) and writes out the broadcast variable. serializeMapStatuses prints out the following INFO message to the logs: Broadcast mapstatuses size = [ length ], actual size = [ length ] serializeMapStatuses is used when ShuffleStatus is requested to serialize shuffle map output statuses .","title":" serializeMapStatuses"},{"location":"scheduler/MapOutputTracker/#deserializemapstatuses","text":"deserializeMapStatuses ( bytes : Array [ Byte ], conf : SparkConf ): Array [ MapStatus ] deserializeMapStatuses ...FIXME deserializeMapStatuses is used when: MapOutputTrackerWorker is requested to getStatuses","title":" deserializeMapStatuses"},{"location":"scheduler/MapOutputTrackerMaster/","text":"MapOutputTrackerMaster \u00b6 MapOutputTrackerMaster is a MapOutputTracker for the driver. MapOutputTrackerMaster is the source of truth of shuffle map output locations . Creating Instance \u00b6 MapOutputTrackerMaster takes the following to be created: SparkConf BroadcastManager isLocal flag (to indicate whether MapOutputTrackerMaster runs in local or a cluster) When created, MapOutputTrackerMaster starts dispatcher threads on the map-output-dispatcher thread pool . MapOutputTrackerMaster is created when: SparkEnv utility is used to create a SparkEnv for the driver maxRpcMessageSize \u00b6 maxRpcMessageSize is...FIXME BroadcastManager \u00b6 MapOutputTrackerMaster is given a BroadcastManager to be created. Shuffle Map Output Status Registry \u00b6 MapOutputTrackerMaster uses an internal registry of ShuffleStatus es by shuffle stages. MapOutputTrackerMaster adds a new shuffle when requested to register one (when DAGScheduler is requested to create a ShuffleMapStage for a ShuffleDependency ). MapOutputTrackerMaster uses the registry when requested for the following: registerMapOutput getStatistics MessageLoop unregisterMapOutput , unregisterAllMapOutput , unregisterShuffle , removeOutputsOnHost , removeOutputsOnExecutor , containsShuffle , getNumAvailableOutputs , findMissingPartitions , getLocationsWithLargestOutputs , getMapSizesByExecutorId MapOutputTrackerMaster removes ( clears ) all shuffles when requested to stop . Configuration Properties \u00b6 MapOutputTrackerMaster uses the following configuration properties: spark.shuffle.mapOutput.minSizeForBroadcast spark.shuffle.mapOutput.dispatcher.numThreads spark.shuffle.reduceLocality.enabled Map and Reduce Task Thresholds for Preferred Locations \u00b6 MapOutputTrackerMaster defines 1000 (tasks) as the hardcoded threshold of the number of map and reduce tasks when requested to compute preferred locations with spark.shuffle.reduceLocality.enabled . Map Output Threshold for Preferred Location of Reduce Tasks \u00b6 MapOutputTrackerMaster defines 0.2 as the fraction of total map output that must be at a location for it to considered as a preferred location for a reduce task. Making this larger will focus on fewer locations where most data can be read locally, but may lead to more delay in scheduling if those locations are busy. MapOutputTrackerMaster uses the fraction when requested for the preferred locations of shuffle RDDs . GetMapOutputMessage Queue \u00b6 MapOutputTrackerMaster uses a blocking queue (a Java LinkedBlockingQueue ) for requests for map output statuses. GetMapOutputMessage ( shuffleId : Int , context : RpcCallContext ) GetMapOutputMessage holds the shuffle ID and the RpcCallContext of the caller. A new GetMapOutputMessage is added to the queue when MapOutputTrackerMaster is requested to post one . MapOutputTrackerMaster uses MessageLoop Dispatcher Threads to process GetMapOutputMessages . MessageLoop Dispatcher Thread \u00b6 MessageLoop is a thread of execution to handle GetMapOutputMessage s until a PoisonPill marker message arrives (when MapOutputTrackerMaster is requested to stop ). MessageLoop takes a GetMapOutputMessage and prints out the following DEBUG message to the logs: Handling request to send map output locations for shuffle [shuffleId] to [hostPort] MessageLoop then finds the ShuffleStatus by the shuffle ID in the shuffleStatuses internal registry and replies back (to the RPC client) with a serialized map output status (with the BroadcastManager and spark.shuffle.mapOutput.minSizeForBroadcast configuration property). MessageLoop threads run on the map-output-dispatcher Thread Pool . map-output-dispatcher Thread Pool \u00b6 threadpool : ThreadPoolExecutor threadpool is a daemon fixed thread pool registered with map-output-dispatcher thread name prefix. threadpool uses spark.shuffle.mapOutput.dispatcher.numThreads configuration property for the number of MessageLoop dispatcher threads to process received GetMapOutputMessage messages. The dispatcher threads are started immediately when MapOutputTrackerMaster is created . The thread pool is shut down when MapOutputTrackerMaster is requested to stop . Epoch Number \u00b6 MapOutputTrackerMaster uses an epoch number to...FIXME getEpoch is used when: DAGScheduler is requested to removeExecutorAndUnregisterOutputs TaskSetManager is created (and sets the epoch to tasks) Enqueueing GetMapOutputMessage \u00b6 post ( message : GetMapOutputMessage ): Unit post simply adds the input GetMapOutputMessage to the mapOutputRequests internal queue. post is used when MapOutputTrackerMasterEndpoint is requested to handle a GetMapOutputStatuses message . Stopping MapOutputTrackerMaster \u00b6 stop (): Unit stop ...FIXME stop is part of the MapOutputTracker abstraction. Unregistering Shuffle Map Output \u00b6 unregisterMapOutput ( shuffleId : Int , mapId : Int , bmAddress : BlockManagerId ): Unit unregisterMapOutput ...FIXME unregisterMapOutput is used when DAGScheduler is requested to handle a task completion (due to a fetch failure) . Computing Preferred Locations \u00b6 getPreferredLocationsForShuffle ( dep : ShuffleDependency [ _ , _ , _ ], partitionId : Int ): Seq [ String ] getPreferredLocationsForShuffle computes the locations ( BlockManager s) with the most shuffle map outputs for the input ShuffleDependency and Partition . getPreferredLocationsForShuffle computes the locations when all of the following are met: spark.shuffle.reduceLocality.enabled configuration property is enabled The number of \"map\" partitions (of the RDD of the input ShuffleDependency ) is below SHUFFLE_PREF_MAP_THRESHOLD The number of \"reduce\" partitions (of the Partitioner of the input ShuffleDependency ) is below SHUFFLE_PREF_REDUCE_THRESHOLD Note getPreferredLocationsForShuffle is simply getLocationsWithLargestOutputs with a guard condition. Internally, getPreferredLocationsForShuffle checks whether spark.shuffle.reduceLocality.enabled configuration property is enabled with the number of partitions of the RDD of the input ShuffleDependency and partitions in the partitioner of the input ShuffleDependency both being less than 1000 . Note The thresholds for the number of partitions in the RDD and of the partitioner when computing the preferred locations are 1000 and are not configurable. If the condition holds, getPreferredLocationsForShuffle finds locations with the largest number of shuffle map outputs for the input ShuffleDependency and partitionId (with the number of partitions in the partitioner of the input ShuffleDependency and 0.2 ) and returns the hosts of the preferred BlockManagers . Note 0.2 is the fraction of total map output that must be at a location to be considered as a preferred location for a reduce task. It is not configurable. getPreferredLocationsForShuffle is used when ShuffledRDD and Spark SQL's ShuffledRowRDD are requested for preferred locations of a partition. Finding Locations with Largest Number of Shuffle Map Outputs \u00b6 getLocationsWithLargestOutputs ( shuffleId : Int , reducerId : Int , numReducers : Int , fractionThreshold : Double ): Option [ Array [ BlockManagerId ]] getLocationsWithLargestOutputs returns BlockManagerId s with the largest size (of all the shuffle blocks they manage) above the input fractionThreshold (given the total size of all the shuffle blocks for the shuffle across all BlockManager s). Note getLocationsWithLargestOutputs may return no BlockManagerId if their shuffle blocks do not total up above the input fractionThreshold . Note The input numReducers is not used. Internally, getLocationsWithLargestOutputs queries the mapStatuses internal cache for the input shuffleId . Note One entry in mapStatuses internal cache is a MapStatus array indexed by partition id. MapStatus includes information about the BlockManager (as BlockManagerId ) and estimated size of the reduce blocks . getLocationsWithLargestOutputs iterates over the MapStatus array and builds an interim mapping between BlockManagerId and the cumulative sum of shuffle blocks across BlockManager s. Incrementing Epoch \u00b6 incrementEpoch (): Unit incrementEpoch increments the internal epoch . incrementEpoch prints out the following DEBUG message to the logs: Increasing epoch to [epoch] incrementEpoch is used when: MapOutputTrackerMaster is requested to unregisterMapOutput , unregisterAllMapOutput , removeOutputsOnHost and removeOutputsOnExecutor DAGScheduler is requested to handle a ShuffleMapTask completion (of a ShuffleMapStage ) Checking Availability of Shuffle Map Output Status \u00b6 containsShuffle ( shuffleId : Int ): Boolean containsShuffle checks if the input shuffleId is registered in the cachedSerializedStatuses or mapStatuses internal caches. containsShuffle is used when DAGScheduler is requested to create a createShuffleMapStage (for a ShuffleDependency ). Registering Shuffle \u00b6 registerShuffle ( shuffleId : Int , numMaps : Int ): Unit registerShuffle registers a new ShuffleStatus (for the given shuffle ID and the number of partitions) to the shuffleStatuses internal registry. registerShuffle throws an IllegalArgumentException when the shuffle ID has already been registered: Shuffle ID [shuffleId] registered twice registerShuffle is used when: DAGScheduler is requested to create a ShuffleMapStage (for a ShuffleDependency ) Registering Map Outputs for Shuffle (Possibly with Epoch Change) \u00b6 registerMapOutputs ( shuffleId : Int , statuses : Array [ MapStatus ], changeEpoch : Boolean = false ): Unit registerMapOutputs registers the input statuses (as the shuffle map output) with the input shuffleId in the mapStatuses internal cache. registerMapOutputs increments epoch if the input changeEpoch is enabled (it is not by default). registerMapOutputs is used when DAGScheduler handles successful ShuffleMapTask completion and executor lost events . Finding Serialized Map Output Statuses (And Possibly Broadcasting Them) \u00b6 getSerializedMapOutputStatuses ( shuffleId : Int ): Array [ Byte ] getSerializedMapOutputStatuses finds cached serialized map statuses for the input shuffleId . If found, getSerializedMapOutputStatuses returns the cached serialized map statuses. Otherwise, getSerializedMapOutputStatuses acquires the shuffle lock for shuffleId and finds cached serialized map statuses again since some other thread could not update the cachedSerializedStatuses internal cache. getSerializedMapOutputStatuses returns the serialized map statuses if found. If not, getSerializedMapOutputStatuses serializes the local array of MapStatuses (from checkCachedStatuses ). getSerializedMapOutputStatuses prints out the following INFO message to the logs: Size of output statuses for shuffle [shuffleId] is [bytes] bytes getSerializedMapOutputStatuses saves the serialized map output statuses in cachedSerializedStatuses internal cache if the epoch has not changed in the meantime. getSerializedMapOutputStatuses also saves its broadcast version in cachedSerializedBroadcast internal cache. If the epoch has changed in the meantime, the serialized map output statuses and their broadcast version are not saved, and getSerializedMapOutputStatuses prints out the following INFO message to the logs: Epoch changed, not caching! getSerializedMapOutputStatuses removes the broadcast . getSerializedMapOutputStatuses returns the serialized map statuses. getSerializedMapOutputStatuses is used when MapOutputTrackerMaster responds to GetMapOutputMessage requests and DAGScheduler creates ShuffleMapStage for ShuffleDependency (copying the shuffle map output locations from previous jobs to avoid unnecessarily regenerating data). Finding Cached Serialized Map Statuses \u00b6 checkCachedStatuses (): Boolean checkCachedStatuses is an internal helper method that < > uses to do some bookkeeping (when the < > and < > differ) and set local statuses , retBytes and epochGotten (that getSerializedMapOutputStatuses uses). Internally, checkCachedStatuses acquires the MapOutputTracker.md#epochLock[ epochLock lock] and checks the status of < > to < cacheEpoch >>. If epoch is younger (i.e. greater), checkCachedStatuses clears < > internal cache, < > and sets cacheEpoch to be epoch . checkCachedStatuses gets the serialized map output statuses for the shuffleId (of the owning < >). When the serialized map output status is found, checkCachedStatuses saves it in a local retBytes and returns true . When not found, you should see the following DEBUG message in the logs: cached status not found for : [shuffleId] checkCachedStatuses uses MapOutputTracker.md#mapStatuses[mapStatuses] internal cache to get map output statuses for the shuffleId (of the owning < >) or falls back to an empty array and sets it to a local statuses . checkCachedStatuses sets the local epochGotten to the current < > and returns false . Registering Shuffle Map Output \u00b6 registerMapOutput ( shuffleId : Int , mapId : Int , status : MapStatus ): Unit registerMapOutput finds the ShuffleStatus by the given shuffle ID and adds the given MapStatus : The given mapId is the partitionId of the ShuffleMapTask that finished. The given shuffleId is the shuffleId of the ShuffleDependency of the ShuffleMapStage (for which the ShuffleMapTask completed) registerMapOutput is used when DAGScheduler is requested to handle a ShuffleMapTask completion . Map Output Statistics for ShuffleDependency \u00b6 getStatistics ( dep : ShuffleDependency [ _ , _ , _ ]): MapOutputStatistics getStatistics looks up the ShuffleStatus for the shuffleId (of the input ShuffleDependency ) in the shuffleStatuses registry. Note It is assumed that the shuffleStatuses registry does have the ShuffleStatus . That makes me believe \"someone else\" is taking care of whether it is available or not. getStatistics requests the ShuffleStatus for the MapStatus es (of the ShuffleDependency ). getStatistics uses the spark.shuffle.mapOutput.parallelAggregationThreshold configuration property to decide on parallelism to calculate the statistics. With no parallelism, getStatistics simply traverses over the MapStatus es and requests them (one by one) for the size of every reduce shuffle block. Note getStatistics requests the given ShuffleDependency for the Partitioner that in turn is requested for the number of partitions . The number of reduce blocks is the number of MapStatus es multiplied by the number of partitions. And hence the need for parallelism based on the spark.shuffle.mapOutput.parallelAggregationThreshold configuration property. In the end, getStatistics creates a MapOutputStatistics with the shuffle ID and the total sizes (sumed up for every partition). getStatistics is used when: DAGScheduler is requested to handle a successful ShuffleMapStage submission and markMapStageJobsAsFinished Deregistering All Map Outputs of Shuffle Stage \u00b6 unregisterAllMapOutput ( shuffleId : Int ): Unit unregisterAllMapOutput ...FIXME unregisterAllMapOutput is used when DAGScheduler is requested to handle a task completion (due to a fetch failure) . Deregistering Shuffle \u00b6 unregisterShuffle ( shuffleId : Int ): Unit unregisterShuffle ...FIXME unregisterShuffle is part of the MapOutputTracker abstraction. Deregistering Shuffle Outputs Associated with Host \u00b6 removeOutputsOnHost ( host : String ): Unit removeOutputsOnHost ...FIXME removeOutputsOnHost is used when DAGScheduler is requested to removeExecutorAndUnregisterOutputs and handle a worker removal . Deregistering Shuffle Outputs Associated with Executor \u00b6 removeOutputsOnExecutor ( execId : String ): Unit removeOutputsOnExecutor ...FIXME removeOutputsOnExecutor is used when DAGScheduler is requested to removeExecutorAndUnregisterOutputs . Number of Partitions with Shuffle Map Outputs Available \u00b6 getNumAvailableOutputs ( shuffleId : Int ): Int getNumAvailableOutputs ...FIXME getNumAvailableOutputs is used when ShuffleMapStage is requested for the number of partitions with shuffle outputs available . Finding Missing Partitions \u00b6 findMissingPartitions ( shuffleId : Int ): Option [ Seq [ Int ]] findMissingPartitions ...FIXME findMissingPartitions is used when ShuffleMapStage is requested for missing partitions . Finding Locations with Blocks and Sizes \u00b6 getMapSizesByExecutorId ( shuffleId : Int , startPartition : Int , endPartition : Int ): Iterator [( BlockManagerId , Seq [( BlockId , Long )])] getMapSizesByExecutorId is part of the MapOutputTracker abstraction. getMapSizesByExecutorId returns a collection of BlockManagerId s with their blocks and sizes. When executed, getMapSizesByExecutorId prints out the following DEBUG message to the logs: Fetching outputs for shuffle [id], partitions [startPartition]-[endPartition] getMapSizesByExecutorId finds map outputs for the input shuffleId . Note getMapSizesByExecutorId gets the map outputs for all the partitions (despite the method's signature). In the end, getMapSizesByExecutorId converts shuffle map outputs (as MapStatuses ) into the collection of BlockManagerId s with their blocks and sizes. Logging \u00b6 Enable ALL logging level for org.apache.spark.MapOutputTrackerMaster logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.MapOutputTrackerMaster=ALL Refer to Logging .","title":"MapOutputTrackerMaster"},{"location":"scheduler/MapOutputTrackerMaster/#mapoutputtrackermaster","text":"MapOutputTrackerMaster is a MapOutputTracker for the driver. MapOutputTrackerMaster is the source of truth of shuffle map output locations .","title":"MapOutputTrackerMaster"},{"location":"scheduler/MapOutputTrackerMaster/#creating-instance","text":"MapOutputTrackerMaster takes the following to be created: SparkConf BroadcastManager isLocal flag (to indicate whether MapOutputTrackerMaster runs in local or a cluster) When created, MapOutputTrackerMaster starts dispatcher threads on the map-output-dispatcher thread pool . MapOutputTrackerMaster is created when: SparkEnv utility is used to create a SparkEnv for the driver","title":"Creating Instance"},{"location":"scheduler/MapOutputTrackerMaster/#maxrpcmessagesize","text":"maxRpcMessageSize is...FIXME","title":" maxRpcMessageSize"},{"location":"scheduler/MapOutputTrackerMaster/#broadcastmanager","text":"MapOutputTrackerMaster is given a BroadcastManager to be created.","title":" BroadcastManager"},{"location":"scheduler/MapOutputTrackerMaster/#shuffle-map-output-status-registry","text":"MapOutputTrackerMaster uses an internal registry of ShuffleStatus es by shuffle stages. MapOutputTrackerMaster adds a new shuffle when requested to register one (when DAGScheduler is requested to create a ShuffleMapStage for a ShuffleDependency ). MapOutputTrackerMaster uses the registry when requested for the following: registerMapOutput getStatistics MessageLoop unregisterMapOutput , unregisterAllMapOutput , unregisterShuffle , removeOutputsOnHost , removeOutputsOnExecutor , containsShuffle , getNumAvailableOutputs , findMissingPartitions , getLocationsWithLargestOutputs , getMapSizesByExecutorId MapOutputTrackerMaster removes ( clears ) all shuffles when requested to stop .","title":" Shuffle Map Output Status Registry"},{"location":"scheduler/MapOutputTrackerMaster/#configuration-properties","text":"MapOutputTrackerMaster uses the following configuration properties: spark.shuffle.mapOutput.minSizeForBroadcast spark.shuffle.mapOutput.dispatcher.numThreads spark.shuffle.reduceLocality.enabled","title":"Configuration Properties"},{"location":"scheduler/MapOutputTrackerMaster/#map-and-reduce-task-thresholds-for-preferred-locations","text":"MapOutputTrackerMaster defines 1000 (tasks) as the hardcoded threshold of the number of map and reduce tasks when requested to compute preferred locations with spark.shuffle.reduceLocality.enabled .","title":" Map and Reduce Task Thresholds for Preferred Locations"},{"location":"scheduler/MapOutputTrackerMaster/#map-output-threshold-for-preferred-location-of-reduce-tasks","text":"MapOutputTrackerMaster defines 0.2 as the fraction of total map output that must be at a location for it to considered as a preferred location for a reduce task. Making this larger will focus on fewer locations where most data can be read locally, but may lead to more delay in scheduling if those locations are busy. MapOutputTrackerMaster uses the fraction when requested for the preferred locations of shuffle RDDs .","title":" Map Output Threshold for Preferred Location of Reduce Tasks"},{"location":"scheduler/MapOutputTrackerMaster/#getmapoutputmessage-queue","text":"MapOutputTrackerMaster uses a blocking queue (a Java LinkedBlockingQueue ) for requests for map output statuses. GetMapOutputMessage ( shuffleId : Int , context : RpcCallContext ) GetMapOutputMessage holds the shuffle ID and the RpcCallContext of the caller. A new GetMapOutputMessage is added to the queue when MapOutputTrackerMaster is requested to post one . MapOutputTrackerMaster uses MessageLoop Dispatcher Threads to process GetMapOutputMessages .","title":" GetMapOutputMessage Queue"},{"location":"scheduler/MapOutputTrackerMaster/#messageloop-dispatcher-thread","text":"MessageLoop is a thread of execution to handle GetMapOutputMessage s until a PoisonPill marker message arrives (when MapOutputTrackerMaster is requested to stop ). MessageLoop takes a GetMapOutputMessage and prints out the following DEBUG message to the logs: Handling request to send map output locations for shuffle [shuffleId] to [hostPort] MessageLoop then finds the ShuffleStatus by the shuffle ID in the shuffleStatuses internal registry and replies back (to the RPC client) with a serialized map output status (with the BroadcastManager and spark.shuffle.mapOutput.minSizeForBroadcast configuration property). MessageLoop threads run on the map-output-dispatcher Thread Pool .","title":" MessageLoop Dispatcher Thread"},{"location":"scheduler/MapOutputTrackerMaster/#map-output-dispatcher-thread-pool","text":"threadpool : ThreadPoolExecutor threadpool is a daemon fixed thread pool registered with map-output-dispatcher thread name prefix. threadpool uses spark.shuffle.mapOutput.dispatcher.numThreads configuration property for the number of MessageLoop dispatcher threads to process received GetMapOutputMessage messages. The dispatcher threads are started immediately when MapOutputTrackerMaster is created . The thread pool is shut down when MapOutputTrackerMaster is requested to stop .","title":" map-output-dispatcher Thread Pool"},{"location":"scheduler/MapOutputTrackerMaster/#epoch-number","text":"MapOutputTrackerMaster uses an epoch number to...FIXME getEpoch is used when: DAGScheduler is requested to removeExecutorAndUnregisterOutputs TaskSetManager is created (and sets the epoch to tasks)","title":" Epoch Number"},{"location":"scheduler/MapOutputTrackerMaster/#enqueueing-getmapoutputmessage","text":"post ( message : GetMapOutputMessage ): Unit post simply adds the input GetMapOutputMessage to the mapOutputRequests internal queue. post is used when MapOutputTrackerMasterEndpoint is requested to handle a GetMapOutputStatuses message .","title":" Enqueueing GetMapOutputMessage"},{"location":"scheduler/MapOutputTrackerMaster/#stopping-mapoutputtrackermaster","text":"stop (): Unit stop ...FIXME stop is part of the MapOutputTracker abstraction.","title":" Stopping MapOutputTrackerMaster"},{"location":"scheduler/MapOutputTrackerMaster/#unregistering-shuffle-map-output","text":"unregisterMapOutput ( shuffleId : Int , mapId : Int , bmAddress : BlockManagerId ): Unit unregisterMapOutput ...FIXME unregisterMapOutput is used when DAGScheduler is requested to handle a task completion (due to a fetch failure) .","title":" Unregistering Shuffle Map Output"},{"location":"scheduler/MapOutputTrackerMaster/#computing-preferred-locations","text":"getPreferredLocationsForShuffle ( dep : ShuffleDependency [ _ , _ , _ ], partitionId : Int ): Seq [ String ] getPreferredLocationsForShuffle computes the locations ( BlockManager s) with the most shuffle map outputs for the input ShuffleDependency and Partition . getPreferredLocationsForShuffle computes the locations when all of the following are met: spark.shuffle.reduceLocality.enabled configuration property is enabled The number of \"map\" partitions (of the RDD of the input ShuffleDependency ) is below SHUFFLE_PREF_MAP_THRESHOLD The number of \"reduce\" partitions (of the Partitioner of the input ShuffleDependency ) is below SHUFFLE_PREF_REDUCE_THRESHOLD Note getPreferredLocationsForShuffle is simply getLocationsWithLargestOutputs with a guard condition. Internally, getPreferredLocationsForShuffle checks whether spark.shuffle.reduceLocality.enabled configuration property is enabled with the number of partitions of the RDD of the input ShuffleDependency and partitions in the partitioner of the input ShuffleDependency both being less than 1000 . Note The thresholds for the number of partitions in the RDD and of the partitioner when computing the preferred locations are 1000 and are not configurable. If the condition holds, getPreferredLocationsForShuffle finds locations with the largest number of shuffle map outputs for the input ShuffleDependency and partitionId (with the number of partitions in the partitioner of the input ShuffleDependency and 0.2 ) and returns the hosts of the preferred BlockManagers . Note 0.2 is the fraction of total map output that must be at a location to be considered as a preferred location for a reduce task. It is not configurable. getPreferredLocationsForShuffle is used when ShuffledRDD and Spark SQL's ShuffledRowRDD are requested for preferred locations of a partition.","title":" Computing Preferred Locations"},{"location":"scheduler/MapOutputTrackerMaster/#finding-locations-with-largest-number-of-shuffle-map-outputs","text":"getLocationsWithLargestOutputs ( shuffleId : Int , reducerId : Int , numReducers : Int , fractionThreshold : Double ): Option [ Array [ BlockManagerId ]] getLocationsWithLargestOutputs returns BlockManagerId s with the largest size (of all the shuffle blocks they manage) above the input fractionThreshold (given the total size of all the shuffle blocks for the shuffle across all BlockManager s). Note getLocationsWithLargestOutputs may return no BlockManagerId if their shuffle blocks do not total up above the input fractionThreshold . Note The input numReducers is not used. Internally, getLocationsWithLargestOutputs queries the mapStatuses internal cache for the input shuffleId . Note One entry in mapStatuses internal cache is a MapStatus array indexed by partition id. MapStatus includes information about the BlockManager (as BlockManagerId ) and estimated size of the reduce blocks . getLocationsWithLargestOutputs iterates over the MapStatus array and builds an interim mapping between BlockManagerId and the cumulative sum of shuffle blocks across BlockManager s.","title":" Finding Locations with Largest Number of Shuffle Map Outputs"},{"location":"scheduler/MapOutputTrackerMaster/#incrementing-epoch","text":"incrementEpoch (): Unit incrementEpoch increments the internal epoch . incrementEpoch prints out the following DEBUG message to the logs: Increasing epoch to [epoch] incrementEpoch is used when: MapOutputTrackerMaster is requested to unregisterMapOutput , unregisterAllMapOutput , removeOutputsOnHost and removeOutputsOnExecutor DAGScheduler is requested to handle a ShuffleMapTask completion (of a ShuffleMapStage )","title":" Incrementing Epoch"},{"location":"scheduler/MapOutputTrackerMaster/#checking-availability-of-shuffle-map-output-status","text":"containsShuffle ( shuffleId : Int ): Boolean containsShuffle checks if the input shuffleId is registered in the cachedSerializedStatuses or mapStatuses internal caches. containsShuffle is used when DAGScheduler is requested to create a createShuffleMapStage (for a ShuffleDependency ).","title":" Checking Availability of Shuffle Map Output Status"},{"location":"scheduler/MapOutputTrackerMaster/#registering-shuffle","text":"registerShuffle ( shuffleId : Int , numMaps : Int ): Unit registerShuffle registers a new ShuffleStatus (for the given shuffle ID and the number of partitions) to the shuffleStatuses internal registry. registerShuffle throws an IllegalArgumentException when the shuffle ID has already been registered: Shuffle ID [shuffleId] registered twice registerShuffle is used when: DAGScheduler is requested to create a ShuffleMapStage (for a ShuffleDependency )","title":" Registering Shuffle"},{"location":"scheduler/MapOutputTrackerMaster/#registering-map-outputs-for-shuffle-possibly-with-epoch-change","text":"registerMapOutputs ( shuffleId : Int , statuses : Array [ MapStatus ], changeEpoch : Boolean = false ): Unit registerMapOutputs registers the input statuses (as the shuffle map output) with the input shuffleId in the mapStatuses internal cache. registerMapOutputs increments epoch if the input changeEpoch is enabled (it is not by default). registerMapOutputs is used when DAGScheduler handles successful ShuffleMapTask completion and executor lost events .","title":" Registering Map Outputs for Shuffle (Possibly with Epoch Change)"},{"location":"scheduler/MapOutputTrackerMaster/#finding-serialized-map-output-statuses-and-possibly-broadcasting-them","text":"getSerializedMapOutputStatuses ( shuffleId : Int ): Array [ Byte ] getSerializedMapOutputStatuses finds cached serialized map statuses for the input shuffleId . If found, getSerializedMapOutputStatuses returns the cached serialized map statuses. Otherwise, getSerializedMapOutputStatuses acquires the shuffle lock for shuffleId and finds cached serialized map statuses again since some other thread could not update the cachedSerializedStatuses internal cache. getSerializedMapOutputStatuses returns the serialized map statuses if found. If not, getSerializedMapOutputStatuses serializes the local array of MapStatuses (from checkCachedStatuses ). getSerializedMapOutputStatuses prints out the following INFO message to the logs: Size of output statuses for shuffle [shuffleId] is [bytes] bytes getSerializedMapOutputStatuses saves the serialized map output statuses in cachedSerializedStatuses internal cache if the epoch has not changed in the meantime. getSerializedMapOutputStatuses also saves its broadcast version in cachedSerializedBroadcast internal cache. If the epoch has changed in the meantime, the serialized map output statuses and their broadcast version are not saved, and getSerializedMapOutputStatuses prints out the following INFO message to the logs: Epoch changed, not caching! getSerializedMapOutputStatuses removes the broadcast . getSerializedMapOutputStatuses returns the serialized map statuses. getSerializedMapOutputStatuses is used when MapOutputTrackerMaster responds to GetMapOutputMessage requests and DAGScheduler creates ShuffleMapStage for ShuffleDependency (copying the shuffle map output locations from previous jobs to avoid unnecessarily regenerating data).","title":" Finding Serialized Map Output Statuses (And Possibly Broadcasting Them)"},{"location":"scheduler/MapOutputTrackerMaster/#finding-cached-serialized-map-statuses","text":"checkCachedStatuses (): Boolean checkCachedStatuses is an internal helper method that < > uses to do some bookkeeping (when the < > and < > differ) and set local statuses , retBytes and epochGotten (that getSerializedMapOutputStatuses uses). Internally, checkCachedStatuses acquires the MapOutputTracker.md#epochLock[ epochLock lock] and checks the status of < > to < cacheEpoch >>. If epoch is younger (i.e. greater), checkCachedStatuses clears < > internal cache, < > and sets cacheEpoch to be epoch . checkCachedStatuses gets the serialized map output statuses for the shuffleId (of the owning < >). When the serialized map output status is found, checkCachedStatuses saves it in a local retBytes and returns true . When not found, you should see the following DEBUG message in the logs: cached status not found for : [shuffleId] checkCachedStatuses uses MapOutputTracker.md#mapStatuses[mapStatuses] internal cache to get map output statuses for the shuffleId (of the owning < >) or falls back to an empty array and sets it to a local statuses . checkCachedStatuses sets the local epochGotten to the current < > and returns false .","title":" Finding Cached Serialized Map Statuses"},{"location":"scheduler/MapOutputTrackerMaster/#registering-shuffle-map-output","text":"registerMapOutput ( shuffleId : Int , mapId : Int , status : MapStatus ): Unit registerMapOutput finds the ShuffleStatus by the given shuffle ID and adds the given MapStatus : The given mapId is the partitionId of the ShuffleMapTask that finished. The given shuffleId is the shuffleId of the ShuffleDependency of the ShuffleMapStage (for which the ShuffleMapTask completed) registerMapOutput is used when DAGScheduler is requested to handle a ShuffleMapTask completion .","title":" Registering Shuffle Map Output"},{"location":"scheduler/MapOutputTrackerMaster/#map-output-statistics-for-shuffledependency","text":"getStatistics ( dep : ShuffleDependency [ _ , _ , _ ]): MapOutputStatistics getStatistics looks up the ShuffleStatus for the shuffleId (of the input ShuffleDependency ) in the shuffleStatuses registry. Note It is assumed that the shuffleStatuses registry does have the ShuffleStatus . That makes me believe \"someone else\" is taking care of whether it is available or not. getStatistics requests the ShuffleStatus for the MapStatus es (of the ShuffleDependency ). getStatistics uses the spark.shuffle.mapOutput.parallelAggregationThreshold configuration property to decide on parallelism to calculate the statistics. With no parallelism, getStatistics simply traverses over the MapStatus es and requests them (one by one) for the size of every reduce shuffle block. Note getStatistics requests the given ShuffleDependency for the Partitioner that in turn is requested for the number of partitions . The number of reduce blocks is the number of MapStatus es multiplied by the number of partitions. And hence the need for parallelism based on the spark.shuffle.mapOutput.parallelAggregationThreshold configuration property. In the end, getStatistics creates a MapOutputStatistics with the shuffle ID and the total sizes (sumed up for every partition). getStatistics is used when: DAGScheduler is requested to handle a successful ShuffleMapStage submission and markMapStageJobsAsFinished","title":" Map Output Statistics for ShuffleDependency"},{"location":"scheduler/MapOutputTrackerMaster/#deregistering-all-map-outputs-of-shuffle-stage","text":"unregisterAllMapOutput ( shuffleId : Int ): Unit unregisterAllMapOutput ...FIXME unregisterAllMapOutput is used when DAGScheduler is requested to handle a task completion (due to a fetch failure) .","title":" Deregistering All Map Outputs of Shuffle Stage"},{"location":"scheduler/MapOutputTrackerMaster/#deregistering-shuffle","text":"unregisterShuffle ( shuffleId : Int ): Unit unregisterShuffle ...FIXME unregisterShuffle is part of the MapOutputTracker abstraction.","title":" Deregistering Shuffle"},{"location":"scheduler/MapOutputTrackerMaster/#deregistering-shuffle-outputs-associated-with-host","text":"removeOutputsOnHost ( host : String ): Unit removeOutputsOnHost ...FIXME removeOutputsOnHost is used when DAGScheduler is requested to removeExecutorAndUnregisterOutputs and handle a worker removal .","title":" Deregistering Shuffle Outputs Associated with Host"},{"location":"scheduler/MapOutputTrackerMaster/#deregistering-shuffle-outputs-associated-with-executor","text":"removeOutputsOnExecutor ( execId : String ): Unit removeOutputsOnExecutor ...FIXME removeOutputsOnExecutor is used when DAGScheduler is requested to removeExecutorAndUnregisterOutputs .","title":" Deregistering Shuffle Outputs Associated with Executor"},{"location":"scheduler/MapOutputTrackerMaster/#number-of-partitions-with-shuffle-map-outputs-available","text":"getNumAvailableOutputs ( shuffleId : Int ): Int getNumAvailableOutputs ...FIXME getNumAvailableOutputs is used when ShuffleMapStage is requested for the number of partitions with shuffle outputs available .","title":" Number of Partitions with Shuffle Map Outputs Available"},{"location":"scheduler/MapOutputTrackerMaster/#finding-missing-partitions","text":"findMissingPartitions ( shuffleId : Int ): Option [ Seq [ Int ]] findMissingPartitions ...FIXME findMissingPartitions is used when ShuffleMapStage is requested for missing partitions .","title":" Finding Missing Partitions"},{"location":"scheduler/MapOutputTrackerMaster/#finding-locations-with-blocks-and-sizes","text":"getMapSizesByExecutorId ( shuffleId : Int , startPartition : Int , endPartition : Int ): Iterator [( BlockManagerId , Seq [( BlockId , Long )])] getMapSizesByExecutorId is part of the MapOutputTracker abstraction. getMapSizesByExecutorId returns a collection of BlockManagerId s with their blocks and sizes. When executed, getMapSizesByExecutorId prints out the following DEBUG message to the logs: Fetching outputs for shuffle [id], partitions [startPartition]-[endPartition] getMapSizesByExecutorId finds map outputs for the input shuffleId . Note getMapSizesByExecutorId gets the map outputs for all the partitions (despite the method's signature). In the end, getMapSizesByExecutorId converts shuffle map outputs (as MapStatuses ) into the collection of BlockManagerId s with their blocks and sizes.","title":" Finding Locations with Blocks and Sizes"},{"location":"scheduler/MapOutputTrackerMaster/#logging","text":"Enable ALL logging level for org.apache.spark.MapOutputTrackerMaster logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.MapOutputTrackerMaster=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/","text":"MapOutputTrackerMasterEndpoint \u00b6 MapOutputTrackerMasterEndpoint is an RpcEndpoint for MapOutputTrackerMaster . MapOutputTrackerMasterEndpoint is registered under the name of MapOutputTracker (on the driver ). Creating Instance \u00b6 MapOutputTrackerMasterEndpoint takes the following to be created: RpcEnv MapOutputTrackerMaster SparkConf MapOutputTrackerMasterEndpoint is created when: SparkEnv is created (for the driver and executors) While being created, MapOutputTrackerMasterEndpoint prints out the following DEBUG message to the logs: init Messages \u00b6 GetMapOutputStatuses \u00b6 GetMapOutputStatuses ( shuffleId : Int ) Posted when MapOutputTrackerWorker is requested for shuffle map outputs for a given shuffle ID When received, MapOutputTrackerMasterEndpoint prints out the following INFO message to the logs: Asked to send map output locations for shuffle [shuffleId] to [hostPort] In the end, MapOutputTrackerMasterEndpoint requests the MapOutputTrackerMaster to post a GetMapOutputMessage (with the input shuffleId ). Whatever is returned from MapOutputTrackerMaster becomes the response. StopMapOutputTracker \u00b6 Posted when MapOutputTrackerMaster is requested to stop . When received, MapOutputTrackerMasterEndpoint prints out the following INFO message to the logs: MapOutputTrackerMasterEndpoint stopped! MapOutputTrackerMasterEndpoint confirms the request (by replying true ) and stops . Logging \u00b6 Enable ALL logging level for org.apache.spark.MapOutputTrackerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.MapOutputTrackerMasterEndpoint=ALL Refer to Logging .","title":"MapOutputTrackerMasterEndpoint"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#mapoutputtrackermasterendpoint","text":"MapOutputTrackerMasterEndpoint is an RpcEndpoint for MapOutputTrackerMaster . MapOutputTrackerMasterEndpoint is registered under the name of MapOutputTracker (on the driver ).","title":"MapOutputTrackerMasterEndpoint"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#creating-instance","text":"MapOutputTrackerMasterEndpoint takes the following to be created: RpcEnv MapOutputTrackerMaster SparkConf MapOutputTrackerMasterEndpoint is created when: SparkEnv is created (for the driver and executors) While being created, MapOutputTrackerMasterEndpoint prints out the following DEBUG message to the logs: init","title":"Creating Instance"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#messages","text":"","title":" Messages"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#getmapoutputstatuses","text":"GetMapOutputStatuses ( shuffleId : Int ) Posted when MapOutputTrackerWorker is requested for shuffle map outputs for a given shuffle ID When received, MapOutputTrackerMasterEndpoint prints out the following INFO message to the logs: Asked to send map output locations for shuffle [shuffleId] to [hostPort] In the end, MapOutputTrackerMasterEndpoint requests the MapOutputTrackerMaster to post a GetMapOutputMessage (with the input shuffleId ). Whatever is returned from MapOutputTrackerMaster becomes the response.","title":" GetMapOutputStatuses"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#stopmapoutputtracker","text":"Posted when MapOutputTrackerMaster is requested to stop . When received, MapOutputTrackerMasterEndpoint prints out the following INFO message to the logs: MapOutputTrackerMasterEndpoint stopped! MapOutputTrackerMasterEndpoint confirms the request (by replying true ) and stops .","title":" StopMapOutputTracker"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#logging","text":"Enable ALL logging level for org.apache.spark.MapOutputTrackerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.MapOutputTrackerMasterEndpoint=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/MapOutputTrackerWorker/","text":"MapOutputTrackerWorker \u00b6 MapOutputTrackerWorker is the MapOutputTracker for executors. MapOutputTrackerWorker uses Java's thread-safe java.util.concurrent.ConcurrentHashMap for mapStatuses internal cache and any lookup cache miss triggers a fetch from the driver's MapOutputTrackerMaster . == [[getStatuses]] Finding Shuffle Map Outputs [source, scala] \u00b6 getStatuses( shuffleId: Int): Array[MapStatus] getStatuses finds MapStatus.md[MapStatuses] for the input shuffleId in the < > internal cache and, when not available, fetches them from a remote MapOutputTrackerMaster.md[MapOutputTrackerMaster] (using RPC). Internally, getStatuses first queries the < mapStatuses internal cache>> and returns the map outputs if found. If not found (in the mapStatuses internal cache), you should see the following INFO message in the logs: Don't have map outputs for shuffle [id], fetching them If some other process fetches the map outputs for the shuffleId (as recorded in fetching internal registry), getStatuses waits until it is done. When no other process fetches the map outputs, getStatuses registers the input shuffleId in fetching internal registry (of shuffle map outputs being fetched). You should see the following INFO message in the logs: Doing the fetch; tracker endpoint = [trackerEndpoint] getStatuses sends a GetMapOutputStatuses RPC remote message for the input shuffleId to the trackerEndpoint expecting a Array[Byte] . NOTE: getStatuses requests shuffle map outputs remotely within a timeout and with retries. Refer to rpc:RpcEndpointRef.md[RpcEndpointRef]. getStatuses < > and records the result in the < mapStatuses internal cache>>. You should see the following INFO message in the logs: Got the output locations getStatuses removes the input shuffleId from fetching internal registry. You should see the following DEBUG message in the logs: Fetching map output statuses for shuffle [id] took [time] ms If getStatuses could not find the map output locations for the input shuffleId (locally and remotely), you should see the following ERROR message in the logs and throws a MetadataFetchFailedException . Missing all output locations for shuffle [id] NOTE: getStatuses is used when MapOutputTracker < > and < ShuffleDependency >>. == [[logging]] Logging Enable ALL logging level for org.apache.spark.MapOutputTrackerWorker logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.MapOutputTrackerWorker=ALL \u00b6 Refer to spark-logging.md[Logging].","title":"MapOutputTrackerWorker"},{"location":"scheduler/MapOutputTrackerWorker/#mapoutputtrackerworker","text":"MapOutputTrackerWorker is the MapOutputTracker for executors. MapOutputTrackerWorker uses Java's thread-safe java.util.concurrent.ConcurrentHashMap for mapStatuses internal cache and any lookup cache miss triggers a fetch from the driver's MapOutputTrackerMaster . == [[getStatuses]] Finding Shuffle Map Outputs","title":"MapOutputTrackerWorker"},{"location":"scheduler/MapOutputTrackerWorker/#source-scala","text":"getStatuses( shuffleId: Int): Array[MapStatus] getStatuses finds MapStatus.md[MapStatuses] for the input shuffleId in the < > internal cache and, when not available, fetches them from a remote MapOutputTrackerMaster.md[MapOutputTrackerMaster] (using RPC). Internally, getStatuses first queries the < mapStatuses internal cache>> and returns the map outputs if found. If not found (in the mapStatuses internal cache), you should see the following INFO message in the logs: Don't have map outputs for shuffle [id], fetching them If some other process fetches the map outputs for the shuffleId (as recorded in fetching internal registry), getStatuses waits until it is done. When no other process fetches the map outputs, getStatuses registers the input shuffleId in fetching internal registry (of shuffle map outputs being fetched). You should see the following INFO message in the logs: Doing the fetch; tracker endpoint = [trackerEndpoint] getStatuses sends a GetMapOutputStatuses RPC remote message for the input shuffleId to the trackerEndpoint expecting a Array[Byte] . NOTE: getStatuses requests shuffle map outputs remotely within a timeout and with retries. Refer to rpc:RpcEndpointRef.md[RpcEndpointRef]. getStatuses < > and records the result in the < mapStatuses internal cache>>. You should see the following INFO message in the logs: Got the output locations getStatuses removes the input shuffleId from fetching internal registry. You should see the following DEBUG message in the logs: Fetching map output statuses for shuffle [id] took [time] ms If getStatuses could not find the map output locations for the input shuffleId (locally and remotely), you should see the following ERROR message in the logs and throws a MetadataFetchFailedException . Missing all output locations for shuffle [id] NOTE: getStatuses is used when MapOutputTracker < > and < ShuffleDependency >>. == [[logging]] Logging Enable ALL logging level for org.apache.spark.MapOutputTrackerWorker logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerWorker/#source","text":"","title":"[source]"},{"location":"scheduler/MapOutputTrackerWorker/#log4jloggerorgapachesparkmapoutputtrackerworkerall","text":"Refer to spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.MapOutputTrackerWorker=ALL"},{"location":"scheduler/MapStatus/","text":"MapStatus \u00b6 MapStatus is an abstraction of shuffle map output statuses with an estimated size , location and map Id . MapStatus is a result of executing a ShuffleMapTask . After a ShuffleMapTask has finished execution successfully , DAGScheduler is requested to handle a ShuffleMapTask completion that in turn requests the MapOutputTrackerMaster to register the MapStatus . Contract \u00b6 Estimated Size \u00b6 getSizeForBlock ( reduceId : Int ): Long Estimated size (in bytes) Used when: MapOutputTrackerMaster is requested for a MapOutputStatistics and locations with the largest number of shuffle map outputs MapOutputTracker utility is used to convert MapStatuses OptimizeSkewedJoin ( Spark SQL ) physical optimization is executed Location \u00b6 location : BlockManagerId BlockManagerId of the shuffle map output (i.e. the BlockManager where a ShuffleMapTask ran and the result is stored) Used when: ShuffleStatus is requested to removeMapOutput and removeOutputsByFilter MapOutputTrackerMaster is requested for locations with the largest number of shuffle map outputs and getMapLocation MapOutputTracker utility is used to convert MapStatuses DAGScheduler is requested to handle a ShuffleMapTask completion Map Id \u00b6 mapId : Long Map Id of the shuffle map output Used when: MapOutputTracker utility is used to convert MapStatuses Implementations \u00b6 CompressedMapStatus HighlyCompressedMapStatus Sealed Trait MapStatus is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file). spark.shuffle.minNumPartitionsToHighlyCompress \u00b6 MapStatus utility uses spark.shuffle.minNumPartitionsToHighlyCompress internal configuration property for the minimum number of partitions to prefer a HighlyCompressedMapStatus . Creating MapStatus \u00b6 apply ( loc : BlockManagerId , uncompressedSizes : Array [ Long ], mapTaskId : Long ): MapStatus apply creates a HighlyCompressedMapStatus when the number of uncompressedSizes is above minPartitionsToUseHighlyCompressMapStatus threshold. Otherwise, apply creates a CompressedMapStatus . apply is used when: SortShuffleWriter is requested to write records BypassMergeSortShuffleWriter is requested to write records UnsafeShuffleWriter is requested to close resources and write out merged spill files","title":"MapStatus"},{"location":"scheduler/MapStatus/#mapstatus","text":"MapStatus is an abstraction of shuffle map output statuses with an estimated size , location and map Id . MapStatus is a result of executing a ShuffleMapTask . After a ShuffleMapTask has finished execution successfully , DAGScheduler is requested to handle a ShuffleMapTask completion that in turn requests the MapOutputTrackerMaster to register the MapStatus .","title":"MapStatus"},{"location":"scheduler/MapStatus/#contract","text":"","title":"Contract"},{"location":"scheduler/MapStatus/#estimated-size","text":"getSizeForBlock ( reduceId : Int ): Long Estimated size (in bytes) Used when: MapOutputTrackerMaster is requested for a MapOutputStatistics and locations with the largest number of shuffle map outputs MapOutputTracker utility is used to convert MapStatuses OptimizeSkewedJoin ( Spark SQL ) physical optimization is executed","title":" Estimated Size"},{"location":"scheduler/MapStatus/#location","text":"location : BlockManagerId BlockManagerId of the shuffle map output (i.e. the BlockManager where a ShuffleMapTask ran and the result is stored) Used when: ShuffleStatus is requested to removeMapOutput and removeOutputsByFilter MapOutputTrackerMaster is requested for locations with the largest number of shuffle map outputs and getMapLocation MapOutputTracker utility is used to convert MapStatuses DAGScheduler is requested to handle a ShuffleMapTask completion","title":" Location"},{"location":"scheduler/MapStatus/#map-id","text":"mapId : Long Map Id of the shuffle map output Used when: MapOutputTracker utility is used to convert MapStatuses","title":" Map Id"},{"location":"scheduler/MapStatus/#implementations","text":"CompressedMapStatus HighlyCompressedMapStatus Sealed Trait MapStatus is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).","title":"Implementations"},{"location":"scheduler/MapStatus/#sparkshuffleminnumpartitionstohighlycompress","text":"MapStatus utility uses spark.shuffle.minNumPartitionsToHighlyCompress internal configuration property for the minimum number of partitions to prefer a HighlyCompressedMapStatus .","title":" spark.shuffle.minNumPartitionsToHighlyCompress"},{"location":"scheduler/MapStatus/#creating-mapstatus","text":"apply ( loc : BlockManagerId , uncompressedSizes : Array [ Long ], mapTaskId : Long ): MapStatus apply creates a HighlyCompressedMapStatus when the number of uncompressedSizes is above minPartitionsToUseHighlyCompressMapStatus threshold. Otherwise, apply creates a CompressedMapStatus . apply is used when: SortShuffleWriter is requested to write records BypassMergeSortShuffleWriter is requested to write records UnsafeShuffleWriter is requested to close resources and write out merged spill files","title":" Creating MapStatus"},{"location":"scheduler/Pool/","text":"== [[Pool]] Schedulable Pool Pool is a scheduler:spark-scheduler-Schedulable.md[Schedulable] entity that represents a tree of scheduler:TaskSetManager.md[TaskSetManagers], i.e. it contains a collection of TaskSetManagers or the Pools thereof. A Pool has a mandatory name, a spark-scheduler-SchedulingMode.md[scheduling mode], initial minShare and weight that are defined when it is created. NOTE: An instance of Pool is created when scheduler:TaskSchedulerImpl.md#initialize[TaskSchedulerImpl is initialized]. NOTE: The scheduler:TaskScheduler.md#contract[TaskScheduler Contract] and spark-scheduler-Schedulable.md#contract[Schedulable Contract] both require that their entities have rootPool of type Pool . === [[increaseRunningTasks]] increaseRunningTasks Method CAUTION: FIXME === [[decreaseRunningTasks]] decreaseRunningTasks Method CAUTION: FIXME === [[taskSetSchedulingAlgorithm]] taskSetSchedulingAlgorithm Attribute Using the spark-scheduler-SchedulingMode.md[scheduling mode] (given when a Pool object is created), Pool selects < > and sets taskSetSchedulingAlgorithm : < > for FIFO scheduling mode. < > for FAIR scheduling mode. It throws an IllegalArgumentException when unsupported scheduling mode is passed on: Unsupported spark.scheduler.mode: [schedulingMode] TIP: Read about the scheduling modes in spark-scheduler-SchedulingMode.md[SchedulingMode]. NOTE: taskSetSchedulingAlgorithm is used in < >. === [[getSortedTaskSetQueue]] Getting TaskSetManagers Sorted -- getSortedTaskSetQueue Method NOTE: getSortedTaskSetQueue is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. getSortedTaskSetQueue sorts all the spark-scheduler-Schedulable.md[Schedulables] in spark-scheduler-Schedulable.md#contract[schedulableQueue] queue by a < > (from the internal < >). NOTE: It is called when scheduler:TaskSchedulerImpl.md#resourceOffers[ TaskSchedulerImpl processes executor resource offers]. === [[schedulableNameToSchedulable]] Schedulables by Name -- schedulableNameToSchedulable Registry [source, scala] \u00b6 schedulableNameToSchedulable = new ConcurrentHashMap[String, Schedulable] \u00b6 schedulableNameToSchedulable is a lookup table of spark-scheduler-Schedulable.md[Schedulable] objects by their names. Beside the obvious usage in the housekeeping methods like addSchedulable , removeSchedulable , getSchedulableByName from the spark-scheduler-Schedulable.md#contract[Schedulable Contract], it is exclusively used in SparkContext.md#getPoolForName[SparkContext.getPoolForName]. === [[addSchedulable]] addSchedulable Method NOTE: addSchedulable is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. addSchedulable adds a Schedulable to the spark-scheduler-Schedulable.md#contract[schedulableQueue] and < >. More importantly, it sets the Schedulable entity's spark-scheduler-Schedulable.md#contract[parent] to itself. === [[removeSchedulable]] removeSchedulable Method NOTE: removeSchedulable is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. removeSchedulable removes a Schedulable from the spark-scheduler-Schedulable.md#contract[schedulableQueue] and < >. NOTE: removeSchedulable is the opposite to < addSchedulable method>>. === [[SchedulingAlgorithm]] SchedulingAlgorithm SchedulingAlgorithm is the interface for a sorting algorithm to sort spark-scheduler-Schedulable.md[Schedulables]. There are currently two SchedulingAlgorithms : < > for FIFO scheduling mode. < > for FAIR scheduling mode. ==== [[FIFOSchedulingAlgorithm]] FIFOSchedulingAlgorithm FIFOSchedulingAlgorithm is a scheduling algorithm that compares Schedulables by their priority first and, when equal, by their stageId . NOTE: priority and stageId are part of spark-scheduler-Schedulable.md#contract[Schedulable Contract]. CAUTION: FIXME A picture is worth a thousand words. How to picture the algorithm? ==== [[FairSchedulingAlgorithm]] FairSchedulingAlgorithm FairSchedulingAlgorithm is a scheduling algorithm that compares Schedulables by their minShare , runningTasks , and weight . NOTE: minShare , runningTasks , and weight are part of spark-scheduler-Schedulable.md#contract[Schedulable Contract]. .FairSchedulingAlgorithm image::spark-pool-FairSchedulingAlgorithm.png[align=\"center\"] For each input Schedulable , minShareRatio is computed as runningTasks by minShare (but at least 1 ) while taskToWeightRatio is runningTasks by weight . === [[getSchedulableByName]] Finding Schedulable by Name -- getSchedulableByName Method [source, scala] \u00b6 getSchedulableByName(schedulableName: String): Schedulable \u00b6 NOTE: getSchedulableByName is part of the < > to find a < > by name. getSchedulableByName ...FIXME","title":"Pool"},{"location":"scheduler/Pool/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/Pool/#schedulablenametoschedulable-new-concurrenthashmapstring-schedulable","text":"schedulableNameToSchedulable is a lookup table of spark-scheduler-Schedulable.md[Schedulable] objects by their names. Beside the obvious usage in the housekeeping methods like addSchedulable , removeSchedulable , getSchedulableByName from the spark-scheduler-Schedulable.md#contract[Schedulable Contract], it is exclusively used in SparkContext.md#getPoolForName[SparkContext.getPoolForName]. === [[addSchedulable]] addSchedulable Method NOTE: addSchedulable is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. addSchedulable adds a Schedulable to the spark-scheduler-Schedulable.md#contract[schedulableQueue] and < >. More importantly, it sets the Schedulable entity's spark-scheduler-Schedulable.md#contract[parent] to itself. === [[removeSchedulable]] removeSchedulable Method NOTE: removeSchedulable is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. removeSchedulable removes a Schedulable from the spark-scheduler-Schedulable.md#contract[schedulableQueue] and < >. NOTE: removeSchedulable is the opposite to < addSchedulable method>>. === [[SchedulingAlgorithm]] SchedulingAlgorithm SchedulingAlgorithm is the interface for a sorting algorithm to sort spark-scheduler-Schedulable.md[Schedulables]. There are currently two SchedulingAlgorithms : < > for FIFO scheduling mode. < > for FAIR scheduling mode. ==== [[FIFOSchedulingAlgorithm]] FIFOSchedulingAlgorithm FIFOSchedulingAlgorithm is a scheduling algorithm that compares Schedulables by their priority first and, when equal, by their stageId . NOTE: priority and stageId are part of spark-scheduler-Schedulable.md#contract[Schedulable Contract]. CAUTION: FIXME A picture is worth a thousand words. How to picture the algorithm? ==== [[FairSchedulingAlgorithm]] FairSchedulingAlgorithm FairSchedulingAlgorithm is a scheduling algorithm that compares Schedulables by their minShare , runningTasks , and weight . NOTE: minShare , runningTasks , and weight are part of spark-scheduler-Schedulable.md#contract[Schedulable Contract]. .FairSchedulingAlgorithm image::spark-pool-FairSchedulingAlgorithm.png[align=\"center\"] For each input Schedulable , minShareRatio is computed as runningTasks by minShare (but at least 1 ) while taskToWeightRatio is runningTasks by weight . === [[getSchedulableByName]] Finding Schedulable by Name -- getSchedulableByName Method","title":"schedulableNameToSchedulable = new ConcurrentHashMap[String, Schedulable]"},{"location":"scheduler/Pool/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/Pool/#getschedulablebynameschedulablename-string-schedulable","text":"NOTE: getSchedulableByName is part of the < > to find a < > by name. getSchedulableByName ...FIXME","title":"getSchedulableByName(schedulableName: String): Schedulable"},{"location":"scheduler/ResultStage/","text":"ResultStage \u00b6 ResultStage is the final stage in a job that applies a function on one or many partitions of the target RDD to compute the result of an action. The partitions are given as a collection of partition ids ( partitions ) and the function func: (TaskContext, Iterator[_]) => _ . == [[findMissingPartitions]] Finding Missing Partitions [source, scala] \u00b6 findMissingPartitions(): Seq[Int] \u00b6 NOTE: findMissingPartitions is part of the scheduler:Stage.md#findMissingPartitions[Stage] abstraction. findMissingPartitions...FIXME .ResultStage.findMissingPartitions and ActiveJob image::resultstage-findMissingPartitions.png[align=\"center\"] In the above figure, partitions 1 and 2 are not finished ( F is false while T is true). == [[func]] func Property CAUTION: FIXME == [[setActiveJob]] setActiveJob Method CAUTION: FIXME == [[removeActiveJob]] removeActiveJob Method CAUTION: FIXME == [[activeJob]] activeJob Method [source, scala] \u00b6 activeJob: Option[ActiveJob] \u00b6 activeJob returns the optional ActiveJob associated with a ResultStage . CAUTION: FIXME When/why would that be NONE (empty)?","title":"ResultStage"},{"location":"scheduler/ResultStage/#resultstage","text":"ResultStage is the final stage in a job that applies a function on one or many partitions of the target RDD to compute the result of an action. The partitions are given as a collection of partition ids ( partitions ) and the function func: (TaskContext, Iterator[_]) => _ . == [[findMissingPartitions]] Finding Missing Partitions","title":"ResultStage"},{"location":"scheduler/ResultStage/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/ResultStage/#findmissingpartitions-seqint","text":"NOTE: findMissingPartitions is part of the scheduler:Stage.md#findMissingPartitions[Stage] abstraction. findMissingPartitions...FIXME .ResultStage.findMissingPartitions and ActiveJob image::resultstage-findMissingPartitions.png[align=\"center\"] In the above figure, partitions 1 and 2 are not finished ( F is false while T is true). == [[func]] func Property CAUTION: FIXME == [[setActiveJob]] setActiveJob Method CAUTION: FIXME == [[removeActiveJob]] removeActiveJob Method CAUTION: FIXME == [[activeJob]] activeJob Method","title":"findMissingPartitions(): Seq[Int]"},{"location":"scheduler/ResultStage/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/ResultStage/#activejob-optionactivejob","text":"activeJob returns the optional ActiveJob associated with a ResultStage . CAUTION: FIXME When/why would that be NONE (empty)?","title":"activeJob: Option[ActiveJob]"},{"location":"scheduler/ResultTask/","text":"ResultTask \u00b6 ResultTask[T, U] is a Task that executes a partition processing function on a partition with records (of type T ) to produce a result (of type U ) that is sent back to the driver. T -- [ResultTask] --> U Creating Instance \u00b6 ResultTask takes the following to be created: Stage ID Stage Attempt ID Broadcast Variable with a serialized task ( Broadcast[Array[Byte]] ) Partition to compute TaskLocation Output ID Local Properties Serialized TaskMetrics ( Array[Byte] ) ActiveJob ID (optional) Application ID (optional) Application Attempt ID (optional) isBarrier flag (default: false ) ResultTask is created when: DAGScheduler is requested to submit missing tasks of a ResultStage Running Task \u00b6 runTask ( context : TaskContext ): U runTask is part of the Task abstraction. runTask deserializes a RDD and a partition processing function from the broadcast variable (using the Closure Serializer ). In the end, runTask executes the function (on the records from the partition of the RDD ).","title":"ResultTask"},{"location":"scheduler/ResultTask/#resulttask","text":"ResultTask[T, U] is a Task that executes a partition processing function on a partition with records (of type T ) to produce a result (of type U ) that is sent back to the driver. T -- [ResultTask] --> U","title":"ResultTask"},{"location":"scheduler/ResultTask/#creating-instance","text":"ResultTask takes the following to be created: Stage ID Stage Attempt ID Broadcast Variable with a serialized task ( Broadcast[Array[Byte]] ) Partition to compute TaskLocation Output ID Local Properties Serialized TaskMetrics ( Array[Byte] ) ActiveJob ID (optional) Application ID (optional) Application Attempt ID (optional) isBarrier flag (default: false ) ResultTask is created when: DAGScheduler is requested to submit missing tasks of a ResultStage","title":"Creating Instance"},{"location":"scheduler/ResultTask/#running-task","text":"runTask ( context : TaskContext ): U runTask is part of the Task abstraction. runTask deserializes a RDD and a partition processing function from the broadcast variable (using the Closure Serializer ). In the end, runTask executes the function (on the records from the partition of the RDD ).","title":" Running Task"},{"location":"scheduler/Schedulable/","text":"== [[Schedulable]] Schedulable Contract -- Schedulable Entities Schedulable is the < > of < > that manages the < > and can < >. [[contract]] .Schedulable Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | addSchedulable a| [[addSchedulable]] [source, scala] \u00b6 addSchedulable(schedulable: Schedulable): Unit \u00b6 Registers a < > Used when: FIFOSchedulableBuilder is requested to < > FairSchedulableBuilder is requested to < >, < >, and < > | checkSpeculatableTasks a| [[checkSpeculatableTasks]] [source, scala] \u00b6 checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean \u00b6 Used when...FIXME | executorLost a| [[executorLost]] [source, scala] \u00b6 executorLost( executorId: String, host: String, reason: ExecutorLossReason): Unit Handles an executor lost event Used when: Pool is requested to < > TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#removeExecutor[removeExecutor] | getSchedulableByName a| [[getSchedulableByName]] [source, scala] \u00b6 getSchedulableByName(name: String): Schedulable \u00b6 Finds a < > by name Used when...FIXME | getSortedTaskSetQueue a| [[getSortedTaskSetQueue]] [source, scala] \u00b6 getSortedTaskSetQueue: ArrayBuffer[TaskSetManager] \u00b6 Builds a collection of scheduler:TaskSetManager.md[TaskSetManagers] sorted by < > Used when: Pool is requested to < > (recursively) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers] | minShare a| [[minShare]] [source, scala] \u00b6 minShare: Int \u00b6 Used when...FIXME | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 Used when...FIXME | parent a| [[parent]] [source, scala] \u00b6 parent: Pool \u00b6 Used when...FIXME | priority a| [[priority]] [source, scala] \u00b6 priority: Int \u00b6 Used when...FIXME | removeSchedulable a| [[removeSchedulable]] [source, scala] \u00b6 removeSchedulable(schedulable: Schedulable): Unit \u00b6 Used when...FIXME | runningTasks a| [[runningTasks]] [source, scala] \u00b6 runningTasks: Int \u00b6 Used when...FIXME | schedulableQueue a| [[schedulableQueue]] [source, scala] \u00b6 schedulableQueue: ConcurrentLinkedQueue[Schedulable] \u00b6 Queue of < > (as https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentLinkedQueue.html[ConcurrentLinkedQueue ]) Used when: SparkContext is requested to SparkContext.md#getAllPools[getAllPools] Pool is requested to < >, < >, < >, < >, < >, and < > | schedulingMode a| [[schedulingMode]] [source, scala] \u00b6 schedulingMode: SchedulingMode \u00b6 < > Used when: Pool is < > web UI's PoolTable is requested to render a page with pools ( poolRow ) | stageId a| [[stageId]] [source, scala] \u00b6 stageId: Int \u00b6 Used when...FIXME | weight a| [[weight]] [source, scala] \u00b6 weight: Int \u00b6 Used when...FIXME |=== [[implementations]] .Schedulables [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Schedulable | Description | < > | [[Pool]] Pool of < > (i.e. a recursive data structure for prioritizing task sets) | scheduler:TaskSetManager.md[TaskSetManager] | [[TaskSetManager]] Manages scheduling of tasks of a scheduler:TaskSet.md[TaskSet] |===","title":"Schedulable"},{"location":"scheduler/Schedulable/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#addschedulableschedulable-schedulable-unit","text":"Registers a < > Used when: FIFOSchedulableBuilder is requested to < > FairSchedulableBuilder is requested to < >, < >, and < > | checkSpeculatableTasks a| [[checkSpeculatableTasks]]","title":"addSchedulable(schedulable: Schedulable): Unit"},{"location":"scheduler/Schedulable/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#checkspeculatabletasksmintimetospeculation-int-boolean","text":"Used when...FIXME | executorLost a| [[executorLost]]","title":"checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean"},{"location":"scheduler/Schedulable/#source-scala_2","text":"executorLost( executorId: String, host: String, reason: ExecutorLossReason): Unit Handles an executor lost event Used when: Pool is requested to < > TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#removeExecutor[removeExecutor] | getSchedulableByName a| [[getSchedulableByName]]","title":"[source, scala]"},{"location":"scheduler/Schedulable/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#getschedulablebynamename-string-schedulable","text":"Finds a < > by name Used when...FIXME | getSortedTaskSetQueue a| [[getSortedTaskSetQueue]]","title":"getSchedulableByName(name: String): Schedulable"},{"location":"scheduler/Schedulable/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#getsortedtasksetqueue-arraybuffertasksetmanager","text":"Builds a collection of scheduler:TaskSetManager.md[TaskSetManagers] sorted by < > Used when: Pool is requested to < > (recursively) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers] | minShare a| [[minShare]]","title":"getSortedTaskSetQueue: ArrayBuffer[TaskSetManager]"},{"location":"scheduler/Schedulable/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#minshare-int","text":"Used when...FIXME | name a| [[name]]","title":"minShare: Int"},{"location":"scheduler/Schedulable/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#name-string","text":"Used when...FIXME | parent a| [[parent]]","title":"name: String"},{"location":"scheduler/Schedulable/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#parent-pool","text":"Used when...FIXME | priority a| [[priority]]","title":"parent: Pool"},{"location":"scheduler/Schedulable/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#priority-int","text":"Used when...FIXME | removeSchedulable a| [[removeSchedulable]]","title":"priority: Int"},{"location":"scheduler/Schedulable/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#removeschedulableschedulable-schedulable-unit","text":"Used when...FIXME | runningTasks a| [[runningTasks]]","title":"removeSchedulable(schedulable: Schedulable): Unit"},{"location":"scheduler/Schedulable/#source-scala_10","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#runningtasks-int","text":"Used when...FIXME | schedulableQueue a| [[schedulableQueue]]","title":"runningTasks: Int"},{"location":"scheduler/Schedulable/#source-scala_11","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#schedulablequeue-concurrentlinkedqueueschedulable","text":"Queue of < > (as https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentLinkedQueue.html[ConcurrentLinkedQueue ]) Used when: SparkContext is requested to SparkContext.md#getAllPools[getAllPools] Pool is requested to < >, < >, < >, < >, < >, and < > | schedulingMode a| [[schedulingMode]]","title":"schedulableQueue: ConcurrentLinkedQueue[Schedulable]"},{"location":"scheduler/Schedulable/#source-scala_12","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#schedulingmode-schedulingmode","text":"< > Used when: Pool is < > web UI's PoolTable is requested to render a page with pools ( poolRow ) | stageId a| [[stageId]]","title":"schedulingMode: SchedulingMode"},{"location":"scheduler/Schedulable/#source-scala_13","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#stageid-int","text":"Used when...FIXME | weight a| [[weight]]","title":"stageId: Int"},{"location":"scheduler/Schedulable/#source-scala_14","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#weight-int","text":"Used when...FIXME |=== [[implementations]] .Schedulables [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Schedulable | Description | < > | [[Pool]] Pool of < > (i.e. a recursive data structure for prioritizing task sets) | scheduler:TaskSetManager.md[TaskSetManager] | [[TaskSetManager]] Manages scheduling of tasks of a scheduler:TaskSet.md[TaskSet] |===","title":"weight: Int"},{"location":"scheduler/SchedulableBuilder/","text":"== [[SchedulableBuilder]] SchedulableBuilder Contract -- Builders of Schedulable Pools SchedulableBuilder is the < > of < > that manage a < >, which is to < > and < >. SchedulableBuilder is a private[spark] Scala trait that is used exclusively by scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] (the default Spark scheduler). When requested to scheduler:TaskSchedulerImpl.md#initialize[initialize], TaskSchedulerImpl uses the configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property (default: FIFO ) to select one of the < >. [[contract]] .SchedulableBuilder Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | addTaskSetManager a| [[addTaskSetManager]] [source, scala] \u00b6 addTaskSetManager(manager: Schedulable, properties: Properties): Unit \u00b6 Registers a new < > with the < > Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#submitTasks[submit tasks (of TaskSet) for execution] (and registers a new scheduler:TaskSetManager.md[TaskSetManager] for the TaskSet ) | buildPools a| [[buildPools]] [source, scala] \u00b6 buildPools(): Unit \u00b6 Builds a tree of < > Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#initialize[initialize] (and creates a scheduler:TaskSchedulerImpl.md#schedulableBuilder[SchedulableBuilder] per configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property) | rootPool a| [[rootPool]] [source, scala] \u00b6 rootPool: Pool \u00b6 Root (top-level) < > Used when: FIFOSchedulableBuilder is requested to < > FairSchedulableBuilder is requested to < >, < >, and < > |=== [[implementations]] .SchedulableBuilders [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | SchedulableBuilder | Description | < > | [[FairSchedulableBuilder]] Used when the configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FAIR | < > | [[FIFOSchedulableBuilder]] Default SchedulableBuilder that is used when the configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FIFO (default) |===","title":"SchedulableBuilder"},{"location":"scheduler/SchedulableBuilder/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulableBuilder/#addtasksetmanagermanager-schedulable-properties-properties-unit","text":"Registers a new < > with the < > Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#submitTasks[submit tasks (of TaskSet) for execution] (and registers a new scheduler:TaskSetManager.md[TaskSetManager] for the TaskSet ) | buildPools a| [[buildPools]]","title":"addTaskSetManager(manager: Schedulable, properties: Properties): Unit"},{"location":"scheduler/SchedulableBuilder/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulableBuilder/#buildpools-unit","text":"Builds a tree of < > Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#initialize[initialize] (and creates a scheduler:TaskSchedulerImpl.md#schedulableBuilder[SchedulableBuilder] per configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property) | rootPool a| [[rootPool]]","title":"buildPools(): Unit"},{"location":"scheduler/SchedulableBuilder/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulableBuilder/#rootpool-pool","text":"Root (top-level) < > Used when: FIFOSchedulableBuilder is requested to < > FairSchedulableBuilder is requested to < >, < >, and < > |=== [[implementations]] .SchedulableBuilders [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | SchedulableBuilder | Description | < > | [[FairSchedulableBuilder]] Used when the configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FAIR | < > | [[FIFOSchedulableBuilder]] Default SchedulableBuilder that is used when the configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FIFO (default) |===","title":"rootPool: Pool"},{"location":"scheduler/SchedulerBackend/","text":"SchedulerBackend \u00b6 SchedulerBackend is an abstraction of task scheduling backends that can revive resource offers from cluster managers. SchedulerBackend abstraction allows TaskSchedulerImpl to use variety of cluster managers (with their own resource offers and task scheduling modes). Note Being a scheduler backend system assumes a Apache Mesos -like scheduling model in which \"an application\" gets resource offers as machines become available so it is possible to launch tasks on them. Once required resource allocation is obtained, the scheduler backend can start executors. Contract \u00b6 applicationAttemptId \u00b6 applicationAttemptId (): Option [ String ] Execution attempt ID of this Spark application Default: None (undefined) Used when: TaskSchedulerImpl is requested for the execution attempt ID of a Spark application applicationId \u00b6 applicationId (): String Unique identifier of this Spark application Default: spark-application-[currentTimeMillis] Used when: TaskSchedulerImpl is requested for the unique identifier of a Spark application Default Parallelism \u00b6 defaultParallelism (): Int Default parallelism , i.e. a hint for the number of tasks in stages while sizing jobs Used when: TaskSchedulerImpl is requested for the default parallelism getDriverAttributes \u00b6 getDriverAttributes : Option [ Map [ String , String ]] Default: None Used when: SparkContext is requested to postApplicationStart getDriverLogUrls \u00b6 getDriverLogUrls : Option [ Map [ String , String ]] Driver log URLs Default: None (undefined) Used when: SparkContext is requested to postApplicationStart isReady \u00b6 isReady (): Boolean Controls whether this SchedulerBackend is ready ( true ) or not ( false ) Default: true Used when: TaskSchedulerImpl is requested to wait until scheduling backend is ready Killing Task \u00b6 killTask ( taskId : Long , executorId : String , interruptThread : Boolean , reason : String ): Unit Kills a given task Default: UnsupportedOperationException Used when: TaskSchedulerImpl is requested to killTaskAttempt and killAllTaskAttempts TaskSetManager is requested to handle a successful task attempt maxNumConcurrentTasks \u00b6 maxNumConcurrentTasks (): Int Maximum number of concurrent tasks that can currently be launched Used when: SparkContext is requested to maxNumConcurrentTasks reviveOffers \u00b6 reviveOffers (): Unit Handles resource allocation offers (from the scheduling system) Used when TaskSchedulerImpl is requested to: Submit tasks (from a TaskSet) Handle a task status update Notify the TaskSetManager that a task has failed Check for speculatable tasks Handle a lost executor event Starting SchedulerBackend \u00b6 start (): Unit Starts this SchedulerBackend Used when: TaskSchedulerImpl is requested to start stop \u00b6 stop (): Unit Stops this SchedulerBackend Used when: TaskSchedulerImpl is requested to stop Implementations \u00b6 CoarseGrainedSchedulerBackend LocalSchedulerBackend MesosFineGrainedSchedulerBackend","title":"SchedulerBackend"},{"location":"scheduler/SchedulerBackend/#schedulerbackend","text":"SchedulerBackend is an abstraction of task scheduling backends that can revive resource offers from cluster managers. SchedulerBackend abstraction allows TaskSchedulerImpl to use variety of cluster managers (with their own resource offers and task scheduling modes). Note Being a scheduler backend system assumes a Apache Mesos -like scheduling model in which \"an application\" gets resource offers as machines become available so it is possible to launch tasks on them. Once required resource allocation is obtained, the scheduler backend can start executors.","title":"SchedulerBackend"},{"location":"scheduler/SchedulerBackend/#contract","text":"","title":"Contract"},{"location":"scheduler/SchedulerBackend/#applicationattemptid","text":"applicationAttemptId (): Option [ String ] Execution attempt ID of this Spark application Default: None (undefined) Used when: TaskSchedulerImpl is requested for the execution attempt ID of a Spark application","title":" applicationAttemptId"},{"location":"scheduler/SchedulerBackend/#applicationid","text":"applicationId (): String Unique identifier of this Spark application Default: spark-application-[currentTimeMillis] Used when: TaskSchedulerImpl is requested for the unique identifier of a Spark application","title":" applicationId"},{"location":"scheduler/SchedulerBackend/#default-parallelism","text":"defaultParallelism (): Int Default parallelism , i.e. a hint for the number of tasks in stages while sizing jobs Used when: TaskSchedulerImpl is requested for the default parallelism","title":" Default Parallelism"},{"location":"scheduler/SchedulerBackend/#getdriverattributes","text":"getDriverAttributes : Option [ Map [ String , String ]] Default: None Used when: SparkContext is requested to postApplicationStart","title":" getDriverAttributes"},{"location":"scheduler/SchedulerBackend/#getdriverlogurls","text":"getDriverLogUrls : Option [ Map [ String , String ]] Driver log URLs Default: None (undefined) Used when: SparkContext is requested to postApplicationStart","title":" getDriverLogUrls"},{"location":"scheduler/SchedulerBackend/#isready","text":"isReady (): Boolean Controls whether this SchedulerBackend is ready ( true ) or not ( false ) Default: true Used when: TaskSchedulerImpl is requested to wait until scheduling backend is ready","title":" isReady"},{"location":"scheduler/SchedulerBackend/#killing-task","text":"killTask ( taskId : Long , executorId : String , interruptThread : Boolean , reason : String ): Unit Kills a given task Default: UnsupportedOperationException Used when: TaskSchedulerImpl is requested to killTaskAttempt and killAllTaskAttempts TaskSetManager is requested to handle a successful task attempt","title":" Killing Task"},{"location":"scheduler/SchedulerBackend/#maxnumconcurrenttasks","text":"maxNumConcurrentTasks (): Int Maximum number of concurrent tasks that can currently be launched Used when: SparkContext is requested to maxNumConcurrentTasks","title":" maxNumConcurrentTasks"},{"location":"scheduler/SchedulerBackend/#reviveoffers","text":"reviveOffers (): Unit Handles resource allocation offers (from the scheduling system) Used when TaskSchedulerImpl is requested to: Submit tasks (from a TaskSet) Handle a task status update Notify the TaskSetManager that a task has failed Check for speculatable tasks Handle a lost executor event","title":" reviveOffers"},{"location":"scheduler/SchedulerBackend/#starting-schedulerbackend","text":"start (): Unit Starts this SchedulerBackend Used when: TaskSchedulerImpl is requested to start","title":" Starting SchedulerBackend"},{"location":"scheduler/SchedulerBackend/#stop","text":"stop (): Unit Stops this SchedulerBackend Used when: TaskSchedulerImpl is requested to stop","title":" stop"},{"location":"scheduler/SchedulerBackend/#implementations","text":"CoarseGrainedSchedulerBackend LocalSchedulerBackend MesosFineGrainedSchedulerBackend","title":"Implementations"},{"location":"scheduler/SchedulerBackendUtils/","text":"SchedulerBackendUtils \u00b6 SchedulerBackendUtils is...FIXME","title":"SchedulerBackendUtils"},{"location":"scheduler/SchedulerBackendUtils/#schedulerbackendutils","text":"SchedulerBackendUtils is...FIXME","title":"SchedulerBackendUtils"},{"location":"scheduler/SchedulingMode/","text":"== [[SchedulingMode]] Scheduling Mode -- spark.scheduler.mode Spark Property Scheduling Mode (aka order task policy or scheduling policy or scheduling order ) defines a policy to sort tasks in order for execution. The scheduling mode schedulingMode attribute is part of the scheduler:TaskScheduler.md#schedulingMode[TaskScheduler Contract]. The only implementation of the TaskScheduler contract in Spark -- scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] -- uses configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] setting to configure schedulingMode that is merely used to set up the scheduler:TaskScheduler.md#rootPool[rootPool] attribute (with FIFO being the default). It happens when scheduler:TaskSchedulerImpl.md#initialize[ TaskSchedulerImpl is initialized]. There are three acceptable scheduling modes: [[FIFO]] FIFO with no pools but a single top-level unnamed pool with elements being scheduler:TaskSetManager.md[TaskSetManager] objects; lower priority gets scheduler:spark-scheduler-Schedulable.md[Schedulable] sooner or earlier stage wins. [[FAIR]] FAIR with a scheduler:spark-scheduler-FairSchedulableBuilder.md#buildPools[hierarchy of Schedulable (sub)pools] with the scheduler:TaskScheduler.md#rootPool[rootPool] at the top. [[NONE]] NONE (not used) NOTE: Out of three possible SchedulingMode policies only FIFO and FAIR modes are supported by scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl]. [NOTE] \u00b6 After the root pool is initialized, the scheduling mode is no longer relevant (since the spark-scheduler-Schedulable.md[Schedulable] that represents the root pool is fully set up). The root pool is later used when scheduler:TaskSchedulerImpl.md#submitTasks[ TaskSchedulerImpl submits tasks (as TaskSets ) for execution]. \u00b6 NOTE: The scheduler:TaskScheduler.md#rootPool[root pool] is a Schedulable . Refer to spark-scheduler-Schedulable.md[Schedulable]. === [[fair-scheduling-sparkui]] Monitoring FAIR Scheduling Mode using Spark UI CAUTION: FIXME Describe me...","title":"SchedulingMode"},{"location":"scheduler/SchedulingMode/#note","text":"After the root pool is initialized, the scheduling mode is no longer relevant (since the spark-scheduler-Schedulable.md[Schedulable] that represents the root pool is fully set up).","title":"[NOTE]"},{"location":"scheduler/SchedulingMode/#the-root-pool-is-later-used-when-schedulertaskschedulerimplmdsubmittaskstaskschedulerimpl-submits-tasks-as-tasksets-for-execution","text":"NOTE: The scheduler:TaskScheduler.md#rootPool[root pool] is a Schedulable . Refer to spark-scheduler-Schedulable.md[Schedulable]. === [[fair-scheduling-sparkui]] Monitoring FAIR Scheduling Mode using Spark UI CAUTION: FIXME Describe me...","title":"The root pool is later used when scheduler:TaskSchedulerImpl.md#submitTasks[TaskSchedulerImpl submits tasks (as TaskSets) for execution]."},{"location":"scheduler/ShuffleMapStage/","text":"ShuffleMapStage \u00b6 ShuffleMapStage ( shuffle map stage or simply map stage ) is one of the two types of stages in a physical execution DAG (beside a ResultStage ). NOTE: The logical DAG or logical execution plan is the RDD lineage . ShuffleMapStage corresponds to (and is associated with) a < >. ShuffleMapStage is created when DAGScheduler is requested to DAGScheduler.md#createShuffleMapStage[plan a ShuffleDependency for execution]. ShuffleMapStage can also be DAGScheduler.md#submitMapStage[submitted independently as a Spark job] for DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. ShuffleMapStage is an input for the other following stages in the DAG of stages and is also called a shuffle dependency's map side . Creating Instance \u00b6 ShuffleMapStage takes the following to be created: [[id]] Stage ID [[rdd]] RDD of the < > [[numTasks]] Number of tasks [[parents]] Parent Stage.md[stages] [[firstJobId]] ID of the ActiveJob that created it [[callSite]] CallSite [[shuffleDep]] ShuffleDependency [[mapOutputTrackerMaster]] MapOutputTrackerMaster == [[_mapStageJobs]][[mapStageJobs]][[addActiveJob]][[removeActiveJob]] Jobs Registry ShuffleMapStage keeps track of jobs that were submitted to execute it independently (if any). The registry is used when DAGScheduler is requested to DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished] (FIXME: when DAGSchedulerEventProcessLoop.md#handleTaskCompletion[ DAGScheduler is notified that a ShuffleMapTask has finished successfully] and the task made ShuffleMapStage completed and so marks any map-stage jobs waiting on this stage as finished). A new job is registered ( added ) when DAGScheduler is DAGScheduler.md#handleMapStageSubmitted[notified that a ShuffleDependency was submitted for execution (as a MapStageSubmitted event)]. An active job is deregistered ( removed ) when DAGScheduler is requested to DAGScheduler.md#cleanupStateForJobAndIndependentStages[clean up after a job and independent stages]. == [[isAvailable]][[numAvailableOutputs]] ShuffleMapStage is Available (Fully Computed) When executed, a ShuffleMapStage saves map output files (for reduce tasks). When all < > have shuffle map outputs available, ShuffleMapStage is considered available ( done or ready ). ShuffleMapStage is asked about its availability when DAGScheduler is requested for DAGScheduler.md#getMissingParentStages[missing parent map stages for a stage], DAGScheduler.md#handleMapStageSubmitted[handleMapStageSubmitted], DAGScheduler.md#submitMissingTasks[submitMissingTasks], DAGScheduler.md#handleTaskCompletion[handleTaskCompletion], DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished], DAGScheduler.md#stageDependsOn[stageDependsOn]. ShuffleMapStage uses the < > for the MapOutputTrackerMaster.md#getNumAvailableOutputs[number of partitions with shuffle map outputs available] (of the < > by the shuffle ID). == [[findMissingPartitions]] Finding Missing Partitions [source, scala] \u00b6 findMissingPartitions(): Seq[Int] \u00b6 findMissingPartitions requests the < > for the MapOutputTrackerMaster.md#findMissingPartitions[missing partitions] (of the < > by the shuffle ID) and returns them. If MapOutputTrackerMaster does not track the ShuffleDependency yet, findMissingPartitions simply returns all the Stage.md#numPartitions[partitions] as missing. findMissingPartitions is part of the Stage.md#findMissingPartitions[Stage] abstraction. == [[stage-sharing]] ShuffleMapStage Sharing A ShuffleMapStage can be shared across multiple jobs, if these jobs reuse the same RDDs. .Skipped Stages are already-computed ShuffleMapStages image::dagscheduler-webui-skipped-stages.png[align=\"center\"] [source, scala] \u00b6 val rdd = sc.parallelize(0 to 5).map((_,1)).sortByKey() // <1> rdd.count // <2> rdd.count // <3> <1> Shuffle at sortByKey() <2> Submits a job with two stages with two being executed <3> Intentionally repeat the last action that submits a new job with two stages with one being shared as already-being-computed","title":"ShuffleMapStage"},{"location":"scheduler/ShuffleMapStage/#shufflemapstage","text":"ShuffleMapStage ( shuffle map stage or simply map stage ) is one of the two types of stages in a physical execution DAG (beside a ResultStage ). NOTE: The logical DAG or logical execution plan is the RDD lineage . ShuffleMapStage corresponds to (and is associated with) a < >. ShuffleMapStage is created when DAGScheduler is requested to DAGScheduler.md#createShuffleMapStage[plan a ShuffleDependency for execution]. ShuffleMapStage can also be DAGScheduler.md#submitMapStage[submitted independently as a Spark job] for DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. ShuffleMapStage is an input for the other following stages in the DAG of stages and is also called a shuffle dependency's map side .","title":"ShuffleMapStage"},{"location":"scheduler/ShuffleMapStage/#creating-instance","text":"ShuffleMapStage takes the following to be created: [[id]] Stage ID [[rdd]] RDD of the < > [[numTasks]] Number of tasks [[parents]] Parent Stage.md[stages] [[firstJobId]] ID of the ActiveJob that created it [[callSite]] CallSite [[shuffleDep]] ShuffleDependency [[mapOutputTrackerMaster]] MapOutputTrackerMaster == [[_mapStageJobs]][[mapStageJobs]][[addActiveJob]][[removeActiveJob]] Jobs Registry ShuffleMapStage keeps track of jobs that were submitted to execute it independently (if any). The registry is used when DAGScheduler is requested to DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished] (FIXME: when DAGSchedulerEventProcessLoop.md#handleTaskCompletion[ DAGScheduler is notified that a ShuffleMapTask has finished successfully] and the task made ShuffleMapStage completed and so marks any map-stage jobs waiting on this stage as finished). A new job is registered ( added ) when DAGScheduler is DAGScheduler.md#handleMapStageSubmitted[notified that a ShuffleDependency was submitted for execution (as a MapStageSubmitted event)]. An active job is deregistered ( removed ) when DAGScheduler is requested to DAGScheduler.md#cleanupStateForJobAndIndependentStages[clean up after a job and independent stages]. == [[isAvailable]][[numAvailableOutputs]] ShuffleMapStage is Available (Fully Computed) When executed, a ShuffleMapStage saves map output files (for reduce tasks). When all < > have shuffle map outputs available, ShuffleMapStage is considered available ( done or ready ). ShuffleMapStage is asked about its availability when DAGScheduler is requested for DAGScheduler.md#getMissingParentStages[missing parent map stages for a stage], DAGScheduler.md#handleMapStageSubmitted[handleMapStageSubmitted], DAGScheduler.md#submitMissingTasks[submitMissingTasks], DAGScheduler.md#handleTaskCompletion[handleTaskCompletion], DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished], DAGScheduler.md#stageDependsOn[stageDependsOn]. ShuffleMapStage uses the < > for the MapOutputTrackerMaster.md#getNumAvailableOutputs[number of partitions with shuffle map outputs available] (of the < > by the shuffle ID). == [[findMissingPartitions]] Finding Missing Partitions","title":"Creating Instance"},{"location":"scheduler/ShuffleMapStage/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/ShuffleMapStage/#findmissingpartitions-seqint","text":"findMissingPartitions requests the < > for the MapOutputTrackerMaster.md#findMissingPartitions[missing partitions] (of the < > by the shuffle ID) and returns them. If MapOutputTrackerMaster does not track the ShuffleDependency yet, findMissingPartitions simply returns all the Stage.md#numPartitions[partitions] as missing. findMissingPartitions is part of the Stage.md#findMissingPartitions[Stage] abstraction. == [[stage-sharing]] ShuffleMapStage Sharing A ShuffleMapStage can be shared across multiple jobs, if these jobs reuse the same RDDs. .Skipped Stages are already-computed ShuffleMapStages image::dagscheduler-webui-skipped-stages.png[align=\"center\"]","title":"findMissingPartitions(): Seq[Int]"},{"location":"scheduler/ShuffleMapStage/#source-scala_1","text":"val rdd = sc.parallelize(0 to 5).map((_,1)).sortByKey() // <1> rdd.count // <2> rdd.count // <3> <1> Shuffle at sortByKey() <2> Submits a job with two stages with two being executed <3> Intentionally repeat the last action that submits a new job with two stages with one being shared as already-being-computed","title":"[source, scala]"},{"location":"scheduler/ShuffleMapTask/","text":"ShuffleMapTask \u00b6 ShuffleMapTask is a Task to produce a MapStatus ( Task[MapStatus] ). ShuffleMapTask is one of the two types of Task s. When executed , ShuffleMapTask writes the result of executing a serialized task code over the records (of a RDD partition ) to the shuffle system and returns a MapStatus (with the BlockManager and estimated size of the result shuffle blocks). Creating Instance \u00b6 ShuffleMapTask takes the following to be created: Stage ID Stage Attempt ID Broadcast variable with a serialized task binary Partition TaskLocation s Local Properties Serialized task metrics Job ID (default: None ) Application ID (default: None ) Application Attempt ID (default: None ) isBarrier Flag (default: false ) ShuffleMapTask is created when DAGScheduler is requested to submit tasks for all missing partitions of a ShuffleMapStage . Serialized Task Binary \u00b6 taskBinary : Broadcast [ Array [ Byte ]] ShuffleMapTask is given a broadcast variable with a reference to a serialized task binary. runTask expects that the serialized task binary is a tuple of an RDD and a ShuffleDependency . Running Task \u00b6 runTask ( context : TaskContext ): MapStatus runTask writes the result ( records ) of executing the serialized task code over the records (in the RDD partition ) to the shuffle system and returns a MapStatus (with the BlockManager and an estimated size of the result shuffle blocks). Internally, runTask requests the SparkEnv for the new instance of closure serializer and requests it to deserialize the serialized task code (into a tuple of a RDD and a ShuffleDependency ). runTask measures the thread and CPU deserialization times. runTask requests the SparkEnv for the ShuffleManager and requests it for a ShuffleWriter (for the ShuffleHandle and the partition ). runTask then requests the RDD for the records (of the partition ) that the ShuffleWriter is requested to write out (to the shuffle system). In the end, runTask requests the ShuffleWriter to stop (with the success flag on) and returns the shuffle map output status . Note This is the moment in Task 's lifecycle (and its corresponding RDD) when a RDD partition is computed and in turn becomes a sequence of records (i.e. real data) on an executor. In case of any exceptions, runTask requests the ShuffleWriter to stop (with the success flag off) and (re)throws the exception. runTask may also print out the following DEBUG message to the logs when the ShuffleWriter could not be stopped . Could not stop writer runTask is part of the Task abstraction. Preferred Locations \u00b6 preferredLocations : Seq [ TaskLocation ] preferredLocations is part of the Task abstraction. preferredLocations returns preferredLocs internal property. ShuffleMapTask tracks TaskLocation s as unique entries in the given locs (with the only rule that when locs is not defined, it is empty, and no task location preferences are defined). ShuffleMapTask initializes the preferredLocs internal property when created Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.ShuffleMapTask logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.ShuffleMapTask=ALL Refer to Logging .","title":"ShuffleMapTask"},{"location":"scheduler/ShuffleMapTask/#shufflemaptask","text":"ShuffleMapTask is a Task to produce a MapStatus ( Task[MapStatus] ). ShuffleMapTask is one of the two types of Task s. When executed , ShuffleMapTask writes the result of executing a serialized task code over the records (of a RDD partition ) to the shuffle system and returns a MapStatus (with the BlockManager and estimated size of the result shuffle blocks).","title":"ShuffleMapTask"},{"location":"scheduler/ShuffleMapTask/#creating-instance","text":"ShuffleMapTask takes the following to be created: Stage ID Stage Attempt ID Broadcast variable with a serialized task binary Partition TaskLocation s Local Properties Serialized task metrics Job ID (default: None ) Application ID (default: None ) Application Attempt ID (default: None ) isBarrier Flag (default: false ) ShuffleMapTask is created when DAGScheduler is requested to submit tasks for all missing partitions of a ShuffleMapStage .","title":"Creating Instance"},{"location":"scheduler/ShuffleMapTask/#serialized-task-binary","text":"taskBinary : Broadcast [ Array [ Byte ]] ShuffleMapTask is given a broadcast variable with a reference to a serialized task binary. runTask expects that the serialized task binary is a tuple of an RDD and a ShuffleDependency .","title":" Serialized Task Binary"},{"location":"scheduler/ShuffleMapTask/#running-task","text":"runTask ( context : TaskContext ): MapStatus runTask writes the result ( records ) of executing the serialized task code over the records (in the RDD partition ) to the shuffle system and returns a MapStatus (with the BlockManager and an estimated size of the result shuffle blocks). Internally, runTask requests the SparkEnv for the new instance of closure serializer and requests it to deserialize the serialized task code (into a tuple of a RDD and a ShuffleDependency ). runTask measures the thread and CPU deserialization times. runTask requests the SparkEnv for the ShuffleManager and requests it for a ShuffleWriter (for the ShuffleHandle and the partition ). runTask then requests the RDD for the records (of the partition ) that the ShuffleWriter is requested to write out (to the shuffle system). In the end, runTask requests the ShuffleWriter to stop (with the success flag on) and returns the shuffle map output status . Note This is the moment in Task 's lifecycle (and its corresponding RDD) when a RDD partition is computed and in turn becomes a sequence of records (i.e. real data) on an executor. In case of any exceptions, runTask requests the ShuffleWriter to stop (with the success flag off) and (re)throws the exception. runTask may also print out the following DEBUG message to the logs when the ShuffleWriter could not be stopped . Could not stop writer runTask is part of the Task abstraction.","title":" Running Task"},{"location":"scheduler/ShuffleMapTask/#preferred-locations","text":"preferredLocations : Seq [ TaskLocation ] preferredLocations is part of the Task abstraction. preferredLocations returns preferredLocs internal property. ShuffleMapTask tracks TaskLocation s as unique entries in the given locs (with the only rule that when locs is not defined, it is empty, and no task location preferences are defined). ShuffleMapTask initializes the preferredLocs internal property when created","title":"  Preferred Locations"},{"location":"scheduler/ShuffleMapTask/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.ShuffleMapTask logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.ShuffleMapTask=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/ShuffleStatus/","text":"ShuffleStatus \u00b6 ShuffleStatus is used by MapOutputTrackerMaster to keep track of the shuffle map outputs of a ShuffleMapStage . Creating Instance \u00b6 ShuffleStatus takes the following to be created: Number of Partitions (of the RDD of a ShuffleDependency ) ShuffleStatus is created when: MapOutputTrackerMaster is requested to register a shuffle (when DAGScheduler is requested to createShuffleMapStage ) Registering Shuffle Map Output \u00b6 addMapOutput ( mapIndex : Int , status : MapStatus ): Unit addMapOutput ...FIXME addMapOutput is used when: MapOutputTrackerMaster is requested to registerMapOutput Deregistering Shuffle Map Output \u00b6 removeMapOutput ( mapIndex : Int , bmAddress : BlockManagerId ): Unit removeMapOutput ...FIXME removeMapOutput is used when: MapOutputTrackerMaster is requested to unregisterMapOutput Missing Partitions \u00b6 findMissingPartitions (): Seq [ Int ] findMissingPartitions ...FIXME findMissingPartitions is used when: MapOutputTrackerMaster is requested to findMissingPartitions Serializing Shuffle Map Output Statuses \u00b6 serializedMapStatus ( broadcastManager : BroadcastManager , isLocal : Boolean , minBroadcastSize : Int , conf : SparkConf ): Array [ Byte ] serializedMapStatus ...FIXME serializedMapStatus is used when: MessageLoop (of the MapOutputTrackerMaster ) is requested to send map output locations for shuffle Logging \u00b6 Enable ALL logging level for org.apache.spark.ShuffleStatus logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ShuffleStatus=ALL Refer to Logging .","title":"ShuffleStatus"},{"location":"scheduler/ShuffleStatus/#shufflestatus","text":"ShuffleStatus is used by MapOutputTrackerMaster to keep track of the shuffle map outputs of a ShuffleMapStage .","title":"ShuffleStatus"},{"location":"scheduler/ShuffleStatus/#creating-instance","text":"ShuffleStatus takes the following to be created: Number of Partitions (of the RDD of a ShuffleDependency ) ShuffleStatus is created when: MapOutputTrackerMaster is requested to register a shuffle (when DAGScheduler is requested to createShuffleMapStage )","title":"Creating Instance"},{"location":"scheduler/ShuffleStatus/#registering-shuffle-map-output","text":"addMapOutput ( mapIndex : Int , status : MapStatus ): Unit addMapOutput ...FIXME addMapOutput is used when: MapOutputTrackerMaster is requested to registerMapOutput","title":" Registering Shuffle Map Output"},{"location":"scheduler/ShuffleStatus/#deregistering-shuffle-map-output","text":"removeMapOutput ( mapIndex : Int , bmAddress : BlockManagerId ): Unit removeMapOutput ...FIXME removeMapOutput is used when: MapOutputTrackerMaster is requested to unregisterMapOutput","title":" Deregistering Shuffle Map Output"},{"location":"scheduler/ShuffleStatus/#missing-partitions","text":"findMissingPartitions (): Seq [ Int ] findMissingPartitions ...FIXME findMissingPartitions is used when: MapOutputTrackerMaster is requested to findMissingPartitions","title":" Missing Partitions"},{"location":"scheduler/ShuffleStatus/#serializing-shuffle-map-output-statuses","text":"serializedMapStatus ( broadcastManager : BroadcastManager , isLocal : Boolean , minBroadcastSize : Int , conf : SparkConf ): Array [ Byte ] serializedMapStatus ...FIXME serializedMapStatus is used when: MessageLoop (of the MapOutputTrackerMaster ) is requested to send map output locations for shuffle","title":" Serializing Shuffle Map Output Statuses"},{"location":"scheduler/ShuffleStatus/#logging","text":"Enable ALL logging level for org.apache.spark.ShuffleStatus logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ShuffleStatus=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/Stage/","text":"Stage \u00b6 Stage is a unit of execution ( step ) in a physical execution plan. A stage is a set of parallel tasks -- one task per partition (of an RDD that computes partial results of a function executed as part of a Spark job). In other words, a Spark job is a computation with that computation sliced into stages. A stage is uniquely identified by id . When a stage is created, DAGScheduler.md[DAGScheduler] increments internal counter nextStageId to track the number of DAGScheduler.md#submitStage[stage submissions]. [[rdd]] A stage can only work on the partitions of a single RDD (identified by rdd ), but can be associated with many other dependent parent stages (via internal field parents ), with the boundary of a stage marked by shuffle dependencies. Submitting a stage can therefore trigger execution of a series of dependent parent stages (refer to DAGScheduler.md#runJob[RDDs, Job Execution, Stages, and Partitions]). Finally, every stage has a firstJobId that is the id of the job that submitted the stage. There are two types of stages: ShuffleMapStage.md[ShuffleMapStage] is an intermediate stage (in the execution DAG) that produces data for other stage(s). It writes map output files for a shuffle. It can also be the final stage in a job in DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. ResultStage.md[ResultStage] is the final stage that executes rdd:index.md#actions[a Spark action] in a user program by running a function on an RDD. When a job is submitted, a new stage is created with the parent ShuffleMapStage.md[ShuffleMapStage] linked -- they can be created from scratch or linked to, i.e. shared, if other jobs use them already. A stage tracks the jobs (their ids) it belongs to (using the internal jobIds registry). DAGScheduler splits up a job into a collection of stages. Each stage contains a sequence of rdd:index.md[narrow transformations] that can be completed without rdd:spark-rdd-shuffle.md[shuffling] the entire data set, separated at shuffle boundaries , i.e. where shuffle occurs. Stages are thus a result of breaking the RDD graph at shuffle boundaries. Shuffle boundaries introduce a barrier where stages/tasks must wait for the previous stage to finish before they fetch map outputs. RDD operations with rdd:index.md[narrow dependencies], like map() and filter() , are pipelined together into one set of tasks in each stage, but operations with shuffle dependencies require multiple stages, i.e. one to write a set of map output files, and another to read those files after a barrier. In the end, every stage will have only shuffle dependencies on other stages, and may compute multiple operations inside it. The actual pipelining of these operations happens in the RDD.compute() functions of various RDDs, e.g. MappedRDD , FilteredRDD , etc. At some point of time in a stage's life, every partition of the stage gets transformed into a task - ShuffleMapTask.md[ShuffleMapTask] or ResultTask.md[ResultTask] for ShuffleMapStage.md[ShuffleMapStage] and ResultStage.md[ResultStage], respectively. Partitions are computed in jobs, and result stages may not always need to compute all partitions in their target RDD, e.g. for actions like first() and lookup() . DAGScheduler prints the following INFO message when there are tasks to submit: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[86] at reduceByKey at <console>:24) There is also the following DEBUG message with pending partitions: New pending partitions: Set(0) Tasks are later submitted to TaskScheduler.md[Task Scheduler] (via taskScheduler.submitTasks ). When no tasks in a stage can be submitted, the following DEBUG message shows in the logs: FIXME Latest StageInfo Registry \u00b6 _latestInfo : StageInfo Stage uses _latestInfo internal registry for...FIXME Making New Stage Attempt \u00b6 makeNewStageAttempt ( numPartitionsToCompute : Int , taskLocalityPreferences : Seq [ Seq [ TaskLocation ]] = Seq . empty ): Unit makeNewStageAttempt creates a new TaskMetrics and requests it to register itself with the SparkContext of the RDD . makeNewStageAttempt creates a StageInfo from this Stage (and the nextAttemptId ). This StageInfo is saved in the _latestInfo internal registry. In the end, makeNewStageAttempt increments the nextAttemptId internal counter. Note makeNewStageAttempt returns Unit (nothing) and its purpose is to update the latest StageInfo internal registry. makeNewStageAttempt is used when: DAGScheduler is requested to submit the missing tasks of a stage Others to be Reviewed \u00b6 == [[findMissingPartitions]] Finding Missing Partitions [source, scala] \u00b6 findMissingPartitions(): Seq[Int] \u00b6 findMissingPartitions gives the partition ids that are missing and need to be computed. findMissingPartitions is used when DAGScheduler is requested to DAGScheduler.md#submitMissingTasks[submitMissingTasks] and DAGScheduler.md#handleTaskCompletion[handleTaskCompletion]. == [[failedOnFetchAndShouldAbort]] failedOnFetchAndShouldAbort Method Stage.failedOnFetchAndShouldAbort(stageAttemptId: Int): Boolean checks whether the number of fetch failed attempts (using fetchFailedAttemptIds ) exceeds the number of consecutive failures allowed for a given stage (that should then be aborted) NOTE: The number of consecutive failures for a stage is not configurable. == [[latestInfo]] Getting StageInfo For Most Recent Attempt [source, scala] \u00b6 latestInfo: StageInfo \u00b6 latestInfo simply returns the <<_latestInfo, most recent StageInfo >> (i.e. makes it accessible). == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[details]] details | Long description of the stage Used when...FIXME | [[fetchFailedAttemptIds]] fetchFailedAttemptIds | FIXME Used when...FIXME | [[jobIds]] jobIds | Set of spark-scheduler-ActiveJob.md[jobs] the stage belongs to. Used when...FIXME | [[name]] name | Name of the stage Used when...FIXME | [[nextAttemptId]] nextAttemptId | The ID for the next attempt of the stage. Used when...FIXME | [[numPartitions]] numPartitions | Number of partitions Used when...FIXME | [[pendingPartitions]] pendingPartitions | Set of pending spark-rdd-partitions.md[partitions] Used when...FIXME |===","title":"Stage"},{"location":"scheduler/Stage/#stage","text":"Stage is a unit of execution ( step ) in a physical execution plan. A stage is a set of parallel tasks -- one task per partition (of an RDD that computes partial results of a function executed as part of a Spark job). In other words, a Spark job is a computation with that computation sliced into stages. A stage is uniquely identified by id . When a stage is created, DAGScheduler.md[DAGScheduler] increments internal counter nextStageId to track the number of DAGScheduler.md#submitStage[stage submissions]. [[rdd]] A stage can only work on the partitions of a single RDD (identified by rdd ), but can be associated with many other dependent parent stages (via internal field parents ), with the boundary of a stage marked by shuffle dependencies. Submitting a stage can therefore trigger execution of a series of dependent parent stages (refer to DAGScheduler.md#runJob[RDDs, Job Execution, Stages, and Partitions]). Finally, every stage has a firstJobId that is the id of the job that submitted the stage. There are two types of stages: ShuffleMapStage.md[ShuffleMapStage] is an intermediate stage (in the execution DAG) that produces data for other stage(s). It writes map output files for a shuffle. It can also be the final stage in a job in DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. ResultStage.md[ResultStage] is the final stage that executes rdd:index.md#actions[a Spark action] in a user program by running a function on an RDD. When a job is submitted, a new stage is created with the parent ShuffleMapStage.md[ShuffleMapStage] linked -- they can be created from scratch or linked to, i.e. shared, if other jobs use them already. A stage tracks the jobs (their ids) it belongs to (using the internal jobIds registry). DAGScheduler splits up a job into a collection of stages. Each stage contains a sequence of rdd:index.md[narrow transformations] that can be completed without rdd:spark-rdd-shuffle.md[shuffling] the entire data set, separated at shuffle boundaries , i.e. where shuffle occurs. Stages are thus a result of breaking the RDD graph at shuffle boundaries. Shuffle boundaries introduce a barrier where stages/tasks must wait for the previous stage to finish before they fetch map outputs. RDD operations with rdd:index.md[narrow dependencies], like map() and filter() , are pipelined together into one set of tasks in each stage, but operations with shuffle dependencies require multiple stages, i.e. one to write a set of map output files, and another to read those files after a barrier. In the end, every stage will have only shuffle dependencies on other stages, and may compute multiple operations inside it. The actual pipelining of these operations happens in the RDD.compute() functions of various RDDs, e.g. MappedRDD , FilteredRDD , etc. At some point of time in a stage's life, every partition of the stage gets transformed into a task - ShuffleMapTask.md[ShuffleMapTask] or ResultTask.md[ResultTask] for ShuffleMapStage.md[ShuffleMapStage] and ResultStage.md[ResultStage], respectively. Partitions are computed in jobs, and result stages may not always need to compute all partitions in their target RDD, e.g. for actions like first() and lookup() . DAGScheduler prints the following INFO message when there are tasks to submit: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[86] at reduceByKey at <console>:24) There is also the following DEBUG message with pending partitions: New pending partitions: Set(0) Tasks are later submitted to TaskScheduler.md[Task Scheduler] (via taskScheduler.submitTasks ). When no tasks in a stage can be submitted, the following DEBUG message shows in the logs: FIXME","title":"Stage"},{"location":"scheduler/Stage/#latest-stageinfo-registry","text":"_latestInfo : StageInfo Stage uses _latestInfo internal registry for...FIXME","title":" Latest StageInfo Registry"},{"location":"scheduler/Stage/#making-new-stage-attempt","text":"makeNewStageAttempt ( numPartitionsToCompute : Int , taskLocalityPreferences : Seq [ Seq [ TaskLocation ]] = Seq . empty ): Unit makeNewStageAttempt creates a new TaskMetrics and requests it to register itself with the SparkContext of the RDD . makeNewStageAttempt creates a StageInfo from this Stage (and the nextAttemptId ). This StageInfo is saved in the _latestInfo internal registry. In the end, makeNewStageAttempt increments the nextAttemptId internal counter. Note makeNewStageAttempt returns Unit (nothing) and its purpose is to update the latest StageInfo internal registry. makeNewStageAttempt is used when: DAGScheduler is requested to submit the missing tasks of a stage","title":" Making New Stage Attempt"},{"location":"scheduler/Stage/#others-to-be-reviewed","text":"== [[findMissingPartitions]] Finding Missing Partitions","title":"Others to be Reviewed"},{"location":"scheduler/Stage/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/Stage/#findmissingpartitions-seqint","text":"findMissingPartitions gives the partition ids that are missing and need to be computed. findMissingPartitions is used when DAGScheduler is requested to DAGScheduler.md#submitMissingTasks[submitMissingTasks] and DAGScheduler.md#handleTaskCompletion[handleTaskCompletion]. == [[failedOnFetchAndShouldAbort]] failedOnFetchAndShouldAbort Method Stage.failedOnFetchAndShouldAbort(stageAttemptId: Int): Boolean checks whether the number of fetch failed attempts (using fetchFailedAttemptIds ) exceeds the number of consecutive failures allowed for a given stage (that should then be aborted) NOTE: The number of consecutive failures for a stage is not configurable. == [[latestInfo]] Getting StageInfo For Most Recent Attempt","title":"findMissingPartitions(): Seq[Int]"},{"location":"scheduler/Stage/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/Stage/#latestinfo-stageinfo","text":"latestInfo simply returns the <<_latestInfo, most recent StageInfo >> (i.e. makes it accessible). == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[details]] details | Long description of the stage Used when...FIXME | [[fetchFailedAttemptIds]] fetchFailedAttemptIds | FIXME Used when...FIXME | [[jobIds]] jobIds | Set of spark-scheduler-ActiveJob.md[jobs] the stage belongs to. Used when...FIXME | [[name]] name | Name of the stage Used when...FIXME | [[nextAttemptId]] nextAttemptId | The ID for the next attempt of the stage. Used when...FIXME | [[numPartitions]] numPartitions | Number of partitions Used when...FIXME | [[pendingPartitions]] pendingPartitions | Set of pending spark-rdd-partitions.md[partitions] Used when...FIXME |===","title":"latestInfo: StageInfo"},{"location":"scheduler/StageInfo/","text":"StageInfo \u00b6 StageInfo is a metadata about a stage to pass from the scheduler to SparkListeners. Creating Instance \u00b6 StageInfo takes the following to be created: Stage ID Stage Attempt ID Name Number of Tasks RDDInfo s Parent IDs Details TaskMetrics (default: null ) Task Locality Preferences (default: empty) Optional Shuffle Dependency ID (default: undefined) StageInfo is created when: StageInfo utility is used to fromStage JsonProtocol (History Server) is used to stageInfoFromJson fromStage Utility \u00b6 fromStage ( stage : Stage , attemptId : Int , numTasks : Option [ Int ] = None , taskMetrics : TaskMetrics = null , taskLocalityPreferences : Seq [ Seq [ TaskLocation ]] = Seq . empty ): StageInfo fromStage ...FIXME fromStage is used when: Stage is created and make a new Stage attempt","title":"StageInfo"},{"location":"scheduler/StageInfo/#stageinfo","text":"StageInfo is a metadata about a stage to pass from the scheduler to SparkListeners.","title":"StageInfo"},{"location":"scheduler/StageInfo/#creating-instance","text":"StageInfo takes the following to be created: Stage ID Stage Attempt ID Name Number of Tasks RDDInfo s Parent IDs Details TaskMetrics (default: null ) Task Locality Preferences (default: empty) Optional Shuffle Dependency ID (default: undefined) StageInfo is created when: StageInfo utility is used to fromStage JsonProtocol (History Server) is used to stageInfoFromJson","title":"Creating Instance"},{"location":"scheduler/StageInfo/#fromstage-utility","text":"fromStage ( stage : Stage , attemptId : Int , numTasks : Option [ Int ] = None , taskMetrics : TaskMetrics = null , taskLocalityPreferences : Seq [ Seq [ TaskLocation ]] = Seq . empty ): StageInfo fromStage ...FIXME fromStage is used when: Stage is created and make a new Stage attempt","title":" fromStage Utility"},{"location":"scheduler/Task/","text":"Task \u00b6 Task is an abstraction of mallest individual units of execution that can be executed to compute an RDD partition. Task is created when DAGScheduler is requested to submit missing tasks of a stage . Contract \u00b6 Running Task \u00b6 runTask ( context : TaskContext ): T Runs the task (in a TaskContext ) Used when Task is requested to run Implementations \u00b6 ResultTask ShuffleMapTask Creating Instance \u00b6 Task takes the following to be created: Stage ID Stage (execution) Attempt ID Partition ID to compute Local Properties Serialized TaskMetrics ( Array[Byte] ) ActiveJob ID (default: None ) Application ID (default: None ) Application Attempt ID (default: None ) isBarrier flag (default: false ) Abstract Class Task is an abstract class and cannot be created directly. It is created indirectly for the concrete Tasks . Serializable \u00b6 Task is a Java Serializable so it can be serialized and send over the wire from the driver to executors. Preferred Locations \u00b6 preferredLocations : Seq [ TaskLocation ] TaskLocations that represent preferred locations (executors) to execute the task on. Empty by default and so no task location preferences are defined that says the task could be launched on any executor. Note Defined by the concrete tasks (i.e. ShuffleMapTask and ResultTask ). preferredLocations is used when TaskSetManager is requested to register a task as pending execution and dequeueSpeculativeTask . Running Task Thread \u00b6 run ( taskAttemptId : Long , attemptNumber : Int , metricsSystem : MetricsSystem ): T run registers the task (identified as taskAttemptId ) with the local BlockManager . Note run uses SparkEnv to access the current BlockManager . run creates a TaskContextImpl that in turn becomes the task's TaskContext . Note run is a final method and so must not be overriden. run checks _killed flag and, if enabled, kills the task (with interruptThread flag disabled). run creates a Hadoop CallerContext and sets it. run runs the task . Note This is the moment when the custom Task 's runTask is executed. In the end, run notifies TaskContextImpl that the task has completed (regardless of the final outcome -- a success or a failure). In case of any exceptions, run notifies TaskContextImpl that the task has failed . run requests MemoryStore to release unroll memory for this task (for both ON_HEAP and OFF_HEAP memory modes). Note run uses SparkEnv to access the current BlockManager that it uses to access MemoryStore . run requests MemoryManager to notify any tasks waiting for execution memory to be freed to wake up and try to acquire memory again . run unsets the task's TaskContext . Note run uses SparkEnv to access the current MemoryManager . run is used when TaskRunner is requested to run (when Executor is requested to launch a task (on \"Executor task launch worker\" thread pool sometime in the future) ). Task States \u00b6 Task can be in one of the following states (as described by TaskState enumeration): LAUNCHING RUNNING when the task is being started. FINISHED when the task finished with the serialized result. FAILED when the task fails, e.g. when FetchFailedException , CommitDeniedException or any Throwable occurs KILLED when an executor kills a task. LOST States are the values of org.apache.spark.TaskState . Note Task status updates are sent from executors to the driver through ExecutorBackend . Task is finished when it is in one of FINISHED , FAILED , KILLED , LOST . LOST and FAILED states are considered failures. Collecting Latest Values of Accumulators \u00b6 collectAccumulatorUpdates ( taskFailed : Boolean = false ): Seq [ AccumulableInfo ] collectAccumulatorUpdates collects the latest values of internal and external accumulators from a task (and returns the values as a collection of AccumulableInfo ). Internally, collectAccumulatorUpdates takes TaskMetrics . Note collectAccumulatorUpdates uses TaskContextImpl to access the task's TaskMetrics . collectAccumulatorUpdates collects the latest values of: internal accumulators whose current value is not the zero value and the RESULT_SIZE accumulator (regardless whether the value is its zero or not). external accumulators when taskFailed is disabled ( false ) or which should be included on failures . collectAccumulatorUpdates returns an empty collection when TaskContextImpl is not initialized. collectAccumulatorUpdates is used when TaskRunner runs a task (and sends a task's final results back to the driver). Killing Task \u00b6 kill ( interruptThread : Boolean ): Unit kill marks the task to be killed, i.e. it sets the internal _killed flag to true . kill calls TaskContextImpl.markInterrupted when context is set. If interruptThread is enabled and the internal taskThread is available, kill interrupts it. CAUTION: FIXME When could context and interruptThread not be set?","title":"Task"},{"location":"scheduler/Task/#task","text":"Task is an abstraction of mallest individual units of execution that can be executed to compute an RDD partition. Task is created when DAGScheduler is requested to submit missing tasks of a stage .","title":"Task"},{"location":"scheduler/Task/#contract","text":"","title":"Contract"},{"location":"scheduler/Task/#running-task","text":"runTask ( context : TaskContext ): T Runs the task (in a TaskContext ) Used when Task is requested to run","title":" Running Task"},{"location":"scheduler/Task/#implementations","text":"ResultTask ShuffleMapTask","title":"Implementations"},{"location":"scheduler/Task/#creating-instance","text":"Task takes the following to be created: Stage ID Stage (execution) Attempt ID Partition ID to compute Local Properties Serialized TaskMetrics ( Array[Byte] ) ActiveJob ID (default: None ) Application ID (default: None ) Application Attempt ID (default: None ) isBarrier flag (default: false ) Abstract Class Task is an abstract class and cannot be created directly. It is created indirectly for the concrete Tasks .","title":"Creating Instance"},{"location":"scheduler/Task/#serializable","text":"Task is a Java Serializable so it can be serialized and send over the wire from the driver to executors.","title":"Serializable"},{"location":"scheduler/Task/#preferred-locations","text":"preferredLocations : Seq [ TaskLocation ] TaskLocations that represent preferred locations (executors) to execute the task on. Empty by default and so no task location preferences are defined that says the task could be launched on any executor. Note Defined by the concrete tasks (i.e. ShuffleMapTask and ResultTask ). preferredLocations is used when TaskSetManager is requested to register a task as pending execution and dequeueSpeculativeTask .","title":" Preferred Locations"},{"location":"scheduler/Task/#running-task-thread","text":"run ( taskAttemptId : Long , attemptNumber : Int , metricsSystem : MetricsSystem ): T run registers the task (identified as taskAttemptId ) with the local BlockManager . Note run uses SparkEnv to access the current BlockManager . run creates a TaskContextImpl that in turn becomes the task's TaskContext . Note run is a final method and so must not be overriden. run checks _killed flag and, if enabled, kills the task (with interruptThread flag disabled). run creates a Hadoop CallerContext and sets it. run runs the task . Note This is the moment when the custom Task 's runTask is executed. In the end, run notifies TaskContextImpl that the task has completed (regardless of the final outcome -- a success or a failure). In case of any exceptions, run notifies TaskContextImpl that the task has failed . run requests MemoryStore to release unroll memory for this task (for both ON_HEAP and OFF_HEAP memory modes). Note run uses SparkEnv to access the current BlockManager that it uses to access MemoryStore . run requests MemoryManager to notify any tasks waiting for execution memory to be freed to wake up and try to acquire memory again . run unsets the task's TaskContext . Note run uses SparkEnv to access the current MemoryManager . run is used when TaskRunner is requested to run (when Executor is requested to launch a task (on \"Executor task launch worker\" thread pool sometime in the future) ).","title":" Running Task Thread"},{"location":"scheduler/Task/#task-states","text":"Task can be in one of the following states (as described by TaskState enumeration): LAUNCHING RUNNING when the task is being started. FINISHED when the task finished with the serialized result. FAILED when the task fails, e.g. when FetchFailedException , CommitDeniedException or any Throwable occurs KILLED when an executor kills a task. LOST States are the values of org.apache.spark.TaskState . Note Task status updates are sent from executors to the driver through ExecutorBackend . Task is finished when it is in one of FINISHED , FAILED , KILLED , LOST . LOST and FAILED states are considered failures.","title":" Task States"},{"location":"scheduler/Task/#collecting-latest-values-of-accumulators","text":"collectAccumulatorUpdates ( taskFailed : Boolean = false ): Seq [ AccumulableInfo ] collectAccumulatorUpdates collects the latest values of internal and external accumulators from a task (and returns the values as a collection of AccumulableInfo ). Internally, collectAccumulatorUpdates takes TaskMetrics . Note collectAccumulatorUpdates uses TaskContextImpl to access the task's TaskMetrics . collectAccumulatorUpdates collects the latest values of: internal accumulators whose current value is not the zero value and the RESULT_SIZE accumulator (regardless whether the value is its zero or not). external accumulators when taskFailed is disabled ( false ) or which should be included on failures . collectAccumulatorUpdates returns an empty collection when TaskContextImpl is not initialized. collectAccumulatorUpdates is used when TaskRunner runs a task (and sends a task's final results back to the driver).","title":" Collecting Latest Values of Accumulators"},{"location":"scheduler/Task/#killing-task","text":"kill ( interruptThread : Boolean ): Unit kill marks the task to be killed, i.e. it sets the internal _killed flag to true . kill calls TaskContextImpl.markInterrupted when context is set. If interruptThread is enabled and the internal taskThread is available, kill interrupts it. CAUTION: FIXME When could context and interruptThread not be set?","title":" Killing Task"},{"location":"scheduler/TaskContext/","text":"TaskContext \u00b6 TaskContext is an abstraction of task contexts . Contract \u00b6 addTaskCompletionListener \u00b6 addTaskCompletionListener [ U ]( f : ( TaskContext ) => U ): TaskContext addTaskCompletionListener ( listener : TaskCompletionListener ): TaskContext Registers a TaskCompletionListener val rdd = sc . range ( 0 , 5 , numSlices = 1 ) import org . apache . spark . TaskContext val printTaskInfo = ( tc : TaskContext ) => { val msg = s\"\"\"|------------------- |partitionId: ${ tc . partitionId } |stageId: ${ tc . stageId } |attemptNum: ${ tc . attemptNumber } |taskAttemptId: ${ tc . taskAttemptId } |-------------------\"\"\" . stripMargin println ( msg ) } rdd . foreachPartition { _ => val tc = TaskContext . get tc . addTaskCompletionListener ( printTaskInfo ) } addTaskFailureListener \u00b6 addTaskFailureListener ( f : ( TaskContext , Throwable ) => Unit ): TaskContext addTaskFailureListener ( listener : TaskFailureListener ): TaskContext Registers a TaskFailureListener val rdd = sc . range ( 0 , 2 , numSlices = 2 ) import org . apache . spark . TaskContext val printTaskErrorInfo = ( tc : TaskContext , error : Throwable ) => { val msg = s\"\"\"|------------------- |partitionId: ${ tc . partitionId } |stageId: ${ tc . stageId } |attemptNum: ${ tc . attemptNumber } |taskAttemptId: ${ tc . taskAttemptId } |error: ${ error . toString } |-------------------\"\"\" . stripMargin println ( msg ) } val throwExceptionForOddNumber = ( n : Long ) => { if ( n % 2 == 1 ) { throw new Exception ( s\"No way it will pass for odd number: $ n \" ) } } // FIXME It won't work. rdd . map ( throwExceptionForOddNumber ). foreachPartition { _ => val tc = TaskContext . get tc . addTaskFailureListener ( printTaskErrorInfo ) } // Listener registration matters. rdd . mapPartitions { ( it : Iterator [ Long ]) => val tc = TaskContext . get tc . addTaskFailureListener ( printTaskErrorInfo ) it }. map ( throwExceptionForOddNumber ). count fetchFailed \u00b6 fetchFailed : Option [ FetchFailedException ] Used when: TaskRunner is requested to run getKillReason \u00b6 getKillReason (): Option [ String ] getLocalProperty \u00b6 getLocalProperty ( key : String ): String Looks up a local property by key getMetricsSources \u00b6 getMetricsSources ( sourceName : String ): Seq [ Source ] Looks up Source s by name isCompleted \u00b6 isCompleted (): Boolean isInterrupted \u00b6 isInterrupted (): Boolean killTaskIfInterrupted \u00b6 killTaskIfInterrupted (): Unit Registering Accumulator \u00b6 registerAccumulator ( a : AccumulatorV2 [ _ , _ ]): Unit Registers a AccumulatorV2 Used when: AccumulatorV2 is requested to deserialize itself resources \u00b6 resources (): Map [ String , ResourceInformation ] Resources allocated to the task taskMetrics \u00b6 taskMetrics (): TaskMetrics TaskMetrics others \u00b6 Important There are other methods, but don't seem very interesting. Implementations \u00b6 BarrierTaskContext TaskContextImpl Serializable \u00b6 TaskContext is a Serializable ( Java ). Accessing TaskContext \u00b6 get (): TaskContext get returns the thread-local TaskContext instance. import org . apache . spark . TaskContext val tc = TaskContext . get val rdd = sc . range ( 0 , 3 , numSlices = 3 ) assert ( rdd . partitions . size == 3 ) rdd . foreach { n => import org . apache . spark . TaskContext val tc = TaskContext . get val msg = s\"\"\"|------------------- |partitionId: ${ tc . partitionId } |stageId: ${ tc . stageId } |attemptNum: ${ tc . attemptNumber } |taskAttemptId: ${ tc . taskAttemptId } |-------------------\"\"\" . stripMargin println ( msg ) }","title":"TaskContext"},{"location":"scheduler/TaskContext/#taskcontext","text":"TaskContext is an abstraction of task contexts .","title":"TaskContext"},{"location":"scheduler/TaskContext/#contract","text":"","title":"Contract"},{"location":"scheduler/TaskContext/#addtaskcompletionlistener","text":"addTaskCompletionListener [ U ]( f : ( TaskContext ) => U ): TaskContext addTaskCompletionListener ( listener : TaskCompletionListener ): TaskContext Registers a TaskCompletionListener val rdd = sc . range ( 0 , 5 , numSlices = 1 ) import org . apache . spark . TaskContext val printTaskInfo = ( tc : TaskContext ) => { val msg = s\"\"\"|------------------- |partitionId: ${ tc . partitionId } |stageId: ${ tc . stageId } |attemptNum: ${ tc . attemptNumber } |taskAttemptId: ${ tc . taskAttemptId } |-------------------\"\"\" . stripMargin println ( msg ) } rdd . foreachPartition { _ => val tc = TaskContext . get tc . addTaskCompletionListener ( printTaskInfo ) }","title":" addTaskCompletionListener"},{"location":"scheduler/TaskContext/#addtaskfailurelistener","text":"addTaskFailureListener ( f : ( TaskContext , Throwable ) => Unit ): TaskContext addTaskFailureListener ( listener : TaskFailureListener ): TaskContext Registers a TaskFailureListener val rdd = sc . range ( 0 , 2 , numSlices = 2 ) import org . apache . spark . TaskContext val printTaskErrorInfo = ( tc : TaskContext , error : Throwable ) => { val msg = s\"\"\"|------------------- |partitionId: ${ tc . partitionId } |stageId: ${ tc . stageId } |attemptNum: ${ tc . attemptNumber } |taskAttemptId: ${ tc . taskAttemptId } |error: ${ error . toString } |-------------------\"\"\" . stripMargin println ( msg ) } val throwExceptionForOddNumber = ( n : Long ) => { if ( n % 2 == 1 ) { throw new Exception ( s\"No way it will pass for odd number: $ n \" ) } } // FIXME It won't work. rdd . map ( throwExceptionForOddNumber ). foreachPartition { _ => val tc = TaskContext . get tc . addTaskFailureListener ( printTaskErrorInfo ) } // Listener registration matters. rdd . mapPartitions { ( it : Iterator [ Long ]) => val tc = TaskContext . get tc . addTaskFailureListener ( printTaskErrorInfo ) it }. map ( throwExceptionForOddNumber ). count","title":" addTaskFailureListener"},{"location":"scheduler/TaskContext/#fetchfailed","text":"fetchFailed : Option [ FetchFailedException ] Used when: TaskRunner is requested to run","title":" fetchFailed"},{"location":"scheduler/TaskContext/#getkillreason","text":"getKillReason (): Option [ String ]","title":" getKillReason"},{"location":"scheduler/TaskContext/#getlocalproperty","text":"getLocalProperty ( key : String ): String Looks up a local property by key","title":" getLocalProperty"},{"location":"scheduler/TaskContext/#getmetricssources","text":"getMetricsSources ( sourceName : String ): Seq [ Source ] Looks up Source s by name","title":" getMetricsSources"},{"location":"scheduler/TaskContext/#iscompleted","text":"isCompleted (): Boolean","title":" isCompleted"},{"location":"scheduler/TaskContext/#isinterrupted","text":"isInterrupted (): Boolean","title":" isInterrupted"},{"location":"scheduler/TaskContext/#killtaskifinterrupted","text":"killTaskIfInterrupted (): Unit","title":" killTaskIfInterrupted"},{"location":"scheduler/TaskContext/#registering-accumulator","text":"registerAccumulator ( a : AccumulatorV2 [ _ , _ ]): Unit Registers a AccumulatorV2 Used when: AccumulatorV2 is requested to deserialize itself","title":" Registering Accumulator"},{"location":"scheduler/TaskContext/#resources","text":"resources (): Map [ String , ResourceInformation ] Resources allocated to the task","title":" resources"},{"location":"scheduler/TaskContext/#taskmetrics","text":"taskMetrics (): TaskMetrics TaskMetrics","title":" taskMetrics"},{"location":"scheduler/TaskContext/#others","text":"Important There are other methods, but don't seem very interesting.","title":"others"},{"location":"scheduler/TaskContext/#implementations","text":"BarrierTaskContext TaskContextImpl","title":"Implementations"},{"location":"scheduler/TaskContext/#serializable","text":"TaskContext is a Serializable ( Java ).","title":" Serializable"},{"location":"scheduler/TaskContext/#accessing-taskcontext","text":"get (): TaskContext get returns the thread-local TaskContext instance. import org . apache . spark . TaskContext val tc = TaskContext . get val rdd = sc . range ( 0 , 3 , numSlices = 3 ) assert ( rdd . partitions . size == 3 ) rdd . foreach { n => import org . apache . spark . TaskContext val tc = TaskContext . get val msg = s\"\"\"|------------------- |partitionId: ${ tc . partitionId } |stageId: ${ tc . stageId } |attemptNum: ${ tc . attemptNumber } |taskAttemptId: ${ tc . taskAttemptId } |-------------------\"\"\" . stripMargin println ( msg ) }","title":" Accessing TaskContext"},{"location":"scheduler/TaskContextImpl/","text":"TaskContextImpl \u00b6 TaskContextImpl is a concrete TaskContext . Creating Instance \u00b6 TaskContextImpl takes the following to be created: Stage ID Stage Execution Attempt ID Partition ID Task Execution Attempt ID Attempt Number TaskMemoryManager Local Properties MetricsSystem TaskMetrics Resources ( Map[String, ResourceInformation] ) TaskContextImpl is created when: Task is requested to run BarrierTaskContext \u00b6 TaskContextImpl is available to barrier tasks as a BarrierTaskContext .","title":"TaskContextImpl"},{"location":"scheduler/TaskContextImpl/#taskcontextimpl","text":"TaskContextImpl is a concrete TaskContext .","title":"TaskContextImpl"},{"location":"scheduler/TaskContextImpl/#creating-instance","text":"TaskContextImpl takes the following to be created: Stage ID Stage Execution Attempt ID Partition ID Task Execution Attempt ID Attempt Number TaskMemoryManager Local Properties MetricsSystem TaskMetrics Resources ( Map[String, ResourceInformation] ) TaskContextImpl is created when: Task is requested to run","title":"Creating Instance"},{"location":"scheduler/TaskContextImpl/#barriertaskcontext","text":"TaskContextImpl is available to barrier tasks as a BarrierTaskContext .","title":" BarrierTaskContext"},{"location":"scheduler/TaskDescription/","text":"TaskDescription \u00b6 TaskDescription is a metadata of a Task . Creating Instance \u00b6 TaskDescription takes the following to be created: Task ID Task attempt number Executor ID Task name Task index (within the TaskSet ) Partition ID Added files (as Map[String, Long] ) Added JAR files (as Map[String, Long] ) Properties Resources ( Map[String, ResourceInformation] ) Serialized task (as ByteBuffer ) TaskDescription is created when: TaskSetManager is requested to find a task ready for execution (given a resource offer) Text Representation \u00b6 toString : String toString uses the taskId and index as follows: TaskDescription(TID=[taskId], index=[index]) Decoding TaskDescription (from Serialized Format) \u00b6 decode ( byteBuffer : ByteBuffer ): TaskDescription decode simply decodes (< >) a TaskDescription from the serialized format ( ByteBuffer ). Internally, decode ...FIXME decode is used when: CoarseGrainedExecutorBackend is requested to CoarseGrainedExecutorBackend.md#LaunchTask[handle a LaunchTask message] Spark on Mesos' MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#launchTask[launch a task] Encoding TaskDescription (to Serialized Format) \u00b6 encode ( taskDescription : TaskDescription ): ByteBuffer encode simply encodes the TaskDescription to a serialized format ( ByteBuffer ). Internally, encode ...FIXME encode is used when: DriverEndpoint (of CoarseGrainedSchedulerBackend ) is requested to launchTasks Task Name \u00b6 The name of the task is of the format: task [taskID] in stage [taskSetID]","title":"TaskDescription"},{"location":"scheduler/TaskDescription/#taskdescription","text":"TaskDescription is a metadata of a Task .","title":"TaskDescription"},{"location":"scheduler/TaskDescription/#creating-instance","text":"TaskDescription takes the following to be created: Task ID Task attempt number Executor ID Task name Task index (within the TaskSet ) Partition ID Added files (as Map[String, Long] ) Added JAR files (as Map[String, Long] ) Properties Resources ( Map[String, ResourceInformation] ) Serialized task (as ByteBuffer ) TaskDescription is created when: TaskSetManager is requested to find a task ready for execution (given a resource offer)","title":"Creating Instance"},{"location":"scheduler/TaskDescription/#text-representation","text":"toString : String toString uses the taskId and index as follows: TaskDescription(TID=[taskId], index=[index])","title":" Text Representation"},{"location":"scheduler/TaskDescription/#decoding-taskdescription-from-serialized-format","text":"decode ( byteBuffer : ByteBuffer ): TaskDescription decode simply decodes (< >) a TaskDescription from the serialized format ( ByteBuffer ). Internally, decode ...FIXME decode is used when: CoarseGrainedExecutorBackend is requested to CoarseGrainedExecutorBackend.md#LaunchTask[handle a LaunchTask message] Spark on Mesos' MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#launchTask[launch a task]","title":" Decoding TaskDescription (from Serialized Format)"},{"location":"scheduler/TaskDescription/#encoding-taskdescription-to-serialized-format","text":"encode ( taskDescription : TaskDescription ): ByteBuffer encode simply encodes the TaskDescription to a serialized format ( ByteBuffer ). Internally, encode ...FIXME encode is used when: DriverEndpoint (of CoarseGrainedSchedulerBackend ) is requested to launchTasks","title":" Encoding TaskDescription (to Serialized Format)"},{"location":"scheduler/TaskDescription/#task-name","text":"The name of the task is of the format: task [taskID] in stage [taskSetID]","title":" Task Name"},{"location":"scheduler/TaskInfo/","text":"== [[TaskInfo]] TaskInfo TaskInfo is information about a running task attempt inside a scheduler:TaskSet.md[TaskSet]. TaskInfo is created when: scheduler:TaskSetManager.md#resourceOffer[ TaskSetManager dequeues a task for execution (given resource offer)] (and records the task as running) TaskUIData does dropInternalAndSQLAccumulables JsonProtocol utility is used to spark-history-server:JsonProtocol.md#taskInfoFromJson[re-create a task details from JSON] NOTE: Back then, at the commit 63051dd2bcc4bf09d413ff7cf89a37967edc33ba, when TaskInfo was first merged to Apache Spark on 07/06/12, TaskInfo was part of spark.scheduler.mesos package -- note \"Mesos\" in the name of the package that shows how much Spark and Mesos influenced each other at that time. [[internal-registries]] .TaskInfo's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[finishTime]] finishTime | Time when TaskInfo was < >. Used when...FIXME |=== === [[creating-instance]] Creating TaskInfo Instance TaskInfo takes the following when created: [[taskId]] Task ID [[index]] Index of the task within its scheduler:TaskSet.md[TaskSet] that may not necessarily be the same as the ID of the RDD partition that the task is computing. [[attemptNumber]] Task attempt ID [[launchTime]] Time when the task was dequeued for execution [[executorId]] Executor that has been offered (as a resource) to run the task [[host]] Host of the < > [[taskLocality]] scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality], i.e. locality preference of the task [[speculative]] Flag whether a task is speculative or not TaskInfo initializes the < >. === [[markFinished]] Marking Task As Finished (Successfully or Not) -- markFinished Method [source, scala] \u00b6 markFinished(state: TaskState, time: Long = System.currentTimeMillis): Unit \u00b6 markFinished records the input time as < >. markFinished marks TaskInfo as < > when the input state is FAILED or < > for state being KILLED . NOTE: markFinished is used when TaskSetManager is notified that a task has finished scheduler:TaskSetManager.md#handleSuccessfulTask[successfully] or scheduler:TaskSetManager.md#handleFailedTask[failed].","title":"TaskInfo"},{"location":"scheduler/TaskInfo/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/TaskInfo/#markfinishedstate-taskstate-time-long-systemcurrenttimemillis-unit","text":"markFinished records the input time as < >. markFinished marks TaskInfo as < > when the input state is FAILED or < > for state being KILLED . NOTE: markFinished is used when TaskSetManager is notified that a task has finished scheduler:TaskSetManager.md#handleSuccessfulTask[successfully] or scheduler:TaskSetManager.md#handleFailedTask[failed].","title":"markFinished(state: TaskState, time: Long = System.currentTimeMillis): Unit"},{"location":"scheduler/TaskLocation/","text":"TaskLocation \u00b6 TaskLocation represents a placement preference of an RDD partition, i.e. a hint of the location to submit scheduler:Task.md[tasks] for execution. TaskLocations are tracked by scheduler:DAGScheduler.md#cacheLocs[DAGScheduler] for scheduler:DAGScheduler.md#submitMissingTasks[submitting missing tasks of a stage]. TaskLocation is available as scheduler:Task.md#preferredLocations[preferredLocations] of a task. [[host]] Every TaskLocation describes the location by host name, but could also use other location-related metadata. TaskLocations of an RDD and a partition is available using SparkContext.md#getPreferredLocs[SparkContext.getPreferredLocs] method. Sealed TaskLocation is a Scala private[spark] sealed trait so all the available implementations of TaskLocation trait are in a single Scala file. == [[ExecutorCacheTaskLocation]] ExecutorCacheTaskLocation ExecutorCacheTaskLocation describes a < > and an executor. ExecutorCacheTaskLocation informs the Scheduler to prefer a given executor, but the next level of preference is any executor on the same host if this is not possible. == [[HDFSCacheTaskLocation]] HDFSCacheTaskLocation HDFSCacheTaskLocation describes a < > that is cached by HDFS. Used exclusively when rdd:HadoopRDD.md#getPreferredLocations[HadoopRDD] and rdd:NewHadoopRDD.md#getPreferredLocations[NewHadoopRDD] are requested for their placement preferences (aka preferred locations ). == [[HostTaskLocation]] HostTaskLocation HostTaskLocation describes a < > only.","title":"TaskLocation"},{"location":"scheduler/TaskLocation/#tasklocation","text":"TaskLocation represents a placement preference of an RDD partition, i.e. a hint of the location to submit scheduler:Task.md[tasks] for execution. TaskLocations are tracked by scheduler:DAGScheduler.md#cacheLocs[DAGScheduler] for scheduler:DAGScheduler.md#submitMissingTasks[submitting missing tasks of a stage]. TaskLocation is available as scheduler:Task.md#preferredLocations[preferredLocations] of a task. [[host]] Every TaskLocation describes the location by host name, but could also use other location-related metadata. TaskLocations of an RDD and a partition is available using SparkContext.md#getPreferredLocs[SparkContext.getPreferredLocs] method. Sealed TaskLocation is a Scala private[spark] sealed trait so all the available implementations of TaskLocation trait are in a single Scala file. == [[ExecutorCacheTaskLocation]] ExecutorCacheTaskLocation ExecutorCacheTaskLocation describes a < > and an executor. ExecutorCacheTaskLocation informs the Scheduler to prefer a given executor, but the next level of preference is any executor on the same host if this is not possible. == [[HDFSCacheTaskLocation]] HDFSCacheTaskLocation HDFSCacheTaskLocation describes a < > that is cached by HDFS. Used exclusively when rdd:HadoopRDD.md#getPreferredLocations[HadoopRDD] and rdd:NewHadoopRDD.md#getPreferredLocations[NewHadoopRDD] are requested for their placement preferences (aka preferred locations ). == [[HostTaskLocation]] HostTaskLocation HostTaskLocation describes a < > only.","title":"TaskLocation"},{"location":"scheduler/TaskResult/","text":"TaskResult \u00b6 TaskResult is an abstraction of task results (of type T ). The decision what TaskResult type to use is made when TaskRunner finishes running a task . Sealed Trait TaskResult is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file). sealed trait TaskResult [ T ] DirectTaskResult \u00b6 DirectTaskResult is a TaskResult to be serialized and sent over the wire to the driver together with the following: Value Bytes ( java.nio.ByteBuffer ) Accumulator updates Metric Peaks DirectTaskResult is used when the size of a task result is below spark.driver.maxResultSize and the maximum size of direct results . IndirectTaskResult \u00b6 IndirectTaskResult is a \"pointer\" to a task result that is available in a BlockManager : BlockId Size IndirectTaskResult is a java.io.Serializable . Externalizable \u00b6 DirectTaskResult is an Externalizable ( Java ).","title":"TaskResult"},{"location":"scheduler/TaskResult/#taskresult","text":"TaskResult is an abstraction of task results (of type T ). The decision what TaskResult type to use is made when TaskRunner finishes running a task . Sealed Trait TaskResult is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file). sealed trait TaskResult [ T ]","title":"TaskResult"},{"location":"scheduler/TaskResult/#directtaskresult","text":"DirectTaskResult is a TaskResult to be serialized and sent over the wire to the driver together with the following: Value Bytes ( java.nio.ByteBuffer ) Accumulator updates Metric Peaks DirectTaskResult is used when the size of a task result is below spark.driver.maxResultSize and the maximum size of direct results .","title":" DirectTaskResult"},{"location":"scheduler/TaskResult/#indirecttaskresult","text":"IndirectTaskResult is a \"pointer\" to a task result that is available in a BlockManager : BlockId Size IndirectTaskResult is a java.io.Serializable .","title":" IndirectTaskResult"},{"location":"scheduler/TaskResult/#externalizable","text":"DirectTaskResult is an Externalizable ( Java ).","title":" Externalizable"},{"location":"scheduler/TaskResultGetter/","text":"TaskResultGetter \u00b6 TaskResultGetter is a helper class of scheduler:TaskSchedulerImpl.md#statusUpdate[TaskSchedulerImpl] for asynchronous deserialization of < > (possibly fetching remote blocks) or < >. CAUTION: FIXME Image with the dependencies TIP: Consult scheduler:Task.md#states[Task States] in Tasks to learn about the different task states. NOTE: The only instance of TaskResultGetter is created while scheduler:TaskSchedulerImpl.md#creating-instance[ TaskSchedulerImpl is created]. TaskResultGetter requires a core:SparkEnv.md[SparkEnv] and scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] to be created and is stopped when scheduler:TaskSchedulerImpl.md#stop[ TaskSchedulerImpl stops]. TaskResultGetter uses < task-result-getter asynchronous task executor>> for operation. [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.scheduler.TaskResultGetter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskResultGetter=DEBUG Refer to spark-logging.md[Logging]. \u00b6 === [[getTaskResultExecutor]][[task-result-getter]] task-result-getter Asynchronous Task Executor [source, scala] \u00b6 getTaskResultExecutor: ExecutorService \u00b6 getTaskResultExecutor creates a daemon thread pool with < > threads and task-result-getter prefix. TIP: Read up on https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor ] that getTaskResultExecutor uses under the covers. === [[stop]] stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the internal < task-result-getter asynchronous task executor>>. === [[serializer]] serializer Attribute [source, scala] \u00b6 serializer: ThreadLocal[SerializerInstance] \u00b6 serializer is a thread-local serializer:SerializerInstance.md[SerializerInstance] that TaskResultGetter uses to deserialize byte buffers (with TaskResult s or a TaskEndReason ). When created for a new thread, serializer is initialized with a new instance of Serializer (using core:SparkEnv.md#closureSerializer[SparkEnv.closureSerializer]). NOTE: TaskResultGetter uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal ] for the thread-local SerializerInstance variable. === [[taskResultSerializer]] taskResultSerializer Attribute [source, scala] \u00b6 taskResultSerializer: ThreadLocal[SerializerInstance] \u00b6 taskResultSerializer is a thread-local serializer:SerializerInstance.md[SerializerInstance] that TaskResultGetter uses to... When created for a new thread, taskResultSerializer is initialized with a new instance of Serializer (using core:SparkEnv.md#serializer[SparkEnv.serializer]). NOTE: TaskResultGetter uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal ] for the thread-local SerializerInstance variable. Enqueuing Successful Task \u00b6 enqueueSuccessfulTask ( taskSetManager : TaskSetManager , tid : Long , serializedData : ByteBuffer ): Unit enqueueSuccessfulTask submits an asynchronous task (to < > asynchronous task executor) that first deserializes serializedData to a DirectTaskResult , then updates the internal accumulator (with the size of the DirectTaskResult ) and ultimately notifies the TaskSchedulerImpl that the tid task was completed and scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[the task result was received successfully] or scheduler:TaskSchedulerImpl.md#handleFailedTask[not]. NOTE: enqueueSuccessfulTask is just the asynchronous task enqueued for execution by < > asynchronous task executor at some point in the future. Internally, the enqueued task first deserializes serializedData to a TaskResult (using the internal thread-local < >). For a DirectTaskResult , the task scheduler:TaskSetManager.md#canFetchMoreResults[checks the available memory for the task result] and, when the size overflows configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize], it simply returns. Note enqueueSuccessfulTask is a mere thread so returning from a thread is to do nothing else. That is why the check for quota does abort when there is not enough memory. Otherwise, when there is enough memory to hold the task result, it deserializes the DirectTaskResult (using the internal thread-local < >). For an IndirectTaskResult , the task checks the available memory for the task result and, when the size could overflow the maximum result size, it storage:BlockManagerMaster.md#removeBlock[removes the block] and simply returns. Otherwise, when there is enough memory to hold the task result, you should see the following DEBUG message in the logs: Fetching indirect task result for TID [tid] The task scheduler:TaskSchedulerImpl.md#handleTaskGettingResult[notifies TaskSchedulerImpl that it is about to fetch a remote block for a task result]. It then storage:BlockManager.md#getRemoteBytes[gets the block from remote block managers (as serialized bytes)]. When the block could not be fetched, scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl is informed] (with TaskResultLost task failure reason) and the task simply returns. NOTE: enqueueSuccessfulTask is a mere thread so returning from a thread is to do nothing else and so the real handling is when scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl is informed]. The task result (as a serialized byte buffer) is then deserialized to a DirectTaskResult (using the internal thread-local < >) and deserialized again using the internal thread-local < > (just like for the DirectTaskResult case). The storage:BlockManagerMaster.md#removeBlock[block is removed from BlockManagerMaster ] and simply returns. Note A IndirectTaskResult is deserialized twice to become the final deserialized task result (using < > for a DirectTaskResult ). Compare it to a DirectTaskResult task result that is deserialized once only. With no exceptions thrown, enqueueSuccessfulTask scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[informs the TaskSchedulerImpl that the tid task was completed and the task result was received]. A ClassNotFoundException leads to scheduler:TaskSetManager.md#abort[aborting the TaskSet ] (with ClassNotFound with classloader: [loader] error message) while any non-fatal exception shows the following ERROR message in the logs followed by scheduler:TaskSetManager.md#abort[aborting the TaskSet ]. Exception while getting task result enqueueSuccessfulTask is used when TaskSchedulerImpl is requested to handle task status update (and the task has finished successfully). === [[enqueueFailedTask]] Deserializing TaskFailedReason and Notifying TaskSchedulerImpl -- enqueueFailedTask Method [source, scala] \u00b6 enqueueFailedTask( taskSetManager: TaskSetManager, tid: Long, taskState: TaskState.TaskState, serializedData: ByteBuffer): Unit enqueueFailedTask submits an asynchronous task (to < task-result-getter asynchronous task executor>>) that first attempts to deserialize a TaskFailedReason from serializedData (using the internal thread-local < >) and then scheduler:TaskSchedulerImpl.md#handleFailedTask[notifies TaskSchedulerImpl that the task has failed]. Any ClassNotFoundException leads to the following ERROR message in the logs (without breaking the flow of enqueueFailedTask ): ERROR Could not deserialize TaskEndReason: ClassNotFound with classloader [loader] NOTE: enqueueFailedTask is called when scheduler:TaskSchedulerImpl.md#statusUpdate[ TaskSchedulerImpl is notified about a task that has failed (and is in FAILED , KILLED or LOST state)]. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_resultGetter_threads]] spark.resultGetter.threads | 4 | The number of threads for TaskResultGetter . |===","title":"TaskResultGetter"},{"location":"scheduler/TaskResultGetter/#taskresultgetter","text":"TaskResultGetter is a helper class of scheduler:TaskSchedulerImpl.md#statusUpdate[TaskSchedulerImpl] for asynchronous deserialization of < > (possibly fetching remote blocks) or < >. CAUTION: FIXME Image with the dependencies TIP: Consult scheduler:Task.md#states[Task States] in Tasks to learn about the different task states. NOTE: The only instance of TaskResultGetter is created while scheduler:TaskSchedulerImpl.md#creating-instance[ TaskSchedulerImpl is created]. TaskResultGetter requires a core:SparkEnv.md[SparkEnv] and scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] to be created and is stopped when scheduler:TaskSchedulerImpl.md#stop[ TaskSchedulerImpl stops]. TaskResultGetter uses < task-result-getter asynchronous task executor>> for operation.","title":"TaskResultGetter"},{"location":"scheduler/TaskResultGetter/#tip","text":"Enable DEBUG logging level for org.apache.spark.scheduler.TaskResultGetter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskResultGetter=DEBUG","title":"[TIP]"},{"location":"scheduler/TaskResultGetter/#refer-to-spark-loggingmdlogging","text":"=== [[getTaskResultExecutor]][[task-result-getter]] task-result-getter Asynchronous Task Executor","title":"Refer to spark-logging.md[Logging]."},{"location":"scheduler/TaskResultGetter/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#gettaskresultexecutor-executorservice","text":"getTaskResultExecutor creates a daemon thread pool with < > threads and task-result-getter prefix. TIP: Read up on https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor ] that getTaskResultExecutor uses under the covers. === [[stop]] stop Method","title":"getTaskResultExecutor: ExecutorService"},{"location":"scheduler/TaskResultGetter/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#stop-unit","text":"stop stops the internal < task-result-getter asynchronous task executor>>. === [[serializer]] serializer Attribute","title":"stop(): Unit"},{"location":"scheduler/TaskResultGetter/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#serializer-threadlocalserializerinstance","text":"serializer is a thread-local serializer:SerializerInstance.md[SerializerInstance] that TaskResultGetter uses to deserialize byte buffers (with TaskResult s or a TaskEndReason ). When created for a new thread, serializer is initialized with a new instance of Serializer (using core:SparkEnv.md#closureSerializer[SparkEnv.closureSerializer]). NOTE: TaskResultGetter uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal ] for the thread-local SerializerInstance variable. === [[taskResultSerializer]] taskResultSerializer Attribute","title":"serializer: ThreadLocal[SerializerInstance]"},{"location":"scheduler/TaskResultGetter/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#taskresultserializer-threadlocalserializerinstance","text":"taskResultSerializer is a thread-local serializer:SerializerInstance.md[SerializerInstance] that TaskResultGetter uses to... When created for a new thread, taskResultSerializer is initialized with a new instance of Serializer (using core:SparkEnv.md#serializer[SparkEnv.serializer]). NOTE: TaskResultGetter uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal ] for the thread-local SerializerInstance variable.","title":"taskResultSerializer: ThreadLocal[SerializerInstance]"},{"location":"scheduler/TaskResultGetter/#enqueuing-successful-task","text":"enqueueSuccessfulTask ( taskSetManager : TaskSetManager , tid : Long , serializedData : ByteBuffer ): Unit enqueueSuccessfulTask submits an asynchronous task (to < > asynchronous task executor) that first deserializes serializedData to a DirectTaskResult , then updates the internal accumulator (with the size of the DirectTaskResult ) and ultimately notifies the TaskSchedulerImpl that the tid task was completed and scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[the task result was received successfully] or scheduler:TaskSchedulerImpl.md#handleFailedTask[not]. NOTE: enqueueSuccessfulTask is just the asynchronous task enqueued for execution by < > asynchronous task executor at some point in the future. Internally, the enqueued task first deserializes serializedData to a TaskResult (using the internal thread-local < >). For a DirectTaskResult , the task scheduler:TaskSetManager.md#canFetchMoreResults[checks the available memory for the task result] and, when the size overflows configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize], it simply returns. Note enqueueSuccessfulTask is a mere thread so returning from a thread is to do nothing else. That is why the check for quota does abort when there is not enough memory. Otherwise, when there is enough memory to hold the task result, it deserializes the DirectTaskResult (using the internal thread-local < >). For an IndirectTaskResult , the task checks the available memory for the task result and, when the size could overflow the maximum result size, it storage:BlockManagerMaster.md#removeBlock[removes the block] and simply returns. Otherwise, when there is enough memory to hold the task result, you should see the following DEBUG message in the logs: Fetching indirect task result for TID [tid] The task scheduler:TaskSchedulerImpl.md#handleTaskGettingResult[notifies TaskSchedulerImpl that it is about to fetch a remote block for a task result]. It then storage:BlockManager.md#getRemoteBytes[gets the block from remote block managers (as serialized bytes)]. When the block could not be fetched, scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl is informed] (with TaskResultLost task failure reason) and the task simply returns. NOTE: enqueueSuccessfulTask is a mere thread so returning from a thread is to do nothing else and so the real handling is when scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl is informed]. The task result (as a serialized byte buffer) is then deserialized to a DirectTaskResult (using the internal thread-local < >) and deserialized again using the internal thread-local < > (just like for the DirectTaskResult case). The storage:BlockManagerMaster.md#removeBlock[block is removed from BlockManagerMaster ] and simply returns. Note A IndirectTaskResult is deserialized twice to become the final deserialized task result (using < > for a DirectTaskResult ). Compare it to a DirectTaskResult task result that is deserialized once only. With no exceptions thrown, enqueueSuccessfulTask scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[informs the TaskSchedulerImpl that the tid task was completed and the task result was received]. A ClassNotFoundException leads to scheduler:TaskSetManager.md#abort[aborting the TaskSet ] (with ClassNotFound with classloader: [loader] error message) while any non-fatal exception shows the following ERROR message in the logs followed by scheduler:TaskSetManager.md#abort[aborting the TaskSet ]. Exception while getting task result enqueueSuccessfulTask is used when TaskSchedulerImpl is requested to handle task status update (and the task has finished successfully). === [[enqueueFailedTask]] Deserializing TaskFailedReason and Notifying TaskSchedulerImpl -- enqueueFailedTask Method","title":" Enqueuing Successful Task"},{"location":"scheduler/TaskResultGetter/#source-scala_4","text":"enqueueFailedTask( taskSetManager: TaskSetManager, tid: Long, taskState: TaskState.TaskState, serializedData: ByteBuffer): Unit enqueueFailedTask submits an asynchronous task (to < task-result-getter asynchronous task executor>>) that first attempts to deserialize a TaskFailedReason from serializedData (using the internal thread-local < >) and then scheduler:TaskSchedulerImpl.md#handleFailedTask[notifies TaskSchedulerImpl that the task has failed]. Any ClassNotFoundException leads to the following ERROR message in the logs (without breaking the flow of enqueueFailedTask ): ERROR Could not deserialize TaskEndReason: ClassNotFound with classloader [loader] NOTE: enqueueFailedTask is called when scheduler:TaskSchedulerImpl.md#statusUpdate[ TaskSchedulerImpl is notified about a task that has failed (and is in FAILED , KILLED or LOST state)]. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_resultGetter_threads]] spark.resultGetter.threads | 4 | The number of threads for TaskResultGetter . |===","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/","text":"TaskScheduler \u00b6 TaskScheduler is an abstraction of < > that can < > in a Spark application (per < >). NOTE: TaskScheduler works closely with scheduler:DAGScheduler.md[DAGScheduler] that < > (for every stage in a Spark job). TaskScheduler can track the executors available in a Spark application using < > and < > interceptors (that inform about active and lost executors, respectively). == [[submitTasks]] Submitting Tasks for Execution [source, scala] \u00b6 submitTasks( taskSet: TaskSet): Unit Submits the tasks (of the given scheduler:TaskSet.md[TaskSet]) for execution. Used when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit missing tasks (of a stage)]. == [[executorHeartbeatReceived]] Handling Executor Heartbeat [source, scala] \u00b6 executorHeartbeatReceived( execId: String, accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], blockManagerId: BlockManagerId): Boolean Handles a heartbeat from an executor Returns true when the execId executor is managed by the TaskScheduler. false indicates that the executor:Executor.md#reportHeartBeat[block manager (on the executor) should re-register]. Used when HeartbeatReceiver RPC endpoint is requested to handle a Heartbeat (with task metrics) from an executor == [[killTaskAttempt]] Killing Task [source, scala] \u00b6 killTaskAttempt( taskId: Long, interruptThread: Boolean, reason: String): Boolean Kills a task (attempt) Used when DAGScheduler is requested to scheduler:DAGScheduler.md#killTaskAttempt[kill a task] == [[workerRemoved]] workerRemoved Notification [source, scala] \u00b6 workerRemoved( workerId: String, host: String, message: String): Unit Used when DriverEndpoint is requested to handle a RemoveWorker event == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | applicationAttemptId a| [[applicationAttemptId]] [source, scala] \u00b6 applicationAttemptId(): Option[String] \u00b6 Unique identifier of an (execution) attempt of the Spark application Used when SparkContext is created | cancelTasks a| [[cancelTasks]] [source, scala] \u00b6 cancelTasks( stageId: Int, interruptThread: Boolean): Unit Cancels all the tasks of a given Stage.md[stage] Used when DAGScheduler is requested to DAGScheduler.md#failJobAndIndependentStages[failJobAndIndependentStages] | defaultParallelism a| [[defaultParallelism]] [source, scala] \u00b6 defaultParallelism(): Int \u00b6 Default level of parallelism Used when SparkContext is requested for the default level of parallelism | executorLost a| [[executorLost]] [source, scala] \u00b6 executorLost( executorId: String, reason: ExecutorLossReason): Unit Handles an executor lost event Used when: HeartbeatReceiver RPC endpoint is requested to expireDeadHosts DriverEndpoint RPC endpoint is requested to removes ( forgets ) and disables a malfunctioning executor (i.e. either lost or blacklisted for some reason) | killAllTaskAttempts a| [[killAllTaskAttempts]] [source, scala] \u00b6 killAllTaskAttempts( stageId: Int, interruptThread: Boolean, reason: String): Unit Used when: DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] TaskSchedulerImpl is requested to TaskSchedulerImpl.md#cancelTasks[cancel all the tasks of a stage] | rootPool a| [[rootPool]] [source, scala] \u00b6 rootPool: Pool \u00b6 Top-level (root) scheduler:spark-scheduler-Pool.md[schedulable pool] Used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#initialize[initialize] SparkContext is requested to SparkContext.md#getAllPools[getAllPools] and SparkContext.md#getPoolForName[getPoolForName] TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers], scheduler:TaskSchedulerImpl.md#checkSpeculatableTasks[checkSpeculatableTasks], and scheduler:TaskSchedulerImpl.md#removeExecutor[removeExecutor] | schedulingMode a| [[schedulingMode]] [source, scala] \u00b6 schedulingMode: SchedulingMode \u00b6 scheduler:spark-scheduler-SchedulingMode.md[Scheduling mode] Used when: TaskSchedulerImpl is scheduler:TaskSchedulerImpl.md#rootPool[created] and scheduler:TaskSchedulerImpl.md#initialize[initialized] SparkContext is requested to SparkContext.md#getSchedulingMode[getSchedulingMode] | setDAGScheduler a| [[setDAGScheduler]] [source, scala] \u00b6 setDAGScheduler(dagScheduler: DAGScheduler): Unit \u00b6 Associates a scheduler:DAGScheduler.md[DAGScheduler] Used when DAGScheduler is scheduler:DAGScheduler.md#creating-instance[created] | start a| [[start]] [source, scala] \u00b6 start(): Unit \u00b6 Starts the TaskScheduler Used when SparkContext is created | stop a| [[stop]] [source, scala] \u00b6 stop(): Unit \u00b6 Stops the TaskScheduler Used when DAGScheduler is requested to scheduler:DAGScheduler.md#stop[stop] |=== == [[implementations]] TaskSchedulers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | TaskScheduler | Description | scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] | [[TaskSchedulerImpl]] Default Spark scheduler | spark-on-yarn:spark-yarn-yarnscheduler.md[YarnScheduler] | [[YarnScheduler]] TaskScheduler for tools:spark-submit.md#deploy-mode[client] deploy mode in spark-on-yarn:index.md[Spark on YARN] | spark-on-yarn:spark-yarn-yarnclusterscheduler.md[YarnClusterScheduler] | [[YarnClusterScheduler]] TaskScheduler for tools:spark-submit.md#deploy-mode[cluster] deploy mode in spark-on-yarn:index.md[Spark on YARN] |=== Lifecycle \u00b6 A TaskScheduler is created while SparkContext is being created (by calling SparkContext.createTaskScheduler for a given master URL and deploy mode ). At this point in SparkContext's lifecycle, the internal _taskScheduler points at the TaskScheduler (and it is \"announced\" by sending a blocking TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint ). The < > right after the blocking TaskSchedulerIsSet message receives a response. The < > and the < > are set at this point (and SparkContext uses the application id to set SparkConf.md#spark.app.id[spark.app.id] Spark property, and configure webui:spark-webui-SparkUI.md[SparkUI], and storage:BlockManager.md[BlockManager]). CAUTION: FIXME The application id is described as \"associated with the job.\" in TaskScheduler, but I think it is \"associated with the application\" and you can have many jobs per application. Right before SparkContext is fully initialized, < > is called. The internal _taskScheduler is cleared (i.e. set to null ) while SparkContext.md#stop[SparkContext is being stopped]. < > while scheduler:DAGScheduler.md#stop[DAGScheduler is being stopped]. WARNING: FIXME If it is SparkContext to start a TaskScheduler, shouldn't SparkContext stop it too? Why is this the way it is now? == [[postStartHook]] Post-Start Initialization [source, scala] \u00b6 postStartHook(): Unit \u00b6 postStartHook does nothing by default, but allows < > for some additional post-start initialization. postStartHook is used when: SparkContext is created Spark on YARN's YarnClusterScheduler is requested to spark-on-yarn:spark-yarn-yarnclusterscheduler.md#postStartHook[postStartHook] == [[applicationId]][[appId]] Unique Identifier of Spark Application [source, scala] \u00b6 applicationId(): String \u00b6 applicationId is the unique identifier of the Spark application and defaults to spark-application-[currentTimeMillis] . applicationId is used when SparkContext is created.","title":"TaskScheduler"},{"location":"scheduler/TaskScheduler/#taskscheduler","text":"TaskScheduler is an abstraction of < > that can < > in a Spark application (per < >). NOTE: TaskScheduler works closely with scheduler:DAGScheduler.md[DAGScheduler] that < > (for every stage in a Spark job). TaskScheduler can track the executors available in a Spark application using < > and < > interceptors (that inform about active and lost executors, respectively). == [[submitTasks]] Submitting Tasks for Execution","title":"TaskScheduler"},{"location":"scheduler/TaskScheduler/#source-scala","text":"submitTasks( taskSet: TaskSet): Unit Submits the tasks (of the given scheduler:TaskSet.md[TaskSet]) for execution. Used when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit missing tasks (of a stage)]. == [[executorHeartbeatReceived]] Handling Executor Heartbeat","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_1","text":"executorHeartbeatReceived( execId: String, accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], blockManagerId: BlockManagerId): Boolean Handles a heartbeat from an executor Returns true when the execId executor is managed by the TaskScheduler. false indicates that the executor:Executor.md#reportHeartBeat[block manager (on the executor) should re-register]. Used when HeartbeatReceiver RPC endpoint is requested to handle a Heartbeat (with task metrics) from an executor == [[killTaskAttempt]] Killing Task","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_2","text":"killTaskAttempt( taskId: Long, interruptThread: Boolean, reason: String): Boolean Kills a task (attempt) Used when DAGScheduler is requested to scheduler:DAGScheduler.md#killTaskAttempt[kill a task] == [[workerRemoved]] workerRemoved Notification","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_3","text":"workerRemoved( workerId: String, host: String, message: String): Unit Used when DriverEndpoint is requested to handle a RemoveWorker event == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | applicationAttemptId a| [[applicationAttemptId]]","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#applicationattemptid-optionstring","text":"Unique identifier of an (execution) attempt of the Spark application Used when SparkContext is created | cancelTasks a| [[cancelTasks]]","title":"applicationAttemptId(): Option[String]"},{"location":"scheduler/TaskScheduler/#source-scala_5","text":"cancelTasks( stageId: Int, interruptThread: Boolean): Unit Cancels all the tasks of a given Stage.md[stage] Used when DAGScheduler is requested to DAGScheduler.md#failJobAndIndependentStages[failJobAndIndependentStages] | defaultParallelism a| [[defaultParallelism]]","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#defaultparallelism-int","text":"Default level of parallelism Used when SparkContext is requested for the default level of parallelism | executorLost a| [[executorLost]]","title":"defaultParallelism(): Int"},{"location":"scheduler/TaskScheduler/#source-scala_7","text":"executorLost( executorId: String, reason: ExecutorLossReason): Unit Handles an executor lost event Used when: HeartbeatReceiver RPC endpoint is requested to expireDeadHosts DriverEndpoint RPC endpoint is requested to removes ( forgets ) and disables a malfunctioning executor (i.e. either lost or blacklisted for some reason) | killAllTaskAttempts a| [[killAllTaskAttempts]]","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_8","text":"killAllTaskAttempts( stageId: Int, interruptThread: Boolean, reason: String): Unit Used when: DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] TaskSchedulerImpl is requested to TaskSchedulerImpl.md#cancelTasks[cancel all the tasks of a stage] | rootPool a| [[rootPool]]","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#rootpool-pool","text":"Top-level (root) scheduler:spark-scheduler-Pool.md[schedulable pool] Used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#initialize[initialize] SparkContext is requested to SparkContext.md#getAllPools[getAllPools] and SparkContext.md#getPoolForName[getPoolForName] TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers], scheduler:TaskSchedulerImpl.md#checkSpeculatableTasks[checkSpeculatableTasks], and scheduler:TaskSchedulerImpl.md#removeExecutor[removeExecutor] | schedulingMode a| [[schedulingMode]]","title":"rootPool: Pool"},{"location":"scheduler/TaskScheduler/#source-scala_10","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#schedulingmode-schedulingmode","text":"scheduler:spark-scheduler-SchedulingMode.md[Scheduling mode] Used when: TaskSchedulerImpl is scheduler:TaskSchedulerImpl.md#rootPool[created] and scheduler:TaskSchedulerImpl.md#initialize[initialized] SparkContext is requested to SparkContext.md#getSchedulingMode[getSchedulingMode] | setDAGScheduler a| [[setDAGScheduler]]","title":"schedulingMode: SchedulingMode"},{"location":"scheduler/TaskScheduler/#source-scala_11","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#setdagschedulerdagscheduler-dagscheduler-unit","text":"Associates a scheduler:DAGScheduler.md[DAGScheduler] Used when DAGScheduler is scheduler:DAGScheduler.md#creating-instance[created] | start a| [[start]]","title":"setDAGScheduler(dagScheduler: DAGScheduler): Unit"},{"location":"scheduler/TaskScheduler/#source-scala_12","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#start-unit","text":"Starts the TaskScheduler Used when SparkContext is created | stop a| [[stop]]","title":"start(): Unit"},{"location":"scheduler/TaskScheduler/#source-scala_13","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#stop-unit","text":"Stops the TaskScheduler Used when DAGScheduler is requested to scheduler:DAGScheduler.md#stop[stop] |=== == [[implementations]] TaskSchedulers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | TaskScheduler | Description | scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] | [[TaskSchedulerImpl]] Default Spark scheduler | spark-on-yarn:spark-yarn-yarnscheduler.md[YarnScheduler] | [[YarnScheduler]] TaskScheduler for tools:spark-submit.md#deploy-mode[client] deploy mode in spark-on-yarn:index.md[Spark on YARN] | spark-on-yarn:spark-yarn-yarnclusterscheduler.md[YarnClusterScheduler] | [[YarnClusterScheduler]] TaskScheduler for tools:spark-submit.md#deploy-mode[cluster] deploy mode in spark-on-yarn:index.md[Spark on YARN] |===","title":"stop(): Unit"},{"location":"scheduler/TaskScheduler/#lifecycle","text":"A TaskScheduler is created while SparkContext is being created (by calling SparkContext.createTaskScheduler for a given master URL and deploy mode ). At this point in SparkContext's lifecycle, the internal _taskScheduler points at the TaskScheduler (and it is \"announced\" by sending a blocking TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint ). The < > right after the blocking TaskSchedulerIsSet message receives a response. The < > and the < > are set at this point (and SparkContext uses the application id to set SparkConf.md#spark.app.id[spark.app.id] Spark property, and configure webui:spark-webui-SparkUI.md[SparkUI], and storage:BlockManager.md[BlockManager]). CAUTION: FIXME The application id is described as \"associated with the job.\" in TaskScheduler, but I think it is \"associated with the application\" and you can have many jobs per application. Right before SparkContext is fully initialized, < > is called. The internal _taskScheduler is cleared (i.e. set to null ) while SparkContext.md#stop[SparkContext is being stopped]. < > while scheduler:DAGScheduler.md#stop[DAGScheduler is being stopped]. WARNING: FIXME If it is SparkContext to start a TaskScheduler, shouldn't SparkContext stop it too? Why is this the way it is now? == [[postStartHook]] Post-Start Initialization","title":"Lifecycle"},{"location":"scheduler/TaskScheduler/#source-scala_14","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#poststarthook-unit","text":"postStartHook does nothing by default, but allows < > for some additional post-start initialization. postStartHook is used when: SparkContext is created Spark on YARN's YarnClusterScheduler is requested to spark-on-yarn:spark-yarn-yarnclusterscheduler.md#postStartHook[postStartHook] == [[applicationId]][[appId]] Unique Identifier of Spark Application","title":"postStartHook(): Unit"},{"location":"scheduler/TaskScheduler/#source-scala_15","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#applicationid-string","text":"applicationId is the unique identifier of the Spark application and defaults to spark-application-[currentTimeMillis] . applicationId is used when SparkContext is created.","title":"applicationId(): String"},{"location":"scheduler/TaskSchedulerImpl/","text":"TaskSchedulerImpl \u00b6 TaskSchedulerImpl is a TaskScheduler that uses a SchedulerBackend to schedule tasks (for execution on a cluster manager). When a Spark application starts (and so an instance of SparkContext is created ) TaskSchedulerImpl with a SchedulerBackend and DAGScheduler are created and soon started. TaskSchedulerImpl generates tasks based on executor resource offers . TaskSchedulerImpl can track racks per host and port (that however is only used with Hadoop YARN cluster manager). Using spark.scheduler.mode configuration property you can select the scheduling policy . TaskSchedulerImpl submits tasks using SchedulableBuilder s. Creating Instance \u00b6 TaskSchedulerImpl takes the following to be created: SparkContext Maximum Number of Task Failures isLocal flag (default: false ) Clock (default: SystemClock ) While being created, TaskSchedulerImpl sets schedulingMode to the value of spark.scheduler.mode configuration property. Note schedulingMode is part of the TaskScheduler abstraction. TaskSchedulerImpl throws a SparkException for unrecognized scheduling mode: Unrecognized spark.scheduler.mode: [schedulingModeConf] In the end, TaskSchedulerImpl creates a TaskResultGetter . TaskSchedulerImpl is created when: SparkContext is requested for a TaskScheduler (for local and spark master URLs) KubernetesClusterManager and MesosClusterManager are requested for a TaskScheduler Maximum Number of Task Failures \u00b6 TaskSchedulerImpl can be given the maximum number of task failures when created or default to spark.task.maxFailures configuration property. The number of task failures is used when submitting tasks (to create a TaskSetManager ). spark.task.cpus \u00b6 TaskSchedulerImpl uses spark.task.cpus configuration property for...FIXME SchedulerBackend \u00b6 backend : SchedulerBackend TaskSchedulerImpl is given a SchedulerBackend when requested to initialize . The lifecycle of the SchedulerBackend is tightly coupled to the lifecycle of the TaskSchedulerImpl : It is started when TaskSchedulerImpl is It is stopped when TaskSchedulerImpl is TaskSchedulerImpl waits until the SchedulerBackend is ready before requesting it for the following: Reviving resource offers when requested to submitTasks , statusUpdate , handleFailedTask , checkSpeculatableTasks , and executorLost Killing tasks when requested to killTaskAttempt and killAllTaskAttempts Default parallelism , applicationId and applicationAttemptId when requested for the defaultParallelism , applicationId and applicationAttemptId , respectively Unique Identifier of Spark Application \u00b6 applicationId (): String applicationId is part of the TaskScheduler abstraction. applicationId simply request the SchedulerBackend for the applicationId . executorHeartbeatReceived \u00b6 executorHeartbeatReceived ( execId : String , accumUpdates : Array [( Long , Seq [ AccumulatorV2 [ _ , _ ]])], blockManagerId : BlockManagerId ): Boolean executorHeartbeatReceived is part of the TaskScheduler abstraction. executorHeartbeatReceived is...FIXME Cancelling All Tasks of Stage \u00b6 cancelTasks ( stageId : Int , interruptThread : Boolean ): Unit cancelTasks is part of the TaskScheduler abstraction. cancelTasks cancels all tasks submitted for execution in a stage stageId . cancelTasks is used when: DAGScheduler is requested to failJobAndIndependentStages handleSuccessfulTask \u00b6 handleSuccessfulTask ( taskSetManager : TaskSetManager , tid : Long , taskResult : DirectTaskResult [ _ ]): Unit handleSuccessfulTask requests the given TaskSetManager to handleSuccessfulTask (with the given tid and taskResult ). handleSuccessfulTask is used when: TaskResultGetter is requested to enqueueSuccessfulTask handleTaskGettingResult \u00b6 handleTaskGettingResult ( taskSetManager : TaskSetManager , tid : Long ): Unit handleTaskGettingResult requests the given TaskSetManager to handleTaskGettingResult . handleTaskGettingResult is used when: TaskResultGetter is requested to enqueueSuccessfulTask Tracking Racks per Hosts and Ports \u00b6 getRackForHost ( value : String ): Option [ String ] getRackForHost is a method to know about the racks per hosts and ports. By default, it assumes that racks are unknown (i.e. the method returns None ). getRackForHost is currently used in two places: < > to track hosts per rack (using the < hostsByRack registry>>) while processing resource offers. < > to...FIXME scheduler:TaskSetManager.md#addPendingTask[TaskSetManager.addPendingTask], scheduler:TaskSetManager.md#[TaskSetManager.dequeueTask], and scheduler:TaskSetManager.md#dequeueSpeculativeTask[TaskSetManager.dequeueSpeculativeTask] Initializing \u00b6 initialize ( backend : SchedulerBackend ): Unit initialize initializes the TaskSchedulerImpl with the given SchedulerBackend . initialize saves the given SchedulerBackend . initialize then sets < Pool >> as an empty-named Pool.md[Pool] (passing in < >, initMinShare and initWeight as 0 ). NOTE: < > and < > are a part of scheduler:TaskScheduler.md#contract[TaskScheduler Contract]. initialize sets < > (based on < >): FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] for FIFO scheduling mode FairSchedulableBuilder.md[FairSchedulableBuilder] for FAIR scheduling mode initialize SchedulableBuilder.md#buildPools[requests SchedulableBuilder to build pools]. CAUTION: FIXME Why are rootPool and schedulableBuilder created only now? What do they need that it is not available when TaskSchedulerImpl is created? NOTE: initialize is called while SparkContext.md#createTaskScheduler[SparkContext is created and creates SchedulerBackend and TaskScheduler ]. Starting TaskSchedulerImpl \u00b6 start (): Unit start starts the SchedulerBackend and the task-scheduler-speculation executor service. Handling Task Status Update \u00b6 statusUpdate ( tid : Long , state : TaskState , serializedData : ByteBuffer ): Unit statusUpdate finds TaskSetManager for the input tid task (in < >). When state is LOST , statusUpdate ...FIXME NOTE: TaskState.LOST is only used by the deprecated Mesos fine-grained scheduling mode. When state is one of the scheduler:Task.md#states[finished states], i.e. FINISHED , FAILED , KILLED or LOST , statusUpdate < > for the input tid . statusUpdate scheduler:TaskSetManager.md#removeRunningTask[requests TaskSetManager to unregister tid from running tasks]. statusUpdate requests < > to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[schedule an asynchrounous task to deserialize the task result (and notify TaskSchedulerImpl back)] for tid in FINISHED state and scheduler:TaskResultGetter.md#enqueueFailedTask[schedule an asynchrounous task to deserialize TaskFailedReason (and notify TaskSchedulerImpl back)] for tid in the other finished states (i.e. FAILED , KILLED , LOST ). If a task is in LOST state, statusUpdate scheduler:DAGScheduler.md#executorLost[notifies DAGScheduler that the executor was lost] (with SlaveLost and the reason Task [tid] was lost, so marking the executor as lost as well. ) and scheduler:SchedulerBackend.md#reviveOffers[requests SchedulerBackend to revive offers]. In case the TaskSetManager for tid could not be found (in < > registry), you should see the following ERROR message in the logs: Ignoring update with state [state] for TID [tid] because its task set is gone (this is likely the result of receiving duplicate task finished status updates) Any exception is caught and reported as ERROR message in the logs: Exception in statusUpdate CAUTION: FIXME image with scheduler backends calling TaskSchedulerImpl.statusUpdate . statusUpdate is used when: DriverEndpoint (of CoarseGrainedSchedulerBackend ) is requested to handle a StatusUpdate message LocalEndpoint is requested to handle a StatusUpdate message task-scheduler-speculation Scheduled Executor Service \u00b6 speculationScheduler is a java.util.concurrent.ScheduledExecutorService with the name task-scheduler-speculation for Speculative Execution of Tasks . When TaskSchedulerImpl is requested to start (in non-local run mode) with spark.speculation enabled, speculationScheduler is used to schedule checkSpeculatableTasks to execute periodically every spark.speculation.interval . speculationScheduler is shut down when TaskSchedulerImpl is requested to stop . Checking for Speculatable Tasks \u00b6 checkSpeculatableTasks (): Unit checkSpeculatableTasks requests rootPool to check for speculatable tasks (if they ran for more than 100 ms) and, if there any, requests scheduler:SchedulerBackend.md#reviveOffers[SchedulerBackend to revive offers]. NOTE: checkSpeculatableTasks is executed periodically as part of speculative-execution-of-tasks.md[]. Cleaning up After Removing Executor \u00b6 removeExecutor ( executorId : String , reason : ExecutorLossReason ): Unit removeExecutor removes the executorId executor from the following < >: < >, executorIdToHost , executorsByHost , and hostsByRack . If the affected hosts and racks are the last entries in executorsByHost and hostsByRack , appropriately, they are removed from the registries. Unless reason is LossReasonPending , the executor is removed from executorIdToHost registry and Schedulable.md#executorLost[TaskSetManagers get notified]. NOTE: The internal removeExecutor is called as part of < > and scheduler:TaskScheduler.md#executorLost[executorLost]. Handling Nearly-Completed SparkContext Initialization \u00b6 postStartHook (): Unit postStartHook is part of the TaskScheduler abstraction. postStartHook waits until a scheduler backend is ready . Waiting Until SchedulerBackend is Ready \u00b6 waitBackendReady (): Unit waitBackendReady waits until the SchedulerBackend is ready . If it is, waitBackendReady returns immediately. Otherwise, waitBackendReady keeps checking every 100 milliseconds (hardcoded) or the < > is SparkContext.md#stopped[stopped]. Note A SchedulerBackend is ready by default. If the SparkContext happens to be stopped while waiting, waitBackendReady throws an IllegalStateException : Spark context stopped while waiting for backend Stopping TaskSchedulerImpl \u00b6 stop (): Unit stop stops all the internal services, i.e. < task-scheduler-speculation executor service>>, scheduler:SchedulerBackend.md[SchedulerBackend], scheduler:TaskResultGetter.md[TaskResultGetter], and < > timer. Default Level of Parallelism \u00b6 defaultParallelism (): Int defaultParallelism is part of the TaskScheduler abstraction. defaultParallelism requests the SchedulerBackend for the default level of parallelism . Note Default level of parallelism is a hint for sizing jobs that SparkContext uses to create RDDs with the right number of partitions unless specified explicitly . Submitting Tasks (of TaskSet) for Execution \u00b6 submitTasks ( taskSet : TaskSet ): Unit submitTasks is part of the TaskScheduler abstraction. In essence, submitTasks registers a new TaskSetManager (for the given TaskSet ) and requests the SchedulerBackend to handle resource allocation offers (from the scheduling system) . Internally, submitTasks prints out the following INFO message to the logs: Adding task set [id] with [length] tasks submitTasks then < > (for the given TaskSet.md[TaskSet] and the < >). submitTasks registers ( adds ) the TaskSetManager per TaskSet.md#stageId[stage] and TaskSet.md#stageAttemptId[stage attempt] IDs (of the TaskSet.md[TaskSet]) in the < > internal registry. NOTE: < > internal registry tracks the TaskSetManager.md[TaskSetManagers] (that represent TaskSet.md[TaskSets]) per stage and stage attempts. In other words, there could be many TaskSetManagers for a single stage, each representing a unique stage attempt. NOTE: Not only could a task be retried (cf. < >), but also a single stage. submitTasks makes sure that there is exactly one active TaskSetManager (with different TaskSet ) across all the managers (for the stage). Otherwise, submitTasks throws an IllegalStateException : more than one active taskSet for stage [stage]: [TaskSet ids] NOTE: TaskSetManager is considered active when it is not a zombie . submitTasks requests the < > to SchedulableBuilder.md#addTaskSetManager[add the TaskSetManager to the schedulable pool]. NOTE: The TaskScheduler.md#rootPool[schedulable pool] can be a single flat linked queue (in FIFOSchedulableBuilder.md[FIFO scheduling mode]) or a hierarchy of pools of Schedulables (in FairSchedulableBuilder.md[FAIR scheduling mode]). submitTasks < > to make sure that the requested resources (i.e. CPU and memory) are assigned to the Spark application for a < > (the very first time the Spark application is started per < > flag). NOTE: The very first time (< > flag is false ) in cluster mode only (i.e. isLocal of the TaskSchedulerImpl is false ), starvationTimer is scheduled to execute after configuration-properties.md#spark.starvation.timeout[spark.starvation.timeout] to ensure that the requested resources, i.e. CPUs and memory, were assigned by a cluster manager. NOTE: After the first configuration-properties.md#spark.starvation.timeout[spark.starvation.timeout] passes, the < > internal flag is true . In the end, submitTasks requests the < > to scheduler:SchedulerBackend.md#reviveOffers[reviveOffers]. TIP: Use dag-scheduler-event-loop thread to step through the code in a debugger. Scheduling Starvation Task \u00b6 Every time the starvation timer thread is executed and hasLaunchedTask flag is false , the following WARN message is printed out to the logs: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Otherwise, when the hasLaunchedTask flag is true the timer thread cancels itself. Creating TaskSetManager \u00b6 createTaskSetManager ( taskSet : TaskSet , maxTaskFailures : Int ): TaskSetManager createTaskSetManager creates a TaskSetManager . createTaskSetManager is used when: TaskSchedulerImpl is requested to submits tasks Notifying TaskSetManager that Task Failed \u00b6 handleFailedTask ( taskSetManager : TaskSetManager , tid : Long , taskState : TaskState , reason : TaskFailedReason ): Unit handleFailedTask scheduler:TaskSetManager.md#handleFailedTask[notifies taskSetManager that tid task has failed] and, only when scheduler:TaskSetManager.md#zombie-state[ taskSetManager is not in zombie state] and tid is not in KILLED state, scheduler:SchedulerBackend.md#reviveOffers[requests SchedulerBackend to revive offers]. NOTE: handleFailedTask is called when scheduler:TaskResultGetter.md#enqueueSuccessfulTask[ TaskResultGetter deserializes a TaskFailedReason ] for a failed task. taskSetFinished \u00b6 taskSetFinished ( manager : TaskSetManager ): Unit taskSetFinished looks all scheduler:TaskSet.md[TaskSet]s up by the stage id (in < > registry) and removes the stage attempt from them, possibly with removing the entire stage record from taskSetsByStageIdAndAttempt registry completely (if there are no other attempts registered). taskSetFinished then removes manager from the parent's schedulable pool . You should see the following INFO message in the logs: Removed TaskSet [id], whose tasks have all completed, from pool [name] taskSetFinished is used when: TaskSetManager is requested to maybeFinishTaskSet Notifying DAGScheduler About New Executor \u00b6 executorAdded ( execId : String , host : String ) executorAdded just DAGScheduler.md#executorAdded[notifies DAGScheduler that an executor was added]. NOTE: executorAdded uses < > that was given when < >. Creating TaskDescriptions For Available Executor Resource Offers \u00b6 resourceOffers ( offers : Seq [ WorkerOffer ]): Seq [ Seq [ TaskDescription ]] resourceOffers takes the resources offers (as < >) and generates a collection of tasks (as TaskDescription ) to launch (given the resources available). NOTE: < > represents a resource offer with CPU cores free to use on an executor. Internally, resourceOffers first updates < > and < > lookup tables to record new hosts and executors (given the input offers ). For new executors (not in < >) resourceOffers < DAGScheduler that an executor was added>>. NOTE: TaskSchedulerImpl uses resourceOffers to track active executors. CAUTION: FIXME a picture with executorAdded call from TaskSchedulerImpl to DAGScheduler. resourceOffers requests BlacklistTracker to applyBlacklistTimeout and filters out offers on blacklisted nodes and executors. NOTE: resourceOffers uses the optional < > that was given when < >. CAUTION: FIXME Expand on blacklisting resourceOffers then randomly shuffles offers (to evenly distribute tasks across executors and avoid over-utilizing some executors) and initializes the local data structures tasks and availableCpus (as shown in the figure below). resourceOffers Pool.md#getSortedTaskSetQueue[takes TaskSets in scheduling order] from scheduler:TaskScheduler.md#rootPool[top-level Schedulable Pool]. Note rootPool is configured when < >. rootPool is part of the scheduler:TaskScheduler.md#rootPool[TaskScheduler Contract] and exclusively managed by scheduler:SchedulableBuilder.md[SchedulableBuilders], i.e. scheduler:FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] and scheduler:FairSchedulableBuilder.md[FairSchedulableBuilder] (that scheduler:SchedulableBuilder.md#addTaskSetManager[manage registering TaskSetManagers with the root pool]). scheduler:TaskSetManager.md[TaskSetManager] manages execution of the tasks in a single scheduler:TaskSet.md[TaskSet] that represents a single scheduler:Stage.md[Stage]. For every TaskSetManager (in scheduling order), you should see the following DEBUG message in the logs: parentName: [name], name: [name], runningTasks: [count] Only if a new executor was added, resourceOffers scheduler:TaskSetManager.md#executorAdded[notifies every TaskSetManager about the change] (to recompute locality preferences). resourceOffers then takes every TaskSetManager (in scheduling order) and offers them each node in increasing order of locality levels (per scheduler:TaskSetManager.md#computeValidLocalityLevels[TaskSetManager's valid locality levels]). NOTE: A TaskSetManager scheduler:TaskSetManager.md##computeValidLocalityLevels[computes locality levels of the tasks] it manages. For every TaskSetManager and the TaskSetManager 's valid locality level, resourceOffers tries to < > as long as the TaskSetManager manages to launch a task (given the locality level). If resourceOffers did not manage to offer resources to a TaskSetManager so it could launch any task, resourceOffers scheduler:TaskSetManager.md#abortIfCompletelyBlacklisted[requests the TaskSetManager to abort the TaskSet if completely blacklisted]. When resourceOffers managed to launch a task, the internal < > flag gets enabled (that effectively means what the name says \"there were executors and I managed to launch a task\" ). resourceOffers is used when: CoarseGrainedSchedulerBackend (via DriverEndpoint RPC endpoint) is requested to make executor resource offers LocalEndpoint is requested to revive resource offers maybeInitBarrierCoordinator \u00b6 maybeInitBarrierCoordinator (): Unit maybeInitBarrierCoordinator ...FIXME Finding Tasks from TaskSetManager to Schedule on Executors \u00b6 resourceOfferSingleTaskSet ( taskSet : TaskSetManager , maxLocality : TaskLocality , shuffledOffers : Seq [ WorkerOffer ], availableCpus : Array [ Int ], tasks : Seq [ ArrayBuffer [ TaskDescription ]]): Boolean resourceOfferSingleTaskSet takes every WorkerOffer (from the input shuffledOffers ) and (only if the number of available CPU cores (using the input availableCpus ) is at least configuration-properties.md#spark.task.cpus[spark.task.cpus]) scheduler:TaskSetManager.md#resourceOffer[requests TaskSetManager (as the input taskSet ) to find a Task to execute (given the resource offer)] (as an executor, a host, and the input maxLocality ). resourceOfferSingleTaskSet adds the task to the input tasks collection. resourceOfferSingleTaskSet records the task id and TaskSetManager in the following registries: < > < > < > resourceOfferSingleTaskSet decreases configuration-properties.md#spark.task.cpus[spark.task.cpus] from the input availableCpus (for the WorkerOffer ). NOTE: resourceOfferSingleTaskSet makes sure that the number of available CPU cores (in the input availableCpus per WorkerOffer ) is at least 0 . If there is a TaskNotSerializableException , you should see the following ERROR in the logs: Resource offer failed, task set [name] was not serializable resourceOfferSingleTaskSet returns whether a task was launched or not. resourceOfferSingleTaskSet is used when: TaskSchedulerImpl is requested to resourceOffers Task Locality Preference \u00b6 TaskLocality represents a task locality preference and can be one of the following (from most localized to the widest): . PROCESS_LOCAL . NODE_LOCAL . NO_PREF . RACK_LOCAL . ANY WorkerOffer \u2014 Free CPU Cores on Executor \u00b6 WorkerOffer ( executorId : String , host : String , cores : Int ) WorkerOffer represents a resource offer with free CPU cores available on an executorId executor on a host . workerRemoved \u00b6 workerRemoved ( workerId : String , host : String , message : String ): Unit workerRemoved is part of the TaskScheduler abstraction. workerRemoved prints out the following INFO message to the logs: Handle removed worker [workerId]: [message] In the end, workerRemoved requests the DAGScheduler to workerRemoved . Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.TaskSchedulerImpl logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskSchedulerImpl=ALL Refer to Logging . Internal Properties \u00b6 [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | dagScheduler a| [[dagScheduler]] DAGScheduler.md[DAGScheduler] Used when...FIXME | executorIdToHost a| [[executorIdToHost]] Lookup table of hosts per executor. Used when...FIXME | executorIdToRunningTaskIds a| [[executorIdToRunningTaskIds]] Lookup table of running tasks per executor. Used when...FIXME | executorIdToTaskCount a| [[executorIdToTaskCount]] Lookup table of the number of running tasks by executor:Executor.md[]. | executorsByHost a| [[executorsByHost]] Collection of executor:Executor.md[executors] per host | hasLaunchedTask a| [[hasLaunchedTask]] Flag...FIXME Used when...FIXME | hostToExecutors a| [[hostToExecutors]] Lookup table of executors per hosts in a cluster. Used when...FIXME | hostsByRack a| [[hostsByRack]] Lookup table of hosts per rack. Used when...FIXME | nextTaskId a| [[nextTaskId]] The next scheduler:Task.md[task] id counting from 0 . Used when TaskSchedulerImpl... | rootPool a| [[rootPool]] Pool.md[Schedulable pool] Used when TaskSchedulerImpl... | schedulableBuilder a| [[schedulableBuilder]] < > Created when TaskSchedulerImpl is requested to < > and can be one of two available builders: FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] when scheduling policy is FIFO (which is the default scheduling policy). FairSchedulableBuilder.md[FairSchedulableBuilder] for FAIR scheduling policy. NOTE: Use configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property to select the scheduling policy. | schedulingMode a| [[schedulingMode]] SchedulingMode.md[SchedulingMode] Used when TaskSchedulerImpl... | taskSetsByStageIdAndAttempt a| [[taskSetsByStageIdAndAttempt]] Lookup table of scheduler:TaskSet.md[TaskSet] by stage and attempt ids. | taskIdToExecutorId a| [[taskIdToExecutorId]] Lookup table of executor:Executor.md[] by task id. | taskIdToTaskSetManager a| [[taskIdToTaskSetManager]] Registry of active scheduler:TaskSetManager.md[TaskSetManagers] per task id. |===","title":"TaskSchedulerImpl"},{"location":"scheduler/TaskSchedulerImpl/#taskschedulerimpl","text":"TaskSchedulerImpl is a TaskScheduler that uses a SchedulerBackend to schedule tasks (for execution on a cluster manager). When a Spark application starts (and so an instance of SparkContext is created ) TaskSchedulerImpl with a SchedulerBackend and DAGScheduler are created and soon started. TaskSchedulerImpl generates tasks based on executor resource offers . TaskSchedulerImpl can track racks per host and port (that however is only used with Hadoop YARN cluster manager). Using spark.scheduler.mode configuration property you can select the scheduling policy . TaskSchedulerImpl submits tasks using SchedulableBuilder s.","title":"TaskSchedulerImpl"},{"location":"scheduler/TaskSchedulerImpl/#creating-instance","text":"TaskSchedulerImpl takes the following to be created: SparkContext Maximum Number of Task Failures isLocal flag (default: false ) Clock (default: SystemClock ) While being created, TaskSchedulerImpl sets schedulingMode to the value of spark.scheduler.mode configuration property. Note schedulingMode is part of the TaskScheduler abstraction. TaskSchedulerImpl throws a SparkException for unrecognized scheduling mode: Unrecognized spark.scheduler.mode: [schedulingModeConf] In the end, TaskSchedulerImpl creates a TaskResultGetter . TaskSchedulerImpl is created when: SparkContext is requested for a TaskScheduler (for local and spark master URLs) KubernetesClusterManager and MesosClusterManager are requested for a TaskScheduler","title":"Creating Instance"},{"location":"scheduler/TaskSchedulerImpl/#maximum-number-of-task-failures","text":"TaskSchedulerImpl can be given the maximum number of task failures when created or default to spark.task.maxFailures configuration property. The number of task failures is used when submitting tasks (to create a TaskSetManager ).","title":" Maximum Number of Task Failures"},{"location":"scheduler/TaskSchedulerImpl/#sparktaskcpus","text":"TaskSchedulerImpl uses spark.task.cpus configuration property for...FIXME","title":" spark.task.cpus"},{"location":"scheduler/TaskSchedulerImpl/#schedulerbackend","text":"backend : SchedulerBackend TaskSchedulerImpl is given a SchedulerBackend when requested to initialize . The lifecycle of the SchedulerBackend is tightly coupled to the lifecycle of the TaskSchedulerImpl : It is started when TaskSchedulerImpl is It is stopped when TaskSchedulerImpl is TaskSchedulerImpl waits until the SchedulerBackend is ready before requesting it for the following: Reviving resource offers when requested to submitTasks , statusUpdate , handleFailedTask , checkSpeculatableTasks , and executorLost Killing tasks when requested to killTaskAttempt and killAllTaskAttempts Default parallelism , applicationId and applicationAttemptId when requested for the defaultParallelism , applicationId and applicationAttemptId , respectively","title":" SchedulerBackend"},{"location":"scheduler/TaskSchedulerImpl/#unique-identifier-of-spark-application","text":"applicationId (): String applicationId is part of the TaskScheduler abstraction. applicationId simply request the SchedulerBackend for the applicationId .","title":" Unique Identifier of Spark Application"},{"location":"scheduler/TaskSchedulerImpl/#executorheartbeatreceived","text":"executorHeartbeatReceived ( execId : String , accumUpdates : Array [( Long , Seq [ AccumulatorV2 [ _ , _ ]])], blockManagerId : BlockManagerId ): Boolean executorHeartbeatReceived is part of the TaskScheduler abstraction. executorHeartbeatReceived is...FIXME","title":" executorHeartbeatReceived"},{"location":"scheduler/TaskSchedulerImpl/#cancelling-all-tasks-of-stage","text":"cancelTasks ( stageId : Int , interruptThread : Boolean ): Unit cancelTasks is part of the TaskScheduler abstraction. cancelTasks cancels all tasks submitted for execution in a stage stageId . cancelTasks is used when: DAGScheduler is requested to failJobAndIndependentStages","title":" Cancelling All Tasks of Stage"},{"location":"scheduler/TaskSchedulerImpl/#handlesuccessfultask","text":"handleSuccessfulTask ( taskSetManager : TaskSetManager , tid : Long , taskResult : DirectTaskResult [ _ ]): Unit handleSuccessfulTask requests the given TaskSetManager to handleSuccessfulTask (with the given tid and taskResult ). handleSuccessfulTask is used when: TaskResultGetter is requested to enqueueSuccessfulTask","title":" handleSuccessfulTask"},{"location":"scheduler/TaskSchedulerImpl/#handletaskgettingresult","text":"handleTaskGettingResult ( taskSetManager : TaskSetManager , tid : Long ): Unit handleTaskGettingResult requests the given TaskSetManager to handleTaskGettingResult . handleTaskGettingResult is used when: TaskResultGetter is requested to enqueueSuccessfulTask","title":" handleTaskGettingResult"},{"location":"scheduler/TaskSchedulerImpl/#tracking-racks-per-hosts-and-ports","text":"getRackForHost ( value : String ): Option [ String ] getRackForHost is a method to know about the racks per hosts and ports. By default, it assumes that racks are unknown (i.e. the method returns None ). getRackForHost is currently used in two places: < > to track hosts per rack (using the < hostsByRack registry>>) while processing resource offers. < > to...FIXME scheduler:TaskSetManager.md#addPendingTask[TaskSetManager.addPendingTask], scheduler:TaskSetManager.md#[TaskSetManager.dequeueTask], and scheduler:TaskSetManager.md#dequeueSpeculativeTask[TaskSetManager.dequeueSpeculativeTask]","title":" Tracking Racks per Hosts and Ports"},{"location":"scheduler/TaskSchedulerImpl/#initializing","text":"initialize ( backend : SchedulerBackend ): Unit initialize initializes the TaskSchedulerImpl with the given SchedulerBackend . initialize saves the given SchedulerBackend . initialize then sets < Pool >> as an empty-named Pool.md[Pool] (passing in < >, initMinShare and initWeight as 0 ). NOTE: < > and < > are a part of scheduler:TaskScheduler.md#contract[TaskScheduler Contract]. initialize sets < > (based on < >): FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] for FIFO scheduling mode FairSchedulableBuilder.md[FairSchedulableBuilder] for FAIR scheduling mode initialize SchedulableBuilder.md#buildPools[requests SchedulableBuilder to build pools]. CAUTION: FIXME Why are rootPool and schedulableBuilder created only now? What do they need that it is not available when TaskSchedulerImpl is created? NOTE: initialize is called while SparkContext.md#createTaskScheduler[SparkContext is created and creates SchedulerBackend and TaskScheduler ].","title":" Initializing"},{"location":"scheduler/TaskSchedulerImpl/#starting-taskschedulerimpl","text":"start (): Unit start starts the SchedulerBackend and the task-scheduler-speculation executor service.","title":" Starting TaskSchedulerImpl"},{"location":"scheduler/TaskSchedulerImpl/#handling-task-status-update","text":"statusUpdate ( tid : Long , state : TaskState , serializedData : ByteBuffer ): Unit statusUpdate finds TaskSetManager for the input tid task (in < >). When state is LOST , statusUpdate ...FIXME NOTE: TaskState.LOST is only used by the deprecated Mesos fine-grained scheduling mode. When state is one of the scheduler:Task.md#states[finished states], i.e. FINISHED , FAILED , KILLED or LOST , statusUpdate < > for the input tid . statusUpdate scheduler:TaskSetManager.md#removeRunningTask[requests TaskSetManager to unregister tid from running tasks]. statusUpdate requests < > to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[schedule an asynchrounous task to deserialize the task result (and notify TaskSchedulerImpl back)] for tid in FINISHED state and scheduler:TaskResultGetter.md#enqueueFailedTask[schedule an asynchrounous task to deserialize TaskFailedReason (and notify TaskSchedulerImpl back)] for tid in the other finished states (i.e. FAILED , KILLED , LOST ). If a task is in LOST state, statusUpdate scheduler:DAGScheduler.md#executorLost[notifies DAGScheduler that the executor was lost] (with SlaveLost and the reason Task [tid] was lost, so marking the executor as lost as well. ) and scheduler:SchedulerBackend.md#reviveOffers[requests SchedulerBackend to revive offers]. In case the TaskSetManager for tid could not be found (in < > registry), you should see the following ERROR message in the logs: Ignoring update with state [state] for TID [tid] because its task set is gone (this is likely the result of receiving duplicate task finished status updates) Any exception is caught and reported as ERROR message in the logs: Exception in statusUpdate CAUTION: FIXME image with scheduler backends calling TaskSchedulerImpl.statusUpdate . statusUpdate is used when: DriverEndpoint (of CoarseGrainedSchedulerBackend ) is requested to handle a StatusUpdate message LocalEndpoint is requested to handle a StatusUpdate message","title":" Handling Task Status Update"},{"location":"scheduler/TaskSchedulerImpl/#task-scheduler-speculation-scheduled-executor-service","text":"speculationScheduler is a java.util.concurrent.ScheduledExecutorService with the name task-scheduler-speculation for Speculative Execution of Tasks . When TaskSchedulerImpl is requested to start (in non-local run mode) with spark.speculation enabled, speculationScheduler is used to schedule checkSpeculatableTasks to execute periodically every spark.speculation.interval . speculationScheduler is shut down when TaskSchedulerImpl is requested to stop .","title":" task-scheduler-speculation Scheduled Executor Service"},{"location":"scheduler/TaskSchedulerImpl/#checking-for-speculatable-tasks","text":"checkSpeculatableTasks (): Unit checkSpeculatableTasks requests rootPool to check for speculatable tasks (if they ran for more than 100 ms) and, if there any, requests scheduler:SchedulerBackend.md#reviveOffers[SchedulerBackend to revive offers]. NOTE: checkSpeculatableTasks is executed periodically as part of speculative-execution-of-tasks.md[].","title":" Checking for Speculatable Tasks"},{"location":"scheduler/TaskSchedulerImpl/#cleaning-up-after-removing-executor","text":"removeExecutor ( executorId : String , reason : ExecutorLossReason ): Unit removeExecutor removes the executorId executor from the following < >: < >, executorIdToHost , executorsByHost , and hostsByRack . If the affected hosts and racks are the last entries in executorsByHost and hostsByRack , appropriately, they are removed from the registries. Unless reason is LossReasonPending , the executor is removed from executorIdToHost registry and Schedulable.md#executorLost[TaskSetManagers get notified]. NOTE: The internal removeExecutor is called as part of < > and scheduler:TaskScheduler.md#executorLost[executorLost].","title":" Cleaning up After Removing Executor"},{"location":"scheduler/TaskSchedulerImpl/#handling-nearly-completed-sparkcontext-initialization","text":"postStartHook (): Unit postStartHook is part of the TaskScheduler abstraction. postStartHook waits until a scheduler backend is ready .","title":" Handling Nearly-Completed SparkContext Initialization"},{"location":"scheduler/TaskSchedulerImpl/#waiting-until-schedulerbackend-is-ready","text":"waitBackendReady (): Unit waitBackendReady waits until the SchedulerBackend is ready . If it is, waitBackendReady returns immediately. Otherwise, waitBackendReady keeps checking every 100 milliseconds (hardcoded) or the < > is SparkContext.md#stopped[stopped]. Note A SchedulerBackend is ready by default. If the SparkContext happens to be stopped while waiting, waitBackendReady throws an IllegalStateException : Spark context stopped while waiting for backend","title":" Waiting Until SchedulerBackend is Ready"},{"location":"scheduler/TaskSchedulerImpl/#stopping-taskschedulerimpl","text":"stop (): Unit stop stops all the internal services, i.e. < task-scheduler-speculation executor service>>, scheduler:SchedulerBackend.md[SchedulerBackend], scheduler:TaskResultGetter.md[TaskResultGetter], and < > timer.","title":" Stopping TaskSchedulerImpl"},{"location":"scheduler/TaskSchedulerImpl/#default-level-of-parallelism","text":"defaultParallelism (): Int defaultParallelism is part of the TaskScheduler abstraction. defaultParallelism requests the SchedulerBackend for the default level of parallelism . Note Default level of parallelism is a hint for sizing jobs that SparkContext uses to create RDDs with the right number of partitions unless specified explicitly .","title":" Default Level of Parallelism"},{"location":"scheduler/TaskSchedulerImpl/#submitting-tasks-of-taskset-for-execution","text":"submitTasks ( taskSet : TaskSet ): Unit submitTasks is part of the TaskScheduler abstraction. In essence, submitTasks registers a new TaskSetManager (for the given TaskSet ) and requests the SchedulerBackend to handle resource allocation offers (from the scheduling system) . Internally, submitTasks prints out the following INFO message to the logs: Adding task set [id] with [length] tasks submitTasks then < > (for the given TaskSet.md[TaskSet] and the < >). submitTasks registers ( adds ) the TaskSetManager per TaskSet.md#stageId[stage] and TaskSet.md#stageAttemptId[stage attempt] IDs (of the TaskSet.md[TaskSet]) in the < > internal registry. NOTE: < > internal registry tracks the TaskSetManager.md[TaskSetManagers] (that represent TaskSet.md[TaskSets]) per stage and stage attempts. In other words, there could be many TaskSetManagers for a single stage, each representing a unique stage attempt. NOTE: Not only could a task be retried (cf. < >), but also a single stage. submitTasks makes sure that there is exactly one active TaskSetManager (with different TaskSet ) across all the managers (for the stage). Otherwise, submitTasks throws an IllegalStateException : more than one active taskSet for stage [stage]: [TaskSet ids] NOTE: TaskSetManager is considered active when it is not a zombie . submitTasks requests the < > to SchedulableBuilder.md#addTaskSetManager[add the TaskSetManager to the schedulable pool]. NOTE: The TaskScheduler.md#rootPool[schedulable pool] can be a single flat linked queue (in FIFOSchedulableBuilder.md[FIFO scheduling mode]) or a hierarchy of pools of Schedulables (in FairSchedulableBuilder.md[FAIR scheduling mode]). submitTasks < > to make sure that the requested resources (i.e. CPU and memory) are assigned to the Spark application for a < > (the very first time the Spark application is started per < > flag). NOTE: The very first time (< > flag is false ) in cluster mode only (i.e. isLocal of the TaskSchedulerImpl is false ), starvationTimer is scheduled to execute after configuration-properties.md#spark.starvation.timeout[spark.starvation.timeout] to ensure that the requested resources, i.e. CPUs and memory, were assigned by a cluster manager. NOTE: After the first configuration-properties.md#spark.starvation.timeout[spark.starvation.timeout] passes, the < > internal flag is true . In the end, submitTasks requests the < > to scheduler:SchedulerBackend.md#reviveOffers[reviveOffers]. TIP: Use dag-scheduler-event-loop thread to step through the code in a debugger.","title":" Submitting Tasks (of TaskSet) for Execution"},{"location":"scheduler/TaskSchedulerImpl/#scheduling-starvation-task","text":"Every time the starvation timer thread is executed and hasLaunchedTask flag is false , the following WARN message is printed out to the logs: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Otherwise, when the hasLaunchedTask flag is true the timer thread cancels itself.","title":" Scheduling Starvation Task"},{"location":"scheduler/TaskSchedulerImpl/#creating-tasksetmanager","text":"createTaskSetManager ( taskSet : TaskSet , maxTaskFailures : Int ): TaskSetManager createTaskSetManager creates a TaskSetManager . createTaskSetManager is used when: TaskSchedulerImpl is requested to submits tasks","title":" Creating TaskSetManager"},{"location":"scheduler/TaskSchedulerImpl/#notifying-tasksetmanager-that-task-failed","text":"handleFailedTask ( taskSetManager : TaskSetManager , tid : Long , taskState : TaskState , reason : TaskFailedReason ): Unit handleFailedTask scheduler:TaskSetManager.md#handleFailedTask[notifies taskSetManager that tid task has failed] and, only when scheduler:TaskSetManager.md#zombie-state[ taskSetManager is not in zombie state] and tid is not in KILLED state, scheduler:SchedulerBackend.md#reviveOffers[requests SchedulerBackend to revive offers]. NOTE: handleFailedTask is called when scheduler:TaskResultGetter.md#enqueueSuccessfulTask[ TaskResultGetter deserializes a TaskFailedReason ] for a failed task.","title":" Notifying TaskSetManager that Task Failed"},{"location":"scheduler/TaskSchedulerImpl/#tasksetfinished","text":"taskSetFinished ( manager : TaskSetManager ): Unit taskSetFinished looks all scheduler:TaskSet.md[TaskSet]s up by the stage id (in < > registry) and removes the stage attempt from them, possibly with removing the entire stage record from taskSetsByStageIdAndAttempt registry completely (if there are no other attempts registered). taskSetFinished then removes manager from the parent's schedulable pool . You should see the following INFO message in the logs: Removed TaskSet [id], whose tasks have all completed, from pool [name] taskSetFinished is used when: TaskSetManager is requested to maybeFinishTaskSet","title":" taskSetFinished"},{"location":"scheduler/TaskSchedulerImpl/#notifying-dagscheduler-about-new-executor","text":"executorAdded ( execId : String , host : String ) executorAdded just DAGScheduler.md#executorAdded[notifies DAGScheduler that an executor was added]. NOTE: executorAdded uses < > that was given when < >.","title":" Notifying DAGScheduler About New Executor"},{"location":"scheduler/TaskSchedulerImpl/#creating-taskdescriptions-for-available-executor-resource-offers","text":"resourceOffers ( offers : Seq [ WorkerOffer ]): Seq [ Seq [ TaskDescription ]] resourceOffers takes the resources offers (as < >) and generates a collection of tasks (as TaskDescription ) to launch (given the resources available). NOTE: < > represents a resource offer with CPU cores free to use on an executor. Internally, resourceOffers first updates < > and < > lookup tables to record new hosts and executors (given the input offers ). For new executors (not in < >) resourceOffers < DAGScheduler that an executor was added>>. NOTE: TaskSchedulerImpl uses resourceOffers to track active executors. CAUTION: FIXME a picture with executorAdded call from TaskSchedulerImpl to DAGScheduler. resourceOffers requests BlacklistTracker to applyBlacklistTimeout and filters out offers on blacklisted nodes and executors. NOTE: resourceOffers uses the optional < > that was given when < >. CAUTION: FIXME Expand on blacklisting resourceOffers then randomly shuffles offers (to evenly distribute tasks across executors and avoid over-utilizing some executors) and initializes the local data structures tasks and availableCpus (as shown in the figure below). resourceOffers Pool.md#getSortedTaskSetQueue[takes TaskSets in scheduling order] from scheduler:TaskScheduler.md#rootPool[top-level Schedulable Pool]. Note rootPool is configured when < >. rootPool is part of the scheduler:TaskScheduler.md#rootPool[TaskScheduler Contract] and exclusively managed by scheduler:SchedulableBuilder.md[SchedulableBuilders], i.e. scheduler:FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] and scheduler:FairSchedulableBuilder.md[FairSchedulableBuilder] (that scheduler:SchedulableBuilder.md#addTaskSetManager[manage registering TaskSetManagers with the root pool]). scheduler:TaskSetManager.md[TaskSetManager] manages execution of the tasks in a single scheduler:TaskSet.md[TaskSet] that represents a single scheduler:Stage.md[Stage]. For every TaskSetManager (in scheduling order), you should see the following DEBUG message in the logs: parentName: [name], name: [name], runningTasks: [count] Only if a new executor was added, resourceOffers scheduler:TaskSetManager.md#executorAdded[notifies every TaskSetManager about the change] (to recompute locality preferences). resourceOffers then takes every TaskSetManager (in scheduling order) and offers them each node in increasing order of locality levels (per scheduler:TaskSetManager.md#computeValidLocalityLevels[TaskSetManager's valid locality levels]). NOTE: A TaskSetManager scheduler:TaskSetManager.md##computeValidLocalityLevels[computes locality levels of the tasks] it manages. For every TaskSetManager and the TaskSetManager 's valid locality level, resourceOffers tries to < > as long as the TaskSetManager manages to launch a task (given the locality level). If resourceOffers did not manage to offer resources to a TaskSetManager so it could launch any task, resourceOffers scheduler:TaskSetManager.md#abortIfCompletelyBlacklisted[requests the TaskSetManager to abort the TaskSet if completely blacklisted]. When resourceOffers managed to launch a task, the internal < > flag gets enabled (that effectively means what the name says \"there were executors and I managed to launch a task\" ). resourceOffers is used when: CoarseGrainedSchedulerBackend (via DriverEndpoint RPC endpoint) is requested to make executor resource offers LocalEndpoint is requested to revive resource offers","title":" Creating TaskDescriptions For Available Executor Resource Offers"},{"location":"scheduler/TaskSchedulerImpl/#maybeinitbarriercoordinator","text":"maybeInitBarrierCoordinator (): Unit maybeInitBarrierCoordinator ...FIXME","title":" maybeInitBarrierCoordinator"},{"location":"scheduler/TaskSchedulerImpl/#finding-tasks-from-tasksetmanager-to-schedule-on-executors","text":"resourceOfferSingleTaskSet ( taskSet : TaskSetManager , maxLocality : TaskLocality , shuffledOffers : Seq [ WorkerOffer ], availableCpus : Array [ Int ], tasks : Seq [ ArrayBuffer [ TaskDescription ]]): Boolean resourceOfferSingleTaskSet takes every WorkerOffer (from the input shuffledOffers ) and (only if the number of available CPU cores (using the input availableCpus ) is at least configuration-properties.md#spark.task.cpus[spark.task.cpus]) scheduler:TaskSetManager.md#resourceOffer[requests TaskSetManager (as the input taskSet ) to find a Task to execute (given the resource offer)] (as an executor, a host, and the input maxLocality ). resourceOfferSingleTaskSet adds the task to the input tasks collection. resourceOfferSingleTaskSet records the task id and TaskSetManager in the following registries: < > < > < > resourceOfferSingleTaskSet decreases configuration-properties.md#spark.task.cpus[spark.task.cpus] from the input availableCpus (for the WorkerOffer ). NOTE: resourceOfferSingleTaskSet makes sure that the number of available CPU cores (in the input availableCpus per WorkerOffer ) is at least 0 . If there is a TaskNotSerializableException , you should see the following ERROR in the logs: Resource offer failed, task set [name] was not serializable resourceOfferSingleTaskSet returns whether a task was launched or not. resourceOfferSingleTaskSet is used when: TaskSchedulerImpl is requested to resourceOffers","title":" Finding Tasks from TaskSetManager to Schedule on Executors"},{"location":"scheduler/TaskSchedulerImpl/#task-locality-preference","text":"TaskLocality represents a task locality preference and can be one of the following (from most localized to the widest): . PROCESS_LOCAL . NODE_LOCAL . NO_PREF . RACK_LOCAL . ANY","title":" Task Locality Preference"},{"location":"scheduler/TaskSchedulerImpl/#workeroffer-free-cpu-cores-on-executor","text":"WorkerOffer ( executorId : String , host : String , cores : Int ) WorkerOffer represents a resource offer with free CPU cores available on an executorId executor on a host .","title":" WorkerOffer &mdash; Free CPU Cores on Executor"},{"location":"scheduler/TaskSchedulerImpl/#workerremoved","text":"workerRemoved ( workerId : String , host : String , message : String ): Unit workerRemoved is part of the TaskScheduler abstraction. workerRemoved prints out the following INFO message to the logs: Handle removed worker [workerId]: [message] In the end, workerRemoved requests the DAGScheduler to workerRemoved .","title":" workerRemoved"},{"location":"scheduler/TaskSchedulerImpl/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.TaskSchedulerImpl logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskSchedulerImpl=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/TaskSchedulerImpl/#internal-properties","text":"[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | dagScheduler a| [[dagScheduler]] DAGScheduler.md[DAGScheduler] Used when...FIXME | executorIdToHost a| [[executorIdToHost]] Lookup table of hosts per executor. Used when...FIXME | executorIdToRunningTaskIds a| [[executorIdToRunningTaskIds]] Lookup table of running tasks per executor. Used when...FIXME | executorIdToTaskCount a| [[executorIdToTaskCount]] Lookup table of the number of running tasks by executor:Executor.md[]. | executorsByHost a| [[executorsByHost]] Collection of executor:Executor.md[executors] per host | hasLaunchedTask a| [[hasLaunchedTask]] Flag...FIXME Used when...FIXME | hostToExecutors a| [[hostToExecutors]] Lookup table of executors per hosts in a cluster. Used when...FIXME | hostsByRack a| [[hostsByRack]] Lookup table of hosts per rack. Used when...FIXME | nextTaskId a| [[nextTaskId]] The next scheduler:Task.md[task] id counting from 0 . Used when TaskSchedulerImpl... | rootPool a| [[rootPool]] Pool.md[Schedulable pool] Used when TaskSchedulerImpl... | schedulableBuilder a| [[schedulableBuilder]] < > Created when TaskSchedulerImpl is requested to < > and can be one of two available builders: FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] when scheduling policy is FIFO (which is the default scheduling policy). FairSchedulableBuilder.md[FairSchedulableBuilder] for FAIR scheduling policy. NOTE: Use configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property to select the scheduling policy. | schedulingMode a| [[schedulingMode]] SchedulingMode.md[SchedulingMode] Used when TaskSchedulerImpl... | taskSetsByStageIdAndAttempt a| [[taskSetsByStageIdAndAttempt]] Lookup table of scheduler:TaskSet.md[TaskSet] by stage and attempt ids. | taskIdToExecutorId a| [[taskIdToExecutorId]] Lookup table of executor:Executor.md[] by task id. | taskIdToTaskSetManager a| [[taskIdToTaskSetManager]] Registry of active scheduler:TaskSetManager.md[TaskSetManagers] per task id. |===","title":"Internal Properties"},{"location":"scheduler/TaskSet/","text":"TaskSet \u00b6 TaskSet is a collection of independent tasks of a stage (and a stage execution attempt ) that are missing ( uncomputed ), i.e. for which computation results are unavailable (as RDD blocks on BlockManagers on executors). In other words, a TaskSet represents the missing partitions of a stage that (as tasks) can be run right away based on the data that is already on the cluster, e.g. map output files from previous stages, though they may fail if this data becomes unavailable. Since the tasks are only the missing tasks, their number does not necessarily have to be the number of all the tasks of a stage . For a brand new stage (that has never been attempted to compute) their numbers are exactly the same. Once DAGScheduler submits the missing tasks for execution (to the TaskScheduler ), the execution of the TaskSet is managed by a TaskSetManager that allows for spark.task.maxFailures . Creating Instance \u00b6 TaskSet takes the following to be created: Task s Stage ID Stage (Execution) Attempt ID FIFO Priority Local Properties Resource Profile ID TaskSet is created when: DAGScheduler is requested to submit the missing tasks of a stage ID \u00b6 id : String TaskSet is uniquely identified by an id that uses the stageId followed by the stageAttemptId with the comma ( . ) in-between: [stageId].[stageAttemptId] Textual Representation \u00b6 toString : String toString follows the pattern: TaskSet [stageId].[stageAttemptId] Task Scheduling Prioritization (FIFO Scheduling) \u00b6 TaskSet is given a priority when created . The priority is the ID of the earliest-created active job that needs the stage (that is given when DAGScheduler is requested to submit the missing tasks of a stage ). Once submitted for execution, the priority is the priority of the TaskSetManager (which is a Schedulable ) that is used for task prioritization ( prioritizing scheduling of tasks ) in the FIFO scheduling mode.","title":"TaskSet"},{"location":"scheduler/TaskSet/#taskset","text":"TaskSet is a collection of independent tasks of a stage (and a stage execution attempt ) that are missing ( uncomputed ), i.e. for which computation results are unavailable (as RDD blocks on BlockManagers on executors). In other words, a TaskSet represents the missing partitions of a stage that (as tasks) can be run right away based on the data that is already on the cluster, e.g. map output files from previous stages, though they may fail if this data becomes unavailable. Since the tasks are only the missing tasks, their number does not necessarily have to be the number of all the tasks of a stage . For a brand new stage (that has never been attempted to compute) their numbers are exactly the same. Once DAGScheduler submits the missing tasks for execution (to the TaskScheduler ), the execution of the TaskSet is managed by a TaskSetManager that allows for spark.task.maxFailures .","title":"TaskSet"},{"location":"scheduler/TaskSet/#creating-instance","text":"TaskSet takes the following to be created: Task s Stage ID Stage (Execution) Attempt ID FIFO Priority Local Properties Resource Profile ID TaskSet is created when: DAGScheduler is requested to submit the missing tasks of a stage","title":"Creating Instance"},{"location":"scheduler/TaskSet/#id","text":"id : String TaskSet is uniquely identified by an id that uses the stageId followed by the stageAttemptId with the comma ( . ) in-between: [stageId].[stageAttemptId]","title":" ID"},{"location":"scheduler/TaskSet/#textual-representation","text":"toString : String toString follows the pattern: TaskSet [stageId].[stageAttemptId]","title":" Textual Representation"},{"location":"scheduler/TaskSet/#task-scheduling-prioritization-fifo-scheduling","text":"TaskSet is given a priority when created . The priority is the ID of the earliest-created active job that needs the stage (that is given when DAGScheduler is requested to submit the missing tasks of a stage ). Once submitted for execution, the priority is the priority of the TaskSetManager (which is a Schedulable ) that is used for task prioritization ( prioritizing scheduling of tasks ) in the FIFO scheduling mode.","title":" Task Scheduling Prioritization (FIFO Scheduling)"},{"location":"scheduler/TaskSetBlacklist/","text":"== [[TaskSetBlacklist]] TaskSetBlacklist -- Blacklisting Executors and Nodes For TaskSet CAUTION: FIXME === [[updateBlacklistForFailedTask]] updateBlacklistForFailedTask Method CAUTION: FIXME === [[isExecutorBlacklistedForTaskSet]] isExecutorBlacklistedForTaskSet Method CAUTION: FIXME === [[isNodeBlacklistedForTaskSet]] isNodeBlacklistedForTaskSet Method CAUTION: FIXME","title":"TaskSetBlacklist"},{"location":"scheduler/TaskSetManager/","text":"TaskSetManager \u00b6 TaskSetManager is a < > that manages scheduling of tasks of a < >. NOTE: A TaskSet.md[TaskSet] represents a set of Task.md[tasks] that correspond to missing spark-rdd-partitions.md[partitions] of a Stage.md[stage]. TaskSetManager is < > exclusively when TaskSchedulerImpl is requested to TaskSchedulerImpl.md#createTaskSetManager[create one] (when submitting tasks for a given TaskSet ). .TaskSetManager and its Dependencies image::TaskSetManager-TaskSchedulerImpl-TaskSet.png[align=\"center\"] When < > with a given < >, TaskSetManager < >. TaskSetManager is notified when a task (from the TaskSet it manages) finishes -- < > or due to a < > (in task execution or < >). TaskSetManager uses < > to control how many times a < > before an < TaskSet gets aborted>> that can take the following values: 1 for local/spark-local.md[ local run mode] maxFailures in local/spark-local.md#local-with-retries[Spark local-with-retries] (i.e. local[N, maxFailures] ) configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property for local/spark-local.md[Spark local-cluster] and spark-cluster.md[Spark clustered] (using Spark Standalone, Mesos and YARN) The responsibilities of a TaskSetManager include: < > < > < > [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.scheduler.TaskSchedulerImpl (or org.apache.spark.scheduler.cluster.YarnScheduler for YARN) and org.apache.spark.scheduler.TaskSetManager and execute the following two-stage job to see their low-level innerworkings. A cluster manager is recommended since it gives more task localization choices (with YARN additionally supporting rack localization). $ ./bin/spark-shell --master yarn --conf spark.ui.showConsoleProgress=false // Keep # partitions low to keep # messages low scala> sc.parallelize(0 to 9, 3).groupBy(_ % 3).count INFO YarnScheduler: Adding task set 0.0 with 3 tasks DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.2.87, executor 1, partition 0, PROCESS_LOCAL, 7541 bytes) INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.2.87, executor 2, partition 1, PROCESS_LOCAL, 7541 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 1 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.0.2.87, executor 1, partition 2, PROCESS_LOCAL, 7598 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 1 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 518 ms on 10.0.2.87 (executor 1) (1/3) INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 512 ms on 10.0.2.87 (executor 2) (2/3) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 51 ms on 10.0.2.87 (executor 1) (3/3) INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool INFO YarnScheduler: Adding task set 1.0 with 3 tasks DEBUG TaskSetManager: Epoch for TaskSet 1.0: 1 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NODE_LOCAL, RACK_LOCAL, ANY DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, 10.0.2.87, executor 2, partition 0, NODE_LOCAL, 7348 bytes) INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, 10.0.2.87, executor 1, partition 1, NODE_LOCAL, 7348 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 1 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, 10.0.2.87, executor 1, partition 2, NODE_LOCAL, 7348 bytes) INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 130 ms on 10.0.2.87 (executor 1) (1/3) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 1 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level RACK_LOCAL DEBUG TaskSetManager: No tasks for locality level RACK_LOCAL, so moving to locality level ANY INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 133 ms on 10.0.2.87 (executor 2) (2/3) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 0 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 21 ms on 10.0.2.87 (executor 1) (3/3) INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool res0: Long = 3 ==== [[internal-registries]] .TaskSetManager's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[allPendingTasks]] allPendingTasks | Indices of all the pending tasks to execute (regardless of their localization preferences). Updated with an task index when TaskSetManager < >. | [[calculatedTasks]] calculatedTasks | The number of the tasks that have already completed execution. Starts from 0 when a < TaskSetManager is created>> and is only incremented when the < TaskSetManager checks that there is enough memory to fetch a task result>>. | [[copiesRunning]] copiesRunning | The number of task copies currently running per task (index in its task set). The number of task copies of a task is increased when < > or < > and decreased when < > or < > (for a shuffle map stage and no external shuffle service). [[currentLocalityIndex]] currentLocalityIndex | [[epoch]] epoch | Current scheduler:MapOutputTracker.md#getEpoch[map output tracker epoch]. | [[failedExecutors]] failedExecutors | Lookup table of TaskInfo indices that failed to executor ids and the time of the failure. Used in < >. | [[isZombie]] isZombie | Disabled, i.e. false , by default. Read < > in this document. [[lastLaunchTime]] lastLaunchTime [[localityWaits]] localityWaits | [[myLocalityLevels]] myLocalityLevels | scheduler:TaskSchedulerImpl.md#TaskLocality[ TaskLocality locality preferences] of the pending tasks in the < > ranging from PROCESS_LOCAL through NODE_LOCAL , NO_PREF , and RACK_LOCAL to ANY . NOTE: myLocalityLevels may contain only a few of all the available TaskLocality preferences with ANY as a mandatory task locality preference. < > immediately when < TaskSetManager is created>>. < > every change in the status of executors. [[name]] name | [[numFailures]] numFailures | Array of the number of task failures per < >. Incremented when TaskSetManager < > and immediatelly checked if above < >. | [[numTasks]] numTasks | Number of < > to compute. | [[pendingTasksForExecutor]] pendingTasksForExecutor | Lookup table of the indices of tasks pending execution per executor. Updated with an task index and executor when TaskSetManager < > (and the location is a ExecutorCacheTaskLocation or HDFSCacheTaskLocation ). | [[pendingTasksForHost]] pendingTasksForHost | Lookup table of the indices of tasks pending execution per host. Updated with an task index and host when TaskSetManager < >. | [[pendingTasksForRack]] pendingTasksForRack | Lookup table of the indices of tasks pending execution per rack. Updated with an task index and rack when TaskSetManager < >. | [[pendingTasksWithNoPrefs]] pendingTasksWithNoPrefs | Lookup table of the indices of tasks pending execution with no location preferences. Updated with an task index when TaskSetManager < >. [[priority]] priority [[recentExceptions]] recentExceptions | [[runningTasksSet]] runningTasksSet | Collection of running tasks that a TaskSetManager manages. Used to implement < > (that is simply the size of runningTasksSet but a required part of any spark-scheduler-Schedulable.md#contract[Schedulable]). runningTasksSet is expanded when < > and shrinked when < >. Used in scheduler:TaskSchedulerImpl.md#cancelTasks[ TaskSchedulerImpl to cancel tasks]. [[speculatableTasks]] speculatableTasks | [[stageId]] stageId | The stage's id a TaskSetManager runs for. Set when < TaskSetManager is created>>. NOTE: stageId is part of spark-scheduler-Schedulable.md#contract[Schedulable contract]. | [[successful]] successful | Status of < > (with a boolean flag, i.e. true or false , per task). All tasks start with their flags disabled, i.e. false , when < TaskSetManager is created>>. The flag for a task is turned on, i.e. true , when a task finishes < > but also < >. A flag is explicitly turned off only for < ShuffleMapTask tasks when their executor is lost>>. | [[taskAttempts]] taskAttempts | Registry of TaskInfo s per every task attempt per task. | [[taskInfos]] taskInfos | Registry of TaskInfo s per task id. Updated with the task (id) and the corresponding TaskInfo when TaskSetManager < >. NOTE: It appears that the entires stay forever, i.e. are never removed (perhaps because the maintenance overhead is not needed given a TaskSetManager is a short-lived entity). | [[tasks]] tasks | Lookup table of scheduler:Task.md[Tasks] (per partition id) to schedule execution of. NOTE: The tasks all belong to a single < > that was given when < TaskSetManager was created>> (which actually represent a single scheduler:Stage.md[Stage]). [[tasksSuccessful]] tasksSuccessful | [[totalResultSize]] totalResultSize | The current total size of the result of all the tasks that have finished. Starts from 0 when < TaskSetManager is created>>. Only increased with the size of a task result whenever a TaskSetManager < >. |=== [[logging]] [TIP] ==== Enable DEBUG logging level for org.apache.spark.scheduler.TaskSetManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG Refer to spark-logging.md[Logging]. \u00b6 spark.driver.maxResultSize \u00b6 TaskSetManager uses spark.driver.maxResultSize configuration property to check available memory for more task results . === [[isTaskBlacklistedOnExecOrNode]] isTaskBlacklistedOnExecOrNode Internal Method [source, scala] \u00b6 isTaskBlacklistedOnExecOrNode( index: Int, execId: String, host: String): Boolean isTaskBlacklistedOnExecOrNode ...FIXME NOTE: isTaskBlacklistedOnExecOrNode is used when TaskSetManager is requested to < > and < >. === [[getLocalityIndex]] getLocalityIndex Method [source, scala] \u00b6 getLocalityIndex(locality: TaskLocality.TaskLocality): Int \u00b6 getLocalityIndex ...FIXME NOTE: getLocalityIndex is used when TaskSetManager is requested to < > and < >. === [[dequeueSpeculativeTask]] dequeueSpeculativeTask Internal Method [source, scala] \u00b6 dequeueSpeculativeTask( execId: String, host: String, locality: TaskLocality.Value): Option[(Int, TaskLocality.Value)] dequeueSpeculativeTask ...FIXME NOTE: dequeueSpeculativeTask is used exclusively when TaskSetManager is requested to < >. === [[executorAdded]] executorAdded Method [source, scala] \u00b6 executorAdded(): Unit \u00b6 executorAdded simply < >. NOTE: executorAdded is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers]. === [[abortIfCompletelyBlacklisted]] abortIfCompletelyBlacklisted Internal Method [source, scala] \u00b6 abortIfCompletelyBlacklisted( hostToExecutors: HashMap[String, HashSet[String]]): Unit abortIfCompletelyBlacklisted ...FIXME NOTE: abortIfCompletelyBlacklisted is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers]. === [[schedulable]] TaskSetManager is Schedulable TaskSetManager is a spark-scheduler-Schedulable.md[Schedulable] with the following implementation: name is TaskSet_[taskSet.stageId.toString] no parent is ever assigned, i.e. it is always null . + It means that it can only be a leaf in the tree of Schedulables (with spark-scheduler-Pool.md[Pools] being the nodes). schedulingMode always returns SchedulingMode.NONE (since there is nothing to schedule). weight is always 1 . minShare is always 0 . runningTasks is the number of running tasks in the internal runningTasksSet . priority is the priority of the owned scheduler:TaskSet.md[TaskSet] (using taskSet.priority ). stageId is the stage id of the owned scheduler:TaskSet.md[TaskSet] (using taskSet.stageId ). schedulableQueue returns no queue, i.e. null . addSchedulable and removeSchedulable do nothing. getSchedulableByName always returns null . getSortedTaskSetQueue returns a one-element collection with the sole element being itself. < > < > === [[handleTaskGettingResult]] Marking Task As Fetching Indirect Result -- handleTaskGettingResult Method [source, scala] \u00b6 handleTaskGettingResult(tid: Long): Unit \u00b6 handleTaskGettingResult finds TaskInfo for tid task in < > internal registry and marks it as fetching indirect task result. It then scheduler:DAGScheduler.md#taskGettingResult[notifies DAGScheduler ]. NOTE: handleTaskGettingResult is executed when scheduler:TaskSchedulerImpl.md#handleTaskGettingResult[ TaskSchedulerImpl is notified about fetching indirect task result]. === [[addRunningTask]] Registering Running Task -- addRunningTask Method [source, scala] \u00b6 addRunningTask(tid: Long): Unit \u00b6 addRunningTask adds tid to < > internal registry and spark-scheduler-Pool.md#increaseRunningTasks[requests the parent pool to increase the number of running tasks] (if defined). === [[removeRunningTask]] Unregistering Running Task -- removeRunningTask Method [source, scala] \u00b6 removeRunningTask(tid: Long): Unit \u00b6 removeRunningTask removes tid from < > internal registry and spark-scheduler-Pool.md#decreaseRunningTasks[requests the parent pool to decrease the number of running task] (if defined). === [[checkSpeculatableTasks]] Checking Speculatable Tasks -- checkSpeculatableTasks Method [source, scala] \u00b6 checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean \u00b6 NOTE: checkSpeculatableTasks is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. checkSpeculatableTasks checks whether there are speculatable tasks in a TaskSet . NOTE: checkSpeculatableTasks is called when for speculative-execution-of-tasks.md[]. If the TaskSetManager is < > or has a single task in TaskSet, it assumes no speculatable tasks. The method goes on with the assumption of no speculatable tasks by default. It computes the minimum number of finished tasks for speculation (as configuration-properties.md#spark.speculation.quantile[spark.speculation.quantile] of all the finished tasks). You should see the DEBUG message in the logs: DEBUG Checking for speculative tasks: minFinished = [minFinishedForSpeculation] It then checks whether the number is equal or greater than the number of tasks completed successfully (using tasksSuccessful ). Having done that, it computes the median duration of all the successfully completed tasks (using < taskInfos internal registry>>) and task length threshold using the median duration multiplied by configuration-properties.md#spark.speculation.multiplier[spark.speculation.multiplier] that has to be equal or less than 100 . You should see the DEBUG message in the logs: DEBUG Task length threshold for speculation: [threshold] For each task (using < taskInfos internal registry>>) that is not marked as successful yet (using successful ) for which there is only one copy running (using copiesRunning ) and the task takes more time than the calculated threshold, but it was not in speculatableTasks it is assumed speculatable . You should see the following INFO message in the logs: INFO Marking task [index] in stage [taskSet.id] (on [info.host]) as speculatable because it ran more than [threshold] ms The task gets added to the internal speculatableTasks collection. The method responds positively. === [[getAllowedLocalityLevel]] getAllowedLocalityLevel Internal Method [source, scala] \u00b6 getAllowedLocalityLevel(curTime: Long): TaskLocality.TaskLocality \u00b6 getAllowedLocalityLevel ...FIXME NOTE: getAllowedLocalityLevel is used exclusively when TaskSetManager is requested to < >. === [[resourceOffer]] Finding Task For Execution (Given Resource Offer) -- resourceOffer Method [source, scala] \u00b6 resourceOffer( execId: String, host: String, maxLocality: TaskLocality): Option[TaskDescription] (only if < > is defined) resourceOffer requests TaskSetBlacklist to check if the input spark-scheduler-TaskSetBlacklist.md#isExecutorBlacklistedForTaskSet[ execId executor] or spark-scheduler-TaskSetBlacklist.md#isNodeBlacklistedForTaskSet[ host node] are blacklisted. When TaskSetManager is a < > or the resource offer (as executor and host) is blacklisted, resourceOffer finds no tasks to execute (and returns no TaskDescription ). NOTE: resourceOffer finds a task to schedule for a resource offer when neither TaskSetManager is a < > nor the resource offer is blacklisted. resourceOffer calculates the allowed task locality for task selection. When the input maxLocality is not NO_PREF task locality, resourceOffer < > (for the current time) and sets it as the current task locality if more localized (specific). NOTE: scheduler:TaskSchedulerImpl.md[TaskLocality] can be the most localized PROCESS_LOCAL , NODE_LOCAL through NO_PREF and RACK_LOCAL to ANY . resourceOffer < >. If a task (index) is found, resourceOffer takes the scheduler:Task.md[Task] (from < > registry). resourceOffer scheduler:TaskSchedulerImpl.md#newTaskId[requests TaskSchedulerImpl for the id for the new task]. resourceOffer increments the < > and finds the task attempt number (as the size of < > entries for the task index). resourceOffer creates a TaskInfo that is then registered in < > and < >. If the maximum acceptable task locality is not NO_PREF , resourceOffer < > (using the task's locality) and records it as < > with the current time as < >. resourceOffer serializes the task. NOTE: resourceOffer uses core:SparkEnv.md#closureSerializer[ SparkEnv to access the closure Serializer ] and serializer:Serializer.md#newInstance[create an instance thereof]. If the task serialization fails, you should see the following ERROR message in the logs: Failed to serialize task [taskId], not attempting to retry it. resourceOffer < TaskSet >> with the following message and reports a TaskNotSerializableException . [options=\"wrap\"] \u00b6 Failed to serialize task [taskId], not attempting to retry it. Exception during serialization: [exception] \u00b6 resourceOffer checks the size of the serialized task. If it is greater than 100 kB, you should see the following WARN message in the logs: [options=\"wrap\"] \u00b6 WARN Stage [id] contains a task of very large size ([size] KB). The maximum recommended task size is 100 KB. \u00b6 NOTE: The size of the serializable task, i.e. 100 kB, is not configurable. If however the serialization went well and the size is fine too, resourceOffer < >. You should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO TaskSetManager: Starting [name] (TID [id], [host], executor [id], partition [id], [taskLocality], [size] bytes) \u00b6 For example: [options=\"wrap\"] \u00b6 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 2054 bytes) \u00b6 resourceOffer scheduler:DAGScheduler.md#taskStarted[notifies DAGScheduler that the task has been started]. IMPORTANT: This is the moment when TaskSetManager informs DAGScheduler that a task has started. NOTE: resourceOffer is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOfferSingleTaskSet[resourceOfferSingleTaskSet]. === [[dequeueTask]] Dequeueing Task For Execution (Given Locality Information) -- dequeueTask Internal Method [source, scala] \u00b6 dequeueTask(execId: String, host: String, maxLocality: TaskLocality): Option[(Int, TaskLocality, Boolean)] \u00b6 dequeueTask tries to < > (meeting localization requirements) using < execId executor>>. If a task is found, dequeueTask returns its index, PROCESS_LOCAL task locality and the speculative marker disabled. dequeueTask then goes over all the possible scheduler:TaskSchedulerImpl.md#TaskLocality[task localities] and checks what locality is allowed given the input maxLocality . dequeueTask checks out NODE_LOCAL , NO_PREF , RACK_LOCAL and ANY in that order. For NODE_LOCAL dequeueTask tries to < > (meeting localization requirements) using < host host>> and if found returns its index, NODE_LOCAL task locality and the speculative marker disabled. For NO_PREF dequeueTask tries to < > (meeting localization requirements) using < > internal registry and if found returns its index, PROCESS_LOCAL task locality and the speculative marker disabled. NOTE: For NO_PREF the task locality is PROCESS_LOCAL . For RACK_LOCAL dequeueTask scheduler:TaskSchedulerImpl.md#getRackForHost[finds the rack for the input host ] and if available tries to < > (meeting localization requirements) using < >. If a task is found, dequeueTask returns its index, RACK_LOCAL task locality and the speculative marker disabled. For ANY dequeueTask tries to < > (meeting localization requirements) using < > internal registry and if found returns its index, ANY task locality and the speculative marker disabled. In the end, when no task could be found, dequeueTask < > and if found returns its index, locality and the speculative marker enabled. NOTE: The speculative marker is enabled for a task only when dequeueTask did not manage to find a task for the available task localities and did find a speculative task. NOTE: dequeueTask is used exclusively when TaskSetManager is requested to < >. === [[dequeueTaskFromList]] Finding Higest Task Index (Not Blacklisted, With No Copies Running and Not Completed Already) -- dequeueTaskFromList Internal Method [source, scala] \u00b6 dequeueTaskFromList( execId: String, host: String, list: ArrayBuffer[Int]): Option[Int] dequeueTaskFromList takes task indices from the input list backwards (from the last to the first entry). For every index dequeueTaskFromList checks if it is not < execId executor and host >> and if not, checks that: < > is 0 the task has not been marked as < > If so, dequeueTaskFromList returns the task index. If dequeueTaskFromList has checked all the indices and no index has passed the checks, dequeueTaskFromList returns None (to indicate that no index has met the requirements). NOTE: dequeueTaskFromList is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForExecutor]] Finding Tasks (Indices) Registered For Execution on Executor -- getPendingTasksForExecutor Internal Method [source, scala] \u00b6 getPendingTasksForExecutor(executorId: String): ArrayBuffer[Int] \u00b6 getPendingTasksForExecutor finds pending tasks (indices) registered for execution on the input executorId executor (in < > internal registry). NOTE: getPendingTasksForExecutor may find no matching tasks and return an empty collection. NOTE: getPendingTasksForExecutor is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForHost]] Finding Tasks (Indices) Registered For Execution on Host -- getPendingTasksForHost Internal Method [source, scala] \u00b6 getPendingTasksForHost(host: String): ArrayBuffer[Int] \u00b6 getPendingTasksForHost finds pending tasks (indices) registered for execution on the input host host (in < > internal registry). NOTE: getPendingTasksForHost may find no matching tasks and return an empty collection. NOTE: getPendingTasksForHost is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForRack]] Finding Tasks (Indices) Registered For Execution on Rack -- getPendingTasksForRack Internal Method [source, scala] \u00b6 getPendingTasksForRack(rack: String): ArrayBuffer[Int] \u00b6 getPendingTasksForRack finds pending tasks (indices) registered for execution on the input rack rack (in < > internal registry). NOTE: getPendingTasksForRack may find no matching tasks and return an empty collection. NOTE: getPendingTasksForRack is used exclusively when TaskSetManager is requested to < >. === [[scheduling-tasks]] Scheduling Tasks in TaskSet CAUTION: FIXME For each submitted < >, a new TaskSetManager is created. The TaskSetManager completely and exclusively owns a TaskSet submitted for execution. CAUTION: FIXME A picture with TaskSetManager owning TaskSet CAUTION: FIXME What component knows about TaskSet and TaskSetManager. Isn't it that TaskSets are created by DAGScheduler while TaskSetManager is used by TaskSchedulerImpl only? TaskSetManager keeps track of the tasks pending execution per executor, host, rack or with no locality preferences. === [[locality-aware-scheduling]] Locality-Aware Scheduling aka Delay Scheduling TaskSetManager computes locality levels for the TaskSet for delay scheduling. While computing you should see the following DEBUG in the logs: DEBUG Valid locality levels for [taskSet]: [levels] CAUTION: FIXME What's delay scheduling? === [[events]] Events Once a task has finished, TaskSetManager informs scheduler:DAGScheduler.md#taskEnded[DAGScheduler]. CAUTION: FIXME === [[handleSuccessfulTask]] Recording Successful Task And Notifying DAGScheduler -- handleSuccessfulTask Method [source, scala] \u00b6 handleSuccessfulTask( tid: Long, result: DirectTaskResult[_]): Unit handleSuccessfulTask records the tid task as finished, scheduler:DAGScheduler.md#taskEnded[notifies the DAGScheduler that the task has ended] and < TaskSet finished>>. NOTE: handleSuccessfulTask is executed after scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[ TaskSchedulerImpl has been informed that tid task finished successfully (and the task result was deserialized)]. Internally, handleSuccessfulTask finds TaskInfo (in < > internal registry) and marks it as FINISHED . It then removes tid task from < > internal registry. handleSuccessfulTask scheduler:DAGScheduler.md#taskEnded[notifies DAGScheduler that tid task ended successfully] (with the Task object from < > internal registry and the result as Success ). At this point, handleSuccessfulTask finds the other < > of tid task and scheduler:SchedulerBackend.md#killTask[requests SchedulerBackend to kill them] (since they are no longer necessary now when at least one task attempt has completed successfully). You should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO Killing attempt [attemptNumber] for task [id] in stage [id] (TID [id]) on [host] as the attempt [attemptNumber] succeeded on [host] \u00b6 CAUTION: FIXME Review taskAttempts If tid has not yet been recorded as < >, handleSuccessfulTask increases < > counter. You should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO Finished task [id] in stage [id] (TID [taskId]) in [duration] ms on [host] (executor [executorId]) ([tasksSuccessful]/[numTasks]) \u00b6 tid task is marked as < >. If the number of task that have finished successfully is exactly the number of the tasks to execute (in the TaskSet ), the TaskSetManager becomes a < >. If tid task was already recorded as < >, you should merely see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO Ignoring task-finished event for [id] in stage [id] because task [index] has already completed successfully \u00b6 Ultimately, handleSuccessfulTask < TaskSet finished>>. NOTE: handleSuccessfulTask is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[handleSuccessfulTask]. === [[maybeFinishTaskSet]] Attempting to Mark TaskSet Finished -- maybeFinishTaskSet Internal Method [source, scala] \u00b6 maybeFinishTaskSet(): Unit \u00b6 maybeFinishTaskSet scheduler:TaskSchedulerImpl.md#taskSetFinished[notifies TaskSchedulerImpl that a TaskSet has finished] when there are no other < > and the < >. === [[task-retries]] Retrying Tasks on Failure CAUTION: FIXME Up to configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] attempts === Task retries and spark.task.maxFailures When you start Spark program you set up configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] for the number of failures that are acceptable until TaskSetManager gives up and marks a job failed. TIP: In Spark shell with local master, configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] is fixed to 1 and you need to use local/spark-local.md[local-with-retries master] to change it to some other value. In the following example, you are going to execute a job with two partitions and keep one failing at all times (by throwing an exception). The aim is to learn the behavior of retrying task execution in a stage in TaskSet. You will only look at a single task execution, namely 0.0 . $ ./bin/spark-shell --master \"local[*, 5]\" ... scala> sc.textFile(\"README.md\", 2).mapPartitionsWithIndex((idx, it) => if (idx == 0) throw new Exception(\"Partition 2 marked failed\") else it).count ... 15/10/27 17:24:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitionsWithIndex at <console>:25) 15/10/27 17:24:56 DEBUG DAGScheduler: New pending partitions: Set(0, 1) 15/10/27 17:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks ... 15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2062 bytes) ... 15/10/27 17:24:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 2) ... 15/10/27 17:24:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2062 bytes) 15/10/27 17:24:56 INFO Executor: Running task 0.1 in stage 1.0 (TID 4) 15/10/27 17:24:56 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/27 17:24:56 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 4) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 ERROR Executor: Exception in task 0.4 in stage 1.0 (TID 7) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 INFO TaskSetManager: Lost task 0.4 in stage 1.0 (TID 7) on executor localhost: java.lang.Exception (Partition 2 marked failed) [duplicate 4] 15/10/27 17:24:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 5 times; aborting job 15/10/27 17:24:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 15/10/27 17:24:56 INFO TaskSchedulerImpl: Cancelling stage 1 15/10/27 17:24:56 INFO DAGScheduler: ResultStage 1 (count at <console>:25) failed in 0.058 s 15/10/27 17:24:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0 15/10/27 17:24:56 INFO DAGScheduler: Job 1 failed: count at <console>:25, took 0.085810 s org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 5 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 7, localhost): java.lang.Exception: Partition 2 marked failed === [[zombie-state]] Zombie state A TaskSetManager is in zombie state when all tasks in a taskset have completed successfully (regardless of the number of task attempts), or if the taskset has been < >. While in zombie state, a TaskSetManager can launch no new tasks and < TaskDescription to resourceOffers>>. A TaskSetManager remains in the zombie state until all tasks have finished running, i.e. to continue to track and account for the running tasks. === [[abort]] Aborting TaskSet -- abort Method [source, scala] \u00b6 abort( message: String, exception: Option[Throwable] = None): Unit abort informs scheduler:DAGScheduler.md#taskSetFailed[ DAGScheduler that the TaskSet has been aborted]. CAUTION: FIXME image with DAGScheduler call The TaskSetManager enters < >. In the end, abort < TaskSet finished>>. abort is used when: TaskResultGetter is requested to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[enqueueSuccessfulTask] (that has failed) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#cancelTasks[cancelTasks] and scheduler:TaskSchedulerImpl.md#error[error] TaskSetManager is requested to < >, < >, < >, and < > DriverEndpoint is requested to launch tasks on executors Creating Instance \u00b6 TaskSetManager takes the following to be created: [[sched]] TaskSchedulerImpl.md[TaskSchedulerImpl] [[taskSet]] TaskSet.md[TaskSet] [[maxTaskFailures]] Number of task failures, i.e. how many times a < > before an entire TaskSet is < > [[blacklistTracker]] (optional) BlacklistTracker (default: None ) [[clock]] Clock (default: SystemClock ) TaskSetManager initializes the < >. NOTE: maxTaskFailures is 1 for local run mode, maxFailures for Spark local-with-retries, and configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property for Spark local-cluster and Spark with cluster managers (Spark Standalone, Mesos and YARN). TaskSetManager MapOutputTracker.md#getEpoch[requests the current epoch from MapOutputTracker ] and sets it on all tasks in the taskset. NOTE: TaskSetManager uses < > (that was given when < >) to TaskSchedulerImpl.md#mapOutputTracker[access the current MapOutputTracker ]. You should see the following DEBUG in the logs: DEBUG Epoch for [taskSet]: [epoch] CAUTION: FIXME Why is the epoch important? NOTE: TaskSetManager requests TaskSchedulerImpl.md#mapOutputTracker[ MapOutputTracker from TaskSchedulerImpl ] which is likely for unit testing only since core:SparkEnv.md#mapOutputTracker[ MapOutputTracker is available using SparkEnv ]. TaskSetManager < > (in reverse order from the highest partition to the lowest). CAUTION: FIXME Why is reverse order important? The code says it's to execute tasks with low indices first. === [[handleFailedTask]] Getting Notified that Task Failed -- handleFailedTask Method [source, scala] \u00b6 handleFailedTask( tid: Long, state: TaskState, reason: TaskFailedReason): Unit handleFailedTask finds TaskInfo of tid task in < > internal registry and simply quits if the task is already marked as failed or killed. .TaskSetManager Gets Notified that Task Has Failed image::TaskSetManager-handleFailedTask.png[align=\"center\"] NOTE: handleFailedTask is executed after TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl has been informed that tid task failed] or < >. In either case, tasks could not finish successfully or could not report their status back. handleFailedTask < tid task from the internal registry of running tasks>> and then marks the corresponding TaskInfo as finished (passing in the input state ). handleFailedTask decrements the number of the running copies of tid task (in < > internal registry). NOTE: With speculative-execution-of-tasks.md[] enabled, there can be many copies of a task running simultaneuosly. handleFailedTask uses the following pattern as the reason of the failure: Lost task [id] in stage [taskSetId] (TID [tid], [host], executor [executorId]): [reason] handleFailedTask then calculates the failure exception per the input reason (follow the links for more details): < > < > < > < > NOTE: Description of how the final failure exception is \"computed\" was moved to respective sections below to make the reading slightly more pleasant and comprehensible. handleFailedTask DAGScheduler.md#taskEnded[informs DAGScheduler that tid task has ended] (passing on the Task instance from < > internal registry, the input reason , null result, calculated accumUpdates per failure, and the TaskInfo ). IMPORTANT: This is the moment when TaskSetManager informs DAGScheduler that a task has ended. If tid task has already been marked as completed (in < > internal registry) you should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO Task [id] in stage [id] (TID [tid]) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded). \u00b6 TIP: Read up on speculative-execution-of-tasks.md[] to find out why a single task could be executed multiple times. If however tid task was not recorded as < >, handleFailedTask < >. If the TaskSetManager is not a < > and the task failed reason should be counted towards the maximum number of times the task is allowed to fail before the stage is aborted (i.e. TaskFailedReason.countTowardsTaskFailures attribute is enabled), the optional spark-scheduler-TaskSetBlacklist.md#updateBlacklistForFailedTask[ TaskSetBlacklist is notified] (passing on the host, executor and the task's index). handleFailedTask then increments the < > for tid task and checks if the number of failures is equal or greater than the < TaskSet >> (as defined when the < TaskSetManager was created>>). If so, i.e. the number of task failures of tid task reached the maximum value, you should see the following ERROR message in the logs: ERROR Task [id] in stage [id] failed [maxTaskFailures] times; aborting job And handleFailedTask < TaskSet >> with the following message and then quits: Task [index] in stage [id] failed [maxTaskFailures] times, most recent failure: [failureReason] In the end (except when the number of failures of tid task grew beyond the acceptable number), handleFailedTask < TaskSet as finished>>. [NOTE] \u00b6 handleFailedTask is used when: TaskSchedulerImpl is requested to TaskSchedulerImpl.md#handleFailedTask[handle a failed task] * TaskSetManager is requested to < > and < > \u00b6 ==== [[handleFailedTask-FetchFailed]] FetchFailed TaskFailedReason For FetchFailed you should see the following WARN message in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] Unless tid has already been marked as successful (in < > internal registry), it becomes so and the < TaskSet >> gets increased. The TaskSetManager enters < >. The failure exception is empty. ==== [[handleFailedTask-ExceptionFailure]] ExceptionFailure TaskFailedReason For ExceptionFailure , handleFailedTask checks if the exception is of type NotSerializableException . If so, you should see the following ERROR message in the logs: ERROR Task [id] in stage [id] (TID [tid]) had a not serializable result: [description]; not retrying And handleFailedTask < TaskSet >> and then quits. Otherwise, if the exception is not of type NotSerializableException , handleFailedTask accesses accumulators and calculates whether to print the WARN message (with the failure reason) or the INFO message. If the failure has already been reported (and is therefore a duplication), configuration-properties.md#spark.logging.exceptionPrintInterval[spark.logging.exceptionPrintInterval] is checked before reprinting the duplicate exception in its entirety. For full printout of the ExceptionFailure , the following WARN appears in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] Otherwise, the following INFO appears in the logs: INFO Lost task [id] in stage [id] (TID [tid]) on [host], executor [id]: [className] ([description]) [duplicate [dupCount]] The exception in ExceptionFailure becomes the failure exception. ==== [[handleFailedTask-ExecutorLostFailure]] ExecutorLostFailure TaskFailedReason For ExecutorLostFailure if not exitCausedByApp , you should see the following INFO in the logs: INFO Task [tid] failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task. The failure exception is empty. ==== [[handleFailedTask-TaskFailedReason]] Other TaskFailedReasons For the other TaskFailedReasons, you should see the following WARN message in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] The failure exception is empty. === [[addPendingTask]] Registering Task As Pending Execution (Per Preferred Locations) -- addPendingTask Internal Method [source, scala] \u00b6 addPendingTask(index: Int): Unit \u00b6 addPendingTask registers a index task in the pending-task lists that the task should be eventually scheduled to (per its preferred locations). Internally, addPendingTask takes the Task.md#preferredLocations[preferred locations of the task] (given index ) and registers the task in the internal pending-task registries for every preferred location: < > when the TaskLocation.md[TaskLocation] is ExecutorCacheTaskLocation . < > for the hosts of a TaskLocation.md[TaskLocation]. < > for the TaskSchedulerImpl.md#getRackForHost[racks from TaskSchedulerImpl per the host] (of a TaskLocation.md[TaskLocation]). For a TaskLocation.md[TaskLocation] being HDFSCacheTaskLocation , addPendingTask TaskSchedulerImpl.md#getExecutorsAliveOnHost[requests TaskSchedulerImpl for the executors on the host] (of a preferred location) and registers the task in < > for every executor (if available). You should see the following INFO message in the logs: INFO Pending task [index] has a cached location at [host] , where there are executors [executors] When addPendingTask could not find executors for a HDFSCacheTaskLocation preferred location, you should see the following DEBUG message in the logs: DEBUG Pending task [index] has a cached location at [host] , but there are no executors alive there. If the task has no location preferences, addPendingTask registers it in < >. addPendingTask always registers the task in < >. NOTE: addPendingTask is used immediatelly when TaskSetManager < > and later when handling a < > or < >. === [[executorLost]] Re-enqueuing ShuffleMapTasks (with no ExternalShuffleService) and Reporting All Running Tasks on Lost Executor as Failed -- executorLost Method [source, scala] \u00b6 executorLost(execId: String, host: String, reason: ExecutorLossReason): Unit \u00b6 executorLost re-enqueues all the ShuffleMapTask.md[ShuffleMapTasks] that have completed already on the lost executor (when external shuffle service is not in use) and < >. NOTE: executorLost is part of the spark-scheduler-Schedulable.md#contract[Schedulable contract] that TaskSchedulerImpl.md#removeExecutor[ TaskSchedulerImpl uses to inform TaskSetManagers about lost executors]. NOTE: Since TaskSetManager manages execution of the tasks in a single TaskSet.md[TaskSet], when an executor gets lost, the affected tasks that have been running on the failed executor need to be re-enqueued. executorLost is the mechanism to \"announce\" the event to all TaskSetManagers . Internally, executorLost first checks whether the < > are ShuffleMapTask.md[ShuffleMapTasks] and whether an external shuffle service is enabled (that could serve the map shuffle outputs in case of failure). NOTE: executorLost checks out the first task in < > as it is assumed the other belong to the same stage. If the task is a ShuffleMapTask.md[ShuffleMapTask], the entire < > is for a ShuffleMapStage.md[ShuffleMapStage]. NOTE: executorLost uses core:SparkEnv.md#blockManager[ SparkEnv to access the current BlockManager ] and finds out whether an storage:BlockManager.md#externalShuffleServiceEnabled[external shuffle service is enabled] or not (based on spark.shuffle.service.enabled configuration property). If executorLost is indeed due to an executor lost that executed tasks for a ShuffleMapStage.md[ShuffleMapStage] (that this TaskSetManager manages) and no external shuffle server is enabled, executorLost finds < > that were scheduled on this lost executor and marks the < > as not executed yet. NOTE: executorLost uses records every tasks on the lost executor in < > (as false ) and decrements < >, and < > for every task. executorLost < > and DAGScheduler.md#taskEnded[informs DAGScheduler that the tasks (on the lost executor) have ended] (with DAGScheduler.md#handleTaskCompletion-Resubmitted[Resubmitted] reason). NOTE: executorLost uses TaskSchedulerImpl.md#dagScheduler[ TaskSchedulerImpl to access the DAGScheduler ]. TaskSchedulerImpl is given when the < TaskSetManager was created>>. Regardless of whether this TaskSetManager manages ShuffleMapTasks or not (it could also manage ResultTask.md[ResultTasks]) and whether the external shuffle service is used or not, executorLost finds all < > on this lost executor and < > (with the task state FAILED ). NOTE: executorLost finds out if the reason for the executor lost is due to application fault, i.e. assumes ExecutorExited 's exit status as the indicator, ExecutorKilled for non-application's fault and any other reason is an application fault. executorLost < >. === [[recomputeLocality]] Recomputing Task Locality Preferences -- recomputeLocality Method [source, scala] \u00b6 recomputeLocality(): Unit \u00b6 recomputeLocality recomputes the internal caches: < >, < > and < >. CAUTION: FIXME But why are the caches important (and have to be recomputed)? recomputeLocality records the current TaskSchedulerImpl.md#TaskLocality[TaskLocality] level of this TaskSetManager (that is < > in < >). NOTE: TaskLocality is one of PROCESS_LOCAL , NODE_LOCAL , NO_PREF , RACK_LOCAL and ANY values. recomputeLocality < > and saves the result in < > internal cache. recomputeLocality computes < > (by < > for every locality level in < > internal cache). In the end, recomputeLocality < > of the previous locality level and records it in < >. NOTE: recomputeLocality is used when TaskSetManager gets notified about status change in executors, i.e. when an executor is < > or < >. === [[computeValidLocalityLevels]] Computing Locality Levels (for Scheduled Tasks) -- computeValidLocalityLevels Internal Method [source, scala] \u00b6 computeValidLocalityLevels(): Array[TaskLocality] \u00b6 computeValidLocalityLevels computes valid locality levels for tasks that were registered in corresponding registries per locality level. NOTE: TaskSchedulerImpl.md[TaskLocality] is a task locality preference and can be the most localized PROCESS_LOCAL , NODE_LOCAL through NO_PREF and RACK_LOCAL to ANY . .TaskLocalities and Corresponding Internal Registries [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskLocality | Internal Registry | PROCESS_LOCAL | < > | NODE_LOCAL | < > | NO_PREF | < > | RACK_LOCAL | < > |=== computeValidLocalityLevels walks over every internal registry and if it is not empty < > for the corresponding TaskLocality and proceeds with it only when the locality wait is not 0 . For TaskLocality with pending tasks, computeValidLocalityLevels asks TaskSchedulerImpl whether there is at least one executor alive (for TaskSchedulerImpl.md#isExecutorAlive[PROCESS_LOCAL], TaskSchedulerImpl.md#hasExecutorsAliveOnHost[NODE_LOCAL] and TaskSchedulerImpl.md#hasHostAliveOnRack[RACK_LOCAL]) and if so registers the TaskLocality . NOTE: computeValidLocalityLevels uses < > that was given when < TaskSetManager was created>>. computeValidLocalityLevels always registers ANY task locality level. In the end, you should see the following DEBUG message in the logs: DEBUG TaskSetManager: Valid locality levels for [taskSet]: [comma-separated levels] NOTE: computeValidLocalityLevels is used when TaskSetManager < > and later to < >. === [[getLocalityWait]] Finding Locality Wait -- getLocalityWait Internal Method [source, scala] \u00b6 getLocalityWait(level: TaskLocality): Long \u00b6 getLocalityWait finds locality wait (in milliseconds) for a given TaskSchedulerImpl.md#TaskLocality[TaskLocality]. getLocalityWait uses configuration-properties.md#spark.locality.wait[spark.locality.wait] (default: 3s ) when the TaskLocality -specific property is not defined or 0 for NO_PREF and ANY . NOTE: NO_PREF and ANY task localities have no locality wait. .TaskLocalities and Corresponding Spark Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskLocality | Spark Property | PROCESS_LOCAL | configuration-properties.md#spark.locality.wait.process[spark.locality.wait.process] | NODE_LOCAL | configuration-properties.md#spark.locality.wait.node[spark.locality.wait.node] | RACK_LOCAL | configuration-properties.md#spark.locality.wait.rack[spark.locality.wait.rack] |=== NOTE: getLocalityWait is used when TaskSetManager calculates < >, < > and < >. Checking Available Memory For More Task Results \u00b6 canFetchMoreResults ( size : Long ): Boolean canFetchMoreResults checks whether there is enough memory to fetch the result of a task. Internally, canFetchMoreResults increments the internal < > with the input size (which is the size of the result of a task) and increments the internal < >. If the current internal < > is bigger than the configuration-properties.md#maxResultSize[maximum result size], canFetchMoreResults prints out the following ERROR message to the logs: Total size of serialized results of [calculatedTasks] tasks ([totalResultSize]) is bigger than spark.driver.maxResultSize ([maxResultSize]) In the end, canFetchMoreResults < > the < > and returns false . Otherwise, canFetchMoreResults returns true . canFetchMoreResults is used when TaskResultGetter is requested to enqueue a successful task .","title":"TaskSetManager"},{"location":"scheduler/TaskSetManager/#tasksetmanager","text":"TaskSetManager is a < > that manages scheduling of tasks of a < >. NOTE: A TaskSet.md[TaskSet] represents a set of Task.md[tasks] that correspond to missing spark-rdd-partitions.md[partitions] of a Stage.md[stage]. TaskSetManager is < > exclusively when TaskSchedulerImpl is requested to TaskSchedulerImpl.md#createTaskSetManager[create one] (when submitting tasks for a given TaskSet ). .TaskSetManager and its Dependencies image::TaskSetManager-TaskSchedulerImpl-TaskSet.png[align=\"center\"] When < > with a given < >, TaskSetManager < >. TaskSetManager is notified when a task (from the TaskSet it manages) finishes -- < > or due to a < > (in task execution or < >). TaskSetManager uses < > to control how many times a < > before an < TaskSet gets aborted>> that can take the following values: 1 for local/spark-local.md[ local run mode] maxFailures in local/spark-local.md#local-with-retries[Spark local-with-retries] (i.e. local[N, maxFailures] ) configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property for local/spark-local.md[Spark local-cluster] and spark-cluster.md[Spark clustered] (using Spark Standalone, Mesos and YARN) The responsibilities of a TaskSetManager include: < > < > < >","title":"TaskSetManager"},{"location":"scheduler/TaskSetManager/#tip","text":"Enable DEBUG logging level for org.apache.spark.scheduler.TaskSchedulerImpl (or org.apache.spark.scheduler.cluster.YarnScheduler for YARN) and org.apache.spark.scheduler.TaskSetManager and execute the following two-stage job to see their low-level innerworkings. A cluster manager is recommended since it gives more task localization choices (with YARN additionally supporting rack localization). $ ./bin/spark-shell --master yarn --conf spark.ui.showConsoleProgress=false // Keep # partitions low to keep # messages low scala> sc.parallelize(0 to 9, 3).groupBy(_ % 3).count INFO YarnScheduler: Adding task set 0.0 with 3 tasks DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.2.87, executor 1, partition 0, PROCESS_LOCAL, 7541 bytes) INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.2.87, executor 2, partition 1, PROCESS_LOCAL, 7541 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 1 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.0.2.87, executor 1, partition 2, PROCESS_LOCAL, 7598 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 1 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 518 ms on 10.0.2.87 (executor 1) (1/3) INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 512 ms on 10.0.2.87 (executor 2) (2/3) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 51 ms on 10.0.2.87 (executor 1) (3/3) INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool INFO YarnScheduler: Adding task set 1.0 with 3 tasks DEBUG TaskSetManager: Epoch for TaskSet 1.0: 1 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NODE_LOCAL, RACK_LOCAL, ANY DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, 10.0.2.87, executor 2, partition 0, NODE_LOCAL, 7348 bytes) INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, 10.0.2.87, executor 1, partition 1, NODE_LOCAL, 7348 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 1 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, 10.0.2.87, executor 1, partition 2, NODE_LOCAL, 7348 bytes) INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 130 ms on 10.0.2.87 (executor 1) (1/3) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 1 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level RACK_LOCAL DEBUG TaskSetManager: No tasks for locality level RACK_LOCAL, so moving to locality level ANY INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 133 ms on 10.0.2.87 (executor 2) (2/3) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 0 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 21 ms on 10.0.2.87 (executor 1) (3/3) INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool res0: Long = 3 ==== [[internal-registries]] .TaskSetManager's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[allPendingTasks]] allPendingTasks | Indices of all the pending tasks to execute (regardless of their localization preferences). Updated with an task index when TaskSetManager < >. | [[calculatedTasks]] calculatedTasks | The number of the tasks that have already completed execution. Starts from 0 when a < TaskSetManager is created>> and is only incremented when the < TaskSetManager checks that there is enough memory to fetch a task result>>. | [[copiesRunning]] copiesRunning | The number of task copies currently running per task (index in its task set). The number of task copies of a task is increased when < > or < > and decreased when < > or < > (for a shuffle map stage and no external shuffle service). [[currentLocalityIndex]] currentLocalityIndex | [[epoch]] epoch | Current scheduler:MapOutputTracker.md#getEpoch[map output tracker epoch]. | [[failedExecutors]] failedExecutors | Lookup table of TaskInfo indices that failed to executor ids and the time of the failure. Used in < >. | [[isZombie]] isZombie | Disabled, i.e. false , by default. Read < > in this document. [[lastLaunchTime]] lastLaunchTime [[localityWaits]] localityWaits | [[myLocalityLevels]] myLocalityLevels | scheduler:TaskSchedulerImpl.md#TaskLocality[ TaskLocality locality preferences] of the pending tasks in the < > ranging from PROCESS_LOCAL through NODE_LOCAL , NO_PREF , and RACK_LOCAL to ANY . NOTE: myLocalityLevels may contain only a few of all the available TaskLocality preferences with ANY as a mandatory task locality preference. < > immediately when < TaskSetManager is created>>. < > every change in the status of executors. [[name]] name | [[numFailures]] numFailures | Array of the number of task failures per < >. Incremented when TaskSetManager < > and immediatelly checked if above < >. | [[numTasks]] numTasks | Number of < > to compute. | [[pendingTasksForExecutor]] pendingTasksForExecutor | Lookup table of the indices of tasks pending execution per executor. Updated with an task index and executor when TaskSetManager < > (and the location is a ExecutorCacheTaskLocation or HDFSCacheTaskLocation ). | [[pendingTasksForHost]] pendingTasksForHost | Lookup table of the indices of tasks pending execution per host. Updated with an task index and host when TaskSetManager < >. | [[pendingTasksForRack]] pendingTasksForRack | Lookup table of the indices of tasks pending execution per rack. Updated with an task index and rack when TaskSetManager < >. | [[pendingTasksWithNoPrefs]] pendingTasksWithNoPrefs | Lookup table of the indices of tasks pending execution with no location preferences. Updated with an task index when TaskSetManager < >. [[priority]] priority [[recentExceptions]] recentExceptions | [[runningTasksSet]] runningTasksSet | Collection of running tasks that a TaskSetManager manages. Used to implement < > (that is simply the size of runningTasksSet but a required part of any spark-scheduler-Schedulable.md#contract[Schedulable]). runningTasksSet is expanded when < > and shrinked when < >. Used in scheduler:TaskSchedulerImpl.md#cancelTasks[ TaskSchedulerImpl to cancel tasks]. [[speculatableTasks]] speculatableTasks | [[stageId]] stageId | The stage's id a TaskSetManager runs for. Set when < TaskSetManager is created>>. NOTE: stageId is part of spark-scheduler-Schedulable.md#contract[Schedulable contract]. | [[successful]] successful | Status of < > (with a boolean flag, i.e. true or false , per task). All tasks start with their flags disabled, i.e. false , when < TaskSetManager is created>>. The flag for a task is turned on, i.e. true , when a task finishes < > but also < >. A flag is explicitly turned off only for < ShuffleMapTask tasks when their executor is lost>>. | [[taskAttempts]] taskAttempts | Registry of TaskInfo s per every task attempt per task. | [[taskInfos]] taskInfos | Registry of TaskInfo s per task id. Updated with the task (id) and the corresponding TaskInfo when TaskSetManager < >. NOTE: It appears that the entires stay forever, i.e. are never removed (perhaps because the maintenance overhead is not needed given a TaskSetManager is a short-lived entity). | [[tasks]] tasks | Lookup table of scheduler:Task.md[Tasks] (per partition id) to schedule execution of. NOTE: The tasks all belong to a single < > that was given when < TaskSetManager was created>> (which actually represent a single scheduler:Stage.md[Stage]). [[tasksSuccessful]] tasksSuccessful | [[totalResultSize]] totalResultSize | The current total size of the result of all the tasks that have finished. Starts from 0 when < TaskSetManager is created>>. Only increased with the size of a task result whenever a TaskSetManager < >. |=== [[logging]] [TIP] ==== Enable DEBUG logging level for org.apache.spark.scheduler.TaskSetManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG","title":"[TIP]"},{"location":"scheduler/TaskSetManager/#refer-to-spark-loggingmdlogging","text":"","title":"Refer to spark-logging.md[Logging]."},{"location":"scheduler/TaskSetManager/#sparkdrivermaxresultsize","text":"TaskSetManager uses spark.driver.maxResultSize configuration property to check available memory for more task results . === [[isTaskBlacklistedOnExecOrNode]] isTaskBlacklistedOnExecOrNode Internal Method","title":" spark.driver.maxResultSize"},{"location":"scheduler/TaskSetManager/#source-scala","text":"isTaskBlacklistedOnExecOrNode( index: Int, execId: String, host: String): Boolean isTaskBlacklistedOnExecOrNode ...FIXME NOTE: isTaskBlacklistedOnExecOrNode is used when TaskSetManager is requested to < > and < >. === [[getLocalityIndex]] getLocalityIndex Method","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getlocalityindexlocality-tasklocalitytasklocality-int","text":"getLocalityIndex ...FIXME NOTE: getLocalityIndex is used when TaskSetManager is requested to < > and < >. === [[dequeueSpeculativeTask]] dequeueSpeculativeTask Internal Method","title":"getLocalityIndex(locality: TaskLocality.TaskLocality): Int"},{"location":"scheduler/TaskSetManager/#source-scala_2","text":"dequeueSpeculativeTask( execId: String, host: String, locality: TaskLocality.Value): Option[(Int, TaskLocality.Value)] dequeueSpeculativeTask ...FIXME NOTE: dequeueSpeculativeTask is used exclusively when TaskSetManager is requested to < >. === [[executorAdded]] executorAdded Method","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#executoradded-unit","text":"executorAdded simply < >. NOTE: executorAdded is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers]. === [[abortIfCompletelyBlacklisted]] abortIfCompletelyBlacklisted Internal Method","title":"executorAdded(): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_4","text":"abortIfCompletelyBlacklisted( hostToExecutors: HashMap[String, HashSet[String]]): Unit abortIfCompletelyBlacklisted ...FIXME NOTE: abortIfCompletelyBlacklisted is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers]. === [[schedulable]] TaskSetManager is Schedulable TaskSetManager is a spark-scheduler-Schedulable.md[Schedulable] with the following implementation: name is TaskSet_[taskSet.stageId.toString] no parent is ever assigned, i.e. it is always null . + It means that it can only be a leaf in the tree of Schedulables (with spark-scheduler-Pool.md[Pools] being the nodes). schedulingMode always returns SchedulingMode.NONE (since there is nothing to schedule). weight is always 1 . minShare is always 0 . runningTasks is the number of running tasks in the internal runningTasksSet . priority is the priority of the owned scheduler:TaskSet.md[TaskSet] (using taskSet.priority ). stageId is the stage id of the owned scheduler:TaskSet.md[TaskSet] (using taskSet.stageId ). schedulableQueue returns no queue, i.e. null . addSchedulable and removeSchedulable do nothing. getSchedulableByName always returns null . getSortedTaskSetQueue returns a one-element collection with the sole element being itself. < > < > === [[handleTaskGettingResult]] Marking Task As Fetching Indirect Result -- handleTaskGettingResult Method","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#handletaskgettingresulttid-long-unit","text":"handleTaskGettingResult finds TaskInfo for tid task in < > internal registry and marks it as fetching indirect task result. It then scheduler:DAGScheduler.md#taskGettingResult[notifies DAGScheduler ]. NOTE: handleTaskGettingResult is executed when scheduler:TaskSchedulerImpl.md#handleTaskGettingResult[ TaskSchedulerImpl is notified about fetching indirect task result]. === [[addRunningTask]] Registering Running Task -- addRunningTask Method","title":"handleTaskGettingResult(tid: Long): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#addrunningtasktid-long-unit","text":"addRunningTask adds tid to < > internal registry and spark-scheduler-Pool.md#increaseRunningTasks[requests the parent pool to increase the number of running tasks] (if defined). === [[removeRunningTask]] Unregistering Running Task -- removeRunningTask Method","title":"addRunningTask(tid: Long): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#removerunningtasktid-long-unit","text":"removeRunningTask removes tid from < > internal registry and spark-scheduler-Pool.md#decreaseRunningTasks[requests the parent pool to decrease the number of running task] (if defined). === [[checkSpeculatableTasks]] Checking Speculatable Tasks -- checkSpeculatableTasks Method","title":"removeRunningTask(tid: Long): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#checkspeculatabletasksmintimetospeculation-int-boolean","text":"NOTE: checkSpeculatableTasks is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. checkSpeculatableTasks checks whether there are speculatable tasks in a TaskSet . NOTE: checkSpeculatableTasks is called when for speculative-execution-of-tasks.md[]. If the TaskSetManager is < > or has a single task in TaskSet, it assumes no speculatable tasks. The method goes on with the assumption of no speculatable tasks by default. It computes the minimum number of finished tasks for speculation (as configuration-properties.md#spark.speculation.quantile[spark.speculation.quantile] of all the finished tasks). You should see the DEBUG message in the logs: DEBUG Checking for speculative tasks: minFinished = [minFinishedForSpeculation] It then checks whether the number is equal or greater than the number of tasks completed successfully (using tasksSuccessful ). Having done that, it computes the median duration of all the successfully completed tasks (using < taskInfos internal registry>>) and task length threshold using the median duration multiplied by configuration-properties.md#spark.speculation.multiplier[spark.speculation.multiplier] that has to be equal or less than 100 . You should see the DEBUG message in the logs: DEBUG Task length threshold for speculation: [threshold] For each task (using < taskInfos internal registry>>) that is not marked as successful yet (using successful ) for which there is only one copy running (using copiesRunning ) and the task takes more time than the calculated threshold, but it was not in speculatableTasks it is assumed speculatable . You should see the following INFO message in the logs: INFO Marking task [index] in stage [taskSet.id] (on [info.host]) as speculatable because it ran more than [threshold] ms The task gets added to the internal speculatableTasks collection. The method responds positively. === [[getAllowedLocalityLevel]] getAllowedLocalityLevel Internal Method","title":"checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean"},{"location":"scheduler/TaskSetManager/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getallowedlocalitylevelcurtime-long-tasklocalitytasklocality","text":"getAllowedLocalityLevel ...FIXME NOTE: getAllowedLocalityLevel is used exclusively when TaskSetManager is requested to < >. === [[resourceOffer]] Finding Task For Execution (Given Resource Offer) -- resourceOffer Method","title":"getAllowedLocalityLevel(curTime: Long): TaskLocality.TaskLocality"},{"location":"scheduler/TaskSetManager/#source-scala_10","text":"resourceOffer( execId: String, host: String, maxLocality: TaskLocality): Option[TaskDescription] (only if < > is defined) resourceOffer requests TaskSetBlacklist to check if the input spark-scheduler-TaskSetBlacklist.md#isExecutorBlacklistedForTaskSet[ execId executor] or spark-scheduler-TaskSetBlacklist.md#isNodeBlacklistedForTaskSet[ host node] are blacklisted. When TaskSetManager is a < > or the resource offer (as executor and host) is blacklisted, resourceOffer finds no tasks to execute (and returns no TaskDescription ). NOTE: resourceOffer finds a task to schedule for a resource offer when neither TaskSetManager is a < > nor the resource offer is blacklisted. resourceOffer calculates the allowed task locality for task selection. When the input maxLocality is not NO_PREF task locality, resourceOffer < > (for the current time) and sets it as the current task locality if more localized (specific). NOTE: scheduler:TaskSchedulerImpl.md[TaskLocality] can be the most localized PROCESS_LOCAL , NODE_LOCAL through NO_PREF and RACK_LOCAL to ANY . resourceOffer < >. If a task (index) is found, resourceOffer takes the scheduler:Task.md[Task] (from < > registry). resourceOffer scheduler:TaskSchedulerImpl.md#newTaskId[requests TaskSchedulerImpl for the id for the new task]. resourceOffer increments the < > and finds the task attempt number (as the size of < > entries for the task index). resourceOffer creates a TaskInfo that is then registered in < > and < >. If the maximum acceptable task locality is not NO_PREF , resourceOffer < > (using the task's locality) and records it as < > with the current time as < >. resourceOffer serializes the task. NOTE: resourceOffer uses core:SparkEnv.md#closureSerializer[ SparkEnv to access the closure Serializer ] and serializer:Serializer.md#newInstance[create an instance thereof]. If the task serialization fails, you should see the following ERROR message in the logs: Failed to serialize task [taskId], not attempting to retry it. resourceOffer < TaskSet >> with the following message and reports a TaskNotSerializableException .","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#failed-to-serialize-task-taskid-not-attempting-to-retry-it-exception-during-serialization-exception","text":"resourceOffer checks the size of the serialized task. If it is greater than 100 kB, you should see the following WARN message in the logs:","title":"Failed to serialize task [taskId], not attempting to retry it. Exception during serialization: [exception]"},{"location":"scheduler/TaskSetManager/#optionswrap_1","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#warn-stage-id-contains-a-task-of-very-large-size-size-kb-the-maximum-recommended-task-size-is-100-kb","text":"NOTE: The size of the serializable task, i.e. 100 kB, is not configurable. If however the serialization went well and the size is fine too, resourceOffer < >. You should see the following INFO message in the logs:","title":"WARN Stage [id] contains a task of very large size ([size] KB). The maximum recommended task size is 100 KB."},{"location":"scheduler/TaskSetManager/#optionswrap_2","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-tasksetmanager-starting-name-tid-id-host-executor-id-partition-id-tasklocality-size-bytes","text":"For example:","title":"INFO TaskSetManager: Starting [name] (TID [id], [host], executor [id], partition [id], [taskLocality], [size] bytes)"},{"location":"scheduler/TaskSetManager/#optionswrap_3","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-tasksetmanager-starting-task-10-in-stage-00-tid-1-localhost-partition-1-process_local-2054-bytes","text":"resourceOffer scheduler:DAGScheduler.md#taskStarted[notifies DAGScheduler that the task has been started]. IMPORTANT: This is the moment when TaskSetManager informs DAGScheduler that a task has started. NOTE: resourceOffer is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOfferSingleTaskSet[resourceOfferSingleTaskSet]. === [[dequeueTask]] Dequeueing Task For Execution (Given Locality Information) -- dequeueTask Internal Method","title":"INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 2054 bytes)"},{"location":"scheduler/TaskSetManager/#source-scala_11","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#dequeuetaskexecid-string-host-string-maxlocality-tasklocality-optionint-tasklocality-boolean","text":"dequeueTask tries to < > (meeting localization requirements) using < execId executor>>. If a task is found, dequeueTask returns its index, PROCESS_LOCAL task locality and the speculative marker disabled. dequeueTask then goes over all the possible scheduler:TaskSchedulerImpl.md#TaskLocality[task localities] and checks what locality is allowed given the input maxLocality . dequeueTask checks out NODE_LOCAL , NO_PREF , RACK_LOCAL and ANY in that order. For NODE_LOCAL dequeueTask tries to < > (meeting localization requirements) using < host host>> and if found returns its index, NODE_LOCAL task locality and the speculative marker disabled. For NO_PREF dequeueTask tries to < > (meeting localization requirements) using < > internal registry and if found returns its index, PROCESS_LOCAL task locality and the speculative marker disabled. NOTE: For NO_PREF the task locality is PROCESS_LOCAL . For RACK_LOCAL dequeueTask scheduler:TaskSchedulerImpl.md#getRackForHost[finds the rack for the input host ] and if available tries to < > (meeting localization requirements) using < >. If a task is found, dequeueTask returns its index, RACK_LOCAL task locality and the speculative marker disabled. For ANY dequeueTask tries to < > (meeting localization requirements) using < > internal registry and if found returns its index, ANY task locality and the speculative marker disabled. In the end, when no task could be found, dequeueTask < > and if found returns its index, locality and the speculative marker enabled. NOTE: The speculative marker is enabled for a task only when dequeueTask did not manage to find a task for the available task localities and did find a speculative task. NOTE: dequeueTask is used exclusively when TaskSetManager is requested to < >. === [[dequeueTaskFromList]] Finding Higest Task Index (Not Blacklisted, With No Copies Running and Not Completed Already) -- dequeueTaskFromList Internal Method","title":"dequeueTask(execId: String, host: String, maxLocality: TaskLocality): Option[(Int, TaskLocality, Boolean)]"},{"location":"scheduler/TaskSetManager/#source-scala_12","text":"dequeueTaskFromList( execId: String, host: String, list: ArrayBuffer[Int]): Option[Int] dequeueTaskFromList takes task indices from the input list backwards (from the last to the first entry). For every index dequeueTaskFromList checks if it is not < execId executor and host >> and if not, checks that: < > is 0 the task has not been marked as < > If so, dequeueTaskFromList returns the task index. If dequeueTaskFromList has checked all the indices and no index has passed the checks, dequeueTaskFromList returns None (to indicate that no index has met the requirements). NOTE: dequeueTaskFromList is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForExecutor]] Finding Tasks (Indices) Registered For Execution on Executor -- getPendingTasksForExecutor Internal Method","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#source-scala_13","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getpendingtasksforexecutorexecutorid-string-arraybufferint","text":"getPendingTasksForExecutor finds pending tasks (indices) registered for execution on the input executorId executor (in < > internal registry). NOTE: getPendingTasksForExecutor may find no matching tasks and return an empty collection. NOTE: getPendingTasksForExecutor is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForHost]] Finding Tasks (Indices) Registered For Execution on Host -- getPendingTasksForHost Internal Method","title":"getPendingTasksForExecutor(executorId: String): ArrayBuffer[Int]"},{"location":"scheduler/TaskSetManager/#source-scala_14","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getpendingtasksforhosthost-string-arraybufferint","text":"getPendingTasksForHost finds pending tasks (indices) registered for execution on the input host host (in < > internal registry). NOTE: getPendingTasksForHost may find no matching tasks and return an empty collection. NOTE: getPendingTasksForHost is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForRack]] Finding Tasks (Indices) Registered For Execution on Rack -- getPendingTasksForRack Internal Method","title":"getPendingTasksForHost(host: String): ArrayBuffer[Int]"},{"location":"scheduler/TaskSetManager/#source-scala_15","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getpendingtasksforrackrack-string-arraybufferint","text":"getPendingTasksForRack finds pending tasks (indices) registered for execution on the input rack rack (in < > internal registry). NOTE: getPendingTasksForRack may find no matching tasks and return an empty collection. NOTE: getPendingTasksForRack is used exclusively when TaskSetManager is requested to < >. === [[scheduling-tasks]] Scheduling Tasks in TaskSet CAUTION: FIXME For each submitted < >, a new TaskSetManager is created. The TaskSetManager completely and exclusively owns a TaskSet submitted for execution. CAUTION: FIXME A picture with TaskSetManager owning TaskSet CAUTION: FIXME What component knows about TaskSet and TaskSetManager. Isn't it that TaskSets are created by DAGScheduler while TaskSetManager is used by TaskSchedulerImpl only? TaskSetManager keeps track of the tasks pending execution per executor, host, rack or with no locality preferences. === [[locality-aware-scheduling]] Locality-Aware Scheduling aka Delay Scheduling TaskSetManager computes locality levels for the TaskSet for delay scheduling. While computing you should see the following DEBUG in the logs: DEBUG Valid locality levels for [taskSet]: [levels] CAUTION: FIXME What's delay scheduling? === [[events]] Events Once a task has finished, TaskSetManager informs scheduler:DAGScheduler.md#taskEnded[DAGScheduler]. CAUTION: FIXME === [[handleSuccessfulTask]] Recording Successful Task And Notifying DAGScheduler -- handleSuccessfulTask Method","title":"getPendingTasksForRack(rack: String): ArrayBuffer[Int]"},{"location":"scheduler/TaskSetManager/#source-scala_16","text":"handleSuccessfulTask( tid: Long, result: DirectTaskResult[_]): Unit handleSuccessfulTask records the tid task as finished, scheduler:DAGScheduler.md#taskEnded[notifies the DAGScheduler that the task has ended] and < TaskSet finished>>. NOTE: handleSuccessfulTask is executed after scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[ TaskSchedulerImpl has been informed that tid task finished successfully (and the task result was deserialized)]. Internally, handleSuccessfulTask finds TaskInfo (in < > internal registry) and marks it as FINISHED . It then removes tid task from < > internal registry. handleSuccessfulTask scheduler:DAGScheduler.md#taskEnded[notifies DAGScheduler that tid task ended successfully] (with the Task object from < > internal registry and the result as Success ). At this point, handleSuccessfulTask finds the other < > of tid task and scheduler:SchedulerBackend.md#killTask[requests SchedulerBackend to kill them] (since they are no longer necessary now when at least one task attempt has completed successfully). You should see the following INFO message in the logs:","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#optionswrap_4","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-killing-attempt-attemptnumber-for-task-id-in-stage-id-tid-id-on-host-as-the-attempt-attemptnumber-succeeded-on-host","text":"CAUTION: FIXME Review taskAttempts If tid has not yet been recorded as < >, handleSuccessfulTask increases < > counter. You should see the following INFO message in the logs:","title":"INFO Killing attempt [attemptNumber] for task [id] in stage [id] (TID [id]) on [host] as the attempt [attemptNumber] succeeded on [host]"},{"location":"scheduler/TaskSetManager/#optionswrap_5","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-finished-task-id-in-stage-id-tid-taskid-in-duration-ms-on-host-executor-executorid-taskssuccessfulnumtasks","text":"tid task is marked as < >. If the number of task that have finished successfully is exactly the number of the tasks to execute (in the TaskSet ), the TaskSetManager becomes a < >. If tid task was already recorded as < >, you should merely see the following INFO message in the logs:","title":"INFO Finished task [id] in stage [id] (TID [taskId]) in [duration] ms on [host] (executor [executorId]) ([tasksSuccessful]/[numTasks])"},{"location":"scheduler/TaskSetManager/#optionswrap_6","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-ignoring-task-finished-event-for-id-in-stage-id-because-task-index-has-already-completed-successfully","text":"Ultimately, handleSuccessfulTask < TaskSet finished>>. NOTE: handleSuccessfulTask is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[handleSuccessfulTask]. === [[maybeFinishTaskSet]] Attempting to Mark TaskSet Finished -- maybeFinishTaskSet Internal Method","title":"INFO Ignoring task-finished event for [id] in stage [id] because task [index] has already completed successfully"},{"location":"scheduler/TaskSetManager/#source-scala_17","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#maybefinishtaskset-unit","text":"maybeFinishTaskSet scheduler:TaskSchedulerImpl.md#taskSetFinished[notifies TaskSchedulerImpl that a TaskSet has finished] when there are no other < > and the < >. === [[task-retries]] Retrying Tasks on Failure CAUTION: FIXME Up to configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] attempts === Task retries and spark.task.maxFailures When you start Spark program you set up configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] for the number of failures that are acceptable until TaskSetManager gives up and marks a job failed. TIP: In Spark shell with local master, configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] is fixed to 1 and you need to use local/spark-local.md[local-with-retries master] to change it to some other value. In the following example, you are going to execute a job with two partitions and keep one failing at all times (by throwing an exception). The aim is to learn the behavior of retrying task execution in a stage in TaskSet. You will only look at a single task execution, namely 0.0 . $ ./bin/spark-shell --master \"local[*, 5]\" ... scala> sc.textFile(\"README.md\", 2).mapPartitionsWithIndex((idx, it) => if (idx == 0) throw new Exception(\"Partition 2 marked failed\") else it).count ... 15/10/27 17:24:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitionsWithIndex at <console>:25) 15/10/27 17:24:56 DEBUG DAGScheduler: New pending partitions: Set(0, 1) 15/10/27 17:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks ... 15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2062 bytes) ... 15/10/27 17:24:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 2) ... 15/10/27 17:24:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2062 bytes) 15/10/27 17:24:56 INFO Executor: Running task 0.1 in stage 1.0 (TID 4) 15/10/27 17:24:56 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/27 17:24:56 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 4) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 ERROR Executor: Exception in task 0.4 in stage 1.0 (TID 7) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 INFO TaskSetManager: Lost task 0.4 in stage 1.0 (TID 7) on executor localhost: java.lang.Exception (Partition 2 marked failed) [duplicate 4] 15/10/27 17:24:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 5 times; aborting job 15/10/27 17:24:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 15/10/27 17:24:56 INFO TaskSchedulerImpl: Cancelling stage 1 15/10/27 17:24:56 INFO DAGScheduler: ResultStage 1 (count at <console>:25) failed in 0.058 s 15/10/27 17:24:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0 15/10/27 17:24:56 INFO DAGScheduler: Job 1 failed: count at <console>:25, took 0.085810 s org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 5 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 7, localhost): java.lang.Exception: Partition 2 marked failed === [[zombie-state]] Zombie state A TaskSetManager is in zombie state when all tasks in a taskset have completed successfully (regardless of the number of task attempts), or if the taskset has been < >. While in zombie state, a TaskSetManager can launch no new tasks and < TaskDescription to resourceOffers>>. A TaskSetManager remains in the zombie state until all tasks have finished running, i.e. to continue to track and account for the running tasks. === [[abort]] Aborting TaskSet -- abort Method","title":"maybeFinishTaskSet(): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_18","text":"abort( message: String, exception: Option[Throwable] = None): Unit abort informs scheduler:DAGScheduler.md#taskSetFailed[ DAGScheduler that the TaskSet has been aborted]. CAUTION: FIXME image with DAGScheduler call The TaskSetManager enters < >. In the end, abort < TaskSet finished>>. abort is used when: TaskResultGetter is requested to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[enqueueSuccessfulTask] (that has failed) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#cancelTasks[cancelTasks] and scheduler:TaskSchedulerImpl.md#error[error] TaskSetManager is requested to < >, < >, < >, and < > DriverEndpoint is requested to launch tasks on executors","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#creating-instance","text":"TaskSetManager takes the following to be created: [[sched]] TaskSchedulerImpl.md[TaskSchedulerImpl] [[taskSet]] TaskSet.md[TaskSet] [[maxTaskFailures]] Number of task failures, i.e. how many times a < > before an entire TaskSet is < > [[blacklistTracker]] (optional) BlacklistTracker (default: None ) [[clock]] Clock (default: SystemClock ) TaskSetManager initializes the < >. NOTE: maxTaskFailures is 1 for local run mode, maxFailures for Spark local-with-retries, and configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property for Spark local-cluster and Spark with cluster managers (Spark Standalone, Mesos and YARN). TaskSetManager MapOutputTracker.md#getEpoch[requests the current epoch from MapOutputTracker ] and sets it on all tasks in the taskset. NOTE: TaskSetManager uses < > (that was given when < >) to TaskSchedulerImpl.md#mapOutputTracker[access the current MapOutputTracker ]. You should see the following DEBUG in the logs: DEBUG Epoch for [taskSet]: [epoch] CAUTION: FIXME Why is the epoch important? NOTE: TaskSetManager requests TaskSchedulerImpl.md#mapOutputTracker[ MapOutputTracker from TaskSchedulerImpl ] which is likely for unit testing only since core:SparkEnv.md#mapOutputTracker[ MapOutputTracker is available using SparkEnv ]. TaskSetManager < > (in reverse order from the highest partition to the lowest). CAUTION: FIXME Why is reverse order important? The code says it's to execute tasks with low indices first. === [[handleFailedTask]] Getting Notified that Task Failed -- handleFailedTask Method","title":"Creating Instance"},{"location":"scheduler/TaskSetManager/#source-scala_19","text":"handleFailedTask( tid: Long, state: TaskState, reason: TaskFailedReason): Unit handleFailedTask finds TaskInfo of tid task in < > internal registry and simply quits if the task is already marked as failed or killed. .TaskSetManager Gets Notified that Task Has Failed image::TaskSetManager-handleFailedTask.png[align=\"center\"] NOTE: handleFailedTask is executed after TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl has been informed that tid task failed] or < >. In either case, tasks could not finish successfully or could not report their status back. handleFailedTask < tid task from the internal registry of running tasks>> and then marks the corresponding TaskInfo as finished (passing in the input state ). handleFailedTask decrements the number of the running copies of tid task (in < > internal registry). NOTE: With speculative-execution-of-tasks.md[] enabled, there can be many copies of a task running simultaneuosly. handleFailedTask uses the following pattern as the reason of the failure: Lost task [id] in stage [taskSetId] (TID [tid], [host], executor [executorId]): [reason] handleFailedTask then calculates the failure exception per the input reason (follow the links for more details): < > < > < > < > NOTE: Description of how the final failure exception is \"computed\" was moved to respective sections below to make the reading slightly more pleasant and comprehensible. handleFailedTask DAGScheduler.md#taskEnded[informs DAGScheduler that tid task has ended] (passing on the Task instance from < > internal registry, the input reason , null result, calculated accumUpdates per failure, and the TaskInfo ). IMPORTANT: This is the moment when TaskSetManager informs DAGScheduler that a task has ended. If tid task has already been marked as completed (in < > internal registry) you should see the following INFO message in the logs:","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#optionswrap_7","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-task-id-in-stage-id-tid-tid-failed-but-the-task-will-not-be-re-executed-either-because-the-task-failed-with-a-shuffle-data-fetch-failure-so-the-previous-stage-needs-to-be-re-run-or-because-a-different-copy-of-the-task-has-already-succeeded","text":"TIP: Read up on speculative-execution-of-tasks.md[] to find out why a single task could be executed multiple times. If however tid task was not recorded as < >, handleFailedTask < >. If the TaskSetManager is not a < > and the task failed reason should be counted towards the maximum number of times the task is allowed to fail before the stage is aborted (i.e. TaskFailedReason.countTowardsTaskFailures attribute is enabled), the optional spark-scheduler-TaskSetBlacklist.md#updateBlacklistForFailedTask[ TaskSetBlacklist is notified] (passing on the host, executor and the task's index). handleFailedTask then increments the < > for tid task and checks if the number of failures is equal or greater than the < TaskSet >> (as defined when the < TaskSetManager was created>>). If so, i.e. the number of task failures of tid task reached the maximum value, you should see the following ERROR message in the logs: ERROR Task [id] in stage [id] failed [maxTaskFailures] times; aborting job And handleFailedTask < TaskSet >> with the following message and then quits: Task [index] in stage [id] failed [maxTaskFailures] times, most recent failure: [failureReason] In the end (except when the number of failures of tid task grew beyond the acceptable number), handleFailedTask < TaskSet as finished>>.","title":"INFO Task [id] in stage [id] (TID [tid]) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded)."},{"location":"scheduler/TaskSetManager/#note","text":"handleFailedTask is used when: TaskSchedulerImpl is requested to TaskSchedulerImpl.md#handleFailedTask[handle a failed task]","title":"[NOTE]"},{"location":"scheduler/TaskSetManager/#tasksetmanager-is-requested-to-and","text":"==== [[handleFailedTask-FetchFailed]] FetchFailed TaskFailedReason For FetchFailed you should see the following WARN message in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] Unless tid has already been marked as successful (in < > internal registry), it becomes so and the < TaskSet >> gets increased. The TaskSetManager enters < >. The failure exception is empty. ==== [[handleFailedTask-ExceptionFailure]] ExceptionFailure TaskFailedReason For ExceptionFailure , handleFailedTask checks if the exception is of type NotSerializableException . If so, you should see the following ERROR message in the logs: ERROR Task [id] in stage [id] (TID [tid]) had a not serializable result: [description]; not retrying And handleFailedTask < TaskSet >> and then quits. Otherwise, if the exception is not of type NotSerializableException , handleFailedTask accesses accumulators and calculates whether to print the WARN message (with the failure reason) or the INFO message. If the failure has already been reported (and is therefore a duplication), configuration-properties.md#spark.logging.exceptionPrintInterval[spark.logging.exceptionPrintInterval] is checked before reprinting the duplicate exception in its entirety. For full printout of the ExceptionFailure , the following WARN appears in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] Otherwise, the following INFO appears in the logs: INFO Lost task [id] in stage [id] (TID [tid]) on [host], executor [id]: [className] ([description]) [duplicate [dupCount]] The exception in ExceptionFailure becomes the failure exception. ==== [[handleFailedTask-ExecutorLostFailure]] ExecutorLostFailure TaskFailedReason For ExecutorLostFailure if not exitCausedByApp , you should see the following INFO in the logs: INFO Task [tid] failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task. The failure exception is empty. ==== [[handleFailedTask-TaskFailedReason]] Other TaskFailedReasons For the other TaskFailedReasons, you should see the following WARN message in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] The failure exception is empty. === [[addPendingTask]] Registering Task As Pending Execution (Per Preferred Locations) -- addPendingTask Internal Method","title":"* TaskSetManager is requested to &lt;&gt; and &lt;&gt;"},{"location":"scheduler/TaskSetManager/#source-scala_20","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#addpendingtaskindex-int-unit","text":"addPendingTask registers a index task in the pending-task lists that the task should be eventually scheduled to (per its preferred locations). Internally, addPendingTask takes the Task.md#preferredLocations[preferred locations of the task] (given index ) and registers the task in the internal pending-task registries for every preferred location: < > when the TaskLocation.md[TaskLocation] is ExecutorCacheTaskLocation . < > for the hosts of a TaskLocation.md[TaskLocation]. < > for the TaskSchedulerImpl.md#getRackForHost[racks from TaskSchedulerImpl per the host] (of a TaskLocation.md[TaskLocation]). For a TaskLocation.md[TaskLocation] being HDFSCacheTaskLocation , addPendingTask TaskSchedulerImpl.md#getExecutorsAliveOnHost[requests TaskSchedulerImpl for the executors on the host] (of a preferred location) and registers the task in < > for every executor (if available). You should see the following INFO message in the logs: INFO Pending task [index] has a cached location at [host] , where there are executors [executors] When addPendingTask could not find executors for a HDFSCacheTaskLocation preferred location, you should see the following DEBUG message in the logs: DEBUG Pending task [index] has a cached location at [host] , but there are no executors alive there. If the task has no location preferences, addPendingTask registers it in < >. addPendingTask always registers the task in < >. NOTE: addPendingTask is used immediatelly when TaskSetManager < > and later when handling a < > or < >. === [[executorLost]] Re-enqueuing ShuffleMapTasks (with no ExternalShuffleService) and Reporting All Running Tasks on Lost Executor as Failed -- executorLost Method","title":"addPendingTask(index: Int): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_21","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#executorlostexecid-string-host-string-reason-executorlossreason-unit","text":"executorLost re-enqueues all the ShuffleMapTask.md[ShuffleMapTasks] that have completed already on the lost executor (when external shuffle service is not in use) and < >. NOTE: executorLost is part of the spark-scheduler-Schedulable.md#contract[Schedulable contract] that TaskSchedulerImpl.md#removeExecutor[ TaskSchedulerImpl uses to inform TaskSetManagers about lost executors]. NOTE: Since TaskSetManager manages execution of the tasks in a single TaskSet.md[TaskSet], when an executor gets lost, the affected tasks that have been running on the failed executor need to be re-enqueued. executorLost is the mechanism to \"announce\" the event to all TaskSetManagers . Internally, executorLost first checks whether the < > are ShuffleMapTask.md[ShuffleMapTasks] and whether an external shuffle service is enabled (that could serve the map shuffle outputs in case of failure). NOTE: executorLost checks out the first task in < > as it is assumed the other belong to the same stage. If the task is a ShuffleMapTask.md[ShuffleMapTask], the entire < > is for a ShuffleMapStage.md[ShuffleMapStage]. NOTE: executorLost uses core:SparkEnv.md#blockManager[ SparkEnv to access the current BlockManager ] and finds out whether an storage:BlockManager.md#externalShuffleServiceEnabled[external shuffle service is enabled] or not (based on spark.shuffle.service.enabled configuration property). If executorLost is indeed due to an executor lost that executed tasks for a ShuffleMapStage.md[ShuffleMapStage] (that this TaskSetManager manages) and no external shuffle server is enabled, executorLost finds < > that were scheduled on this lost executor and marks the < > as not executed yet. NOTE: executorLost uses records every tasks on the lost executor in < > (as false ) and decrements < >, and < > for every task. executorLost < > and DAGScheduler.md#taskEnded[informs DAGScheduler that the tasks (on the lost executor) have ended] (with DAGScheduler.md#handleTaskCompletion-Resubmitted[Resubmitted] reason). NOTE: executorLost uses TaskSchedulerImpl.md#dagScheduler[ TaskSchedulerImpl to access the DAGScheduler ]. TaskSchedulerImpl is given when the < TaskSetManager was created>>. Regardless of whether this TaskSetManager manages ShuffleMapTasks or not (it could also manage ResultTask.md[ResultTasks]) and whether the external shuffle service is used or not, executorLost finds all < > on this lost executor and < > (with the task state FAILED ). NOTE: executorLost finds out if the reason for the executor lost is due to application fault, i.e. assumes ExecutorExited 's exit status as the indicator, ExecutorKilled for non-application's fault and any other reason is an application fault. executorLost < >. === [[recomputeLocality]] Recomputing Task Locality Preferences -- recomputeLocality Method","title":"executorLost(execId: String, host: String, reason: ExecutorLossReason): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_22","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#recomputelocality-unit","text":"recomputeLocality recomputes the internal caches: < >, < > and < >. CAUTION: FIXME But why are the caches important (and have to be recomputed)? recomputeLocality records the current TaskSchedulerImpl.md#TaskLocality[TaskLocality] level of this TaskSetManager (that is < > in < >). NOTE: TaskLocality is one of PROCESS_LOCAL , NODE_LOCAL , NO_PREF , RACK_LOCAL and ANY values. recomputeLocality < > and saves the result in < > internal cache. recomputeLocality computes < > (by < > for every locality level in < > internal cache). In the end, recomputeLocality < > of the previous locality level and records it in < >. NOTE: recomputeLocality is used when TaskSetManager gets notified about status change in executors, i.e. when an executor is < > or < >. === [[computeValidLocalityLevels]] Computing Locality Levels (for Scheduled Tasks) -- computeValidLocalityLevels Internal Method","title":"recomputeLocality(): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_23","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#computevalidlocalitylevels-arraytasklocality","text":"computeValidLocalityLevels computes valid locality levels for tasks that were registered in corresponding registries per locality level. NOTE: TaskSchedulerImpl.md[TaskLocality] is a task locality preference and can be the most localized PROCESS_LOCAL , NODE_LOCAL through NO_PREF and RACK_LOCAL to ANY . .TaskLocalities and Corresponding Internal Registries [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskLocality | Internal Registry | PROCESS_LOCAL | < > | NODE_LOCAL | < > | NO_PREF | < > | RACK_LOCAL | < > |=== computeValidLocalityLevels walks over every internal registry and if it is not empty < > for the corresponding TaskLocality and proceeds with it only when the locality wait is not 0 . For TaskLocality with pending tasks, computeValidLocalityLevels asks TaskSchedulerImpl whether there is at least one executor alive (for TaskSchedulerImpl.md#isExecutorAlive[PROCESS_LOCAL], TaskSchedulerImpl.md#hasExecutorsAliveOnHost[NODE_LOCAL] and TaskSchedulerImpl.md#hasHostAliveOnRack[RACK_LOCAL]) and if so registers the TaskLocality . NOTE: computeValidLocalityLevels uses < > that was given when < TaskSetManager was created>>. computeValidLocalityLevels always registers ANY task locality level. In the end, you should see the following DEBUG message in the logs: DEBUG TaskSetManager: Valid locality levels for [taskSet]: [comma-separated levels] NOTE: computeValidLocalityLevels is used when TaskSetManager < > and later to < >. === [[getLocalityWait]] Finding Locality Wait -- getLocalityWait Internal Method","title":"computeValidLocalityLevels(): Array[TaskLocality]"},{"location":"scheduler/TaskSetManager/#source-scala_24","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getlocalitywaitlevel-tasklocality-long","text":"getLocalityWait finds locality wait (in milliseconds) for a given TaskSchedulerImpl.md#TaskLocality[TaskLocality]. getLocalityWait uses configuration-properties.md#spark.locality.wait[spark.locality.wait] (default: 3s ) when the TaskLocality -specific property is not defined or 0 for NO_PREF and ANY . NOTE: NO_PREF and ANY task localities have no locality wait. .TaskLocalities and Corresponding Spark Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskLocality | Spark Property | PROCESS_LOCAL | configuration-properties.md#spark.locality.wait.process[spark.locality.wait.process] | NODE_LOCAL | configuration-properties.md#spark.locality.wait.node[spark.locality.wait.node] | RACK_LOCAL | configuration-properties.md#spark.locality.wait.rack[spark.locality.wait.rack] |=== NOTE: getLocalityWait is used when TaskSetManager calculates < >, < > and < >.","title":"getLocalityWait(level: TaskLocality): Long"},{"location":"scheduler/TaskSetManager/#checking-available-memory-for-more-task-results","text":"canFetchMoreResults ( size : Long ): Boolean canFetchMoreResults checks whether there is enough memory to fetch the result of a task. Internally, canFetchMoreResults increments the internal < > with the input size (which is the size of the result of a task) and increments the internal < >. If the current internal < > is bigger than the configuration-properties.md#maxResultSize[maximum result size], canFetchMoreResults prints out the following ERROR message to the logs: Total size of serialized results of [calculatedTasks] tasks ([totalResultSize]) is bigger than spark.driver.maxResultSize ([maxResultSize]) In the end, canFetchMoreResults < > the < > and returns false . Otherwise, canFetchMoreResults returns true . canFetchMoreResults is used when TaskResultGetter is requested to enqueue a successful task .","title":" Checking Available Memory For More Task Results"},{"location":"serializer/","text":"Serialization System \u00b6 Serialization System is a core component of Apache Spark with pluggable serializers for task closures and block data. Serialization System uses SerializerManager to select the Serializer (based on spark.serializer configuration property).","title":"Serialization System"},{"location":"serializer/#serialization-system","text":"Serialization System is a core component of Apache Spark with pluggable serializers for task closures and block data. Serialization System uses SerializerManager to select the Serializer (based on spark.serializer configuration property).","title":"Serialization System"},{"location":"serializer/DeserializationStream/","text":"= DeserializationStream DeserializationStream is an abstraction of streams for reading serialized objects. == [[readObject]] readObject Method [source, scala] \u00b6 readObject T: ClassTag : T \u00b6 readObject...FIXME readObject is used when...FIXME == [[readKey]] readKey Method [source, scala] \u00b6 readKey T: ClassTag : T \u00b6 readKey < > representing the key of a key-value record. readKey is used when...FIXME == [[readValue]] readValue Method [source, scala] \u00b6 readValue T: ClassTag : T \u00b6 readValue < > representing the value of a key-value record. readValue is used when...FIXME == [[asIterator]] asIterator Method [source, scala] \u00b6 asIterator: Iterator[Any] \u00b6 asIterator...FIXME asIterator is used when...FIXME == [[asKeyValueIterator]] asKeyValueIterator Method [source, scala] \u00b6 asKeyValueIterator: Iterator[Any] \u00b6 asKeyValueIterator...FIXME asKeyValueIterator is used when...FIXME","title":"DeserializationStream"},{"location":"serializer/DeserializationStream/#source-scala","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#readobjectt-classtag-t","text":"readObject...FIXME readObject is used when...FIXME == [[readKey]] readKey Method","title":"readObjectT: ClassTag: T"},{"location":"serializer/DeserializationStream/#source-scala_1","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#readkeyt-classtag-t","text":"readKey < > representing the key of a key-value record. readKey is used when...FIXME == [[readValue]] readValue Method","title":"readKeyT: ClassTag: T"},{"location":"serializer/DeserializationStream/#source-scala_2","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#readvaluet-classtag-t","text":"readValue < > representing the value of a key-value record. readValue is used when...FIXME == [[asIterator]] asIterator Method","title":"readValueT: ClassTag: T"},{"location":"serializer/DeserializationStream/#source-scala_3","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#asiterator-iteratorany","text":"asIterator...FIXME asIterator is used when...FIXME == [[asKeyValueIterator]] asKeyValueIterator Method","title":"asIterator: Iterator[Any]"},{"location":"serializer/DeserializationStream/#source-scala_4","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#askeyvalueiterator-iteratorany","text":"asKeyValueIterator...FIXME asKeyValueIterator is used when...FIXME","title":"asKeyValueIterator: Iterator[Any]"},{"location":"serializer/JavaSerializerInstance/","text":"JavaSerializerInstance \u00b6 JavaSerializerInstance is...FIXME","title":"JavaSerializerInstance"},{"location":"serializer/JavaSerializerInstance/#javaserializerinstance","text":"JavaSerializerInstance is...FIXME","title":"JavaSerializerInstance"},{"location":"serializer/KryoSerializer/","text":"KryoSerializer \u00b6 KryoSerializer is a Serializer that uses the Kryo serialization library . Creating Instance \u00b6 KryoSerializer takes the following to be created: SparkConf KryoSerializer is created when: SerializerManager is created SparkConf is requested to registerKryoClasses SerializerSupport (Spark SQL) is requested for a SerializerInstance useUnsafe Flag \u00b6 KryoSerializer uses the spark.kryo.unsafe configuration property for useUnsafe flag (initialized when KryoSerializer is created ). useUnsafe is used when KryoSerializer is requested to create the following: KryoSerializerInstance KryoOutput Creating New SerializerInstance \u00b6 newInstance (): SerializerInstance newInstance is part of the Serializer abstraction. newInstance creates a KryoSerializerInstance with this KryoSerializer (and the useUnsafe and usePool flags). newKryoOutput \u00b6 newKryoOutput (): KryoOutput newKryoOutput ...FIXME newKryoOutput is used when: KryoSerializerInstance is requested for the output newKryo \u00b6 newKryo (): Kryo newKryo ...FIXME newKryo is used when: KryoSerializer is requested for a KryoFactory KryoSerializerInstance is requested to borrowKryo KryoFactory \u00b6 factory : KryoFactory KryoSerializer creates a KryoFactory lazily (on demand and once only) for internalPool . KryoPool \u00b6 KryoSerializer creates a custom KryoPool lazily (on demand and once only). KryoPool is used when: pool setDefaultClassLoader supportsRelocationOfSerializedObjects \u00b6 supportsRelocationOfSerializedObjects : Boolean supportsRelocationOfSerializedObjects is part of the Serializer abstraction. supportsRelocationOfSerializedObjects creates a new SerializerInstance (that is assumed to be a KryoSerializerInstance ) and requests it to get the value of the autoReset field .","title":"KryoSerializer"},{"location":"serializer/KryoSerializer/#kryoserializer","text":"KryoSerializer is a Serializer that uses the Kryo serialization library .","title":"KryoSerializer"},{"location":"serializer/KryoSerializer/#creating-instance","text":"KryoSerializer takes the following to be created: SparkConf KryoSerializer is created when: SerializerManager is created SparkConf is requested to registerKryoClasses SerializerSupport (Spark SQL) is requested for a SerializerInstance","title":"Creating Instance"},{"location":"serializer/KryoSerializer/#useunsafe-flag","text":"KryoSerializer uses the spark.kryo.unsafe configuration property for useUnsafe flag (initialized when KryoSerializer is created ). useUnsafe is used when KryoSerializer is requested to create the following: KryoSerializerInstance KryoOutput","title":" useUnsafe Flag"},{"location":"serializer/KryoSerializer/#creating-new-serializerinstance","text":"newInstance (): SerializerInstance newInstance is part of the Serializer abstraction. newInstance creates a KryoSerializerInstance with this KryoSerializer (and the useUnsafe and usePool flags).","title":" Creating New SerializerInstance"},{"location":"serializer/KryoSerializer/#newkryooutput","text":"newKryoOutput (): KryoOutput newKryoOutput ...FIXME newKryoOutput is used when: KryoSerializerInstance is requested for the output","title":" newKryoOutput"},{"location":"serializer/KryoSerializer/#newkryo","text":"newKryo (): Kryo newKryo ...FIXME newKryo is used when: KryoSerializer is requested for a KryoFactory KryoSerializerInstance is requested to borrowKryo","title":" newKryo"},{"location":"serializer/KryoSerializer/#kryofactory","text":"factory : KryoFactory KryoSerializer creates a KryoFactory lazily (on demand and once only) for internalPool .","title":" KryoFactory"},{"location":"serializer/KryoSerializer/#kryopool","text":"KryoSerializer creates a custom KryoPool lazily (on demand and once only). KryoPool is used when: pool setDefaultClassLoader","title":" KryoPool"},{"location":"serializer/KryoSerializer/#supportsrelocationofserializedobjects","text":"supportsRelocationOfSerializedObjects : Boolean supportsRelocationOfSerializedObjects is part of the Serializer abstraction. supportsRelocationOfSerializedObjects creates a new SerializerInstance (that is assumed to be a KryoSerializerInstance ) and requests it to get the value of the autoReset field .","title":" supportsRelocationOfSerializedObjects"},{"location":"serializer/KryoSerializerInstance/","text":"KryoSerializerInstance \u00b6 KryoSerializerInstance is a SerializerInstance . Creating Instance \u00b6 KryoSerializerInstance takes the following to be created: KryoSerializer useUnsafe flag usePool flag KryoSerializerInstance is created when: KryoSerializer is requested for a new SerializerInstance Output \u00b6 KryoSerializerInstance creates Kryo's Output lazily (on demand and once only). KryoSerializerInstance requests the KryoSerializer for a newKryoOutput . output is used for serialization . serialize \u00b6 serialize [ T : ClassTag ]( t : T ): ByteBuffer serialize is part of the SerializerInstance abstraction. serialize ...FIXME deserialize \u00b6 deserialize [ T : ClassTag ]( bytes : ByteBuffer ): T deserialize is part of the SerializerInstance abstraction. deserialize ...FIXME Releasing Kryo Instance \u00b6 releaseKryo ( kryo : Kryo ): Unit releaseKryo ...FIXME releaseKryo is used when: KryoSerializationStream is requested to close KryoDeserializationStream is requested to close KryoSerializerInstance is requested to serialize and deserialize (and getAutoReset ) getAutoReset \u00b6 getAutoReset (): Boolean getAutoReset uses Java Reflection to access the value of the autoReset field of the Kryo class. getAutoReset is used when: KryoSerializer is requested for the supportsRelocationOfSerializedObjects flag","title":"KryoSerializerInstance"},{"location":"serializer/KryoSerializerInstance/#kryoserializerinstance","text":"KryoSerializerInstance is a SerializerInstance .","title":"KryoSerializerInstance"},{"location":"serializer/KryoSerializerInstance/#creating-instance","text":"KryoSerializerInstance takes the following to be created: KryoSerializer useUnsafe flag usePool flag KryoSerializerInstance is created when: KryoSerializer is requested for a new SerializerInstance","title":"Creating Instance"},{"location":"serializer/KryoSerializerInstance/#output","text":"KryoSerializerInstance creates Kryo's Output lazily (on demand and once only). KryoSerializerInstance requests the KryoSerializer for a newKryoOutput . output is used for serialization .","title":" Output"},{"location":"serializer/KryoSerializerInstance/#serialize","text":"serialize [ T : ClassTag ]( t : T ): ByteBuffer serialize is part of the SerializerInstance abstraction. serialize ...FIXME","title":" serialize"},{"location":"serializer/KryoSerializerInstance/#deserialize","text":"deserialize [ T : ClassTag ]( bytes : ByteBuffer ): T deserialize is part of the SerializerInstance abstraction. deserialize ...FIXME","title":" deserialize"},{"location":"serializer/KryoSerializerInstance/#releasing-kryo-instance","text":"releaseKryo ( kryo : Kryo ): Unit releaseKryo ...FIXME releaseKryo is used when: KryoSerializationStream is requested to close KryoDeserializationStream is requested to close KryoSerializerInstance is requested to serialize and deserialize (and getAutoReset )","title":" Releasing Kryo Instance"},{"location":"serializer/KryoSerializerInstance/#getautoreset","text":"getAutoReset (): Boolean getAutoReset uses Java Reflection to access the value of the autoReset field of the Kryo class. getAutoReset is used when: KryoSerializer is requested for the supportsRelocationOfSerializedObjects flag","title":" getAutoReset"},{"location":"serializer/SerializationStream/","text":"SerializationStream \u00b6 SerializationStream is an abstraction of serialized streams for writing out serialized key-value records . Contract \u00b6 Closing Stream \u00b6 close (): Unit Flushing Stream \u00b6 flush (): Unit Used when: UnsafeShuffleWriter is requested to insert a record into a ShuffleExternalSorter DiskBlockObjectWriter is requested to commitAndGet Writing Out Object \u00b6 writeObject [ T : ClassTag ]( t : T ): SerializationStream Used when: MemoryStore is requested to putIteratorAsBytes JavaSerializerInstance is requested to serialize RequestMessage is requested to serialize (for NettyRpcEnv ) ParallelCollectionPartition is requested to writeObject (for ParallelCollectionRDD ) ReliableRDDCheckpointData is requested to doCheckpoint TorrentBroadcast is created (and requested to writeBlocks ) RangePartitioner is requested to writeObject SerializationStream is requested to writeKey , writeValue or writeAll FileSystemPersistenceEngine is requested to serializeIntoFile (for Spark Standalone's Master ) Implementations \u00b6 JavaSerializationStream KryoSerializationStream Writing Out All Records \u00b6 writeAll [ T : ClassTag ]( iter : Iterator [ T ]): SerializationStream writeAll writes out records of the given iterator ( one by one as objects ). writeAll is used when: ReliableCheckpointRDD is requested to doCheckpoint SerializerManager is requested to dataSerializeStream and dataSerializeWithExplicitClassTag Writing Out Key \u00b6 writeKey [ T : ClassTag ]( key : T ): SerializationStream Writes out the key writeKey is used when: UnsafeShuffleWriter is requested to insert a record into a ShuffleExternalSorter DiskBlockObjectWriter is requested to write the key and value of a record Writing Out Value \u00b6 writeValue [ T : ClassTag ]( value : T ): SerializationStream Writes out the value writeValue is used when: UnsafeShuffleWriter is requested to insert a record into a ShuffleExternalSorter DiskBlockObjectWriter is requested to write the key and value of a record","title":"SerializationStream"},{"location":"serializer/SerializationStream/#serializationstream","text":"SerializationStream is an abstraction of serialized streams for writing out serialized key-value records .","title":"SerializationStream"},{"location":"serializer/SerializationStream/#contract","text":"","title":"Contract"},{"location":"serializer/SerializationStream/#closing-stream","text":"close (): Unit","title":" Closing Stream"},{"location":"serializer/SerializationStream/#flushing-stream","text":"flush (): Unit Used when: UnsafeShuffleWriter is requested to insert a record into a ShuffleExternalSorter DiskBlockObjectWriter is requested to commitAndGet","title":" Flushing Stream"},{"location":"serializer/SerializationStream/#writing-out-object","text":"writeObject [ T : ClassTag ]( t : T ): SerializationStream Used when: MemoryStore is requested to putIteratorAsBytes JavaSerializerInstance is requested to serialize RequestMessage is requested to serialize (for NettyRpcEnv ) ParallelCollectionPartition is requested to writeObject (for ParallelCollectionRDD ) ReliableRDDCheckpointData is requested to doCheckpoint TorrentBroadcast is created (and requested to writeBlocks ) RangePartitioner is requested to writeObject SerializationStream is requested to writeKey , writeValue or writeAll FileSystemPersistenceEngine is requested to serializeIntoFile (for Spark Standalone's Master )","title":" Writing Out Object"},{"location":"serializer/SerializationStream/#implementations","text":"JavaSerializationStream KryoSerializationStream","title":"Implementations"},{"location":"serializer/SerializationStream/#writing-out-all-records","text":"writeAll [ T : ClassTag ]( iter : Iterator [ T ]): SerializationStream writeAll writes out records of the given iterator ( one by one as objects ). writeAll is used when: ReliableCheckpointRDD is requested to doCheckpoint SerializerManager is requested to dataSerializeStream and dataSerializeWithExplicitClassTag","title":" Writing Out All Records"},{"location":"serializer/SerializationStream/#writing-out-key","text":"writeKey [ T : ClassTag ]( key : T ): SerializationStream Writes out the key writeKey is used when: UnsafeShuffleWriter is requested to insert a record into a ShuffleExternalSorter DiskBlockObjectWriter is requested to write the key and value of a record","title":" Writing Out Key"},{"location":"serializer/SerializationStream/#writing-out-value","text":"writeValue [ T : ClassTag ]( value : T ): SerializationStream Writes out the value writeValue is used when: UnsafeShuffleWriter is requested to insert a record into a ShuffleExternalSorter DiskBlockObjectWriter is requested to write the key and value of a record","title":" Writing Out Value"},{"location":"serializer/Serializer/","text":"Serializer \u00b6 Serializer is an abstraction of serializers for serialization and deserialization of tasks (closures) and data blocks in a Spark application. Contract \u00b6 Creating New SerializerInstance \u00b6 newInstance (): SerializerInstance Creates a new SerializerInstance Used when: Task is created (only used in tests) SerializerSupport (Spark SQL) utility is used to newSerializer RangePartitioner is requested to writeObject and readObject TorrentBroadcast utility is used to blockifyObject and unBlockifyObject TaskRunner is requested to run NettyBlockRpcServer is requested to deserializeMetadata NettyBlockTransferService is requested to uploadBlock PairRDDFunctions is requested to...FIXME ParallelCollectionPartition is requested to...FIXME RDD is requested to...FIXME ReliableCheckpointRDD utility is used to...FIXME NettyRpcEnvFactory is requested to create a RpcEnv DAGScheduler is created others Implementations \u00b6 JavaSerializer KryoSerializer UnsafeRowSerializer ( Spark SQL ) Accessing Serializer \u00b6 Serializer is available using SparkEnv as the closureSerializer and serializer . closureSerializer \u00b6 SparkEnv . get . closureSerializer serializer \u00b6 SparkEnv . get . serializer Serialized Objects Relocation Requirements \u00b6 supportsRelocationOfSerializedObjects : Boolean supportsRelocationOfSerializedObjects is disabled ( false ) by default. supportsRelocationOfSerializedObjects is used when: BlockStoreShuffleReader is requested to fetchContinuousBlocksInBatch SortShuffleManager is requested to create a ShuffleHandle for a given ShuffleDependency (and checks out SerializedShuffleHandle requirements )","title":"Serializer"},{"location":"serializer/Serializer/#serializer","text":"Serializer is an abstraction of serializers for serialization and deserialization of tasks (closures) and data blocks in a Spark application.","title":"Serializer"},{"location":"serializer/Serializer/#contract","text":"","title":"Contract"},{"location":"serializer/Serializer/#creating-new-serializerinstance","text":"newInstance (): SerializerInstance Creates a new SerializerInstance Used when: Task is created (only used in tests) SerializerSupport (Spark SQL) utility is used to newSerializer RangePartitioner is requested to writeObject and readObject TorrentBroadcast utility is used to blockifyObject and unBlockifyObject TaskRunner is requested to run NettyBlockRpcServer is requested to deserializeMetadata NettyBlockTransferService is requested to uploadBlock PairRDDFunctions is requested to...FIXME ParallelCollectionPartition is requested to...FIXME RDD is requested to...FIXME ReliableCheckpointRDD utility is used to...FIXME NettyRpcEnvFactory is requested to create a RpcEnv DAGScheduler is created others","title":" Creating New SerializerInstance"},{"location":"serializer/Serializer/#implementations","text":"JavaSerializer KryoSerializer UnsafeRowSerializer ( Spark SQL )","title":"Implementations"},{"location":"serializer/Serializer/#accessing-serializer","text":"Serializer is available using SparkEnv as the closureSerializer and serializer .","title":"Accessing Serializer"},{"location":"serializer/Serializer/#closureserializer","text":"SparkEnv . get . closureSerializer","title":" closureSerializer"},{"location":"serializer/Serializer/#serializer_1","text":"SparkEnv . get . serializer","title":" serializer"},{"location":"serializer/Serializer/#serialized-objects-relocation-requirements","text":"supportsRelocationOfSerializedObjects : Boolean supportsRelocationOfSerializedObjects is disabled ( false ) by default. supportsRelocationOfSerializedObjects is used when: BlockStoreShuffleReader is requested to fetchContinuousBlocksInBatch SortShuffleManager is requested to create a ShuffleHandle for a given ShuffleDependency (and checks out SerializedShuffleHandle requirements )","title":" Serialized Objects Relocation Requirements"},{"location":"serializer/SerializerInstance/","text":"SerializerInstance \u00b6 SerializerInstance is an abstraction of serializer instances (for use by one thread at a time). Contract \u00b6 Deserializing (from ByteBuffer) \u00b6 deserialize [ T : ClassTag ]( bytes : ByteBuffer ): T deserialize [ T : ClassTag ]( bytes : ByteBuffer , loader : ClassLoader ): T Used when: TaskRunner is requested to run ResultTask is requested to run ShuffleMapTask is requested to run TaskResultGetter is requested to enqueueFailedTask others Deserializing (from InputStream) \u00b6 deserializeStream ( s : InputStream ): DeserializationStream Serializing (to ByteBuffer) \u00b6 serialize [ T : ClassTag ]( t : T ): ByteBuffer Serializing (to OutputStream) \u00b6 serializeStream ( s : OutputStream ): SerializationStream Implementations \u00b6 JavaSerializerInstance KryoSerializerInstance UnsafeRowSerializerInstance ( Spark SQL )","title":"SerializerInstance"},{"location":"serializer/SerializerInstance/#serializerinstance","text":"SerializerInstance is an abstraction of serializer instances (for use by one thread at a time).","title":"SerializerInstance"},{"location":"serializer/SerializerInstance/#contract","text":"","title":"Contract"},{"location":"serializer/SerializerInstance/#deserializing-from-bytebuffer","text":"deserialize [ T : ClassTag ]( bytes : ByteBuffer ): T deserialize [ T : ClassTag ]( bytes : ByteBuffer , loader : ClassLoader ): T Used when: TaskRunner is requested to run ResultTask is requested to run ShuffleMapTask is requested to run TaskResultGetter is requested to enqueueFailedTask others","title":" Deserializing (from ByteBuffer)"},{"location":"serializer/SerializerInstance/#deserializing-from-inputstream","text":"deserializeStream ( s : InputStream ): DeserializationStream","title":" Deserializing (from InputStream)"},{"location":"serializer/SerializerInstance/#serializing-to-bytebuffer","text":"serialize [ T : ClassTag ]( t : T ): ByteBuffer","title":" Serializing (to ByteBuffer)"},{"location":"serializer/SerializerInstance/#serializing-to-outputstream","text":"serializeStream ( s : OutputStream ): SerializationStream","title":" Serializing (to OutputStream)"},{"location":"serializer/SerializerInstance/#implementations","text":"JavaSerializerInstance KryoSerializerInstance UnsafeRowSerializerInstance ( Spark SQL )","title":"Implementations"},{"location":"serializer/SerializerManager/","text":"SerializerManager \u00b6 SerializerManager is used to select the Serializer for shuffle blocks. Creating Instance \u00b6 SerializerManager takes the following to be created: Default Serializer SparkConf (optional) Encryption Key ( Option[Array[Byte]] ) SerializerManager is created when: SparkEnv utility is used to create a SparkEnv (for the driver and executors) Kryo-Compatible Types \u00b6 Kryo-Compatible Types are the following primitive types, Array s of the primitive types and String s: Boolean Byte Char Double Float Int Long Null Short Default Serializer \u00b6 SerializerManager is given a Serializer when created (based on spark.serializer configuration property). The Serializer is used when SerializerManager is requested for a Serializer . Tip Enable DEBUG logging level of SparkEnv to be told about the selected Serializer . Using serializer: [serializer] Accessing SerializerManager \u00b6 SerializerManager is available using SparkEnv on the driver and executors. import org . apache . spark . SparkEnv SparkEnv . get . serializerManager KryoSerializer \u00b6 SerializerManager creates a KryoSerializer when created . KryoSerializer is used as the serializer when the types of a given key and value are Kryo-compatible . Selecting Serializer \u00b6 getSerializer ( ct : ClassTag [ _ ], autoPick : Boolean ): Serializer getSerializer ( keyClassTag : ClassTag [ _ ], valueClassTag : ClassTag [ _ ]): Serializer getSerializer returns the KryoSerializer when the given ClassTag s are Kryo-compatible and the autoPick flag is true . Otherwise, getSerializer returns the default Serializer . autoPick flag is true for all BlockId s but Spark Streaming's StreamBlockId s. getSerializer (with autoPick flag) is used when: SerializerManager is requested to dataSerializeStream , dataSerializeWithExplicitClassTag and dataDeserializeStream SerializedValuesHolder (of MemoryStore ) is requested for a SerializationStream getSerializer (with key and value ClassTag s only) is used when: ShuffledRDD is requested for dependencies dataSerializeStream \u00b6 dataSerializeStream [ T : ClassTag ]( blockId : BlockId , outputStream : OutputStream , values : Iterator [ T ]): Unit dataSerializeStream ...FIXME dataSerializeStream is used when: BlockManager is requested to doPutIterator and dropFromMemory dataSerializeWithExplicitClassTag \u00b6 dataSerializeWithExplicitClassTag ( blockId : BlockId , values : Iterator [ _ ], classTag : ClassTag [ _ ]): ChunkedByteBuffer dataSerializeWithExplicitClassTag ...FIXME dataSerializeWithExplicitClassTag is used when: BlockManager is requested to doGetLocalBytes SerializerManager is requested to dataSerialize dataDeserializeStream \u00b6 dataDeserializeStream [ T ]( blockId : BlockId , inputStream : InputStream ) ( classTag : ClassTag [ T ]): Iterator [ T ] dataDeserializeStream ...FIXME dataDeserializeStream is used when: BlockStoreUpdater is requested to saveDeserializedValuesToMemoryStore BlockManager is requested to getLocalValues and getRemoteValues MemoryStore is requested to putIteratorAsBytes (when PartiallySerializedBlock is requested for a PartiallyUnrolledIterator )","title":"SerializerManager"},{"location":"serializer/SerializerManager/#serializermanager","text":"SerializerManager is used to select the Serializer for shuffle blocks.","title":"SerializerManager"},{"location":"serializer/SerializerManager/#creating-instance","text":"SerializerManager takes the following to be created: Default Serializer SparkConf (optional) Encryption Key ( Option[Array[Byte]] ) SerializerManager is created when: SparkEnv utility is used to create a SparkEnv (for the driver and executors)","title":"Creating Instance"},{"location":"serializer/SerializerManager/#kryo-compatible-types","text":"Kryo-Compatible Types are the following primitive types, Array s of the primitive types and String s: Boolean Byte Char Double Float Int Long Null Short","title":" Kryo-Compatible Types"},{"location":"serializer/SerializerManager/#default-serializer","text":"SerializerManager is given a Serializer when created (based on spark.serializer configuration property). The Serializer is used when SerializerManager is requested for a Serializer . Tip Enable DEBUG logging level of SparkEnv to be told about the selected Serializer . Using serializer: [serializer]","title":" Default Serializer"},{"location":"serializer/SerializerManager/#accessing-serializermanager","text":"SerializerManager is available using SparkEnv on the driver and executors. import org . apache . spark . SparkEnv SparkEnv . get . serializerManager","title":" Accessing SerializerManager"},{"location":"serializer/SerializerManager/#kryoserializer","text":"SerializerManager creates a KryoSerializer when created . KryoSerializer is used as the serializer when the types of a given key and value are Kryo-compatible .","title":" KryoSerializer"},{"location":"serializer/SerializerManager/#selecting-serializer","text":"getSerializer ( ct : ClassTag [ _ ], autoPick : Boolean ): Serializer getSerializer ( keyClassTag : ClassTag [ _ ], valueClassTag : ClassTag [ _ ]): Serializer getSerializer returns the KryoSerializer when the given ClassTag s are Kryo-compatible and the autoPick flag is true . Otherwise, getSerializer returns the default Serializer . autoPick flag is true for all BlockId s but Spark Streaming's StreamBlockId s. getSerializer (with autoPick flag) is used when: SerializerManager is requested to dataSerializeStream , dataSerializeWithExplicitClassTag and dataDeserializeStream SerializedValuesHolder (of MemoryStore ) is requested for a SerializationStream getSerializer (with key and value ClassTag s only) is used when: ShuffledRDD is requested for dependencies","title":" Selecting Serializer"},{"location":"serializer/SerializerManager/#dataserializestream","text":"dataSerializeStream [ T : ClassTag ]( blockId : BlockId , outputStream : OutputStream , values : Iterator [ T ]): Unit dataSerializeStream ...FIXME dataSerializeStream is used when: BlockManager is requested to doPutIterator and dropFromMemory","title":" dataSerializeStream"},{"location":"serializer/SerializerManager/#dataserializewithexplicitclasstag","text":"dataSerializeWithExplicitClassTag ( blockId : BlockId , values : Iterator [ _ ], classTag : ClassTag [ _ ]): ChunkedByteBuffer dataSerializeWithExplicitClassTag ...FIXME dataSerializeWithExplicitClassTag is used when: BlockManager is requested to doGetLocalBytes SerializerManager is requested to dataSerialize","title":" dataSerializeWithExplicitClassTag"},{"location":"serializer/SerializerManager/#datadeserializestream","text":"dataDeserializeStream [ T ]( blockId : BlockId , inputStream : InputStream ) ( classTag : ClassTag [ T ]): Iterator [ T ] dataDeserializeStream ...FIXME dataDeserializeStream is used when: BlockStoreUpdater is requested to saveDeserializedValuesToMemoryStore BlockManager is requested to getLocalValues and getRemoteValues MemoryStore is requested to putIteratorAsBytes (when PartiallySerializedBlock is requested for a PartiallyUnrolledIterator )","title":" dataDeserializeStream"},{"location":"shuffle/","text":"Shuffle System \u00b6 Shuffle System is a core service of Apache Spark that is responsible for shuffle block management. The core abstraction is ShuffleManager with the default and only known implementation being SortShuffleManager . spark.shuffle.manager configuration property allows for a custom ShuffleManager . Shuffle System uses shuffle handles , readers and writers . Resources \u00b6 Improving Apache Spark Downscaling by Christopher Crosbie (Google) Ben Sidhom (Google) Spark shuffle introduction by Raymond Liu (aka colorant )","title":"Shuffle System"},{"location":"shuffle/#shuffle-system","text":"Shuffle System is a core service of Apache Spark that is responsible for shuffle block management. The core abstraction is ShuffleManager with the default and only known implementation being SortShuffleManager . spark.shuffle.manager configuration property allows for a custom ShuffleManager . Shuffle System uses shuffle handles , readers and writers .","title":"Shuffle System"},{"location":"shuffle/#resources","text":"Improving Apache Spark Downscaling by Christopher Crosbie (Google) Ben Sidhom (Google) Spark shuffle introduction by Raymond Liu (aka colorant )","title":"Resources"},{"location":"shuffle/BaseShuffleHandle/","text":"BaseShuffleHandle \u00b6 BaseShuffleHandle is a fall-back ShuffleHandle that is used to capture the parameters when SortShuffleManager is requested for a ShuffleHandle (and other ShuffleHandle s could not be used): Shuffle ID ShuffleDependency Demo \u00b6 // Start a Spark application, e.g. spark-shell, with the Spark properties to trigger selection of BaseShuffleHandle: // 1. spark.shuffle.spill.numElementsForceSpillThreshold=1 // 2. spark.shuffle.sort.bypassMergeThreshold=1 // numSlices > spark.shuffle.sort.bypassMergeThreshold scala> val rdd = sc.parallelize(0 to 4, numSlices = 2).groupBy(_ % 2) rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24 scala> rdd.dependencies DEBUG SortShuffleManager: Can't use serialized shuffle for shuffle 0 because an aggregator is defined res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@1160c54b) scala> rdd.getNumPartitions res1: Int = 2 scala> import org.apache.spark.ShuffleDependency import org.apache.spark.ShuffleDependency scala> val shuffleDep = rdd.dependencies(0).asInstanceOf[ShuffleDependency[Int, Int, Int]] shuffleDep: org.apache.spark.ShuffleDependency[Int,Int,Int] = org.apache.spark.ShuffleDependency@1160c54b // mapSideCombine is disabled scala> shuffleDep.mapSideCombine res2: Boolean = false // aggregator defined scala> shuffleDep.aggregator res3: Option[org.apache.spark.Aggregator[Int,Int,Int]] = Some(Aggregator(<function1>,<function2>,<function2>)) // the number of reduce partitions < spark.shuffle.sort.bypassMergeThreshold scala> shuffleDep.partitioner.numPartitions res4: Int = 2 scala> shuffleDep.shuffleHandle res5: org.apache.spark.shuffle.ShuffleHandle = org.apache.spark.shuffle.BaseShuffleHandle@22b0fe7e","title":"BaseShuffleHandle"},{"location":"shuffle/BaseShuffleHandle/#baseshufflehandle","text":"BaseShuffleHandle is a fall-back ShuffleHandle that is used to capture the parameters when SortShuffleManager is requested for a ShuffleHandle (and other ShuffleHandle s could not be used): Shuffle ID ShuffleDependency","title":"BaseShuffleHandle"},{"location":"shuffle/BaseShuffleHandle/#demo","text":"// Start a Spark application, e.g. spark-shell, with the Spark properties to trigger selection of BaseShuffleHandle: // 1. spark.shuffle.spill.numElementsForceSpillThreshold=1 // 2. spark.shuffle.sort.bypassMergeThreshold=1 // numSlices > spark.shuffle.sort.bypassMergeThreshold scala> val rdd = sc.parallelize(0 to 4, numSlices = 2).groupBy(_ % 2) rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24 scala> rdd.dependencies DEBUG SortShuffleManager: Can't use serialized shuffle for shuffle 0 because an aggregator is defined res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@1160c54b) scala> rdd.getNumPartitions res1: Int = 2 scala> import org.apache.spark.ShuffleDependency import org.apache.spark.ShuffleDependency scala> val shuffleDep = rdd.dependencies(0).asInstanceOf[ShuffleDependency[Int, Int, Int]] shuffleDep: org.apache.spark.ShuffleDependency[Int,Int,Int] = org.apache.spark.ShuffleDependency@1160c54b // mapSideCombine is disabled scala> shuffleDep.mapSideCombine res2: Boolean = false // aggregator defined scala> shuffleDep.aggregator res3: Option[org.apache.spark.Aggregator[Int,Int,Int]] = Some(Aggregator(<function1>,<function2>,<function2>)) // the number of reduce partitions < spark.shuffle.sort.bypassMergeThreshold scala> shuffleDep.partitioner.numPartitions res4: Int = 2 scala> shuffleDep.shuffleHandle res5: org.apache.spark.shuffle.ShuffleHandle = org.apache.spark.shuffle.BaseShuffleHandle@22b0fe7e","title":"Demo"},{"location":"shuffle/BlockStoreShuffleReader/","text":"BlockStoreShuffleReader \u00b6 BlockStoreShuffleReader is a ShuffleReader . Creating Instance \u00b6 BlockStoreShuffleReader takes the following to be created: BaseShuffleHandle Block s by Address ( Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])] ) TaskContext ShuffleReadMetricsReporter SerializerManager BlockManager MapOutputTracker shouldBatchFetch flag (default: false ) BlockStoreShuffleReader is created when: SortShuffleManager is requested for a ShuffleReader (for a ShuffleHandle and a range of reduce partitions) Reading Combined Records for Reduce Task \u00b6 read (): Iterator [ Product2 [ K , C ]] read is part of the ShuffleReader abstraction. read creates a ShuffleBlockFetcherIterator . read ...FIXME Review Me \u00b6 === [[read]] Reading Combined Records For Reduce Task Internally, read first storage:ShuffleBlockFetcherIterator.md#creating-instance[creates a ShuffleBlockFetcherIterator ] (passing in the values of < >, < > and < > Spark properties). NOTE: read uses scheduler:MapOutputTracker.md#getMapSizesByExecutorId[ MapOutputTracker to find the BlockManagers with the shuffle blocks and sizes] to create ShuffleBlockFetcherIterator . read creates a new serializer:SerializerInstance.md[SerializerInstance] (using Serializer from ShuffleDependency ). read creates a key/value iterator by deserializeStream every shuffle block stream. read updates the context task metrics for each record read. NOTE: read uses CompletionIterator (to count the records read) and spark-InterruptibleIterator.md[InterruptibleIterator] (to support task cancellation). If the ShuffleDependency has an Aggregator defined , read wraps the current iterator inside an iterator defined by Aggregator.combineCombinersByKey (for mapSideCombine enabled ) or Aggregator.combineValuesByKey otherwise. NOTE: run reports an exception when ShuffleDependency has no Aggregator defined with mapSideCombine flag enabled . For keyOrdering defined in the ShuffleDependency , run does the following: shuffle:ExternalSorter.md#creating-instance[Creates an ExternalSorter ] shuffle:ExternalSorter.md#insertAll[Inserts all the records] into the ExternalSorter Updates context TaskMetrics Returns a CompletionIterator for the ExternalSorter","title":"BlockStoreShuffleReader"},{"location":"shuffle/BlockStoreShuffleReader/#blockstoreshufflereader","text":"BlockStoreShuffleReader is a ShuffleReader .","title":"BlockStoreShuffleReader"},{"location":"shuffle/BlockStoreShuffleReader/#creating-instance","text":"BlockStoreShuffleReader takes the following to be created: BaseShuffleHandle Block s by Address ( Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])] ) TaskContext ShuffleReadMetricsReporter SerializerManager BlockManager MapOutputTracker shouldBatchFetch flag (default: false ) BlockStoreShuffleReader is created when: SortShuffleManager is requested for a ShuffleReader (for a ShuffleHandle and a range of reduce partitions)","title":"Creating Instance"},{"location":"shuffle/BlockStoreShuffleReader/#reading-combined-records-for-reduce-task","text":"read (): Iterator [ Product2 [ K , C ]] read is part of the ShuffleReader abstraction. read creates a ShuffleBlockFetcherIterator . read ...FIXME","title":" Reading Combined Records for Reduce Task"},{"location":"shuffle/BlockStoreShuffleReader/#review-me","text":"=== [[read]] Reading Combined Records For Reduce Task Internally, read first storage:ShuffleBlockFetcherIterator.md#creating-instance[creates a ShuffleBlockFetcherIterator ] (passing in the values of < >, < > and < > Spark properties). NOTE: read uses scheduler:MapOutputTracker.md#getMapSizesByExecutorId[ MapOutputTracker to find the BlockManagers with the shuffle blocks and sizes] to create ShuffleBlockFetcherIterator . read creates a new serializer:SerializerInstance.md[SerializerInstance] (using Serializer from ShuffleDependency ). read creates a key/value iterator by deserializeStream every shuffle block stream. read updates the context task metrics for each record read. NOTE: read uses CompletionIterator (to count the records read) and spark-InterruptibleIterator.md[InterruptibleIterator] (to support task cancellation). If the ShuffleDependency has an Aggregator defined , read wraps the current iterator inside an iterator defined by Aggregator.combineCombinersByKey (for mapSideCombine enabled ) or Aggregator.combineValuesByKey otherwise. NOTE: run reports an exception when ShuffleDependency has no Aggregator defined with mapSideCombine flag enabled . For keyOrdering defined in the ShuffleDependency , run does the following: shuffle:ExternalSorter.md#creating-instance[Creates an ExternalSorter ] shuffle:ExternalSorter.md#insertAll[Inserts all the records] into the ExternalSorter Updates context TaskMetrics Returns a CompletionIterator for the ExternalSorter","title":"Review Me"},{"location":"shuffle/BypassMergeSortShuffleHandle/","text":"BypassMergeSortShuffleHandle \u2014 Marker Interface for Bypass Merge Sort Shuffle Handles \u00b6 BypassMergeSortShuffleHandles is a BaseShuffleHandle with no additional methods or fields and serves only to identify the choice of bypass merge sort shuffle . Like BaseShuffleHandle , BypassMergeSortShuffleHandles takes shuffleId , numMaps , and a ShuffleDependency . BypassMergeSortShuffleHandle is created when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ). Demo \u00b6 scala> val rdd = sc.parallelize(0 to 8).groupBy(_ % 3) rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24 scala> rdd.dependencies res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@655875bb) scala> rdd.getNumPartitions res1: Int = 8 scala> import org.apache.spark.ShuffleDependency import org.apache.spark.ShuffleDependency scala> val shuffleDep = rdd.dependencies(0).asInstanceOf[ShuffleDependency[Int, Int, Int]] shuffleDep: org.apache.spark.ShuffleDependency[Int,Int,Int] = org.apache.spark.ShuffleDependency@655875bb // mapSideCombine is disabled scala> shuffleDep.mapSideCombine res2: Boolean = false // aggregator defined scala> shuffleDep.aggregator res3: Option[org.apache.spark.Aggregator[Int,Int,Int]] = Some(Aggregator(<function1>,<function2>,<function2>)) // spark.shuffle.sort.bypassMergeThreshold == 200 // the number of reduce partitions < spark.shuffle.sort.bypassMergeThreshold scala> shuffleDep.partitioner.numPartitions res4: Int = 8 scala> shuffleDep.shuffleHandle res5: org.apache.spark.shuffle.ShuffleHandle = org.apache.spark.shuffle.sort.BypassMergeSortShuffleHandle@68893394","title":"BypassMergeSortShuffleHandle"},{"location":"shuffle/BypassMergeSortShuffleHandle/#bypassmergesortshufflehandle-marker-interface-for-bypass-merge-sort-shuffle-handles","text":"BypassMergeSortShuffleHandles is a BaseShuffleHandle with no additional methods or fields and serves only to identify the choice of bypass merge sort shuffle . Like BaseShuffleHandle , BypassMergeSortShuffleHandles takes shuffleId , numMaps , and a ShuffleDependency . BypassMergeSortShuffleHandle is created when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ).","title":"BypassMergeSortShuffleHandle &mdash; Marker Interface for Bypass Merge Sort Shuffle Handles"},{"location":"shuffle/BypassMergeSortShuffleHandle/#demo","text":"scala> val rdd = sc.parallelize(0 to 8).groupBy(_ % 3) rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24 scala> rdd.dependencies res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@655875bb) scala> rdd.getNumPartitions res1: Int = 8 scala> import org.apache.spark.ShuffleDependency import org.apache.spark.ShuffleDependency scala> val shuffleDep = rdd.dependencies(0).asInstanceOf[ShuffleDependency[Int, Int, Int]] shuffleDep: org.apache.spark.ShuffleDependency[Int,Int,Int] = org.apache.spark.ShuffleDependency@655875bb // mapSideCombine is disabled scala> shuffleDep.mapSideCombine res2: Boolean = false // aggregator defined scala> shuffleDep.aggregator res3: Option[org.apache.spark.Aggregator[Int,Int,Int]] = Some(Aggregator(<function1>,<function2>,<function2>)) // spark.shuffle.sort.bypassMergeThreshold == 200 // the number of reduce partitions < spark.shuffle.sort.bypassMergeThreshold scala> shuffleDep.partitioner.numPartitions res4: Int = 8 scala> shuffleDep.shuffleHandle res5: org.apache.spark.shuffle.ShuffleHandle = org.apache.spark.shuffle.sort.BypassMergeSortShuffleHandle@68893394","title":"Demo"},{"location":"shuffle/BypassMergeSortShuffleWriter/","text":"BypassMergeSortShuffleWriter \u00b6 BypassMergeSortShuffleWriter&lt;K, V&gt; is a ShuffleWriter for ShuffleMapTasks to write records into one single shuffle block data file . Creating Instance \u00b6 BypassMergeSortShuffleWriter takes the following to be created: BlockManager BypassMergeSortShuffleHandle (of K keys and V values) Map ID SparkConf ShuffleWriteMetricsReporter ShuffleExecutorComponents BypassMergeSortShuffleWriter is created when: SortShuffleManager is requested for a ShuffleWriter (for a BypassMergeSortShuffleHandle ) DiskBlockObjectWriters \u00b6 DiskBlockObjectWriter [] partitionWriters BypassMergeSortShuffleWriter uses a DiskBlockObjectWriter per partition (based on the Partitioner ). BypassMergeSortShuffleWriter asserts that no partitionWriters are created while writing out records to a shuffle file . While writing , BypassMergeSortShuffleWriter requests the BlockManager for as many DiskBlockObjectWriter s as there are partition s (in the Partitioner ). While writing , BypassMergeSortShuffleWriter requests the Partitioner for a partition for records (using keys) and finds the per-partition DiskBlockObjectWriter that is requested to write out the partition records . After all records are written out to their shuffle files, the DiskBlockObjectWriter s are requested to commitAndGet . BypassMergeSortShuffleWriter uses the partition writers while writing out partition data and removes references to them ( null ify them) in the end. In other words, after writing out partition data partitionWriters internal registry is null . partitionWriters internal registry becomes null after BypassMergeSortShuffleWriter has finished: Writing out partition data Stopping IndexShuffleBlockResolver \u00b6 BypassMergeSortShuffleWriter is given a IndexShuffleBlockResolver when created . BypassMergeSortShuffleWriter uses the IndexShuffleBlockResolver for writing out records (to writeIndexFileAndCommit and getDataFile ). Serializer \u00b6 When created, BypassMergeSortShuffleWriter requests the ShuffleDependency (of the given BypassMergeSortShuffleHandle ) for the Serializer . BypassMergeSortShuffleWriter creates a new instance of the Serializer for writing out records . Configuration Properties \u00b6 spark.shuffle.file.buffer \u00b6 BypassMergeSortShuffleWriter uses spark.shuffle.file.buffer configuration property for...FIXME spark.file.transferTo \u00b6 BypassMergeSortShuffleWriter uses spark.file.transferTo configuration property to control whether to use Java New I/O while writing to a partitioned file . Writing Out Records to Shuffle File \u00b6 void write ( Iterator < Product2 < K , V >> records ) write is part of the ShuffleWriter abstraction. write creates a new instance of the Serializer . write initializes the partitionWriters and partitionWriterSegments internal registries (for DiskBlockObjectWriters and FileSegments for every partition , respectively). write requests the BlockManager for the DiskBlockManager and for every partition write requests it for a shuffle block ID and the file . write creates a DiskBlockObjectWriter for the shuffle block (using the BlockManager ). write stores the reference to DiskBlockObjectWriters in the partitionWriters internal registry. After all DiskBlockObjectWriters are created, write requests the ShuffleWriteMetrics to increment shuffle write time metric . For every record (a key-value pair), write requests the Partitioner for the partition ID for the key. The partition ID is then used as an index of the partition writer (among the DiskBlockObjectWriters ) to write the current record out to a block file . Once all records have been writted out to their respective block files, write does the following for every DiskBlockObjectWriter : Requests the DiskBlockObjectWriter to commit and return a corresponding FileSegment of the shuffle block Saves the (reference to) FileSegments in the partitionWriterSegments internal registry Requests the DiskBlockObjectWriter to close Note At this point, all the records are in shuffle block files on a local disk. The records are split across block files by key. write requests the IndexShuffleBlockResolver for the shuffle file for the shuffle and the map Ds>>. write creates a temporary file (based on the name of the shuffle file) and writes all the per-partition shuffle files to it . The size of every per-partition shuffle files is saved as the partitionLengths internal registry. Note At this point, all the per-partition shuffle block files are one single map shuffle data file. write requests the IndexShuffleBlockResolver to write shuffle index and data files for the shuffle and the map IDs (with the partitionLengths and the temporary shuffle output file). write returns a shuffle map output status (with the shuffle server ID and the partitionLengths ). No Records \u00b6 When there is no records to write out, write initializes the partitionLengths internal array (of numPartitions size) with all elements being 0. write requests the IndexShuffleBlockResolver to write shuffle index and data files , but the difference (compared to when there are records to write) is that the dataTmp argument is simply null . write sets the internal mapStatus (with the address of BlockManager in use and partitionLengths ). Requirements \u00b6 write requires that there are no DiskBlockObjectWriters . Writing Out Partitioned Data \u00b6 long [] writePartitionedData ( ShuffleMapOutputWriter mapOutputWriter ) writePartitionedData makes sure that DiskBlockObjectWriter s are available ( partitionWriters != null ). For every partition , writePartitionedData takes the partition file (from the FileSegment s). Only when the partition file exists, writePartitionedData requests the given ShuffleMapOutputWriter for a ShufflePartitionWriter and writes out the partitioned data. At the end, writePartitionedData deletes the file. writePartitionedData requests the ShuffleWriteMetricsReporter to increment the write time . In the end, writePartitionedData requests the ShuffleMapOutputWriter to commitAllPartitions and returns the size of each partition of the output map file. Copying Raw Bytes Between Input Streams \u00b6 copyStream ( in : InputStream , out : OutputStream , closeStreams : Boolean = false , transferToEnabled : Boolean = false ): Long copyStream branches off depending on the type of in and out streams, i.e. whether they are both FileInputStream with transferToEnabled input flag is enabled. If they are both FileInputStream with transferToEnabled enabled, copyStream gets their FileChannels and transfers bytes from the input file to the output file and counts the number of bytes, possibly zero, that were actually transferred. NOTE: copyStream uses Java's {java-javadoc-url}/java/nio/channels/FileChannel.html[java.nio.channels.FileChannel] to manage file channels. If either in and out input streams are not FileInputStream or transferToEnabled flag is disabled (default), copyStream reads data from in to write to out and counts the number of bytes written. copyStream can optionally close in and out streams (depending on the input closeStreams -- disabled by default). NOTE: Utils.copyStream is used when < > (among other places). Tip Visit the official web site of JSR 51: New I/O APIs for the Java Platform and read up on java.nio package . Stopping ShuffleWriter \u00b6 Option < MapStatus > stop ( boolean success ) stop ...FIXME stop is part of the ShuffleWriter abstraction. Temporary Array of Partition Lengths \u00b6 long [] partitionLengths Temporary array of partition lengths after records are written to a shuffle system . Initialized every time BypassMergeSortShuffleWriter writes out records (before passing it in to IndexShuffleBlockResolver ). After IndexShuffleBlockResolver finishes, it is used to initialize mapStatus internal property. Logging \u00b6 Enable ALL logging level for org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter=ALL Refer to Logging . Internal Properties \u00b6 numPartitions \u00b6 partitionWriterSegments \u00b6 mapStatus \u00b6 MapStatus that BypassMergeSortShuffleWriter returns when stopped Initialized every time BypassMergeSortShuffleWriter writes out records . Used when BypassMergeSortShuffleWriter stops (with success enabled) as a marker if any records were written and returned if they did .","title":"BypassMergeSortShuffleWriter"},{"location":"shuffle/BypassMergeSortShuffleWriter/#bypassmergesortshufflewriter","text":"BypassMergeSortShuffleWriter&lt;K, V&gt; is a ShuffleWriter for ShuffleMapTasks to write records into one single shuffle block data file .","title":"BypassMergeSortShuffleWriter"},{"location":"shuffle/BypassMergeSortShuffleWriter/#creating-instance","text":"BypassMergeSortShuffleWriter takes the following to be created: BlockManager BypassMergeSortShuffleHandle (of K keys and V values) Map ID SparkConf ShuffleWriteMetricsReporter ShuffleExecutorComponents BypassMergeSortShuffleWriter is created when: SortShuffleManager is requested for a ShuffleWriter (for a BypassMergeSortShuffleHandle )","title":"Creating Instance"},{"location":"shuffle/BypassMergeSortShuffleWriter/#diskblockobjectwriters","text":"DiskBlockObjectWriter [] partitionWriters BypassMergeSortShuffleWriter uses a DiskBlockObjectWriter per partition (based on the Partitioner ). BypassMergeSortShuffleWriter asserts that no partitionWriters are created while writing out records to a shuffle file . While writing , BypassMergeSortShuffleWriter requests the BlockManager for as many DiskBlockObjectWriter s as there are partition s (in the Partitioner ). While writing , BypassMergeSortShuffleWriter requests the Partitioner for a partition for records (using keys) and finds the per-partition DiskBlockObjectWriter that is requested to write out the partition records . After all records are written out to their shuffle files, the DiskBlockObjectWriter s are requested to commitAndGet . BypassMergeSortShuffleWriter uses the partition writers while writing out partition data and removes references to them ( null ify them) in the end. In other words, after writing out partition data partitionWriters internal registry is null . partitionWriters internal registry becomes null after BypassMergeSortShuffleWriter has finished: Writing out partition data Stopping","title":" DiskBlockObjectWriters"},{"location":"shuffle/BypassMergeSortShuffleWriter/#indexshuffleblockresolver","text":"BypassMergeSortShuffleWriter is given a IndexShuffleBlockResolver when created . BypassMergeSortShuffleWriter uses the IndexShuffleBlockResolver for writing out records (to writeIndexFileAndCommit and getDataFile ).","title":" IndexShuffleBlockResolver"},{"location":"shuffle/BypassMergeSortShuffleWriter/#serializer","text":"When created, BypassMergeSortShuffleWriter requests the ShuffleDependency (of the given BypassMergeSortShuffleHandle ) for the Serializer . BypassMergeSortShuffleWriter creates a new instance of the Serializer for writing out records .","title":" Serializer"},{"location":"shuffle/BypassMergeSortShuffleWriter/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"shuffle/BypassMergeSortShuffleWriter/#sparkshufflefilebuffer","text":"BypassMergeSortShuffleWriter uses spark.shuffle.file.buffer configuration property for...FIXME","title":" spark.shuffle.file.buffer"},{"location":"shuffle/BypassMergeSortShuffleWriter/#sparkfiletransferto","text":"BypassMergeSortShuffleWriter uses spark.file.transferTo configuration property to control whether to use Java New I/O while writing to a partitioned file .","title":" spark.file.transferTo"},{"location":"shuffle/BypassMergeSortShuffleWriter/#writing-out-records-to-shuffle-file","text":"void write ( Iterator < Product2 < K , V >> records ) write is part of the ShuffleWriter abstraction. write creates a new instance of the Serializer . write initializes the partitionWriters and partitionWriterSegments internal registries (for DiskBlockObjectWriters and FileSegments for every partition , respectively). write requests the BlockManager for the DiskBlockManager and for every partition write requests it for a shuffle block ID and the file . write creates a DiskBlockObjectWriter for the shuffle block (using the BlockManager ). write stores the reference to DiskBlockObjectWriters in the partitionWriters internal registry. After all DiskBlockObjectWriters are created, write requests the ShuffleWriteMetrics to increment shuffle write time metric . For every record (a key-value pair), write requests the Partitioner for the partition ID for the key. The partition ID is then used as an index of the partition writer (among the DiskBlockObjectWriters ) to write the current record out to a block file . Once all records have been writted out to their respective block files, write does the following for every DiskBlockObjectWriter : Requests the DiskBlockObjectWriter to commit and return a corresponding FileSegment of the shuffle block Saves the (reference to) FileSegments in the partitionWriterSegments internal registry Requests the DiskBlockObjectWriter to close Note At this point, all the records are in shuffle block files on a local disk. The records are split across block files by key. write requests the IndexShuffleBlockResolver for the shuffle file for the shuffle and the map Ds>>. write creates a temporary file (based on the name of the shuffle file) and writes all the per-partition shuffle files to it . The size of every per-partition shuffle files is saved as the partitionLengths internal registry. Note At this point, all the per-partition shuffle block files are one single map shuffle data file. write requests the IndexShuffleBlockResolver to write shuffle index and data files for the shuffle and the map IDs (with the partitionLengths and the temporary shuffle output file). write returns a shuffle map output status (with the shuffle server ID and the partitionLengths ).","title":" Writing Out Records to Shuffle File"},{"location":"shuffle/BypassMergeSortShuffleWriter/#no-records","text":"When there is no records to write out, write initializes the partitionLengths internal array (of numPartitions size) with all elements being 0. write requests the IndexShuffleBlockResolver to write shuffle index and data files , but the difference (compared to when there are records to write) is that the dataTmp argument is simply null . write sets the internal mapStatus (with the address of BlockManager in use and partitionLengths ).","title":" No Records"},{"location":"shuffle/BypassMergeSortShuffleWriter/#requirements","text":"write requires that there are no DiskBlockObjectWriters .","title":" Requirements"},{"location":"shuffle/BypassMergeSortShuffleWriter/#writing-out-partitioned-data","text":"long [] writePartitionedData ( ShuffleMapOutputWriter mapOutputWriter ) writePartitionedData makes sure that DiskBlockObjectWriter s are available ( partitionWriters != null ). For every partition , writePartitionedData takes the partition file (from the FileSegment s). Only when the partition file exists, writePartitionedData requests the given ShuffleMapOutputWriter for a ShufflePartitionWriter and writes out the partitioned data. At the end, writePartitionedData deletes the file. writePartitionedData requests the ShuffleWriteMetricsReporter to increment the write time . In the end, writePartitionedData requests the ShuffleMapOutputWriter to commitAllPartitions and returns the size of each partition of the output map file.","title":" Writing Out Partitioned Data"},{"location":"shuffle/BypassMergeSortShuffleWriter/#copying-raw-bytes-between-input-streams","text":"copyStream ( in : InputStream , out : OutputStream , closeStreams : Boolean = false , transferToEnabled : Boolean = false ): Long copyStream branches off depending on the type of in and out streams, i.e. whether they are both FileInputStream with transferToEnabled input flag is enabled. If they are both FileInputStream with transferToEnabled enabled, copyStream gets their FileChannels and transfers bytes from the input file to the output file and counts the number of bytes, possibly zero, that were actually transferred. NOTE: copyStream uses Java's {java-javadoc-url}/java/nio/channels/FileChannel.html[java.nio.channels.FileChannel] to manage file channels. If either in and out input streams are not FileInputStream or transferToEnabled flag is disabled (default), copyStream reads data from in to write to out and counts the number of bytes written. copyStream can optionally close in and out streams (depending on the input closeStreams -- disabled by default). NOTE: Utils.copyStream is used when < > (among other places). Tip Visit the official web site of JSR 51: New I/O APIs for the Java Platform and read up on java.nio package .","title":" Copying Raw Bytes Between Input Streams"},{"location":"shuffle/BypassMergeSortShuffleWriter/#stopping-shufflewriter","text":"Option < MapStatus > stop ( boolean success ) stop ...FIXME stop is part of the ShuffleWriter abstraction.","title":" Stopping ShuffleWriter"},{"location":"shuffle/BypassMergeSortShuffleWriter/#temporary-array-of-partition-lengths","text":"long [] partitionLengths Temporary array of partition lengths after records are written to a shuffle system . Initialized every time BypassMergeSortShuffleWriter writes out records (before passing it in to IndexShuffleBlockResolver ). After IndexShuffleBlockResolver finishes, it is used to initialize mapStatus internal property.","title":" Temporary Array of Partition Lengths"},{"location":"shuffle/BypassMergeSortShuffleWriter/#logging","text":"Enable ALL logging level for org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter=ALL Refer to Logging .","title":"Logging"},{"location":"shuffle/BypassMergeSortShuffleWriter/#internal-properties","text":"","title":"Internal Properties"},{"location":"shuffle/BypassMergeSortShuffleWriter/#numpartitions","text":"","title":" numPartitions"},{"location":"shuffle/BypassMergeSortShuffleWriter/#partitionwritersegments","text":"","title":" partitionWriterSegments"},{"location":"shuffle/BypassMergeSortShuffleWriter/#mapstatus","text":"MapStatus that BypassMergeSortShuffleWriter returns when stopped Initialized every time BypassMergeSortShuffleWriter writes out records . Used when BypassMergeSortShuffleWriter stops (with success enabled) as a marker if any records were written and returned if they did .","title":" mapStatus"},{"location":"shuffle/DownloadFileManager/","text":"DownloadFileManager \u00b6 DownloadFileManager is an abstraction of file managers that can createTempFile and registerTempFileToClean . Contract \u00b6 createTempFile \u00b6 DownloadFile createTempFile ( TransportConf transportConf ) Used when: DownloadCallback (of OneForOneBlockFetcher ) is created registerTempFileToClean \u00b6 boolean registerTempFileToClean ( DownloadFile file ) Used when: DownloadCallback (of OneForOneBlockFetcher ) is requested to onComplete Implementations \u00b6 RemoteBlockDownloadFileManager ShuffleBlockFetcherIterator","title":"DownloadFileManager"},{"location":"shuffle/DownloadFileManager/#downloadfilemanager","text":"DownloadFileManager is an abstraction of file managers that can createTempFile and registerTempFileToClean .","title":"DownloadFileManager"},{"location":"shuffle/DownloadFileManager/#contract","text":"","title":"Contract"},{"location":"shuffle/DownloadFileManager/#createtempfile","text":"DownloadFile createTempFile ( TransportConf transportConf ) Used when: DownloadCallback (of OneForOneBlockFetcher ) is created","title":" createTempFile"},{"location":"shuffle/DownloadFileManager/#registertempfiletoclean","text":"boolean registerTempFileToClean ( DownloadFile file ) Used when: DownloadCallback (of OneForOneBlockFetcher ) is requested to onComplete","title":" registerTempFileToClean"},{"location":"shuffle/DownloadFileManager/#implementations","text":"RemoteBlockDownloadFileManager ShuffleBlockFetcherIterator","title":"Implementations"},{"location":"shuffle/ExecutorDiskUtils/","text":"ExecutorDiskUtils \u00b6","title":"ExecutorDiskUtils"},{"location":"shuffle/ExecutorDiskUtils/#executordiskutils","text":"","title":"ExecutorDiskUtils"},{"location":"shuffle/ExternalAppendOnlyMap/","text":"ExternalAppendOnlyMap \u00b6 ExternalAppendOnlyMap is a Spillable of SizeTrackers. ExternalAppendOnlyMap[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. Creating Instance \u00b6 ExternalAppendOnlyMap takes the following to be created: [[createCombiner]] createCombiner function ( V => C ) [[mergeValue]] mergeValue function ( (C, V) => C ) [[mergeCombiners]] mergeCombiners function ( (C, C) => C ) [[serializer]] Optional serializer:Serializer.md[Serializer] (default: core:SparkEnv.md#serializer[system Serializer]) [[blockManager]] Optional storage:BlockManager.md[BlockManager] (default: core:SparkEnv.md#blockManager[system BlockManager]) [[context]] TaskContext [[serializerManager]] Optional serializer:SerializerManager.md[SerializerManager] (default: core:SparkEnv.md#serializerManager[system SerializerManager]) ExternalAppendOnlyMap is created when: Aggregator is requested to rdd:Aggregator.md#combineValuesByKey[combineValuesByKey] and rdd:Aggregator.md#combineCombinersByKey[combineCombinersByKey] CoGroupedRDD is requested to compute a partition == [[currentMap]] SizeTrackingAppendOnlyMap ExternalAppendOnlyMap manages a SizeTrackingAppendOnlyMap. A SizeTrackingAppendOnlyMap is created immediately when ExternalAppendOnlyMap is and every time when < > and < > spilled to disk. SizeTrackingAppendOnlyMap are dereferenced ( null ed) for the memory to be garbage-collected when < > and < >. SizeTrackingAppendOnlyMap is used when < >, < >, < > and < >. == [[insertAll]] Inserting All Key-Value Pairs (from Iterator) [source, scala] \u00b6 insertAll( entries: Iterator[Product2[K, V]]): Unit [[insertAll-update-function]] insertAll creates an update function that uses the < > function for an existing value or the < > function for a new value. For every key-value pair (from the input iterator), insertAll does the following: Requests the < > for the estimated size and, if greater than the <<_peakMemoryUsedBytes, _peakMemoryUsedBytes>> metric, updates it. shuffle:Spillable.md#maybeSpill[Spills to a disk if necessary] and, if spilled, creates a new < > Requests the < > to change value for the current value (with the < > function) shuffle:Spillable.md#addElementsRead[Increments the elements read counter] === [[insertAll-usage]] Usage insertAll is used when: Aggregator is requested to rdd:Aggregator.md#combineValuesByKey[combineValuesByKey] and rdd:Aggregator.md#combineCombinersByKey[combineCombinersByKey] CoGroupedRDD is requested to compute a partition ExternalAppendOnlyMap is requested to < > === [[insertAll-requirements]] Requirements insertAll throws an IllegalStateException when the < > internal registry is null : [source,plaintext] \u00b6 Cannot insert new elements into a map after calling iterator \u00b6 == [[iterator]] Iterator of \"Combined\" Pairs [source, scala] \u00b6 iterator: Iterator[(K, C)] \u00b6 iterator...FIXME iterator is used when...FIXME == [[spill]] Spilling to Disk if Necessary [source, scala] \u00b6 spill( collection: SizeTracker): Unit spill...FIXME spill is used when...FIXME == [[forceSpill]] Forcing Disk Spilling [source, scala] \u00b6 forceSpill(): Boolean \u00b6 forceSpill returns a flag to indicate whether spilling to disk has really happened ( true ) or not ( false ). forceSpill branches off per the current state it is in (and should rather use a state-aware implementation). When a < > is in use, forceSpill requests it to spill and, if it did, dereferences ( null ify) the < >. forceSpill returns whatever the spilling of the < > returned. When there is at least one element in the < >, forceSpill < > it. forceSpill then creates a new < > and always returns true . In other cases, forceSpill simply returns false . forceSpill is part of the shuffle:Spillable.md[Spillable] abstraction. == [[freeCurrentMap]] Freeing Up SizeTrackingAppendOnlyMap and Releasing Memory [source, scala] \u00b6 freeCurrentMap(): Unit \u00b6 freeCurrentMap dereferences ( null ify) the < > (if there still was one) followed by shuffle:Spillable.md#releaseMemory[releasing all memory]. freeCurrentMap is used when SpillableIterator is requested to destroy itself. == [[spillMemoryIteratorToDisk]] spillMemoryIteratorToDisk Method [source, scala] \u00b6 spillMemoryIteratorToDisk( inMemoryIterator: Iterator[(K, C)]): DiskMapIterator spillMemoryIteratorToDisk...FIXME spillMemoryIteratorToDisk is used when...FIXME","title":"ExternalAppendOnlyMap"},{"location":"shuffle/ExternalAppendOnlyMap/#externalappendonlymap","text":"ExternalAppendOnlyMap is a Spillable of SizeTrackers. ExternalAppendOnlyMap[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values.","title":"ExternalAppendOnlyMap"},{"location":"shuffle/ExternalAppendOnlyMap/#creating-instance","text":"ExternalAppendOnlyMap takes the following to be created: [[createCombiner]] createCombiner function ( V => C ) [[mergeValue]] mergeValue function ( (C, V) => C ) [[mergeCombiners]] mergeCombiners function ( (C, C) => C ) [[serializer]] Optional serializer:Serializer.md[Serializer] (default: core:SparkEnv.md#serializer[system Serializer]) [[blockManager]] Optional storage:BlockManager.md[BlockManager] (default: core:SparkEnv.md#blockManager[system BlockManager]) [[context]] TaskContext [[serializerManager]] Optional serializer:SerializerManager.md[SerializerManager] (default: core:SparkEnv.md#serializerManager[system SerializerManager]) ExternalAppendOnlyMap is created when: Aggregator is requested to rdd:Aggregator.md#combineValuesByKey[combineValuesByKey] and rdd:Aggregator.md#combineCombinersByKey[combineCombinersByKey] CoGroupedRDD is requested to compute a partition == [[currentMap]] SizeTrackingAppendOnlyMap ExternalAppendOnlyMap manages a SizeTrackingAppendOnlyMap. A SizeTrackingAppendOnlyMap is created immediately when ExternalAppendOnlyMap is and every time when < > and < > spilled to disk. SizeTrackingAppendOnlyMap are dereferenced ( null ed) for the memory to be garbage-collected when < > and < >. SizeTrackingAppendOnlyMap is used when < >, < >, < > and < >. == [[insertAll]] Inserting All Key-Value Pairs (from Iterator)","title":"Creating Instance"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala","text":"insertAll( entries: Iterator[Product2[K, V]]): Unit [[insertAll-update-function]] insertAll creates an update function that uses the < > function for an existing value or the < > function for a new value. For every key-value pair (from the input iterator), insertAll does the following: Requests the < > for the estimated size and, if greater than the <<_peakMemoryUsedBytes, _peakMemoryUsedBytes>> metric, updates it. shuffle:Spillable.md#maybeSpill[Spills to a disk if necessary] and, if spilled, creates a new < > Requests the < > to change value for the current value (with the < > function) shuffle:Spillable.md#addElementsRead[Increments the elements read counter] === [[insertAll-usage]] Usage insertAll is used when: Aggregator is requested to rdd:Aggregator.md#combineValuesByKey[combineValuesByKey] and rdd:Aggregator.md#combineCombinersByKey[combineCombinersByKey] CoGroupedRDD is requested to compute a partition ExternalAppendOnlyMap is requested to < > === [[insertAll-requirements]] Requirements insertAll throws an IllegalStateException when the < > internal registry is null :","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"shuffle/ExternalAppendOnlyMap/#cannot-insert-new-elements-into-a-map-after-calling-iterator","text":"== [[iterator]] Iterator of \"Combined\" Pairs","title":"Cannot insert new elements into a map after calling iterator"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_1","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#iterator-iteratork-c","text":"iterator...FIXME iterator is used when...FIXME == [[spill]] Spilling to Disk if Necessary","title":"iterator: Iterator[(K, C)]"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_2","text":"spill( collection: SizeTracker): Unit spill...FIXME spill is used when...FIXME == [[forceSpill]] Forcing Disk Spilling","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_3","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#forcespill-boolean","text":"forceSpill returns a flag to indicate whether spilling to disk has really happened ( true ) or not ( false ). forceSpill branches off per the current state it is in (and should rather use a state-aware implementation). When a < > is in use, forceSpill requests it to spill and, if it did, dereferences ( null ify) the < >. forceSpill returns whatever the spilling of the < > returned. When there is at least one element in the < >, forceSpill < > it. forceSpill then creates a new < > and always returns true . In other cases, forceSpill simply returns false . forceSpill is part of the shuffle:Spillable.md[Spillable] abstraction. == [[freeCurrentMap]] Freeing Up SizeTrackingAppendOnlyMap and Releasing Memory","title":"forceSpill(): Boolean"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_4","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#freecurrentmap-unit","text":"freeCurrentMap dereferences ( null ify) the < > (if there still was one) followed by shuffle:Spillable.md#releaseMemory[releasing all memory]. freeCurrentMap is used when SpillableIterator is requested to destroy itself. == [[spillMemoryIteratorToDisk]] spillMemoryIteratorToDisk Method","title":"freeCurrentMap(): Unit"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_5","text":"spillMemoryIteratorToDisk( inMemoryIterator: Iterator[(K, C)]): DiskMapIterator spillMemoryIteratorToDisk...FIXME spillMemoryIteratorToDisk is used when...FIXME","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/","text":"ExternalSorter \u00b6 ExternalSorter is a Spillable of WritablePartitionedPairCollection of pairs (of K keys and C values). ExternalSorter[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. ExternalSorter is used for the following: SortShuffleWriter to write records BlockStoreShuffleReader to read records (with a key ordering defined) Creating Instance \u00b6 ExternalSorter takes the following to be created: TaskContext Optional Aggregator (default: undefined) Optional Partitioner (default: undefined) Optional [Ordering] ( Scala ) for keys (default: undefined) Serializer (default: Serializer ) ExternalSorter is created when: BlockStoreShuffleReader is requested to read records (for a reduce task) SortShuffleWriter is requested to write records (as a ExternalSorter[K, V, C] or ExternalSorter[K, V, V] based on Map-Size Partial Aggregation Flag ) Inserting Records \u00b6 insertAll ( records : Iterator [ Product2 [ K , V ]]): Unit insertAll branches off per whether the optional Aggregator was specified or not (when creating the ExternalSorter ). insertAll takes all records eagerly and materializes the given records iterator. Map-Side Aggregator Specified \u00b6 With an Aggregator given, insertAll creates an update function based on the mergeValue and createCombiner functions of the Aggregator . For every record, insertAll increment internal read counter . insertAll requests the PartitionedAppendOnlyMap to changeValue for the key (made up of the partition of the key of the current record and the key itself, i.e. (partition, key) ) with the update function. In the end, insertAll spills the in-memory collection to disk if needed with the usingMap flag enabled (to indicate that the PartitionedAppendOnlyMap was updated). No Map-Side Aggregator Specified \u00b6 With no Aggregator given, insertAll iterates over all the records and uses the PartitionedPairBuffer instead. For every record, insertAll increment internal read counter . insertAll requests the PartitionedPairBuffer to insert with the partition of the key of the current record, the key itself and the value of the current record. In the end, insertAll spills the in-memory collection to disk if needed with the usingMap flag disabled (since this time the PartitionedPairBuffer was updated, not the PartitionedAppendOnlyMap ). Spilling In-Memory Collection to Disk \u00b6 maybeSpillCollection ( usingMap : Boolean ): Unit maybeSpillCollection branches per the input usingMap flag (to indicate which in-memory collection to use, the PartitionedAppendOnlyMap or the PartitionedPairBuffer ). maybeSpillCollection requests the collection to estimate size (in bytes) that is tracked as the peakMemoryUsedBytes metric (for every size bigger than what is currently recorded). maybeSpillCollection spills the collection to disk if needed . If spilled, maybeSpillCollection creates a new collection (a new PartitionedAppendOnlyMap or a new PartitionedPairBuffer ). Usage \u00b6 insertAll is used when: SortShuffleWriter is requested to write records (as a ExternalSorter[K, V, C] or ExternalSorter[K, V, V] based on Map-Size Partial Aggregation Flag ) BlockStoreShuffleReader is requested to read records (with a key sorting defined) In-Memory Collections of Records \u00b6 ExternalSorter uses PartitionedPairBuffer s or PartitionedAppendOnlyMap s to store records in memory before spilling to disk . ExternalSorter uses PartitionedPairBuffer s when created with no Aggregator . Otherwise, ExternalSorter uses PartitionedAppendOnlyMap s. ExternalSorter inserts records to the collections when insertAll . ExternalSorter spills the in-memory collection to disk if needed and, if so , creates a new collection. ExternalSorter releases the collections ( null s them) when requested to forceSpill and stop . That is when the JVM garbage collector takes care of evicting them from memory completely. Peak Size of In-Memory Collection \u00b6 ExternalSorter tracks the peak size (in bytes) of the in-memory collection whenever requested to spill the in-memory collection to disk if needed . The peak size is used when: BlockStoreShuffleReader is requested to read combined records for a reduce task (with an ordering defined) ExternalSorter is requested to writePartitionedMapOutput Spills \u00b6 spills : ArrayBuffer [ SpilledFile ] ExternalSorter creates the spills internal buffer of SpilledFile s when created . A new SpilledFile is added when ExternalSorter is requested to spill . Note No elements in spills indicate that there is only in-memory data. SpilledFile s are deleted physically from disk and the spills buffer is cleared when ExternalSorter is requested to stop . ExternalSorter uses the spills buffer when requested for an partitionedIterator . Number of Spills \u00b6 numSpills : Int numSpills is the number of spill files this sorter has spilled . SpilledFile \u00b6 SpilledFile is a metadata of a spilled file: File ( Java ) BlockId Serializer Batch Sizes ( Array[Long] ) Elements per Partition ( Array[Long] ) Spilling Data to Disk \u00b6 spill ( collection : WritablePartitionedPairCollection [ K , C ]): Unit spill is part of the Spillable abstraction. spill requests the given WritablePartitionedPairCollection for a destructive WritablePartitionedIterator . spill spillMemoryIteratorToDisk (with the destructive WritablePartitionedIterator ) that creates a SpilledFile . In the end, spill adds the SpilledFile to the spills internal registry. spillMemoryIteratorToDisk \u00b6 spillMemoryIteratorToDisk ( inMemoryIterator : WritablePartitionedIterator ): SpilledFile spillMemoryIteratorToDisk ...FIXME spillMemoryIteratorToDisk is used when: ExternalSorter is requested to spill SpillableIterator is requested to spill partitionedIterator \u00b6 partitionedIterator : Iterator [( Int , Iterator [ Product2 [ K , C ]])] partitionedIterator ...FIXME partitionedIterator is used when: ExternalSorter is requested for an iterator and to writePartitionedMapOutput writePartitionedMapOutput \u00b6 writePartitionedMapOutput ( shuffleId : Int , mapId : Long , mapOutputWriter : ShuffleMapOutputWriter ): Unit writePartitionedMapOutput ...FIXME writePartitionedMapOutput is used when: SortShuffleWriter is requested to write records Iterator \u00b6 iterator : Iterator [ Product2 [ K , C ]] iterator turns the isShuffleSort flag off ( false ). iterator partitionedIterator and takes the combined values (the second elements) only. iterator is used when: BlockStoreShuffleReader is requested to read combined records for a reduce task Stopping ExternalSorter \u00b6 stop (): Unit stop ...FIXME stop is used when: BlockStoreShuffleReader is requested to read records (with ordering defined) SortShuffleWriter is requested to stop Logging \u00b6 Enable ALL logging level for org.apache.spark.util.collection.ExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.collection.ExternalSorter=ALL Refer to Logging .","title":"ExternalSorter"},{"location":"shuffle/ExternalSorter/#externalsorter","text":"ExternalSorter is a Spillable of WritablePartitionedPairCollection of pairs (of K keys and C values). ExternalSorter[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. ExternalSorter is used for the following: SortShuffleWriter to write records BlockStoreShuffleReader to read records (with a key ordering defined)","title":"ExternalSorter"},{"location":"shuffle/ExternalSorter/#creating-instance","text":"ExternalSorter takes the following to be created: TaskContext Optional Aggregator (default: undefined) Optional Partitioner (default: undefined) Optional [Ordering] ( Scala ) for keys (default: undefined) Serializer (default: Serializer ) ExternalSorter is created when: BlockStoreShuffleReader is requested to read records (for a reduce task) SortShuffleWriter is requested to write records (as a ExternalSorter[K, V, C] or ExternalSorter[K, V, V] based on Map-Size Partial Aggregation Flag )","title":"Creating Instance"},{"location":"shuffle/ExternalSorter/#inserting-records","text":"insertAll ( records : Iterator [ Product2 [ K , V ]]): Unit insertAll branches off per whether the optional Aggregator was specified or not (when creating the ExternalSorter ). insertAll takes all records eagerly and materializes the given records iterator.","title":" Inserting Records"},{"location":"shuffle/ExternalSorter/#map-side-aggregator-specified","text":"With an Aggregator given, insertAll creates an update function based on the mergeValue and createCombiner functions of the Aggregator . For every record, insertAll increment internal read counter . insertAll requests the PartitionedAppendOnlyMap to changeValue for the key (made up of the partition of the key of the current record and the key itself, i.e. (partition, key) ) with the update function. In the end, insertAll spills the in-memory collection to disk if needed with the usingMap flag enabled (to indicate that the PartitionedAppendOnlyMap was updated).","title":" Map-Side Aggregator Specified"},{"location":"shuffle/ExternalSorter/#no-map-side-aggregator-specified","text":"With no Aggregator given, insertAll iterates over all the records and uses the PartitionedPairBuffer instead. For every record, insertAll increment internal read counter . insertAll requests the PartitionedPairBuffer to insert with the partition of the key of the current record, the key itself and the value of the current record. In the end, insertAll spills the in-memory collection to disk if needed with the usingMap flag disabled (since this time the PartitionedPairBuffer was updated, not the PartitionedAppendOnlyMap ).","title":" No Map-Side Aggregator Specified"},{"location":"shuffle/ExternalSorter/#spilling-in-memory-collection-to-disk","text":"maybeSpillCollection ( usingMap : Boolean ): Unit maybeSpillCollection branches per the input usingMap flag (to indicate which in-memory collection to use, the PartitionedAppendOnlyMap or the PartitionedPairBuffer ). maybeSpillCollection requests the collection to estimate size (in bytes) that is tracked as the peakMemoryUsedBytes metric (for every size bigger than what is currently recorded). maybeSpillCollection spills the collection to disk if needed . If spilled, maybeSpillCollection creates a new collection (a new PartitionedAppendOnlyMap or a new PartitionedPairBuffer ).","title":" Spilling In-Memory Collection to Disk"},{"location":"shuffle/ExternalSorter/#usage","text":"insertAll is used when: SortShuffleWriter is requested to write records (as a ExternalSorter[K, V, C] or ExternalSorter[K, V, V] based on Map-Size Partial Aggregation Flag ) BlockStoreShuffleReader is requested to read records (with a key sorting defined)","title":" Usage"},{"location":"shuffle/ExternalSorter/#in-memory-collections-of-records","text":"ExternalSorter uses PartitionedPairBuffer s or PartitionedAppendOnlyMap s to store records in memory before spilling to disk . ExternalSorter uses PartitionedPairBuffer s when created with no Aggregator . Otherwise, ExternalSorter uses PartitionedAppendOnlyMap s. ExternalSorter inserts records to the collections when insertAll . ExternalSorter spills the in-memory collection to disk if needed and, if so , creates a new collection. ExternalSorter releases the collections ( null s them) when requested to forceSpill and stop . That is when the JVM garbage collector takes care of evicting them from memory completely.","title":" In-Memory Collections of Records"},{"location":"shuffle/ExternalSorter/#peak-size-of-in-memory-collection","text":"ExternalSorter tracks the peak size (in bytes) of the in-memory collection whenever requested to spill the in-memory collection to disk if needed . The peak size is used when: BlockStoreShuffleReader is requested to read combined records for a reduce task (with an ordering defined) ExternalSorter is requested to writePartitionedMapOutput","title":" Peak Size of In-Memory Collection"},{"location":"shuffle/ExternalSorter/#spills","text":"spills : ArrayBuffer [ SpilledFile ] ExternalSorter creates the spills internal buffer of SpilledFile s when created . A new SpilledFile is added when ExternalSorter is requested to spill . Note No elements in spills indicate that there is only in-memory data. SpilledFile s are deleted physically from disk and the spills buffer is cleared when ExternalSorter is requested to stop . ExternalSorter uses the spills buffer when requested for an partitionedIterator .","title":" Spills"},{"location":"shuffle/ExternalSorter/#number-of-spills","text":"numSpills : Int numSpills is the number of spill files this sorter has spilled .","title":" Number of Spills"},{"location":"shuffle/ExternalSorter/#spilledfile","text":"SpilledFile is a metadata of a spilled file: File ( Java ) BlockId Serializer Batch Sizes ( Array[Long] ) Elements per Partition ( Array[Long] )","title":" SpilledFile"},{"location":"shuffle/ExternalSorter/#spilling-data-to-disk","text":"spill ( collection : WritablePartitionedPairCollection [ K , C ]): Unit spill is part of the Spillable abstraction. spill requests the given WritablePartitionedPairCollection for a destructive WritablePartitionedIterator . spill spillMemoryIteratorToDisk (with the destructive WritablePartitionedIterator ) that creates a SpilledFile . In the end, spill adds the SpilledFile to the spills internal registry.","title":" Spilling Data to Disk"},{"location":"shuffle/ExternalSorter/#spillmemoryiteratortodisk","text":"spillMemoryIteratorToDisk ( inMemoryIterator : WritablePartitionedIterator ): SpilledFile spillMemoryIteratorToDisk ...FIXME spillMemoryIteratorToDisk is used when: ExternalSorter is requested to spill SpillableIterator is requested to spill","title":" spillMemoryIteratorToDisk"},{"location":"shuffle/ExternalSorter/#partitionediterator","text":"partitionedIterator : Iterator [( Int , Iterator [ Product2 [ K , C ]])] partitionedIterator ...FIXME partitionedIterator is used when: ExternalSorter is requested for an iterator and to writePartitionedMapOutput","title":" partitionedIterator"},{"location":"shuffle/ExternalSorter/#writepartitionedmapoutput","text":"writePartitionedMapOutput ( shuffleId : Int , mapId : Long , mapOutputWriter : ShuffleMapOutputWriter ): Unit writePartitionedMapOutput ...FIXME writePartitionedMapOutput is used when: SortShuffleWriter is requested to write records","title":" writePartitionedMapOutput"},{"location":"shuffle/ExternalSorter/#iterator","text":"iterator : Iterator [ Product2 [ K , C ]] iterator turns the isShuffleSort flag off ( false ). iterator partitionedIterator and takes the combined values (the second elements) only. iterator is used when: BlockStoreShuffleReader is requested to read combined records for a reduce task","title":" Iterator"},{"location":"shuffle/ExternalSorter/#stopping-externalsorter","text":"stop (): Unit stop ...FIXME stop is used when: BlockStoreShuffleReader is requested to read records (with ordering defined) SortShuffleWriter is requested to stop","title":" Stopping ExternalSorter"},{"location":"shuffle/ExternalSorter/#logging","text":"Enable ALL logging level for org.apache.spark.util.collection.ExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.collection.ExternalSorter=ALL Refer to Logging .","title":"Logging"},{"location":"shuffle/FetchFailedException/","text":"== [[FetchFailedException]] FetchFailedException FetchFailedException exception executor:TaskRunner.md#run-FetchFailedException[may be thrown when a task runs] (and storage:ShuffleBlockFetcherIterator.md#throwFetchFailedException[ ShuffleBlockFetcherIterator did not manage to fetch shuffle blocks]). FetchFailedException contains the following: the unique identifier for a storage:BlockManager.md[BlockManager] (as storage:BlockManagerId.md[]) shuffleId mapId reduceId A short exception message cause - the root Throwable object When FetchFailedException is reported, executor:TaskRunner.md#run-FetchFailedException[ TaskRunner catches it and notifies ExecutorBackend ] (with TaskState.FAILED task state). The root cause of the FetchFailedException is usually because the executor:Executor.md[] (with the storage:BlockManager.md[BlockManager] for the shuffle blocks) is lost (i.e. no longer available) due to: OutOfMemoryError could be thrown (aka OOMed ) or some other unhandled exception. The cluster manager that manages the workers with the executors of your Spark application, e.g. YARN, enforces the container memory limits and eventually decided to kill the executor due to excessive memory usage. You should review the logs of the Spark application using webui:index.md[web UI], spark-history-server:index.md[Spark History Server] or cluster-specific tools like https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YarnCommands.html#logs[yarn logs -applicationId] for Hadoop YARN. A solution is usually to tune the memory of your Spark application. CAUTION: FIXME Image with the call to ExecutorBackend. === [[toTaskFailedReason]] toTaskFailedReason Method CAUTION: FIXME","title":"FetchFailedException"},{"location":"shuffle/IndexShuffleBlockResolver/","text":"IndexShuffleBlockResolver \u00b6 IndexShuffleBlockResolver is a ShuffleBlockResolver that manages shuffle block data and uses shuffle index files for faster shuffle data access. Creating Instance \u00b6 IndexShuffleBlockResolver takes the following to be created: SparkConf BlockManager IndexShuffleBlockResolver is created when: SortShuffleManager is created LocalDiskShuffleExecutorComponents is requested to initializeExecutor getStoredShuffles \u00b6 getStoredShuffles (): Seq [ ShuffleBlockInfo ] getStoredShuffles is part of the MigratableResolver abstraction. getStoredShuffles ...FIXME putShuffleBlockAsStream \u00b6 putShuffleBlockAsStream ( blockId : BlockId , serializerManager : SerializerManager ): StreamCallbackWithID putShuffleBlockAsStream is part of the MigratableResolver abstraction. putShuffleBlockAsStream ...FIXME getMigrationBlocks \u00b6 getMigrationBlocks ( shuffleBlockInfo : ShuffleBlockInfo ): List [( BlockId , ManagedBuffer )] getMigrationBlocks is part of the MigratableResolver abstraction. getMigrationBlocks ...FIXME Writing Shuffle Index and Data Files \u00b6 writeIndexFileAndCommit ( shuffleId : Int , mapId : Long , lengths : Array [ Long ], dataTmp : File ): Unit writeIndexFileAndCommit finds the index and data files for the input shuffleId and mapId . writeIndexFileAndCommit creates a temporary file for the index file (in the same directory) and writes offsets (as the moving sum of the input lengths starting from 0 to the final offset at the end for the end of the output file). Note The offsets are the sizes in the input lengths exactly. writeIndexFileAndCommit ...FIXME (Review me) writeIndexFileAndCommit < > for the input shuffleId and mapId . writeIndexFileAndCommit < > (aka consistency check ). If the consistency check fails, it means that another attempt for the same task has already written the map outputs successfully and so the input dataTmp and temporary index files are deleted (as no longer correct). If the consistency check succeeds, the existing index and data files are deleted (if they exist) and the temporary index and data files become \"official\", i.e. renamed to their final names. In case of any IO-related exception, writeIndexFileAndCommit throws a IOException with the messages: fail to rename file [indexTmp] to [indexFile] or fail to rename file [dataTmp] to [dataFile] writeIndexFileAndCommit is used when: LocalDiskShuffleMapOutputWriter is requested to commitAllPartitions LocalDiskSingleSpillMapOutputWriter is requested to transferMapSpillFile Removing Shuffle Index and Data Files \u00b6 removeDataByMap ( shuffleId : Int , mapId : Long ): Unit removeDataByMap finds and deletes the shuffle data file (for the input shuffleId and mapId ) followed by finding and deleting the shuffle data index file. removeDataByMap is used when: SortShuffleManager is requested to unregister a shuffle (and remove a shuffle from a shuffle system) Creating Shuffle Block Index File \u00b6 getIndexFile ( shuffleId : Int , mapId : Long , dirs : Option [ Array [ String ]] = None ): File getIndexFile creates a ShuffleIndexBlockId . With dirs local directories defined, getIndexFile places the index file of the ShuffleIndexBlockId (by the name) in the local directories (with the spark.diskStore.subDirectories ). Otherwise, with no local directories, getIndexFile requests the DiskBlockManager (of the BlockManager ) to get the data file . getIndexFile is used when: IndexShuffleBlockResolver is requested to getBlockData , removeDataByMap , putShuffleBlockAsStream , getMigrationBlocks , writeIndexFileAndCommit FallbackStorage is requested to copy Creating Shuffle Block Data File \u00b6 getDataFile ( shuffleId : Int , mapId : Long ): File // (1) getDataFile ( shuffleId : Int , mapId : Long , dirs : Option [ Array [ String ]]): File dirs is None (undefined) getDataFile creates a ShuffleDataBlockId . With dirs local directories defined, getDataFile places the data file of the ShuffleDataBlockId (by the name) in the local directories (with the spark.diskStore.subDirectories ). Otherwise, with no local directories, getDataFile requests the DiskBlockManager (of the BlockManager ) to get the data file . getDataFile is used when: IndexShuffleBlockResolver is requested to getBlockData , removeDataByMap , putShuffleBlockAsStream , getMigrationBlocks , writeIndexFileAndCommit LocalDiskShuffleMapOutputWriter is created LocalDiskSingleSpillMapOutputWriter is requested to transferMapSpillFile FallbackStorage is requested to copy Creating ManagedBuffer to Read Shuffle Block Data File \u00b6 getBlockData ( blockId : BlockId , dirs : Option [ Array [ String ]]): ManagedBuffer getBlockData is part of the ShuffleBlockResolver abstraction. getBlockData ...FIXME Checking Consistency of Shuffle Index and Data Files \u00b6 checkIndexAndDataFile ( index : File , data : File , blocks : Int ): Array [ Long ] Danger Review Me checkIndexAndDataFile first checks if the size of the input index file is exactly the input blocks multiplied by 8 . checkIndexAndDataFile returns null when the numbers, and hence the shuffle index and data files, don't match. checkIndexAndDataFile reads the shuffle index file and converts the offsets into lengths of each block. checkIndexAndDataFile makes sure that the size of the input shuffle data file is exactly the sum of the block lengths. checkIndexAndDataFile returns the block lengths if the numbers match, and null otherwise. TransportConf \u00b6 IndexShuffleBlockResolver creates a TransportConf (for shuffle module) when created . transportConf is used in getMigrationBlocks and getBlockData . Logging \u00b6 Enable ALL logging level for org.apache.spark.shuffle.IndexShuffleBlockResolver logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.IndexShuffleBlockResolver=ALL Refer to Logging .","title":"IndexShuffleBlockResolver"},{"location":"shuffle/IndexShuffleBlockResolver/#indexshuffleblockresolver","text":"IndexShuffleBlockResolver is a ShuffleBlockResolver that manages shuffle block data and uses shuffle index files for faster shuffle data access.","title":"IndexShuffleBlockResolver"},{"location":"shuffle/IndexShuffleBlockResolver/#creating-instance","text":"IndexShuffleBlockResolver takes the following to be created: SparkConf BlockManager IndexShuffleBlockResolver is created when: SortShuffleManager is created LocalDiskShuffleExecutorComponents is requested to initializeExecutor","title":"Creating Instance"},{"location":"shuffle/IndexShuffleBlockResolver/#getstoredshuffles","text":"getStoredShuffles (): Seq [ ShuffleBlockInfo ] getStoredShuffles is part of the MigratableResolver abstraction. getStoredShuffles ...FIXME","title":" getStoredShuffles"},{"location":"shuffle/IndexShuffleBlockResolver/#putshuffleblockasstream","text":"putShuffleBlockAsStream ( blockId : BlockId , serializerManager : SerializerManager ): StreamCallbackWithID putShuffleBlockAsStream is part of the MigratableResolver abstraction. putShuffleBlockAsStream ...FIXME","title":" putShuffleBlockAsStream"},{"location":"shuffle/IndexShuffleBlockResolver/#getmigrationblocks","text":"getMigrationBlocks ( shuffleBlockInfo : ShuffleBlockInfo ): List [( BlockId , ManagedBuffer )] getMigrationBlocks is part of the MigratableResolver abstraction. getMigrationBlocks ...FIXME","title":" getMigrationBlocks"},{"location":"shuffle/IndexShuffleBlockResolver/#writing-shuffle-index-and-data-files","text":"writeIndexFileAndCommit ( shuffleId : Int , mapId : Long , lengths : Array [ Long ], dataTmp : File ): Unit writeIndexFileAndCommit finds the index and data files for the input shuffleId and mapId . writeIndexFileAndCommit creates a temporary file for the index file (in the same directory) and writes offsets (as the moving sum of the input lengths starting from 0 to the final offset at the end for the end of the output file). Note The offsets are the sizes in the input lengths exactly. writeIndexFileAndCommit ...FIXME (Review me) writeIndexFileAndCommit < > for the input shuffleId and mapId . writeIndexFileAndCommit < > (aka consistency check ). If the consistency check fails, it means that another attempt for the same task has already written the map outputs successfully and so the input dataTmp and temporary index files are deleted (as no longer correct). If the consistency check succeeds, the existing index and data files are deleted (if they exist) and the temporary index and data files become \"official\", i.e. renamed to their final names. In case of any IO-related exception, writeIndexFileAndCommit throws a IOException with the messages: fail to rename file [indexTmp] to [indexFile] or fail to rename file [dataTmp] to [dataFile] writeIndexFileAndCommit is used when: LocalDiskShuffleMapOutputWriter is requested to commitAllPartitions LocalDiskSingleSpillMapOutputWriter is requested to transferMapSpillFile","title":" Writing Shuffle Index and Data Files"},{"location":"shuffle/IndexShuffleBlockResolver/#removing-shuffle-index-and-data-files","text":"removeDataByMap ( shuffleId : Int , mapId : Long ): Unit removeDataByMap finds and deletes the shuffle data file (for the input shuffleId and mapId ) followed by finding and deleting the shuffle data index file. removeDataByMap is used when: SortShuffleManager is requested to unregister a shuffle (and remove a shuffle from a shuffle system)","title":" Removing Shuffle Index and Data Files"},{"location":"shuffle/IndexShuffleBlockResolver/#creating-shuffle-block-index-file","text":"getIndexFile ( shuffleId : Int , mapId : Long , dirs : Option [ Array [ String ]] = None ): File getIndexFile creates a ShuffleIndexBlockId . With dirs local directories defined, getIndexFile places the index file of the ShuffleIndexBlockId (by the name) in the local directories (with the spark.diskStore.subDirectories ). Otherwise, with no local directories, getIndexFile requests the DiskBlockManager (of the BlockManager ) to get the data file . getIndexFile is used when: IndexShuffleBlockResolver is requested to getBlockData , removeDataByMap , putShuffleBlockAsStream , getMigrationBlocks , writeIndexFileAndCommit FallbackStorage is requested to copy","title":" Creating Shuffle Block Index File"},{"location":"shuffle/IndexShuffleBlockResolver/#creating-shuffle-block-data-file","text":"getDataFile ( shuffleId : Int , mapId : Long ): File // (1) getDataFile ( shuffleId : Int , mapId : Long , dirs : Option [ Array [ String ]]): File dirs is None (undefined) getDataFile creates a ShuffleDataBlockId . With dirs local directories defined, getDataFile places the data file of the ShuffleDataBlockId (by the name) in the local directories (with the spark.diskStore.subDirectories ). Otherwise, with no local directories, getDataFile requests the DiskBlockManager (of the BlockManager ) to get the data file . getDataFile is used when: IndexShuffleBlockResolver is requested to getBlockData , removeDataByMap , putShuffleBlockAsStream , getMigrationBlocks , writeIndexFileAndCommit LocalDiskShuffleMapOutputWriter is created LocalDiskSingleSpillMapOutputWriter is requested to transferMapSpillFile FallbackStorage is requested to copy","title":" Creating Shuffle Block Data File"},{"location":"shuffle/IndexShuffleBlockResolver/#creating-managedbuffer-to-read-shuffle-block-data-file","text":"getBlockData ( blockId : BlockId , dirs : Option [ Array [ String ]]): ManagedBuffer getBlockData is part of the ShuffleBlockResolver abstraction. getBlockData ...FIXME","title":" Creating ManagedBuffer to Read Shuffle Block Data File"},{"location":"shuffle/IndexShuffleBlockResolver/#checking-consistency-of-shuffle-index-and-data-files","text":"checkIndexAndDataFile ( index : File , data : File , blocks : Int ): Array [ Long ] Danger Review Me checkIndexAndDataFile first checks if the size of the input index file is exactly the input blocks multiplied by 8 . checkIndexAndDataFile returns null when the numbers, and hence the shuffle index and data files, don't match. checkIndexAndDataFile reads the shuffle index file and converts the offsets into lengths of each block. checkIndexAndDataFile makes sure that the size of the input shuffle data file is exactly the sum of the block lengths. checkIndexAndDataFile returns the block lengths if the numbers match, and null otherwise.","title":" Checking Consistency of Shuffle Index and Data Files"},{"location":"shuffle/IndexShuffleBlockResolver/#transportconf","text":"IndexShuffleBlockResolver creates a TransportConf (for shuffle module) when created . transportConf is used in getMigrationBlocks and getBlockData .","title":" TransportConf"},{"location":"shuffle/IndexShuffleBlockResolver/#logging","text":"Enable ALL logging level for org.apache.spark.shuffle.IndexShuffleBlockResolver logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.IndexShuffleBlockResolver=ALL Refer to Logging .","title":"Logging"},{"location":"shuffle/LocalDiskShuffleDataIO/","text":"LocalDiskShuffleDataIO \u00b6 LocalDiskShuffleDataIO is a ShuffleDataIO . ShuffleExecutorComponents \u00b6 ShuffleExecutorComponents executor () executor is part of the ShuffleDataIO abstraction. executor creates a new LocalDiskShuffleExecutorComponents .","title":"LocalDiskShuffleDataIO"},{"location":"shuffle/LocalDiskShuffleDataIO/#localdiskshuffledataio","text":"LocalDiskShuffleDataIO is a ShuffleDataIO .","title":"LocalDiskShuffleDataIO"},{"location":"shuffle/LocalDiskShuffleDataIO/#shuffleexecutorcomponents","text":"ShuffleExecutorComponents executor () executor is part of the ShuffleDataIO abstraction. executor creates a new LocalDiskShuffleExecutorComponents .","title":" ShuffleExecutorComponents"},{"location":"shuffle/LocalDiskShuffleExecutorComponents/","text":"LocalDiskShuffleExecutorComponents \u00b6 LocalDiskShuffleExecutorComponents is a ShuffleExecutorComponents . Creating Instance \u00b6 LocalDiskShuffleExecutorComponents takes the following to be created: SparkConf LocalDiskShuffleExecutorComponents is created when: LocalDiskShuffleDataIO is requested for a ShuffleExecutorComponents","title":"LocalDiskShuffleExecutorComponents"},{"location":"shuffle/LocalDiskShuffleExecutorComponents/#localdiskshuffleexecutorcomponents","text":"LocalDiskShuffleExecutorComponents is a ShuffleExecutorComponents .","title":"LocalDiskShuffleExecutorComponents"},{"location":"shuffle/LocalDiskShuffleExecutorComponents/#creating-instance","text":"LocalDiskShuffleExecutorComponents takes the following to be created: SparkConf LocalDiskShuffleExecutorComponents is created when: LocalDiskShuffleDataIO is requested for a ShuffleExecutorComponents","title":"Creating Instance"},{"location":"shuffle/LocalDiskShuffleMapOutputWriter/","text":"LocalDiskShuffleMapOutputWriter \u00b6 LocalDiskShuffleMapOutputWriter is...FIXME","title":"LocalDiskShuffleMapOutputWriter"},{"location":"shuffle/LocalDiskShuffleMapOutputWriter/#localdiskshufflemapoutputwriter","text":"LocalDiskShuffleMapOutputWriter is...FIXME","title":"LocalDiskShuffleMapOutputWriter"},{"location":"shuffle/LocalDiskSingleSpillMapOutputWriter/","text":"LocalDiskSingleSpillMapOutputWriter \u00b6 LocalDiskSingleSpillMapOutputWriter is...FIXME","title":"LocalDiskSingleSpillMapOutputWriter"},{"location":"shuffle/LocalDiskSingleSpillMapOutputWriter/#localdisksinglespillmapoutputwriter","text":"LocalDiskSingleSpillMapOutputWriter is...FIXME","title":"LocalDiskSingleSpillMapOutputWriter"},{"location":"shuffle/MigratableResolver/","text":"MigratableResolver \u00b6 MigratableResolver is an abstraction of resolvers that allow Spark to migrate shuffle blocks. Contract \u00b6 getMigrationBlocks \u00b6 getMigrationBlocks ( shuffleBlockInfo : ShuffleBlockInfo ): List [( BlockId , ManagedBuffer )] Used when: ShuffleMigrationRunnable is requested to run getStoredShuffles \u00b6 getStoredShuffles (): Seq [ ShuffleBlockInfo ] Used when: BlockManagerDecommissioner is requested to refreshOffloadingShuffleBlocks putShuffleBlockAsStream \u00b6 putShuffleBlockAsStream ( blockId : BlockId , serializerManager : SerializerManager ): StreamCallbackWithID Used when: BlockManager is requested to putBlockDataAsStream Implementations \u00b6 IndexShuffleBlockResolver","title":"MigratableResolver"},{"location":"shuffle/MigratableResolver/#migratableresolver","text":"MigratableResolver is an abstraction of resolvers that allow Spark to migrate shuffle blocks.","title":"MigratableResolver"},{"location":"shuffle/MigratableResolver/#contract","text":"","title":"Contract"},{"location":"shuffle/MigratableResolver/#getmigrationblocks","text":"getMigrationBlocks ( shuffleBlockInfo : ShuffleBlockInfo ): List [( BlockId , ManagedBuffer )] Used when: ShuffleMigrationRunnable is requested to run","title":" getMigrationBlocks"},{"location":"shuffle/MigratableResolver/#getstoredshuffles","text":"getStoredShuffles (): Seq [ ShuffleBlockInfo ] Used when: BlockManagerDecommissioner is requested to refreshOffloadingShuffleBlocks","title":" getStoredShuffles"},{"location":"shuffle/MigratableResolver/#putshuffleblockasstream","text":"putShuffleBlockAsStream ( blockId : BlockId , serializerManager : SerializerManager ): StreamCallbackWithID Used when: BlockManager is requested to putBlockDataAsStream","title":" putShuffleBlockAsStream"},{"location":"shuffle/MigratableResolver/#implementations","text":"IndexShuffleBlockResolver","title":"Implementations"},{"location":"shuffle/SerializedShuffleHandle/","text":"SerializedShuffleHandle \u00b6 SerializedShuffleHandle is a ShuffleHandle to identify the choice of a serialized shuffle . SerializedShuffleHandle is used to create an UnsafeShuffleWriter . Creating Instance \u00b6 SerializedShuffleHandle takes the following to be created: [[shuffleId]] Shuffle ID [[numMaps]] Number of mappers [[dependency]] ShuffleDependency[K, V, V] SerializedShuffleHandle is created when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ). SortShuffleManager determines what shuffle handle to use by first checking out the requirements of BypassMergeSortShuffleHandle before SerializedShuffleHandle 's.","title":"SerializedShuffleHandle"},{"location":"shuffle/SerializedShuffleHandle/#serializedshufflehandle","text":"SerializedShuffleHandle is a ShuffleHandle to identify the choice of a serialized shuffle . SerializedShuffleHandle is used to create an UnsafeShuffleWriter .","title":"SerializedShuffleHandle"},{"location":"shuffle/SerializedShuffleHandle/#creating-instance","text":"SerializedShuffleHandle takes the following to be created: [[shuffleId]] Shuffle ID [[numMaps]] Number of mappers [[dependency]] ShuffleDependency[K, V, V] SerializedShuffleHandle is created when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ). SortShuffleManager determines what shuffle handle to use by first checking out the requirements of BypassMergeSortShuffleHandle before SerializedShuffleHandle 's.","title":"Creating Instance"},{"location":"shuffle/ShuffleBlockResolver/","text":"= [[ShuffleBlockResolver]] ShuffleBlockResolver ShuffleBlockResolver is an < > of < > that storage:BlockManager.md[BlockManager] uses to < > for a logical shuffle block identifier (i.e. map, reduce, and shuffle). NOTE: Shuffle block data files are often referred to as map outputs files . [[implementations]] NOTE: shuffle:IndexShuffleBlockResolver.md[IndexShuffleBlockResolver] is the default and only known ShuffleBlockResolver in Apache Spark. [[contract]] .ShuffleBlockResolver Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | getBlockData a| [[getBlockData]] [source, scala] \u00b6 getBlockData( blockId: ShuffleBlockId): ManagedBuffer Retrieves the data (as a network:ManagedBuffer.md[]) for the given storage:BlockId.md#ShuffleBlockId[block] (a tuple of shuffleId , mapId and reduceId ). Used when BlockManager is requested to retrieve a storage:BlockManager.md#getLocalBytes[block data from a local block manager] and storage:BlockManager.md#getBlockData[block data] | stop a| [[stop]] [source, scala] \u00b6 stop(): Unit \u00b6 Stops the ShuffleBlockResolver Used when SortShuffleManager is requested to SortShuffleManager.md#stop[stop] |===","title":"ShuffleBlockResolver"},{"location":"shuffle/ShuffleBlockResolver/#source-scala","text":"getBlockData( blockId: ShuffleBlockId): ManagedBuffer Retrieves the data (as a network:ManagedBuffer.md[]) for the given storage:BlockId.md#ShuffleBlockId[block] (a tuple of shuffleId , mapId and reduceId ). Used when BlockManager is requested to retrieve a storage:BlockManager.md#getLocalBytes[block data from a local block manager] and storage:BlockManager.md#getBlockData[block data] | stop a| [[stop]]","title":"[source, scala]"},{"location":"shuffle/ShuffleBlockResolver/#source-scala_1","text":"","title":"[source, scala]"},{"location":"shuffle/ShuffleBlockResolver/#stop-unit","text":"Stops the ShuffleBlockResolver Used when SortShuffleManager is requested to SortShuffleManager.md#stop[stop] |===","title":"stop(): Unit"},{"location":"shuffle/ShuffleDataIO/","text":"ShuffleDataIO \u00b6 ShuffleDataIO is an abstraction of pluggable temporary shuffle block store plugins for storing shuffle blocks in arbitrary storage backends. Contract \u00b6 ShuffleDriverComponents \u00b6 ShuffleDriverComponents driver () Used when: SparkContext is created ShuffleExecutorComponents \u00b6 ShuffleExecutorComponents executor () Used when: SortShuffleManager utility is used to load the ShuffleExecutorComponents Implementations \u00b6 LocalDiskShuffleDataIO","title":"ShuffleDataIO"},{"location":"shuffle/ShuffleDataIO/#shuffledataio","text":"ShuffleDataIO is an abstraction of pluggable temporary shuffle block store plugins for storing shuffle blocks in arbitrary storage backends.","title":"ShuffleDataIO"},{"location":"shuffle/ShuffleDataIO/#contract","text":"","title":"Contract"},{"location":"shuffle/ShuffleDataIO/#shuffledrivercomponents","text":"ShuffleDriverComponents driver () Used when: SparkContext is created","title":" ShuffleDriverComponents"},{"location":"shuffle/ShuffleDataIO/#shuffleexecutorcomponents","text":"ShuffleExecutorComponents executor () Used when: SortShuffleManager utility is used to load the ShuffleExecutorComponents","title":" ShuffleExecutorComponents"},{"location":"shuffle/ShuffleDataIO/#implementations","text":"LocalDiskShuffleDataIO","title":"Implementations"},{"location":"shuffle/ShuffleDataIOUtils/","text":"ShuffleDataIOUtils \u00b6 Loading ShuffleDataIO \u00b6 loadShuffleDataIO ( conf : SparkConf ): ShuffleDataIO loadShuffleDataIO uses the spark.shuffle.sort.io.plugin.class configuration property to load the ShuffleDataIO . loadShuffleDataIO is used when: SparkContext is created SortShuffleManager utility is used to loadShuffleExecutorComponents","title":"ShuffleDataIOUtils"},{"location":"shuffle/ShuffleDataIOUtils/#shuffledataioutils","text":"","title":"ShuffleDataIOUtils"},{"location":"shuffle/ShuffleDataIOUtils/#loading-shuffledataio","text":"loadShuffleDataIO ( conf : SparkConf ): ShuffleDataIO loadShuffleDataIO uses the spark.shuffle.sort.io.plugin.class configuration property to load the ShuffleDataIO . loadShuffleDataIO is used when: SparkContext is created SortShuffleManager utility is used to loadShuffleExecutorComponents","title":" Loading ShuffleDataIO"},{"location":"shuffle/ShuffleDriverComponents/","text":"ShuffleDriverComponents \u00b6 ShuffleDriverComponents is...FIXME","title":"ShuffleDriverComponents"},{"location":"shuffle/ShuffleDriverComponents/#shuffledrivercomponents","text":"ShuffleDriverComponents is...FIXME","title":"ShuffleDriverComponents"},{"location":"shuffle/ShuffleExecutorComponents/","text":"ShuffleExecutorComponents \u00b6 ShuffleExecutorComponents is an abstraction of executor shuffle builders . Contract \u00b6 createMapOutputWriter \u00b6 ShuffleMapOutputWriter createMapOutputWriter ( int shuffleId , long mapTaskId , int numPartitions ) throws IOException Creates a ShuffleMapOutputWriter Used when: BypassMergeSortShuffleWriter is requested to write records UnsafeShuffleWriter is requested to mergeSpills and mergeSpillsUsingStandardWriter SortShuffleWriter is requested to write records createSingleFileMapOutputWriter \u00b6 Optional < SingleSpillShuffleMapOutputWriter > createSingleFileMapOutputWriter ( int shuffleId , long mapId ) throws IOException Creates a SingleSpillShuffleMapOutputWriter Default: empty Used when: UnsafeShuffleWriter is requested to mergeSpills initializeExecutor \u00b6 void initializeExecutor ( String appId , String execId , Map < String , String > extraConfigs ); Used when: SortShuffleManager utility is used to loadShuffleExecutorComponents Implementations \u00b6 LocalDiskShuffleExecutorComponents","title":"ShuffleExecutorComponents"},{"location":"shuffle/ShuffleExecutorComponents/#shuffleexecutorcomponents","text":"ShuffleExecutorComponents is an abstraction of executor shuffle builders .","title":"ShuffleExecutorComponents"},{"location":"shuffle/ShuffleExecutorComponents/#contract","text":"","title":"Contract"},{"location":"shuffle/ShuffleExecutorComponents/#createmapoutputwriter","text":"ShuffleMapOutputWriter createMapOutputWriter ( int shuffleId , long mapTaskId , int numPartitions ) throws IOException Creates a ShuffleMapOutputWriter Used when: BypassMergeSortShuffleWriter is requested to write records UnsafeShuffleWriter is requested to mergeSpills and mergeSpillsUsingStandardWriter SortShuffleWriter is requested to write records","title":" createMapOutputWriter"},{"location":"shuffle/ShuffleExecutorComponents/#createsinglefilemapoutputwriter","text":"Optional < SingleSpillShuffleMapOutputWriter > createSingleFileMapOutputWriter ( int shuffleId , long mapId ) throws IOException Creates a SingleSpillShuffleMapOutputWriter Default: empty Used when: UnsafeShuffleWriter is requested to mergeSpills","title":" createSingleFileMapOutputWriter"},{"location":"shuffle/ShuffleExecutorComponents/#initializeexecutor","text":"void initializeExecutor ( String appId , String execId , Map < String , String > extraConfigs ); Used when: SortShuffleManager utility is used to loadShuffleExecutorComponents","title":" initializeExecutor"},{"location":"shuffle/ShuffleExecutorComponents/#implementations","text":"LocalDiskShuffleExecutorComponents","title":"Implementations"},{"location":"shuffle/ShuffleExternalSorter/","text":"ShuffleExternalSorter \u00b6 ShuffleExternalSorter is a specialized cache-efficient sorter that sorts arrays of compressed record pointers and partition ids. ShuffleExternalSorter uses only 8 bytes of space per record in the sorting array to fit more of the array into cache. ShuffleExternalSorter is created and used by UnsafeShuffleWriter only. MemoryConsumer \u00b6 ShuffleExternalSorter is a MemoryConsumer with page size of 128 MB (unless TaskMemoryManager uses smaller). ShuffleExternalSorter can spill to disk to free up execution memory . Configuration Properties \u00b6 spark.shuffle.file.buffer \u00b6 ShuffleExternalSorter uses spark.shuffle.file.buffer configuration property for...FIXME spark.shuffle.spill.numElementsForceSpillThreshold \u00b6 ShuffleExternalSorter uses spark.shuffle.spill.numElementsForceSpillThreshold configuration property for...FIXME Creating Instance \u00b6 ShuffleExternalSorter takes the following to be created: TaskMemoryManager BlockManager TaskContext Initial Size Number of Partitions SparkConf ShuffleWriteMetricsReporter ShuffleExternalSorter is created when UnsafeShuffleWriter is requested to open a ShuffleExternalSorter . ShuffleInMemorySorter \u00b6 ShuffleExternalSorter manages a ShuffleInMemorySorter : ShuffleInMemorySorter is created immediately when ShuffleExternalSorter is ShuffleInMemorySorter is requested to free up memory and dereferenced ( null ed) when ShuffleExternalSorter is requested to cleanupResources and closeAndGetSpills ShuffleExternalSorter uses the ShuffleInMemorySorter for the following: writeSortedFile spill getMemoryUsage growPointerArrayIfNecessary insertRecord Spilling To Disk \u00b6 long spill ( long size , MemoryConsumer trigger ) spill returns the memory bytes spilled ( spill size ). spill prints out the following INFO message to the logs: Thread [threadId] spilling sort data of [memoryUsage] to disk ([spillsSize] [time|times] so far) spill writeSortedFile (with the isLastFile flag disabled). spill frees up execution memory (and records the memory bytes spilled as spillSize ). spill requests the ShuffleInMemorySorter to reset . In the end, spill requests the TaskContext for TaskMetrics to increase the memory bytes spilled . spill is part of the MemoryConsumer abstraction. closeAndGetSpills \u00b6 SpillInfo [] closeAndGetSpills () closeAndGetSpills ...FIXME closeAndGetSpills is used when UnsafeShuffleWriter is requested to closeAndWriteOutput . getMemoryUsage \u00b6 long getMemoryUsage () getMemoryUsage ...FIXME getMemoryUsage is used when ShuffleExternalSorter is created and requested to spill and updatePeakMemoryUsed . updatePeakMemoryUsed \u00b6 void updatePeakMemoryUsed () updatePeakMemoryUsed ...FIXME updatePeakMemoryUsed is used when ShuffleExternalSorter is requested to getPeakMemoryUsedBytes and freeMemory . writeSortedFile \u00b6 void writeSortedFile ( boolean isLastFile ) writeSortedFile ...FIXME writeSortedFile is used when ShuffleExternalSorter is requested to spill and closeAndGetSpills . cleanupResources \u00b6 void cleanupResources () cleanupResources ...FIXME cleanupResources is used when UnsafeShuffleWriter is requested to write records and stop . Inserting Serialized Record Into ShuffleInMemorySorter \u00b6 void insertRecord ( Object recordBase , long recordOffset , int length , int partitionId ) insertRecord ...FIXME insertRecord growPointerArrayIfNecessary . insertRecord ...FIXME insertRecord acquireNewPageIfNecessary . insertRecord ...FIXME insertRecord is used when UnsafeShuffleWriter is requested to insertRecordIntoSorter growPointerArrayIfNecessary \u00b6 void growPointerArrayIfNecessary () growPointerArrayIfNecessary ...FIXME acquireNewPageIfNecessary \u00b6 void acquireNewPageIfNecessary ( int required ) acquireNewPageIfNecessary ...FIXME freeMemory \u00b6 long freeMemory () freeMemory ...FIXME freeMemory is used when ShuffleExternalSorter is requested to spill , cleanupResources , and closeAndGetSpills . Peak Memory Used \u00b6 long getPeakMemoryUsedBytes () getPeakMemoryUsedBytes ...FIXME getPeakMemoryUsedBytes is used when UnsafeShuffleWriter is requested to updatePeakMemoryUsed . Logging \u00b6 Enable ALL logging level for org.apache.spark.shuffle.sort.ShuffleExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.ShuffleExternalSorter=ALL Refer to Logging .","title":"ShuffleExternalSorter"},{"location":"shuffle/ShuffleExternalSorter/#shuffleexternalsorter","text":"ShuffleExternalSorter is a specialized cache-efficient sorter that sorts arrays of compressed record pointers and partition ids. ShuffleExternalSorter uses only 8 bytes of space per record in the sorting array to fit more of the array into cache. ShuffleExternalSorter is created and used by UnsafeShuffleWriter only.","title":"ShuffleExternalSorter"},{"location":"shuffle/ShuffleExternalSorter/#memoryconsumer","text":"ShuffleExternalSorter is a MemoryConsumer with page size of 128 MB (unless TaskMemoryManager uses smaller). ShuffleExternalSorter can spill to disk to free up execution memory .","title":" MemoryConsumer"},{"location":"shuffle/ShuffleExternalSorter/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"shuffle/ShuffleExternalSorter/#sparkshufflefilebuffer","text":"ShuffleExternalSorter uses spark.shuffle.file.buffer configuration property for...FIXME","title":" spark.shuffle.file.buffer"},{"location":"shuffle/ShuffleExternalSorter/#sparkshufflespillnumelementsforcespillthreshold","text":"ShuffleExternalSorter uses spark.shuffle.spill.numElementsForceSpillThreshold configuration property for...FIXME","title":" spark.shuffle.spill.numElementsForceSpillThreshold"},{"location":"shuffle/ShuffleExternalSorter/#creating-instance","text":"ShuffleExternalSorter takes the following to be created: TaskMemoryManager BlockManager TaskContext Initial Size Number of Partitions SparkConf ShuffleWriteMetricsReporter ShuffleExternalSorter is created when UnsafeShuffleWriter is requested to open a ShuffleExternalSorter .","title":"Creating Instance"},{"location":"shuffle/ShuffleExternalSorter/#shuffleinmemorysorter","text":"ShuffleExternalSorter manages a ShuffleInMemorySorter : ShuffleInMemorySorter is created immediately when ShuffleExternalSorter is ShuffleInMemorySorter is requested to free up memory and dereferenced ( null ed) when ShuffleExternalSorter is requested to cleanupResources and closeAndGetSpills ShuffleExternalSorter uses the ShuffleInMemorySorter for the following: writeSortedFile spill getMemoryUsage growPointerArrayIfNecessary insertRecord","title":" ShuffleInMemorySorter"},{"location":"shuffle/ShuffleExternalSorter/#spilling-to-disk","text":"long spill ( long size , MemoryConsumer trigger ) spill returns the memory bytes spilled ( spill size ). spill prints out the following INFO message to the logs: Thread [threadId] spilling sort data of [memoryUsage] to disk ([spillsSize] [time|times] so far) spill writeSortedFile (with the isLastFile flag disabled). spill frees up execution memory (and records the memory bytes spilled as spillSize ). spill requests the ShuffleInMemorySorter to reset . In the end, spill requests the TaskContext for TaskMetrics to increase the memory bytes spilled . spill is part of the MemoryConsumer abstraction.","title":" Spilling To Disk"},{"location":"shuffle/ShuffleExternalSorter/#closeandgetspills","text":"SpillInfo [] closeAndGetSpills () closeAndGetSpills ...FIXME closeAndGetSpills is used when UnsafeShuffleWriter is requested to closeAndWriteOutput .","title":" closeAndGetSpills"},{"location":"shuffle/ShuffleExternalSorter/#getmemoryusage","text":"long getMemoryUsage () getMemoryUsage ...FIXME getMemoryUsage is used when ShuffleExternalSorter is created and requested to spill and updatePeakMemoryUsed .","title":" getMemoryUsage"},{"location":"shuffle/ShuffleExternalSorter/#updatepeakmemoryused","text":"void updatePeakMemoryUsed () updatePeakMemoryUsed ...FIXME updatePeakMemoryUsed is used when ShuffleExternalSorter is requested to getPeakMemoryUsedBytes and freeMemory .","title":" updatePeakMemoryUsed"},{"location":"shuffle/ShuffleExternalSorter/#writesortedfile","text":"void writeSortedFile ( boolean isLastFile ) writeSortedFile ...FIXME writeSortedFile is used when ShuffleExternalSorter is requested to spill and closeAndGetSpills .","title":" writeSortedFile"},{"location":"shuffle/ShuffleExternalSorter/#cleanupresources","text":"void cleanupResources () cleanupResources ...FIXME cleanupResources is used when UnsafeShuffleWriter is requested to write records and stop .","title":" cleanupResources"},{"location":"shuffle/ShuffleExternalSorter/#inserting-serialized-record-into-shuffleinmemorysorter","text":"void insertRecord ( Object recordBase , long recordOffset , int length , int partitionId ) insertRecord ...FIXME insertRecord growPointerArrayIfNecessary . insertRecord ...FIXME insertRecord acquireNewPageIfNecessary . insertRecord ...FIXME insertRecord is used when UnsafeShuffleWriter is requested to insertRecordIntoSorter","title":" Inserting Serialized Record Into ShuffleInMemorySorter"},{"location":"shuffle/ShuffleExternalSorter/#growpointerarrayifnecessary","text":"void growPointerArrayIfNecessary () growPointerArrayIfNecessary ...FIXME","title":" growPointerArrayIfNecessary"},{"location":"shuffle/ShuffleExternalSorter/#acquirenewpageifnecessary","text":"void acquireNewPageIfNecessary ( int required ) acquireNewPageIfNecessary ...FIXME","title":" acquireNewPageIfNecessary"},{"location":"shuffle/ShuffleExternalSorter/#freememory","text":"long freeMemory () freeMemory ...FIXME freeMemory is used when ShuffleExternalSorter is requested to spill , cleanupResources , and closeAndGetSpills .","title":" freeMemory"},{"location":"shuffle/ShuffleExternalSorter/#peak-memory-used","text":"long getPeakMemoryUsedBytes () getPeakMemoryUsedBytes ...FIXME getPeakMemoryUsedBytes is used when UnsafeShuffleWriter is requested to updatePeakMemoryUsed .","title":" Peak Memory Used"},{"location":"shuffle/ShuffleExternalSorter/#logging","text":"Enable ALL logging level for org.apache.spark.shuffle.sort.ShuffleExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.ShuffleExternalSorter=ALL Refer to Logging .","title":"Logging"},{"location":"shuffle/ShuffleHandle/","text":"ShuffleHandle \u00b6 ShuffleHandle is...FIXME","title":"ShuffleHandle"},{"location":"shuffle/ShuffleHandle/#shufflehandle","text":"ShuffleHandle is...FIXME","title":"ShuffleHandle"},{"location":"shuffle/ShuffleInMemorySorter/","text":"ShuffleInMemorySorter \u00b6 ShuffleInMemorySorter is used by ShuffleExternalSorter to < > using < > sort algorithms. == [[creating-instance]] Creating Instance ShuffleInMemorySorter takes the following to be created: [[consumer]] memory:MemoryConsumer.md[MemoryConsumer] [[initialSize]] Initial size [[useRadixSort]] useRadixSort flag (to indicate whether to use < >) ShuffleInMemorySorter requests the given < > to memory:MemoryConsumer.md#allocateArray[allocate an array] of the given < > for the < >. ShuffleInMemorySorter is created for a shuffle:ShuffleExternalSorter.md#inMemSorter[ShuffleExternalSorter]. == [[getSortedIterator]] Iterator of Records Sorted [source, java] \u00b6 ShuffleSorterIterator getSortedIterator() \u00b6 getSortedIterator...FIXME getSortedIterator is used when ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#writeSortedFile[writeSortedFile]. == [[reset]] Resetting [source, java] \u00b6 void reset() \u00b6 reset...FIXME reset is used when...FIXME == [[numRecords]] numRecords Method [source, java] \u00b6 int numRecords() \u00b6 numRecords...FIXME numRecords is used when...FIXME == [[getUsableCapacity]] Calculating Usable Capacity [source, java] \u00b6 int getUsableCapacity() \u00b6 getUsableCapacity calculates the capacity that is a half or two-third of the memory used for the < >. getUsableCapacity is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.ShuffleExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.shuffle.sort.ShuffleExternalSorter=ALL \u00b6 Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[array]] Unsafe LongArray of Record Pointers and Partition IDs ShuffleInMemorySorter uses a LongArray. === [[usableCapacity]] Usable Capacity ShuffleInMemorySorter...FIXME","title":"ShuffleInMemorySorter"},{"location":"shuffle/ShuffleInMemorySorter/#shuffleinmemorysorter","text":"ShuffleInMemorySorter is used by ShuffleExternalSorter to < > using < > sort algorithms. == [[creating-instance]] Creating Instance ShuffleInMemorySorter takes the following to be created: [[consumer]] memory:MemoryConsumer.md[MemoryConsumer] [[initialSize]] Initial size [[useRadixSort]] useRadixSort flag (to indicate whether to use < >) ShuffleInMemorySorter requests the given < > to memory:MemoryConsumer.md#allocateArray[allocate an array] of the given < > for the < >. ShuffleInMemorySorter is created for a shuffle:ShuffleExternalSorter.md#inMemSorter[ShuffleExternalSorter]. == [[getSortedIterator]] Iterator of Records Sorted","title":"ShuffleInMemorySorter"},{"location":"shuffle/ShuffleInMemorySorter/#source-java","text":"","title":"[source, java]"},{"location":"shuffle/ShuffleInMemorySorter/#shufflesorteriterator-getsortediterator","text":"getSortedIterator...FIXME getSortedIterator is used when ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#writeSortedFile[writeSortedFile]. == [[reset]] Resetting","title":"ShuffleSorterIterator getSortedIterator()"},{"location":"shuffle/ShuffleInMemorySorter/#source-java_1","text":"","title":"[source, java]"},{"location":"shuffle/ShuffleInMemorySorter/#void-reset","text":"reset...FIXME reset is used when...FIXME == [[numRecords]] numRecords Method","title":"void reset()"},{"location":"shuffle/ShuffleInMemorySorter/#source-java_2","text":"","title":"[source, java]"},{"location":"shuffle/ShuffleInMemorySorter/#int-numrecords","text":"numRecords...FIXME numRecords is used when...FIXME == [[getUsableCapacity]] Calculating Usable Capacity","title":"int numRecords()"},{"location":"shuffle/ShuffleInMemorySorter/#source-java_3","text":"","title":"[source, java]"},{"location":"shuffle/ShuffleInMemorySorter/#int-getusablecapacity","text":"getUsableCapacity calculates the capacity that is a half or two-third of the memory used for the < >. getUsableCapacity is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.ShuffleExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"int getUsableCapacity()"},{"location":"shuffle/ShuffleInMemorySorter/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"shuffle/ShuffleInMemorySorter/#log4jloggerorgapachesparkshufflesortshuffleexternalsorterall","text":"Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[array]] Unsafe LongArray of Record Pointers and Partition IDs ShuffleInMemorySorter uses a LongArray. === [[usableCapacity]] Usable Capacity ShuffleInMemorySorter...FIXME","title":"log4j.logger.org.apache.spark.shuffle.sort.ShuffleExternalSorter=ALL"},{"location":"shuffle/ShuffleManager/","text":"ShuffleManager \u00b6 ShuffleManager is an abstraction of shuffle managers that manage shuffle data. ShuffleManager is specified using spark.shuffle.manager configuration property. ShuffleManager is used to create a BlockManager . Contract \u00b6 Getting ShuffleReader for ShuffleHandle \u00b6 getReader [ K , C ]( handle : ShuffleHandle , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ): ShuffleReader [ K , C ] ShuffleReader to read shuffle data in the ShuffleHandle Used when the following RDDs are requested to compute a partition: CoGroupedRDD is requested to compute a partition ShuffledRDD is requested to compute a partition SubtractedRDD is requested to compute a partition ShuffledRowRDD (Spark SQL) is requested to compute a partition getReaderForRange \u00b6 getReaderForRange [ K , C ]( handle : ShuffleHandle , startMapIndex : Int , endMapIndex : Int , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ): ShuffleReader [ K , C ] ShuffleReader for a range of reduce partitions to read from map output in the ShuffleHandle Used when ShuffledRowRDD (Spark SQL) is requested to compute a partition Getting ShuffleWriter for ShuffleHandle \u00b6 getWriter [ K , V ]( handle : ShuffleHandle , mapId : Long , context : TaskContext , metrics : ShuffleWriteMetricsReporter ): ShuffleWriter [ K , V ] ShuffleWriter to write shuffle data in the ShuffleHandle Used when ShuffleWriteProcessor is requested to write a partition Registering Shuffle of ShuffleDependency (and Getting ShuffleHandle) \u00b6 registerShuffle [ K , V , C ]( shuffleId : Int , dependency : ShuffleDependency [ K , V , C ]): ShuffleHandle Registers a shuffle (by the given shuffleId and ShuffleDependency ) and gives a ShuffleHandle Used when ShuffleDependency is created (and registers with the shuffle system) ShuffleBlockResolver \u00b6 shuffleBlockResolver : ShuffleBlockResolver ShuffleBlockResolver of the shuffle system Used when: SortShuffleManager is requested for a ShuffleWriter for a ShuffleHandle , to unregister a shuffle and stop BlockManager is requested to getLocalBlockData and getHostLocalShuffleData Stopping ShuffleManager \u00b6 stop (): Unit Stops the shuffle system Used when SparkEnv is requested to stop Unregistering Shuffle \u00b6 unregisterShuffle ( shuffleId : Int ): Boolean Unregisters a given shuffle Used when BlockManagerSlaveEndpoint is requested to handle a RemoveShuffle message Implementations \u00b6 SortShuffleManager Accessing ShuffleManager using SparkEnv \u00b6 ShuffleManager is available on the driver and executors using SparkEnv.shuffleManager . val shuffleManager = SparkEnv . get . shuffleManager","title":"ShuffleManager"},{"location":"shuffle/ShuffleManager/#shufflemanager","text":"ShuffleManager is an abstraction of shuffle managers that manage shuffle data. ShuffleManager is specified using spark.shuffle.manager configuration property. ShuffleManager is used to create a BlockManager .","title":"ShuffleManager"},{"location":"shuffle/ShuffleManager/#contract","text":"","title":"Contract"},{"location":"shuffle/ShuffleManager/#getting-shufflereader-for-shufflehandle","text":"getReader [ K , C ]( handle : ShuffleHandle , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ): ShuffleReader [ K , C ] ShuffleReader to read shuffle data in the ShuffleHandle Used when the following RDDs are requested to compute a partition: CoGroupedRDD is requested to compute a partition ShuffledRDD is requested to compute a partition SubtractedRDD is requested to compute a partition ShuffledRowRDD (Spark SQL) is requested to compute a partition","title":" Getting ShuffleReader for ShuffleHandle"},{"location":"shuffle/ShuffleManager/#getreaderforrange","text":"getReaderForRange [ K , C ]( handle : ShuffleHandle , startMapIndex : Int , endMapIndex : Int , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ): ShuffleReader [ K , C ] ShuffleReader for a range of reduce partitions to read from map output in the ShuffleHandle Used when ShuffledRowRDD (Spark SQL) is requested to compute a partition","title":" getReaderForRange"},{"location":"shuffle/ShuffleManager/#getting-shufflewriter-for-shufflehandle","text":"getWriter [ K , V ]( handle : ShuffleHandle , mapId : Long , context : TaskContext , metrics : ShuffleWriteMetricsReporter ): ShuffleWriter [ K , V ] ShuffleWriter to write shuffle data in the ShuffleHandle Used when ShuffleWriteProcessor is requested to write a partition","title":" Getting ShuffleWriter for ShuffleHandle"},{"location":"shuffle/ShuffleManager/#registering-shuffle-of-shuffledependency-and-getting-shufflehandle","text":"registerShuffle [ K , V , C ]( shuffleId : Int , dependency : ShuffleDependency [ K , V , C ]): ShuffleHandle Registers a shuffle (by the given shuffleId and ShuffleDependency ) and gives a ShuffleHandle Used when ShuffleDependency is created (and registers with the shuffle system)","title":" Registering Shuffle of ShuffleDependency (and Getting ShuffleHandle)"},{"location":"shuffle/ShuffleManager/#shuffleblockresolver","text":"shuffleBlockResolver : ShuffleBlockResolver ShuffleBlockResolver of the shuffle system Used when: SortShuffleManager is requested for a ShuffleWriter for a ShuffleHandle , to unregister a shuffle and stop BlockManager is requested to getLocalBlockData and getHostLocalShuffleData","title":" ShuffleBlockResolver"},{"location":"shuffle/ShuffleManager/#stopping-shufflemanager","text":"stop (): Unit Stops the shuffle system Used when SparkEnv is requested to stop","title":" Stopping ShuffleManager"},{"location":"shuffle/ShuffleManager/#unregistering-shuffle","text":"unregisterShuffle ( shuffleId : Int ): Boolean Unregisters a given shuffle Used when BlockManagerSlaveEndpoint is requested to handle a RemoveShuffle message","title":" Unregistering Shuffle"},{"location":"shuffle/ShuffleManager/#implementations","text":"SortShuffleManager","title":"Implementations"},{"location":"shuffle/ShuffleManager/#accessing-shufflemanager-using-sparkenv","text":"ShuffleManager is available on the driver and executors using SparkEnv.shuffleManager . val shuffleManager = SparkEnv . get . shuffleManager","title":" Accessing ShuffleManager using SparkEnv"},{"location":"shuffle/ShuffleMapOutputWriter/","text":"ShuffleMapOutputWriter \u00b6 ShuffleMapOutputWriter is...FIXME","title":"ShuffleMapOutputWriter"},{"location":"shuffle/ShuffleMapOutputWriter/#shufflemapoutputwriter","text":"ShuffleMapOutputWriter is...FIXME","title":"ShuffleMapOutputWriter"},{"location":"shuffle/ShuffleReader/","text":"ShuffleReader \u00b6 ShuffleReader is an abstraction of shuffle block readers that can read combined key-value records for a reduce task . Contract \u00b6 Reading Combined Records (for Reduce Task) \u00b6 read (): Iterator [ Product2 [ K , C ]] Used when: CoGroupedRDD , ShuffledRDD , and SubtractedRDD are requested to compute a partition (for a ShuffleDependency dependency) ShuffledRowRDD ( Spark SQL ) is requested to compute a partition Implementations \u00b6 BlockStoreShuffleReader","title":"ShuffleReader"},{"location":"shuffle/ShuffleReader/#shufflereader","text":"ShuffleReader is an abstraction of shuffle block readers that can read combined key-value records for a reduce task .","title":"ShuffleReader"},{"location":"shuffle/ShuffleReader/#contract","text":"","title":"Contract"},{"location":"shuffle/ShuffleReader/#reading-combined-records-for-reduce-task","text":"read (): Iterator [ Product2 [ K , C ]] Used when: CoGroupedRDD , ShuffledRDD , and SubtractedRDD are requested to compute a partition (for a ShuffleDependency dependency) ShuffledRowRDD ( Spark SQL ) is requested to compute a partition","title":" Reading Combined Records (for Reduce Task)"},{"location":"shuffle/ShuffleReader/#implementations","text":"BlockStoreShuffleReader","title":"Implementations"},{"location":"shuffle/ShuffleWriteMetricsReporter/","text":"ShuffleWriteMetricsReporter \u00b6 ShuffleWriteMetricsReporter is an abstraction of shuffle write metrics reporters . Contract \u00b6 decBytesWritten \u00b6 decBytesWritten ( v : Long ): Unit decRecordsWritten \u00b6 decRecordsWritten ( v : Long ): Unit incBytesWritten \u00b6 incBytesWritten ( v : Long ): Unit incRecordsWritten \u00b6 incRecordsWritten ( v : Long ): Unit incWriteTime \u00b6 incWriteTime ( v : Long ): Unit Used when: BypassMergeSortShuffleWriter is requested to write partition records and writePartitionedData UnsafeShuffleWriter is requested to mergeSpillsWithTransferTo DiskBlockObjectWriter is requested to commitAndGet TimeTrackingOutputStream is requested to write , flush , and close Implementations \u00b6 ShuffleWriteMetrics SQLShuffleWriteMetricsReporter ( Spark SQL )","title":"ShuffleWriteMetricsReporter"},{"location":"shuffle/ShuffleWriteMetricsReporter/#shufflewritemetricsreporter","text":"ShuffleWriteMetricsReporter is an abstraction of shuffle write metrics reporters .","title":"ShuffleWriteMetricsReporter"},{"location":"shuffle/ShuffleWriteMetricsReporter/#contract","text":"","title":"Contract"},{"location":"shuffle/ShuffleWriteMetricsReporter/#decbyteswritten","text":"decBytesWritten ( v : Long ): Unit","title":" decBytesWritten"},{"location":"shuffle/ShuffleWriteMetricsReporter/#decrecordswritten","text":"decRecordsWritten ( v : Long ): Unit","title":" decRecordsWritten"},{"location":"shuffle/ShuffleWriteMetricsReporter/#incbyteswritten","text":"incBytesWritten ( v : Long ): Unit","title":" incBytesWritten"},{"location":"shuffle/ShuffleWriteMetricsReporter/#increcordswritten","text":"incRecordsWritten ( v : Long ): Unit","title":" incRecordsWritten"},{"location":"shuffle/ShuffleWriteMetricsReporter/#incwritetime","text":"incWriteTime ( v : Long ): Unit Used when: BypassMergeSortShuffleWriter is requested to write partition records and writePartitionedData UnsafeShuffleWriter is requested to mergeSpillsWithTransferTo DiskBlockObjectWriter is requested to commitAndGet TimeTrackingOutputStream is requested to write , flush , and close","title":" incWriteTime"},{"location":"shuffle/ShuffleWriteMetricsReporter/#implementations","text":"ShuffleWriteMetrics SQLShuffleWriteMetricsReporter ( Spark SQL )","title":"Implementations"},{"location":"shuffle/ShuffleWriteProcessor/","text":"ShuffleWriteProcessor \u00b6 ShuffleWriteProcessor is used by ShuffleMapTask to write partition records to the shuffle system . Writing Partition Records to Shuffle System \u00b6 write ( rdd : RDD [ _ ], dep : ShuffleDependency [ _ , _ , _ ], mapId : Long , context : TaskContext , partition : Partition ): MapStatus write requests the ShuffleManager for the ShuffleWriter for the ShuffleHandle (of the given ShuffleDependency ). write requests the ShuffleWriter to write out records (of the given Partition and RDD ). In the end, write requests the ShuffleWriter to stop (with the success flag enabled). In case of any Exception s, write requests the ShuffleWriter to stop (with the success flag disabled). write is used when ShuffleMapTask is requested to run . Creating MetricsReporter \u00b6 createMetricsReporter ( context : TaskContext ): ShuffleWriteMetricsReporter createMetricsReporter creates a ShuffleWriteMetricsReporter from the given TaskContext . createMetricsReporter requests the given TaskContext for TaskMetrics and then for the ShuffleWriteMetrics .","title":"ShuffleWriteProcessor"},{"location":"shuffle/ShuffleWriteProcessor/#shufflewriteprocessor","text":"ShuffleWriteProcessor is used by ShuffleMapTask to write partition records to the shuffle system .","title":"ShuffleWriteProcessor"},{"location":"shuffle/ShuffleWriteProcessor/#writing-partition-records-to-shuffle-system","text":"write ( rdd : RDD [ _ ], dep : ShuffleDependency [ _ , _ , _ ], mapId : Long , context : TaskContext , partition : Partition ): MapStatus write requests the ShuffleManager for the ShuffleWriter for the ShuffleHandle (of the given ShuffleDependency ). write requests the ShuffleWriter to write out records (of the given Partition and RDD ). In the end, write requests the ShuffleWriter to stop (with the success flag enabled). In case of any Exception s, write requests the ShuffleWriter to stop (with the success flag disabled). write is used when ShuffleMapTask is requested to run .","title":" Writing Partition Records to Shuffle System"},{"location":"shuffle/ShuffleWriteProcessor/#creating-metricsreporter","text":"createMetricsReporter ( context : TaskContext ): ShuffleWriteMetricsReporter createMetricsReporter creates a ShuffleWriteMetricsReporter from the given TaskContext . createMetricsReporter requests the given TaskContext for TaskMetrics and then for the ShuffleWriteMetrics .","title":" Creating MetricsReporter"},{"location":"shuffle/ShuffleWriter/","text":"ShuffleWriter \u00b6 ShuffleWriter[K, V] (of K keys and V values) is an abstraction of shuffle writers that can write out key-value records (of a RDD partition) to a shuffle system. ShuffleWriter is used when ShuffleMapTask is requested to run (and uses a ShuffleWriteProcessor to write partition records to a shuffle system ). Contract \u00b6 Writing Out Partition Records to Shuffle System \u00b6 write ( records : Iterator [ Product2 [ K , V ]]): Unit Writes key-value records (of a partition) out to a shuffle system Used when: ShuffleWriteProcessor is requested to write Stopping ShuffleWriter \u00b6 stop ( success : Boolean ): Option [ MapStatus ] Stops ( closes ) the ShuffleWriter and returns a MapStatus if the writing completed successfully. The success flag is the status of the task execution. Used when: ShuffleWriteProcessor is requested to write Implementations \u00b6 BypassMergeSortShuffleWriter SortShuffleWriter UnsafeShuffleWriter","title":"ShuffleWriter"},{"location":"shuffle/ShuffleWriter/#shufflewriter","text":"ShuffleWriter[K, V] (of K keys and V values) is an abstraction of shuffle writers that can write out key-value records (of a RDD partition) to a shuffle system. ShuffleWriter is used when ShuffleMapTask is requested to run (and uses a ShuffleWriteProcessor to write partition records to a shuffle system ).","title":"ShuffleWriter"},{"location":"shuffle/ShuffleWriter/#contract","text":"","title":"Contract"},{"location":"shuffle/ShuffleWriter/#writing-out-partition-records-to-shuffle-system","text":"write ( records : Iterator [ Product2 [ K , V ]]): Unit Writes key-value records (of a partition) out to a shuffle system Used when: ShuffleWriteProcessor is requested to write","title":" Writing Out Partition Records to Shuffle System"},{"location":"shuffle/ShuffleWriter/#stopping-shufflewriter","text":"stop ( success : Boolean ): Option [ MapStatus ] Stops ( closes ) the ShuffleWriter and returns a MapStatus if the writing completed successfully. The success flag is the status of the task execution. Used when: ShuffleWriteProcessor is requested to write","title":" Stopping ShuffleWriter"},{"location":"shuffle/ShuffleWriter/#implementations","text":"BypassMergeSortShuffleWriter SortShuffleWriter UnsafeShuffleWriter","title":"Implementations"},{"location":"shuffle/SingleSpillShuffleMapOutputWriter/","text":"SingleSpillShuffleMapOutputWriter \u00b6 SingleSpillShuffleMapOutputWriter is...FIXME","title":"SingleSpillShuffleMapOutputWriter"},{"location":"shuffle/SingleSpillShuffleMapOutputWriter/#singlespillshufflemapoutputwriter","text":"SingleSpillShuffleMapOutputWriter is...FIXME","title":"SingleSpillShuffleMapOutputWriter"},{"location":"shuffle/SortShuffleManager/","text":"SortShuffleManager \u00b6 SortShuffleManager is the default and only ShuffleManager in Apache Spark (with the short name sort or tungsten-sort ). Creating Instance \u00b6 SortShuffleManager takes the following to be created: SparkConf SortShuffleManager is created when SparkEnv is created (on the driver and executors at the very beginning of a Spark application's lifecycle). taskIdMapsForShuffle Registry \u00b6 taskIdMapsForShuffle : ConcurrentHashMap [ Int , OpenHashSet [ Long ]] SortShuffleManager uses taskIdMapsForShuffle internal registry to track task (attempt) IDs by shuffle. A new shuffle and task IDs are added when SortShuffleManager is requested for a ShuffleWriter (for a partition and a ShuffleHandle ). A shuffle ID (and associated task IDs) are removed when SortShuffleManager is requested to unregister a shuffle . Getting ShuffleWriter for Partition and ShuffleHandle \u00b6 getWriter [ K , V ]( handle : ShuffleHandle , mapId : Int , context : TaskContext ): ShuffleWriter [ K , V ] getWriter registers the given ShuffleHandle (by the shuffleId and numMaps ) in the taskIdMapsForShuffle internal registry unless already done. Note getWriter expects that the input ShuffleHandle is a BaseShuffleHandle . Moreover, getWriter expects that in two (out of three cases) it is a more specialized IndexShuffleBlockResolver . getWriter then creates a new ShuffleWriter based on the type of the given ShuffleHandle . ShuffleHandle ShuffleWriter SerializedShuffleHandle UnsafeShuffleWriter BypassMergeSortShuffleHandle BypassMergeSortShuffleWriter BaseShuffleHandle SortShuffleWriter getWriter is part of the ShuffleManager abstraction. ShuffleExecutorComponents \u00b6 shuffleExecutorComponents : ShuffleExecutorComponents SortShuffleManager defines the shuffleExecutorComponents internal registry for a ShuffleExecutorComponents . shuffleExecutorComponents is used when: SortShuffleManager is requested for the ShuffleWriter loadShuffleExecutorComponents \u00b6 loadShuffleExecutorComponents ( conf : SparkConf ): ShuffleExecutorComponents loadShuffleExecutorComponents loads the ShuffleDataIO that is then requested for the ShuffleExecutorComponents . loadShuffleExecutorComponents requests the ShuffleExecutorComponents to initialize before returning it. Creating ShuffleHandle for ShuffleDependency \u00b6 registerShuffle [ K , V , C ]( shuffleId : Int , dependency : ShuffleDependency [ K , V , C ]): ShuffleHandle registerShuffle is part of the ShuffleManager abstraction. registerShuffle creates a new ShuffleHandle (for the given ShuffleDependency ) that is one of the following: BypassMergeSortShuffleHandle (with ShuffleDependency[K, V, V] ) when shouldBypassMergeSort condition holds SerializedShuffleHandle (with ShuffleDependency[K, V, V] ) when canUseSerializedShuffle condition holds BaseShuffleHandle SerializedShuffleHandle Requirements \u00b6 canUseSerializedShuffle ( dependency : ShuffleDependency [ _ , _ , _ ]): Boolean canUseSerializedShuffle is true when all of the following hold for the given ShuffleDependency : Serializer (of the given ShuffleDependency ) supports relocation of serialized objects mapSideCombine flag (of the given ShuffleDependency ) is false Number of partitions (of the Partitioner of the given ShuffleDependency ) is not greater than the supported maximum number With all of the above positive, canUseSerializedShuffle prints out the following DEBUG message to the logs: Can use serialized shuffle for shuffle [shuffleId] Otherwise, canUseSerializedShuffle is false and prints out one of the following DEBUG messages based on the failed requirement: Can't use serialized shuffle for shuffle [id] because the serializer, [name], does not support object relocation Can't use serialized shuffle for shuffle [id] because we need to do map-side aggregation Can't use serialized shuffle for shuffle [id] because it has more than [number] partitions canUseSerializedShuffle is used when: SortShuffleManager is requested to register a shuffle (and creates a ShuffleHandle) Maximum Number of Partition Identifiers for Serialized Mode \u00b6 SortShuffleManager defines MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE internal constant to be (1 << 24) ( 16777216 ) for the maximum number of shuffle output partitions. MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE is used when: UnsafeShuffleWriter is created SortShuffleManager utility is used to check out SerializedShuffleHandle requirements ShuffleExchangeExec ( Spark SQL ) utility is used to needToCopyObjectsBeforeShuffle Creating ShuffleBlockResolver \u00b6 shuffleBlockResolver : IndexShuffleBlockResolver shuffleBlockResolver is part of the ShuffleManager abstraction. shuffleBlockResolver is a IndexShuffleBlockResolver (and is created immediately alongside this SortShuffleManager ). Unregistering Shuffle \u00b6 unregisterShuffle ( shuffleId : Int ): Boolean unregisterShuffle is part of the ShuffleManager abstraction. unregisterShuffle removes the given shuffleId from the taskIdMapsForShuffle internal registry. If the shuffleId was found and removed successfully, unregisterShuffle requests the IndexShuffleBlockResolver to remove the shuffle index and data files for every mapTaskId (mappers producing the output for the shuffle). unregisterShuffle is always true . Getting ShuffleReader for ShuffleHandle \u00b6 getReader [ K , C ]( handle : ShuffleHandle , startMapIndex : Int , endMapIndex : Int , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ): ShuffleReader [ K , C ] getReader is part of the ShuffleManager abstraction. getReader requests the MapOutputTracker (via SparkEnv ) for the getMapSizesByExecutorId for the shuffleId (of the given ShuffleHandle ). In the end, getReader creates a new BlockStoreShuffleReader . Stopping ShuffleManager \u00b6 stop (): Unit stop is part of the ShuffleManager abstraction. stop requests the IndexShuffleBlockResolver to stop . Logging \u00b6 Enable ALL logging level for org.apache.spark.shuffle.sort.SortShuffleManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.SortShuffleManager=ALL Refer to Logging .","title":"SortShuffleManager"},{"location":"shuffle/SortShuffleManager/#sortshufflemanager","text":"SortShuffleManager is the default and only ShuffleManager in Apache Spark (with the short name sort or tungsten-sort ).","title":"SortShuffleManager"},{"location":"shuffle/SortShuffleManager/#creating-instance","text":"SortShuffleManager takes the following to be created: SparkConf SortShuffleManager is created when SparkEnv is created (on the driver and executors at the very beginning of a Spark application's lifecycle).","title":"Creating Instance"},{"location":"shuffle/SortShuffleManager/#taskidmapsforshuffle-registry","text":"taskIdMapsForShuffle : ConcurrentHashMap [ Int , OpenHashSet [ Long ]] SortShuffleManager uses taskIdMapsForShuffle internal registry to track task (attempt) IDs by shuffle. A new shuffle and task IDs are added when SortShuffleManager is requested for a ShuffleWriter (for a partition and a ShuffleHandle ). A shuffle ID (and associated task IDs) are removed when SortShuffleManager is requested to unregister a shuffle .","title":" taskIdMapsForShuffle Registry"},{"location":"shuffle/SortShuffleManager/#getting-shufflewriter-for-partition-and-shufflehandle","text":"getWriter [ K , V ]( handle : ShuffleHandle , mapId : Int , context : TaskContext ): ShuffleWriter [ K , V ] getWriter registers the given ShuffleHandle (by the shuffleId and numMaps ) in the taskIdMapsForShuffle internal registry unless already done. Note getWriter expects that the input ShuffleHandle is a BaseShuffleHandle . Moreover, getWriter expects that in two (out of three cases) it is a more specialized IndexShuffleBlockResolver . getWriter then creates a new ShuffleWriter based on the type of the given ShuffleHandle . ShuffleHandle ShuffleWriter SerializedShuffleHandle UnsafeShuffleWriter BypassMergeSortShuffleHandle BypassMergeSortShuffleWriter BaseShuffleHandle SortShuffleWriter getWriter is part of the ShuffleManager abstraction.","title":" Getting ShuffleWriter for Partition and ShuffleHandle"},{"location":"shuffle/SortShuffleManager/#shuffleexecutorcomponents","text":"shuffleExecutorComponents : ShuffleExecutorComponents SortShuffleManager defines the shuffleExecutorComponents internal registry for a ShuffleExecutorComponents . shuffleExecutorComponents is used when: SortShuffleManager is requested for the ShuffleWriter","title":" ShuffleExecutorComponents"},{"location":"shuffle/SortShuffleManager/#loadshuffleexecutorcomponents","text":"loadShuffleExecutorComponents ( conf : SparkConf ): ShuffleExecutorComponents loadShuffleExecutorComponents loads the ShuffleDataIO that is then requested for the ShuffleExecutorComponents . loadShuffleExecutorComponents requests the ShuffleExecutorComponents to initialize before returning it.","title":" loadShuffleExecutorComponents"},{"location":"shuffle/SortShuffleManager/#creating-shufflehandle-for-shuffledependency","text":"registerShuffle [ K , V , C ]( shuffleId : Int , dependency : ShuffleDependency [ K , V , C ]): ShuffleHandle registerShuffle is part of the ShuffleManager abstraction. registerShuffle creates a new ShuffleHandle (for the given ShuffleDependency ) that is one of the following: BypassMergeSortShuffleHandle (with ShuffleDependency[K, V, V] ) when shouldBypassMergeSort condition holds SerializedShuffleHandle (with ShuffleDependency[K, V, V] ) when canUseSerializedShuffle condition holds BaseShuffleHandle","title":" Creating ShuffleHandle for ShuffleDependency"},{"location":"shuffle/SortShuffleManager/#serializedshufflehandle-requirements","text":"canUseSerializedShuffle ( dependency : ShuffleDependency [ _ , _ , _ ]): Boolean canUseSerializedShuffle is true when all of the following hold for the given ShuffleDependency : Serializer (of the given ShuffleDependency ) supports relocation of serialized objects mapSideCombine flag (of the given ShuffleDependency ) is false Number of partitions (of the Partitioner of the given ShuffleDependency ) is not greater than the supported maximum number With all of the above positive, canUseSerializedShuffle prints out the following DEBUG message to the logs: Can use serialized shuffle for shuffle [shuffleId] Otherwise, canUseSerializedShuffle is false and prints out one of the following DEBUG messages based on the failed requirement: Can't use serialized shuffle for shuffle [id] because the serializer, [name], does not support object relocation Can't use serialized shuffle for shuffle [id] because we need to do map-side aggregation Can't use serialized shuffle for shuffle [id] because it has more than [number] partitions canUseSerializedShuffle is used when: SortShuffleManager is requested to register a shuffle (and creates a ShuffleHandle)","title":" SerializedShuffleHandle Requirements"},{"location":"shuffle/SortShuffleManager/#maximum-number-of-partition-identifiers-for-serialized-mode","text":"SortShuffleManager defines MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE internal constant to be (1 << 24) ( 16777216 ) for the maximum number of shuffle output partitions. MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE is used when: UnsafeShuffleWriter is created SortShuffleManager utility is used to check out SerializedShuffleHandle requirements ShuffleExchangeExec ( Spark SQL ) utility is used to needToCopyObjectsBeforeShuffle","title":" Maximum Number of Partition Identifiers for Serialized Mode"},{"location":"shuffle/SortShuffleManager/#creating-shuffleblockresolver","text":"shuffleBlockResolver : IndexShuffleBlockResolver shuffleBlockResolver is part of the ShuffleManager abstraction. shuffleBlockResolver is a IndexShuffleBlockResolver (and is created immediately alongside this SortShuffleManager ).","title":" Creating ShuffleBlockResolver"},{"location":"shuffle/SortShuffleManager/#unregistering-shuffle","text":"unregisterShuffle ( shuffleId : Int ): Boolean unregisterShuffle is part of the ShuffleManager abstraction. unregisterShuffle removes the given shuffleId from the taskIdMapsForShuffle internal registry. If the shuffleId was found and removed successfully, unregisterShuffle requests the IndexShuffleBlockResolver to remove the shuffle index and data files for every mapTaskId (mappers producing the output for the shuffle). unregisterShuffle is always true .","title":" Unregistering Shuffle"},{"location":"shuffle/SortShuffleManager/#getting-shufflereader-for-shufflehandle","text":"getReader [ K , C ]( handle : ShuffleHandle , startMapIndex : Int , endMapIndex : Int , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ): ShuffleReader [ K , C ] getReader is part of the ShuffleManager abstraction. getReader requests the MapOutputTracker (via SparkEnv ) for the getMapSizesByExecutorId for the shuffleId (of the given ShuffleHandle ). In the end, getReader creates a new BlockStoreShuffleReader .","title":" Getting ShuffleReader for ShuffleHandle"},{"location":"shuffle/SortShuffleManager/#stopping-shufflemanager","text":"stop (): Unit stop is part of the ShuffleManager abstraction. stop requests the IndexShuffleBlockResolver to stop .","title":" Stopping ShuffleManager"},{"location":"shuffle/SortShuffleManager/#logging","text":"Enable ALL logging level for org.apache.spark.shuffle.sort.SortShuffleManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.SortShuffleManager=ALL Refer to Logging .","title":"Logging"},{"location":"shuffle/SortShuffleWriter/","text":"SortShuffleWriter \u2014 Fallback ShuffleWriter \u00b6 SortShuffleWriter is a \"fallback\" ShuffleWriter (when SortShuffleManager is requested for a ShuffleWriter and the more specialized BypassMergeSortShuffleWriter and UnsafeShuffleWriter could not be used). SortShuffleWriter[K, V, C] is a parameterized type with K keys, V values, and C combiner values. Creating Instance \u00b6 SortShuffleWriter takes the following to be created: IndexShuffleBlockResolver (unused) BaseShuffleHandle Map ID TaskContext ShuffleExecutorComponents SortShuffleWriter is created when: SortShuffleManager is requested for a ShuffleWriter (for a given ShuffleHandle ) MapStatus \u00b6 SortShuffleWriter uses mapStatus internal registry for a MapStatus after writing records . Writing records itself does not return a value and SortShuffleWriter uses the registry when requested to stop (which allows returning a MapStatus ). Writing Records (Into Shuffle Partitioned File In Disk Store) \u00b6 write ( records : Iterator [ Product2 [ K , V ]]): Unit write is part of the ShuffleWriter abstraction. write creates an ExternalSorter based on the ShuffleDependency (of the BaseShuffleHandle ), namely the Map-Size Partial Aggregation flag. The ExternalSorter uses the aggregator and key ordering when the flag is enabled. write requests the ExternalSorter to insert all the given records . write ...FIXME Stopping SortShuffleWriter (and Calculating MapStatus) \u00b6 stop ( success : Boolean ): Option [ MapStatus ] stop is part of the ShuffleWriter abstraction. stop turns the stopping flag on and returns the internal mapStatus if the input success is enabled. Otherwise, when stopping flag is already enabled or the input success is disabled, stop returns no MapStatus (i.e. None ). In the end, stop requests the ExternalSorter to stop and increments the shuffle write time task metrics. Requirements of BypassMergeSortShuffleHandle (as ShuffleHandle) \u00b6 shouldBypassMergeSort ( conf : SparkConf , dep : ShuffleDependency [ _ , _ , _ ]): Boolean shouldBypassMergeSort returns true when all of the following hold: No map-side aggregation (the mapSideCombine flag of the given ShuffleDependency is off) Number of partitions (of the Partitioner of the given ShuffleDependency ) is not greater than spark.shuffle.sort.bypassMergeThreshold configuration property Otherwise, shouldBypassMergeSort does not hold ( false ). shouldBypassMergeSort is used when: SortShuffleManager is requested to register a shuffle (and creates a ShuffleHandle) stopping Flag \u00b6 SortShuffleWriter uses stopping internal flag to indicate whether or not this SortShuffleWriter has been stopped . Logging \u00b6 Enable ALL logging level for org.apache.spark.shuffle.sort.SortShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.SortShuffleWriter=ALL Refer to Logging .","title":"SortShuffleWriter"},{"location":"shuffle/SortShuffleWriter/#sortshufflewriter-fallback-shufflewriter","text":"SortShuffleWriter is a \"fallback\" ShuffleWriter (when SortShuffleManager is requested for a ShuffleWriter and the more specialized BypassMergeSortShuffleWriter and UnsafeShuffleWriter could not be used). SortShuffleWriter[K, V, C] is a parameterized type with K keys, V values, and C combiner values.","title":"SortShuffleWriter &mdash; Fallback ShuffleWriter"},{"location":"shuffle/SortShuffleWriter/#creating-instance","text":"SortShuffleWriter takes the following to be created: IndexShuffleBlockResolver (unused) BaseShuffleHandle Map ID TaskContext ShuffleExecutorComponents SortShuffleWriter is created when: SortShuffleManager is requested for a ShuffleWriter (for a given ShuffleHandle )","title":"Creating Instance"},{"location":"shuffle/SortShuffleWriter/#mapstatus","text":"SortShuffleWriter uses mapStatus internal registry for a MapStatus after writing records . Writing records itself does not return a value and SortShuffleWriter uses the registry when requested to stop (which allows returning a MapStatus ).","title":" MapStatus"},{"location":"shuffle/SortShuffleWriter/#writing-records-into-shuffle-partitioned-file-in-disk-store","text":"write ( records : Iterator [ Product2 [ K , V ]]): Unit write is part of the ShuffleWriter abstraction. write creates an ExternalSorter based on the ShuffleDependency (of the BaseShuffleHandle ), namely the Map-Size Partial Aggregation flag. The ExternalSorter uses the aggregator and key ordering when the flag is enabled. write requests the ExternalSorter to insert all the given records . write ...FIXME","title":" Writing Records (Into Shuffle Partitioned File In Disk Store)"},{"location":"shuffle/SortShuffleWriter/#stopping-sortshufflewriter-and-calculating-mapstatus","text":"stop ( success : Boolean ): Option [ MapStatus ] stop is part of the ShuffleWriter abstraction. stop turns the stopping flag on and returns the internal mapStatus if the input success is enabled. Otherwise, when stopping flag is already enabled or the input success is disabled, stop returns no MapStatus (i.e. None ). In the end, stop requests the ExternalSorter to stop and increments the shuffle write time task metrics.","title":" Stopping SortShuffleWriter (and Calculating MapStatus)"},{"location":"shuffle/SortShuffleWriter/#requirements-of-bypassmergesortshufflehandle-as-shufflehandle","text":"shouldBypassMergeSort ( conf : SparkConf , dep : ShuffleDependency [ _ , _ , _ ]): Boolean shouldBypassMergeSort returns true when all of the following hold: No map-side aggregation (the mapSideCombine flag of the given ShuffleDependency is off) Number of partitions (of the Partitioner of the given ShuffleDependency ) is not greater than spark.shuffle.sort.bypassMergeThreshold configuration property Otherwise, shouldBypassMergeSort does not hold ( false ). shouldBypassMergeSort is used when: SortShuffleManager is requested to register a shuffle (and creates a ShuffleHandle)","title":" Requirements of BypassMergeSortShuffleHandle (as ShuffleHandle)"},{"location":"shuffle/SortShuffleWriter/#stopping-flag","text":"SortShuffleWriter uses stopping internal flag to indicate whether or not this SortShuffleWriter has been stopped .","title":" stopping Flag"},{"location":"shuffle/SortShuffleWriter/#logging","text":"Enable ALL logging level for org.apache.spark.shuffle.sort.SortShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.SortShuffleWriter=ALL Refer to Logging .","title":"Logging"},{"location":"shuffle/Spillable/","text":"Spillable \u00b6 Spillable is an extension of the MemoryConsumer abstraction for spillable collections that can spill to disk . Spillable[C] is a parameterized type of C combiner (partial) values. Contract \u00b6 forceSpill \u00b6 forceSpill (): Boolean Force spilling the current in-memory collection to disk to release memory. Used when Spillable is requested to spill spill \u00b6 spill ( collection : C ): Unit Spills the current in-memory collection to disk, and releases the memory. Used when: ExternalAppendOnlyMap is requested to forceSpill Spillable is requested to spilling to disk if necessary Implementations \u00b6 ExternalAppendOnlyMap ExternalSorter Memory Threshold \u00b6 Spillable uses a threshold for the memory size (in bytes) to know when to spill to disk . When the size of the in-memory collection is above the threshold, Spillable will try to acquire more memory. Unless given all requested memory, Spillable spills to disk. The memory threshold starts as spark.shuffle.spill.initialMemoryThreshold configuration property and is increased every time Spillable is requested to spill to disk if needed , but managed to acquire required memory. The threshold goes back to the initial value when requested to release all memory . Used when Spillable is requested to spill and releaseMemory . Creating Instance \u00b6 Spillable takes the following to be created: TaskMemoryManager Abstract Class Spillable is an abstract class and cannot be created directly. It is created indirectly for the concrete Spillables . Configuration Properties \u00b6 spark.shuffle.spill.numElementsForceSpillThreshold \u00b6 Spillable uses spark.shuffle.spill.numElementsForceSpillThreshold configuration property to force spilling in-memory objects to disk when requested to maybeSpill . spark.shuffle.spill.initialMemoryThreshold \u00b6 Spillable uses spark.shuffle.spill.initialMemoryThreshold configuration property as the initial threshold for the size of a collection (and the minimum memory required to operate properly). Spillable uses it when requested to spill and releaseMemory . Releasing All Memory \u00b6 releaseMemory (): Unit releaseMemory ...FIXME releaseMemory is used when: ExternalAppendOnlyMap is requested to freeCurrentMap ExternalSorter is requested to stop Spillable is requested to maybeSpill and spill (and spilled to disk in either case) Spilling In-Memory Collection to Disk (to Release Memory) \u00b6 spill ( collection : C ): Unit spill spills the given in-memory collection to disk to release memory. spill is used when: ExternalAppendOnlyMap is requested to forceSpill Spillable is requested to maybeSpill forceSpill \u00b6 forceSpill (): Boolean forceSpill forcefully spills the Spillable to disk to release memory. forceSpill is used when Spillable is requested to spill an in-memory collection to disk . Spilling to Disk if Necessary \u00b6 maybeSpill ( collection : C , currentMemory : Long ): Boolean maybeSpill ...FIXME maybeSpill is used when: ExternalAppendOnlyMap is requested to insertAll ExternalSorter is requested to attempt to spill an in-memory collection to disk if needed","title":"Spillable"},{"location":"shuffle/Spillable/#spillable","text":"Spillable is an extension of the MemoryConsumer abstraction for spillable collections that can spill to disk . Spillable[C] is a parameterized type of C combiner (partial) values.","title":"Spillable"},{"location":"shuffle/Spillable/#contract","text":"","title":"Contract"},{"location":"shuffle/Spillable/#forcespill","text":"forceSpill (): Boolean Force spilling the current in-memory collection to disk to release memory. Used when Spillable is requested to spill","title":" forceSpill"},{"location":"shuffle/Spillable/#spill","text":"spill ( collection : C ): Unit Spills the current in-memory collection to disk, and releases the memory. Used when: ExternalAppendOnlyMap is requested to forceSpill Spillable is requested to spilling to disk if necessary","title":" spill"},{"location":"shuffle/Spillable/#implementations","text":"ExternalAppendOnlyMap ExternalSorter","title":"Implementations"},{"location":"shuffle/Spillable/#memory-threshold","text":"Spillable uses a threshold for the memory size (in bytes) to know when to spill to disk . When the size of the in-memory collection is above the threshold, Spillable will try to acquire more memory. Unless given all requested memory, Spillable spills to disk. The memory threshold starts as spark.shuffle.spill.initialMemoryThreshold configuration property and is increased every time Spillable is requested to spill to disk if needed , but managed to acquire required memory. The threshold goes back to the initial value when requested to release all memory . Used when Spillable is requested to spill and releaseMemory .","title":" Memory Threshold"},{"location":"shuffle/Spillable/#creating-instance","text":"Spillable takes the following to be created: TaskMemoryManager Abstract Class Spillable is an abstract class and cannot be created directly. It is created indirectly for the concrete Spillables .","title":"Creating Instance"},{"location":"shuffle/Spillable/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"shuffle/Spillable/#sparkshufflespillnumelementsforcespillthreshold","text":"Spillable uses spark.shuffle.spill.numElementsForceSpillThreshold configuration property to force spilling in-memory objects to disk when requested to maybeSpill .","title":" spark.shuffle.spill.numElementsForceSpillThreshold"},{"location":"shuffle/Spillable/#sparkshufflespillinitialmemorythreshold","text":"Spillable uses spark.shuffle.spill.initialMemoryThreshold configuration property as the initial threshold for the size of a collection (and the minimum memory required to operate properly). Spillable uses it when requested to spill and releaseMemory .","title":" spark.shuffle.spill.initialMemoryThreshold"},{"location":"shuffle/Spillable/#releasing-all-memory","text":"releaseMemory (): Unit releaseMemory ...FIXME releaseMemory is used when: ExternalAppendOnlyMap is requested to freeCurrentMap ExternalSorter is requested to stop Spillable is requested to maybeSpill and spill (and spilled to disk in either case)","title":" Releasing All Memory"},{"location":"shuffle/Spillable/#spilling-in-memory-collection-to-disk-to-release-memory","text":"spill ( collection : C ): Unit spill spills the given in-memory collection to disk to release memory. spill is used when: ExternalAppendOnlyMap is requested to forceSpill Spillable is requested to maybeSpill","title":" Spilling In-Memory Collection to Disk (to Release Memory)"},{"location":"shuffle/Spillable/#forcespill_1","text":"forceSpill (): Boolean forceSpill forcefully spills the Spillable to disk to release memory. forceSpill is used when Spillable is requested to spill an in-memory collection to disk .","title":" forceSpill"},{"location":"shuffle/Spillable/#spilling-to-disk-if-necessary","text":"maybeSpill ( collection : C , currentMemory : Long ): Boolean maybeSpill ...FIXME maybeSpill is used when: ExternalAppendOnlyMap is requested to insertAll ExternalSorter is requested to attempt to spill an in-memory collection to disk if needed","title":" Spilling to Disk if Necessary"},{"location":"shuffle/UnsafeShuffleWriter/","text":"UnsafeShuffleWriter \u00b6 UnsafeShuffleWriter<K, V> is a ShuffleWriter for SerializedShuffleHandle s. UnsafeShuffleWriter opens resources (a ShuffleExternalSorter and the buffers) while being created . Creating Instance \u00b6 UnsafeShuffleWriter takes the following to be created: BlockManager TaskMemoryManager SerializedShuffleHandle Map ID TaskContext SparkConf ShuffleWriteMetricsReporter ShuffleExecutorComponents UnsafeShuffleWriter is created when SortShuffleManager is requested for a ShuffleWriter for a SerializedShuffleHandle . UnsafeShuffleWriter makes sure that the number of partitions at most 16MB reduce partitions ( 1 << 24 ) (as the upper bound of the partition identifiers that can be encoded ) or throws an IllegalArgumentException : UnsafeShuffleWriter can only be used for shuffles with at most 16777215 reduce partitions UnsafeShuffleWriter uses the number of partitions of the Partitioner that is used for the ShuffleDependency of the SerializedShuffleHandle . Note The number of shuffle output partitions is first enforced when SortShuffleManager is requested to check out whether a SerializedShuffleHandle can be used for ShuffleHandle (that eventually leads to UnsafeShuffleWriter ). In the end, UnsafeShuffleWriter creates a ShuffleExternalSorter and a SerializationStream . ShuffleExternalSorter \u00b6 UnsafeShuffleWriter uses a ShuffleExternalSorter . ShuffleExternalSorter is created when UnsafeShuffleWriter is requested to open (while being created ) and dereferenced ( null ed) when requested to close internal resources and merge spill files . Used when UnsafeShuffleWriter is requested for the following: Updating peak memory used Writing records Closing internal resources and merging spill files Inserting a record Stopping IndexShuffleBlockResolver \u00b6 UnsafeShuffleWriter is given a IndexShuffleBlockResolver when created . UnsafeShuffleWriter uses the IndexShuffleBlockResolver for...FIXME Initial Serialized Buffer Size \u00b6 UnsafeShuffleWriter uses a fixed buffer size for the output stream of serialized data written into a byte array (default: 1024 * 1024 ). inputBufferSizeInBytes \u00b6 UnsafeShuffleWriter uses the spark.shuffle.file.buffer configuration property for...FIXME outputBufferSizeInBytes \u00b6 UnsafeShuffleWriter uses the spark.shuffle.unsafe.file.output.buffer configuration property for...FIXME transferToEnabled \u00b6 UnsafeShuffleWriter can use a specialized NIO-based fast merge procedure that avoids extra serialization/deserialization when spark.file.transferTo configuration property is enabled. initialSortBufferSize \u00b6 UnsafeShuffleWriter uses the initial buffer size for sorting (default: 4096 ) when creating a ShuffleExternalSorter (when requested to open ). Tip Use spark.shuffle.sort.initialBufferSize configuration property to change the buffer size. Merging Spills \u00b6 long [] mergeSpills ( SpillInfo [] spills , File outputFile ) Many Spills \u00b6 With multiple SpillInfos to merge, mergeSpills selects between fast and slow merge strategies . The fast merge strategy can be transferTo - or fileStream -based. mergeSpills uses the spark.shuffle.unsafe.fastMergeEnabled configuration property to consider one of the fast merge strategies. A fast merge strategy is supported when spark.shuffle.compress configuration property is disabled or the IO compression codec supports decompression of concatenated compressed streams . With spark.shuffle.compress configuration property enabled, mergeSpills will always use the slow merge strategy. With fast merge strategy enabled and supported, transferToEnabled enabled and encryption disabled, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithTransferTo . Using transferTo-based fast merge With fast merge strategy enabled and supported, no transferToEnabled or encryption enabled, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithFileStream (with no compression codec). Using fileStream-based fast merge For slow merge, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithFileStream (with the compression codec). Using slow merge In the end, mergeSpills requests the ShuffleWriteMetrics to decBytesWritten and incBytesWritten , and returns the partition length array. One Spill \u00b6 With one SpillInfo to merge, mergeSpills simply renames the spill file to be the output file and returns the partition length array of the one spill. No Spills \u00b6 With no SpillInfo s to merge, mergeSpills creates an empty output file and returns an array of 0 s of size of the numPartitions of the Partitioner . Usage \u00b6 mergeSpills is used when UnsafeShuffleWriter is requested to close internal resources and merge spill files . mergeSpillsWithTransferTo \u00b6 long [] mergeSpillsWithTransferTo ( SpillInfo [] spills , File outputFile ) mergeSpillsWithTransferTo ...FIXME mergeSpillsWithTransferTo is used when UnsafeShuffleWriter is requested to mergeSpills (with the transferToEnabled flag enabled and no encryption). == [[updatePeakMemoryUsed]] updatePeakMemoryUsed Internal Method [source, java] \u00b6 void updatePeakMemoryUsed() \u00b6 updatePeakMemoryUsed...FIXME updatePeakMemoryUsed is used when UnsafeShuffleWriter is requested for the < > and to < >. Writing Key-Value Records of Partition \u00b6 void write ( Iterator < Product2 < K , V >> records ) write traverses the input sequence of records (for a RDD partition) and insertRecordIntoSorter one by one. When all the records have been processed, write closes internal resources and merges spill files . In the end, write requests ShuffleExternalSorter to clean up . CAUTION: FIXME When requested to < >, UnsafeShuffleWriter simply < > followed by < > (that, among other things, creates the < >). write is part of the ShuffleWriter abstraction. == [[stop]] Stopping ShuffleWriter [source, java] \u00b6 Option stop( boolean success) stop ...FIXME When requested to < >, UnsafeShuffleWriter records the peak execution memory metric and returns the < > (that was created when requested to < >). stop is part of the ShuffleWriter abstraction. == [[insertRecordIntoSorter]] Inserting Record Into ShuffleExternalSorter [source, java] \u00b6 void insertRecordIntoSorter( Product2 record) insertRecordIntoSorter requires that the < > is available. insertRecordIntoSorter requests the < > to reset (so that all currently accumulated output in the output stream is discarded and reusing the already allocated buffer space). insertRecordIntoSorter requests the < > to write out the record (write the serializer:SerializationStream.md#writeKey[key] and the serializer:SerializationStream.md#writeValue[value]) and to serializer:SerializationStream.md#flush[flush]. [[insertRecordIntoSorter-serializedRecordSize]] insertRecordIntoSorter requests the < > for the length of the buffer. [[insertRecordIntoSorter-partitionId]] insertRecordIntoSorter requests the < > for the ../rdd/Partitioner.md#getPartition[partition] for the given record (by the key). In the end, insertRecordIntoSorter requests the < > to ShuffleExternalSorter.md#insertRecord[insert] the < > as a byte array (with the < > and the < >). insertRecordIntoSorter is used when UnsafeShuffleWriter is requested to < >. Closing and Writing Output (Merging Spill Files) \u00b6 void closeAndWriteOutput () closeAndWriteOutput asserts that the ShuffleExternalSorter is created (non- null ). closeAndWriteOutput updates peak memory used . closeAndWriteOutput removes the references to the < > and < > output streams ( null s them). closeAndWriteOutput requests the < > to ShuffleExternalSorter.md#closeAndGetSpills[close and return spill metadata]. closeAndWriteOutput removes the reference to the < > ( null s it). closeAndWriteOutput requests the < > for the IndexShuffleBlockResolver.md#getDataFile[output data file] for the < > and < > IDs. [[closeAndWriteOutput-partitionLengths]][[closeAndWriteOutput-tmp]] closeAndWriteOutput creates a temporary file (along the data output file) and uses it to < > (that gives a partition length array). All spill files are then deleted. closeAndWriteOutput requests the < > to IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write shuffle index and data files] (for the < > and < > IDs, the < > and the < >). In the end, closeAndWriteOutput creates a scheduler:MapStatus.md[MapStatus] with the storage:BlockManager.md#shuffleServerId[location of the local BlockManager] and the < >. closeAndWriteOutput prints out the following ERROR message to the logs if there is an issue with deleting spill files: Error while deleting spill file [path] closeAndWriteOutput prints out the following ERROR message to the logs if there is an issue with deleting the < >: Error while deleting temp file [path] closeAndWriteOutput is used when UnsafeShuffleWriter is requested to write records . == [[mergeSpillsWithFileStream]] mergeSpillsWithFileStream Method [source, java] \u00b6 long[] mergeSpillsWithFileStream( SpillInfo[] spills, File outputFile, CompressionCodec compressionCodec) mergeSpillsWithFileStream will be given an io:CompressionCodec.md[IO compression codec] when shuffle compression is enabled. mergeSpillsWithFileStream...FIXME mergeSpillsWithFileStream requires that there are at least two spills to merge. mergeSpillsWithFileStream is used when UnsafeShuffleWriter is requested to < >. == [[getPeakMemoryUsedBytes]] Getting Peak Memory Used [source, java] \u00b6 long getPeakMemoryUsedBytes() \u00b6 getPeakMemoryUsedBytes simply < > and returns the internal < > registry. getPeakMemoryUsedBytes is used when UnsafeShuffleWriter is requested to < >. == [[open]] Opening UnsafeShuffleWriter and Buffers [source, java] \u00b6 void open() \u00b6 open requires that there is no < > available. open creates a ShuffleExternalSorter.md[ShuffleExternalSorter]. open creates a < > with the capacity of < >. open requests the < > for a serializer:SerializerInstance.md#serializeStream[SerializationStream] to the < > (available internally as the < > reference). open is used when UnsafeShuffleWriter is < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.UnsafeShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=ALL \u00b6 Refer to spark-logging.md[Logging]. Internal Properties \u00b6 MapStatus \u00b6 MapStatus Created when UnsafeShuffleWriter is requested to < > (with the storage:BlockManagerId.md[] of the < > and partitionLengths ) Returned when UnsafeShuffleWriter is requested to < > Partitioner \u00b6 Partitioner (as used by the BaseShuffleHandle.md#dependency[ShuffleDependency] of the < >) Used when UnsafeShuffleWriter is requested for the following: < > (and create a ShuffleExternalSorter.md[ShuffleExternalSorter] with the given ../rdd/Partitioner.md#numPartitions[number of partitions]) < > (and request the ../rdd/Partitioner.md#getPartition[partition for the key]) < >, < > and < > (for the ../rdd/Partitioner.md#numPartitions[number of partitions] to create partition lengths) Peak Memory Used \u00b6 Peak memory used (in bytes) that is updated exclusively in < > (after requesting the < > for ShuffleExternalSorter.md#getPeakMemoryUsedBytes[getPeakMemoryUsedBytes]) Use < > to access the current value ByteArrayOutputStream for Serialized Data \u00b6 {java-javadoc-url}/java/io/ByteArrayOutputStream.html[java.io.ByteArrayOutputStream] of serialized data (written into a byte array of < > initial size) Used when UnsafeShuffleWriter is requested for the following: < > (and create the internal < >) < > Destroyed ( null ) when requested to < >. === [[serializer]] serializer serializer:SerializerInstance.md[SerializerInstance] (that is a new instance of the Serializer of the BaseShuffleHandle.md#dependency[ShuffleDependency] of the < >) Used exclusively when UnsafeShuffleWriter is requested to < > (and creates the < >) === [[serOutputStream]] serOutputStream serializer:SerializationStream.md[SerializationStream] (that is created when the < > is requested to serializer:SerializerInstance.md#serializeStream[serializeStream] with the < >) Used when UnsafeShuffleWriter is requested to < > Destroyed ( null ) when requested to < >. Shuffle ID \u00b6 Shuffle ID (of the ShuffleDependency of the SerializedShuffleHandle ) Used exclusively when requested to < > === [[writeMetrics]] writeMetrics executor:ShuffleWriteMetrics.md[] (of the TaskMetrics of the < >) Used when UnsafeShuffleWriter is requested for the following: < > (and creates the < >) < > < > < >","title":"UnsafeShuffleWriter"},{"location":"shuffle/UnsafeShuffleWriter/#unsafeshufflewriter","text":"UnsafeShuffleWriter<K, V> is a ShuffleWriter for SerializedShuffleHandle s. UnsafeShuffleWriter opens resources (a ShuffleExternalSorter and the buffers) while being created .","title":"UnsafeShuffleWriter"},{"location":"shuffle/UnsafeShuffleWriter/#creating-instance","text":"UnsafeShuffleWriter takes the following to be created: BlockManager TaskMemoryManager SerializedShuffleHandle Map ID TaskContext SparkConf ShuffleWriteMetricsReporter ShuffleExecutorComponents UnsafeShuffleWriter is created when SortShuffleManager is requested for a ShuffleWriter for a SerializedShuffleHandle . UnsafeShuffleWriter makes sure that the number of partitions at most 16MB reduce partitions ( 1 << 24 ) (as the upper bound of the partition identifiers that can be encoded ) or throws an IllegalArgumentException : UnsafeShuffleWriter can only be used for shuffles with at most 16777215 reduce partitions UnsafeShuffleWriter uses the number of partitions of the Partitioner that is used for the ShuffleDependency of the SerializedShuffleHandle . Note The number of shuffle output partitions is first enforced when SortShuffleManager is requested to check out whether a SerializedShuffleHandle can be used for ShuffleHandle (that eventually leads to UnsafeShuffleWriter ). In the end, UnsafeShuffleWriter creates a ShuffleExternalSorter and a SerializationStream .","title":"Creating Instance"},{"location":"shuffle/UnsafeShuffleWriter/#shuffleexternalsorter","text":"UnsafeShuffleWriter uses a ShuffleExternalSorter . ShuffleExternalSorter is created when UnsafeShuffleWriter is requested to open (while being created ) and dereferenced ( null ed) when requested to close internal resources and merge spill files . Used when UnsafeShuffleWriter is requested for the following: Updating peak memory used Writing records Closing internal resources and merging spill files Inserting a record Stopping","title":" ShuffleExternalSorter"},{"location":"shuffle/UnsafeShuffleWriter/#indexshuffleblockresolver","text":"UnsafeShuffleWriter is given a IndexShuffleBlockResolver when created . UnsafeShuffleWriter uses the IndexShuffleBlockResolver for...FIXME","title":" IndexShuffleBlockResolver"},{"location":"shuffle/UnsafeShuffleWriter/#initial-serialized-buffer-size","text":"UnsafeShuffleWriter uses a fixed buffer size for the output stream of serialized data written into a byte array (default: 1024 * 1024 ).","title":" Initial Serialized Buffer Size"},{"location":"shuffle/UnsafeShuffleWriter/#inputbuffersizeinbytes","text":"UnsafeShuffleWriter uses the spark.shuffle.file.buffer configuration property for...FIXME","title":" inputBufferSizeInBytes"},{"location":"shuffle/UnsafeShuffleWriter/#outputbuffersizeinbytes","text":"UnsafeShuffleWriter uses the spark.shuffle.unsafe.file.output.buffer configuration property for...FIXME","title":" outputBufferSizeInBytes"},{"location":"shuffle/UnsafeShuffleWriter/#transfertoenabled","text":"UnsafeShuffleWriter can use a specialized NIO-based fast merge procedure that avoids extra serialization/deserialization when spark.file.transferTo configuration property is enabled.","title":" transferToEnabled"},{"location":"shuffle/UnsafeShuffleWriter/#initialsortbuffersize","text":"UnsafeShuffleWriter uses the initial buffer size for sorting (default: 4096 ) when creating a ShuffleExternalSorter (when requested to open ). Tip Use spark.shuffle.sort.initialBufferSize configuration property to change the buffer size.","title":" initialSortBufferSize"},{"location":"shuffle/UnsafeShuffleWriter/#merging-spills","text":"long [] mergeSpills ( SpillInfo [] spills , File outputFile )","title":" Merging Spills"},{"location":"shuffle/UnsafeShuffleWriter/#many-spills","text":"With multiple SpillInfos to merge, mergeSpills selects between fast and slow merge strategies . The fast merge strategy can be transferTo - or fileStream -based. mergeSpills uses the spark.shuffle.unsafe.fastMergeEnabled configuration property to consider one of the fast merge strategies. A fast merge strategy is supported when spark.shuffle.compress configuration property is disabled or the IO compression codec supports decompression of concatenated compressed streams . With spark.shuffle.compress configuration property enabled, mergeSpills will always use the slow merge strategy. With fast merge strategy enabled and supported, transferToEnabled enabled and encryption disabled, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithTransferTo . Using transferTo-based fast merge With fast merge strategy enabled and supported, no transferToEnabled or encryption enabled, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithFileStream (with no compression codec). Using fileStream-based fast merge For slow merge, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithFileStream (with the compression codec). Using slow merge In the end, mergeSpills requests the ShuffleWriteMetrics to decBytesWritten and incBytesWritten , and returns the partition length array.","title":" Many Spills"},{"location":"shuffle/UnsafeShuffleWriter/#one-spill","text":"With one SpillInfo to merge, mergeSpills simply renames the spill file to be the output file and returns the partition length array of the one spill.","title":" One Spill"},{"location":"shuffle/UnsafeShuffleWriter/#no-spills","text":"With no SpillInfo s to merge, mergeSpills creates an empty output file and returns an array of 0 s of size of the numPartitions of the Partitioner .","title":" No Spills"},{"location":"shuffle/UnsafeShuffleWriter/#usage","text":"mergeSpills is used when UnsafeShuffleWriter is requested to close internal resources and merge spill files .","title":" Usage"},{"location":"shuffle/UnsafeShuffleWriter/#mergespillswithtransferto","text":"long [] mergeSpillsWithTransferTo ( SpillInfo [] spills , File outputFile ) mergeSpillsWithTransferTo ...FIXME mergeSpillsWithTransferTo is used when UnsafeShuffleWriter is requested to mergeSpills (with the transferToEnabled flag enabled and no encryption). == [[updatePeakMemoryUsed]] updatePeakMemoryUsed Internal Method","title":" mergeSpillsWithTransferTo"},{"location":"shuffle/UnsafeShuffleWriter/#source-java","text":"","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#void-updatepeakmemoryused","text":"updatePeakMemoryUsed...FIXME updatePeakMemoryUsed is used when UnsafeShuffleWriter is requested for the < > and to < >.","title":"void updatePeakMemoryUsed()"},{"location":"shuffle/UnsafeShuffleWriter/#writing-key-value-records-of-partition","text":"void write ( Iterator < Product2 < K , V >> records ) write traverses the input sequence of records (for a RDD partition) and insertRecordIntoSorter one by one. When all the records have been processed, write closes internal resources and merges spill files . In the end, write requests ShuffleExternalSorter to clean up . CAUTION: FIXME When requested to < >, UnsafeShuffleWriter simply < > followed by < > (that, among other things, creates the < >). write is part of the ShuffleWriter abstraction. == [[stop]] Stopping ShuffleWriter","title":" Writing Key-Value Records of Partition"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_1","text":"Option stop( boolean success) stop ...FIXME When requested to < >, UnsafeShuffleWriter records the peak execution memory metric and returns the < > (that was created when requested to < >). stop is part of the ShuffleWriter abstraction. == [[insertRecordIntoSorter]] Inserting Record Into ShuffleExternalSorter","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_2","text":"void insertRecordIntoSorter( Product2 record) insertRecordIntoSorter requires that the < > is available. insertRecordIntoSorter requests the < > to reset (so that all currently accumulated output in the output stream is discarded and reusing the already allocated buffer space). insertRecordIntoSorter requests the < > to write out the record (write the serializer:SerializationStream.md#writeKey[key] and the serializer:SerializationStream.md#writeValue[value]) and to serializer:SerializationStream.md#flush[flush]. [[insertRecordIntoSorter-serializedRecordSize]] insertRecordIntoSorter requests the < > for the length of the buffer. [[insertRecordIntoSorter-partitionId]] insertRecordIntoSorter requests the < > for the ../rdd/Partitioner.md#getPartition[partition] for the given record (by the key). In the end, insertRecordIntoSorter requests the < > to ShuffleExternalSorter.md#insertRecord[insert] the < > as a byte array (with the < > and the < >). insertRecordIntoSorter is used when UnsafeShuffleWriter is requested to < >.","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#closing-and-writing-output-merging-spill-files","text":"void closeAndWriteOutput () closeAndWriteOutput asserts that the ShuffleExternalSorter is created (non- null ). closeAndWriteOutput updates peak memory used . closeAndWriteOutput removes the references to the < > and < > output streams ( null s them). closeAndWriteOutput requests the < > to ShuffleExternalSorter.md#closeAndGetSpills[close and return spill metadata]. closeAndWriteOutput removes the reference to the < > ( null s it). closeAndWriteOutput requests the < > for the IndexShuffleBlockResolver.md#getDataFile[output data file] for the < > and < > IDs. [[closeAndWriteOutput-partitionLengths]][[closeAndWriteOutput-tmp]] closeAndWriteOutput creates a temporary file (along the data output file) and uses it to < > (that gives a partition length array). All spill files are then deleted. closeAndWriteOutput requests the < > to IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write shuffle index and data files] (for the < > and < > IDs, the < > and the < >). In the end, closeAndWriteOutput creates a scheduler:MapStatus.md[MapStatus] with the storage:BlockManager.md#shuffleServerId[location of the local BlockManager] and the < >. closeAndWriteOutput prints out the following ERROR message to the logs if there is an issue with deleting spill files: Error while deleting spill file [path] closeAndWriteOutput prints out the following ERROR message to the logs if there is an issue with deleting the < >: Error while deleting temp file [path] closeAndWriteOutput is used when UnsafeShuffleWriter is requested to write records . == [[mergeSpillsWithFileStream]] mergeSpillsWithFileStream Method","title":" Closing and Writing Output (Merging Spill Files)"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_3","text":"long[] mergeSpillsWithFileStream( SpillInfo[] spills, File outputFile, CompressionCodec compressionCodec) mergeSpillsWithFileStream will be given an io:CompressionCodec.md[IO compression codec] when shuffle compression is enabled. mergeSpillsWithFileStream...FIXME mergeSpillsWithFileStream requires that there are at least two spills to merge. mergeSpillsWithFileStream is used when UnsafeShuffleWriter is requested to < >. == [[getPeakMemoryUsedBytes]] Getting Peak Memory Used","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_4","text":"","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#long-getpeakmemoryusedbytes","text":"getPeakMemoryUsedBytes simply < > and returns the internal < > registry. getPeakMemoryUsedBytes is used when UnsafeShuffleWriter is requested to < >. == [[open]] Opening UnsafeShuffleWriter and Buffers","title":"long getPeakMemoryUsedBytes()"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_5","text":"","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#void-open","text":"open requires that there is no < > available. open creates a ShuffleExternalSorter.md[ShuffleExternalSorter]. open creates a < > with the capacity of < >. open requests the < > for a serializer:SerializerInstance.md#serializeStream[SerializationStream] to the < > (available internally as the < > reference). open is used when UnsafeShuffleWriter is < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.UnsafeShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"void open()"},{"location":"shuffle/UnsafeShuffleWriter/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"shuffle/UnsafeShuffleWriter/#log4jloggerorgapachesparkshufflesortunsafeshufflewriterall","text":"Refer to spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=ALL"},{"location":"shuffle/UnsafeShuffleWriter/#internal-properties","text":"","title":"Internal Properties"},{"location":"shuffle/UnsafeShuffleWriter/#mapstatus","text":"MapStatus Created when UnsafeShuffleWriter is requested to < > (with the storage:BlockManagerId.md[] of the < > and partitionLengths ) Returned when UnsafeShuffleWriter is requested to < >","title":" MapStatus"},{"location":"shuffle/UnsafeShuffleWriter/#partitioner","text":"Partitioner (as used by the BaseShuffleHandle.md#dependency[ShuffleDependency] of the < >) Used when UnsafeShuffleWriter is requested for the following: < > (and create a ShuffleExternalSorter.md[ShuffleExternalSorter] with the given ../rdd/Partitioner.md#numPartitions[number of partitions]) < > (and request the ../rdd/Partitioner.md#getPartition[partition for the key]) < >, < > and < > (for the ../rdd/Partitioner.md#numPartitions[number of partitions] to create partition lengths)","title":" Partitioner"},{"location":"shuffle/UnsafeShuffleWriter/#peak-memory-used","text":"Peak memory used (in bytes) that is updated exclusively in < > (after requesting the < > for ShuffleExternalSorter.md#getPeakMemoryUsedBytes[getPeakMemoryUsedBytes]) Use < > to access the current value","title":" Peak Memory Used"},{"location":"shuffle/UnsafeShuffleWriter/#bytearrayoutputstream-for-serialized-data","text":"{java-javadoc-url}/java/io/ByteArrayOutputStream.html[java.io.ByteArrayOutputStream] of serialized data (written into a byte array of < > initial size) Used when UnsafeShuffleWriter is requested for the following: < > (and create the internal < >) < > Destroyed ( null ) when requested to < >. === [[serializer]] serializer serializer:SerializerInstance.md[SerializerInstance] (that is a new instance of the Serializer of the BaseShuffleHandle.md#dependency[ShuffleDependency] of the < >) Used exclusively when UnsafeShuffleWriter is requested to < > (and creates the < >) === [[serOutputStream]] serOutputStream serializer:SerializationStream.md[SerializationStream] (that is created when the < > is requested to serializer:SerializerInstance.md#serializeStream[serializeStream] with the < >) Used when UnsafeShuffleWriter is requested to < > Destroyed ( null ) when requested to < >.","title":" ByteArrayOutputStream for Serialized Data"},{"location":"shuffle/UnsafeShuffleWriter/#shuffle-id","text":"Shuffle ID (of the ShuffleDependency of the SerializedShuffleHandle ) Used exclusively when requested to < > === [[writeMetrics]] writeMetrics executor:ShuffleWriteMetrics.md[] (of the TaskMetrics of the < >) Used when UnsafeShuffleWriter is requested for the following: < > (and creates the < >) < > < > < >","title":" Shuffle ID"},{"location":"spark-standalone/","text":"Spark Standalone \u00b6 Spark Standalone ( Standalone Cluster ) is Spark's own built-in cluster environment. Since Spark Standalone is available in the default distribution of Apache Spark it is the easiest way to run your Spark applications in a clustered environment in many cases. Standalone Master ( standalone Master ) is the resource manager for the Spark Standalone cluster. Standalone Worker is a worker in a Spark Standalone cluster. There can be one or many workers in a standalone cluster. In Standalone cluster mode Spark allocates resources based on cores. By default, an application will grab all the cores in the cluster. Standalone cluster mode is subject to the constraint that only one executor can be allocated on each worker per application. A Spark Standalone cluster is available using spark:// -prefixed master URL. Round-robin Scheduling Across Nodes \u00b6 If enabled (using spark.deploy.spreadOut ), standalone Master attempts to spread out an application's executors on as many workers as possible (instead of trying to consolidate it onto a small number of nodes). scheduleExecutorsOnWorkers \u00b6 scheduleExecutorsOnWorkers ( app : ApplicationInfo , usableWorkers : Array [ WorkerInfo ], spreadOutApps : Boolean ): Array [ Int ] scheduleExecutorsOnWorkers schedules executors on workers. SPARK_WORKER_INSTANCES (and SPARK_WORKER_CORES) \u00b6 There is really no need to run multiple workers per machine in Spark 1.5 (perhaps in 1.4, too). You can run multiple executors on the same machine with one worker. Use SPARK_WORKER_INSTANCES (default: 1 ) in spark-env.sh to define the number of worker instances. If you use SPARK_WORKER_INSTANCES , make sure to set SPARK_WORKER_CORES explicitly to limit the cores per worker, or else each worker will try to use all the cores. You can set up the number of cores as an command line argument when you start a worker daemon using --cores . Multiple executors per worker in Standalone mode \u00b6 CAUTION: It can be a duplicate of the above section. It is possible to start multiple executors in a single JVM process of a worker. To launch multiple executors on a machine you start multiple standalone workers, each with its own JVM. It introduces unnecessary overhead due to these JVM processes, provided that there are enough cores on that worker. If you are running Spark in standalone mode on memory-rich nodes it can be beneficial to have multiple worker instances on the same node as a very large heap size has two disadvantages: Garbage collector pauses can hurt throughput of Spark jobs. Heap size of >32 GB can\u2019t use CompressedOoops. So 35 GB is actually less than 32 GB . Mesos and YARN can, out of the box, support packing multiple, smaller executors onto the same physical host, so requesting smaller executors doesn\u2019t mean your application will have fewer overall resources. SparkDeploySchedulerBackend \u00b6 SparkDeploySchedulerBackend is the xref:scheduler:SchedulerBackend.md[Scheduler Backend] for Spark Standalone, i.e. it is used when you xref:ROOT:SparkContext.md#creating-instance[create a SparkContext] using spark:// link:spark-deployment-environments.md#master-urls[master URL]. It requires a xref:scheduler:TaskScheduler.md[Task Scheduler], a xref:ROOT:SparkContext.md[], and a collection of link:spark-deployment-environments.md#master-urls[master URLs]. It is a specialized xref:scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] that uses < > and is a AppClientListener . .SparkDeploySchedulerBackend.start() (while SparkContext starts) image::SparkDeploySchedulerBackend-AppClient-start.png[align=\"center\"] CAUTION: FIXME AppClientListener & ApplicationDescription It uses < > to talk to executors. AppClient \u00b6 AppClient is an interface to allow Spark applications to talk to a Standalone cluster (using a RPC Environment). It takes an RPC Environment, a collection of master URLs, a ApplicationDescription , and a AppClientListener . It is solely used by < >. AppClient registers AppClient RPC endpoint (using ClientEndpoint class) to a given RPC Environment. AppClient uses a daemon cached thread pool ( askAndReplyThreadPool ) with threads' name in the format of appclient-receive-and-reply-threadpool-ID , where ID is a unique integer for asynchronous asks and replies. It is used for requesting executors (via RequestExecutors message) and kill executors (via KillExecutors ). sendToMaster sends one-way ExecutorStateChanged and UnregisterApplication messages to master. Initialization - AppClient.start() method \u00b6 When AppClient starts, AppClient.start() method is called that merely registers < >. AppClient RPC Endpoint \u00b6 AppClient RPC endpoint is started as part of < > (that is in turn part of < >). It is a xref:rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] that knows about the RPC endpoint of the primary active standalone Master (there can be a couple of them, but only one can be active and hence primary). When it starts, it sends < > message to register an application and itself. RegisterApplication RPC message \u00b6 An AppClient registers the Spark application to a single master (regardless of link:spark-deployment-environments.md#master-urls[the number of the standalone masters given in the master URL]). .AppClient registers application to standalone Master image::appclient-registerapplication.png[align=\"center\"] It uses a dedicated thread pool appclient-register-master-threadpool to asynchronously send RegisterApplication messages, one per standalone master. INFO AppClient$ClientEndpoint: Connecting to master spark://localhost:7077... An AppClient tries connecting to a standalone master 3 times every 20 seconds per master before giving up. They are not configurable parameters. The appclient-register-master-threadpool thread pool is used until the registration is finished, i.e. AppClient is connected to the primary standalone Master or the registration fails. It is then shutdown . RegisteredApplication RPC message \u00b6 RegisteredApplication is a one-way message from the primary master to confirm successful application registration. It comes with the application id and the master's RPC endpoint reference. The AppClientListener gets notified about the event via listener.connected(appId) with appId being an application id. ApplicationRemoved RPC message \u00b6 ApplicationRemoved is received from the primary master to inform about having removed the application. AppClient RPC endpoint is stopped afterwards. It can come from the standalone Master after a kill request from Web UI, application has finished properly or the executor where the application was still running on has been killed, failed, lost or exited. ExecutorAdded RPC message \u00b6 ExecutorAdded is received from the primary master to inform about...FIXME CAUTION: FIXME the message INFO Executor added: %s on %s (%s) with %d cores ExecutorUpdated RPC message \u00b6 ExecutorUpdated is received from the primary master to inform about...FIXME CAUTION: FIXME the message INFO Executor updated: %s is now %s%s MasterChanged RPC message \u00b6 MasterChanged is received from the primary master to inform about...FIXME CAUTION: FIXME the message INFO Master has changed, new master is at StopAppClient RPC message \u00b6 StopAppClient is a reply-response message from the SparkDeploySchedulerBackend to stop the AppClient after the SparkContext has been stopped (and so should the running application on the standalone cluster). It stops the AppClient RPC endpoint. RequestExecutors RPC message \u00b6 RequestExecutors is a reply-response message from the SparkDeploySchedulerBackend that is passed on to the master to request executors for the application. KillExecutors RPC message \u00b6 KillExecutors is a reply-response message from the SparkDeploySchedulerBackend that is passed on to the master to kill executors assigned to the application.","title":"Spark Standalone"},{"location":"spark-standalone/#spark-standalone","text":"Spark Standalone ( Standalone Cluster ) is Spark's own built-in cluster environment. Since Spark Standalone is available in the default distribution of Apache Spark it is the easiest way to run your Spark applications in a clustered environment in many cases. Standalone Master ( standalone Master ) is the resource manager for the Spark Standalone cluster. Standalone Worker is a worker in a Spark Standalone cluster. There can be one or many workers in a standalone cluster. In Standalone cluster mode Spark allocates resources based on cores. By default, an application will grab all the cores in the cluster. Standalone cluster mode is subject to the constraint that only one executor can be allocated on each worker per application. A Spark Standalone cluster is available using spark:// -prefixed master URL.","title":"Spark Standalone"},{"location":"spark-standalone/#round-robin-scheduling-across-nodes","text":"If enabled (using spark.deploy.spreadOut ), standalone Master attempts to spread out an application's executors on as many workers as possible (instead of trying to consolidate it onto a small number of nodes).","title":"Round-robin Scheduling Across Nodes"},{"location":"spark-standalone/#scheduleexecutorsonworkers","text":"scheduleExecutorsOnWorkers ( app : ApplicationInfo , usableWorkers : Array [ WorkerInfo ], spreadOutApps : Boolean ): Array [ Int ] scheduleExecutorsOnWorkers schedules executors on workers.","title":"scheduleExecutorsOnWorkers"},{"location":"spark-standalone/#spark_worker_instances-and-spark_worker_cores","text":"There is really no need to run multiple workers per machine in Spark 1.5 (perhaps in 1.4, too). You can run multiple executors on the same machine with one worker. Use SPARK_WORKER_INSTANCES (default: 1 ) in spark-env.sh to define the number of worker instances. If you use SPARK_WORKER_INSTANCES , make sure to set SPARK_WORKER_CORES explicitly to limit the cores per worker, or else each worker will try to use all the cores. You can set up the number of cores as an command line argument when you start a worker daemon using --cores .","title":"SPARK_WORKER_INSTANCES (and SPARK_WORKER_CORES)"},{"location":"spark-standalone/#multiple-executors-per-worker-in-standalone-mode","text":"CAUTION: It can be a duplicate of the above section. It is possible to start multiple executors in a single JVM process of a worker. To launch multiple executors on a machine you start multiple standalone workers, each with its own JVM. It introduces unnecessary overhead due to these JVM processes, provided that there are enough cores on that worker. If you are running Spark in standalone mode on memory-rich nodes it can be beneficial to have multiple worker instances on the same node as a very large heap size has two disadvantages: Garbage collector pauses can hurt throughput of Spark jobs. Heap size of >32 GB can\u2019t use CompressedOoops. So 35 GB is actually less than 32 GB . Mesos and YARN can, out of the box, support packing multiple, smaller executors onto the same physical host, so requesting smaller executors doesn\u2019t mean your application will have fewer overall resources.","title":"Multiple executors per worker in Standalone mode"},{"location":"spark-standalone/#sparkdeployschedulerbackend","text":"SparkDeploySchedulerBackend is the xref:scheduler:SchedulerBackend.md[Scheduler Backend] for Spark Standalone, i.e. it is used when you xref:ROOT:SparkContext.md#creating-instance[create a SparkContext] using spark:// link:spark-deployment-environments.md#master-urls[master URL]. It requires a xref:scheduler:TaskScheduler.md[Task Scheduler], a xref:ROOT:SparkContext.md[], and a collection of link:spark-deployment-environments.md#master-urls[master URLs]. It is a specialized xref:scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] that uses < > and is a AppClientListener . .SparkDeploySchedulerBackend.start() (while SparkContext starts) image::SparkDeploySchedulerBackend-AppClient-start.png[align=\"center\"] CAUTION: FIXME AppClientListener & ApplicationDescription It uses < > to talk to executors.","title":"SparkDeploySchedulerBackend"},{"location":"spark-standalone/#appclient","text":"AppClient is an interface to allow Spark applications to talk to a Standalone cluster (using a RPC Environment). It takes an RPC Environment, a collection of master URLs, a ApplicationDescription , and a AppClientListener . It is solely used by < >. AppClient registers AppClient RPC endpoint (using ClientEndpoint class) to a given RPC Environment. AppClient uses a daemon cached thread pool ( askAndReplyThreadPool ) with threads' name in the format of appclient-receive-and-reply-threadpool-ID , where ID is a unique integer for asynchronous asks and replies. It is used for requesting executors (via RequestExecutors message) and kill executors (via KillExecutors ). sendToMaster sends one-way ExecutorStateChanged and UnregisterApplication messages to master.","title":"AppClient"},{"location":"spark-standalone/#initialization-appclientstart-method","text":"When AppClient starts, AppClient.start() method is called that merely registers < >.","title":"Initialization - AppClient.start() method"},{"location":"spark-standalone/#appclient-rpc-endpoint","text":"AppClient RPC endpoint is started as part of < > (that is in turn part of < >). It is a xref:rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] that knows about the RPC endpoint of the primary active standalone Master (there can be a couple of them, but only one can be active and hence primary). When it starts, it sends < > message to register an application and itself.","title":"AppClient RPC Endpoint"},{"location":"spark-standalone/#registerapplication-rpc-message","text":"An AppClient registers the Spark application to a single master (regardless of link:spark-deployment-environments.md#master-urls[the number of the standalone masters given in the master URL]). .AppClient registers application to standalone Master image::appclient-registerapplication.png[align=\"center\"] It uses a dedicated thread pool appclient-register-master-threadpool to asynchronously send RegisterApplication messages, one per standalone master. INFO AppClient$ClientEndpoint: Connecting to master spark://localhost:7077... An AppClient tries connecting to a standalone master 3 times every 20 seconds per master before giving up. They are not configurable parameters. The appclient-register-master-threadpool thread pool is used until the registration is finished, i.e. AppClient is connected to the primary standalone Master or the registration fails. It is then shutdown .","title":"RegisterApplication RPC message"},{"location":"spark-standalone/#registeredapplication-rpc-message","text":"RegisteredApplication is a one-way message from the primary master to confirm successful application registration. It comes with the application id and the master's RPC endpoint reference. The AppClientListener gets notified about the event via listener.connected(appId) with appId being an application id.","title":"RegisteredApplication RPC message"},{"location":"spark-standalone/#applicationremoved-rpc-message","text":"ApplicationRemoved is received from the primary master to inform about having removed the application. AppClient RPC endpoint is stopped afterwards. It can come from the standalone Master after a kill request from Web UI, application has finished properly or the executor where the application was still running on has been killed, failed, lost or exited.","title":"ApplicationRemoved RPC message"},{"location":"spark-standalone/#executoradded-rpc-message","text":"ExecutorAdded is received from the primary master to inform about...FIXME CAUTION: FIXME the message INFO Executor added: %s on %s (%s) with %d cores","title":"ExecutorAdded RPC message"},{"location":"spark-standalone/#executorupdated-rpc-message","text":"ExecutorUpdated is received from the primary master to inform about...FIXME CAUTION: FIXME the message INFO Executor updated: %s is now %s%s","title":"ExecutorUpdated RPC message"},{"location":"spark-standalone/#masterchanged-rpc-message","text":"MasterChanged is received from the primary master to inform about...FIXME CAUTION: FIXME the message INFO Master has changed, new master is at","title":"MasterChanged RPC message"},{"location":"spark-standalone/#stopappclient-rpc-message","text":"StopAppClient is a reply-response message from the SparkDeploySchedulerBackend to stop the AppClient after the SparkContext has been stopped (and so should the running application on the standalone cluster). It stops the AppClient RPC endpoint.","title":"StopAppClient RPC message"},{"location":"spark-standalone/#requestexecutors-rpc-message","text":"RequestExecutors is a reply-response message from the SparkDeploySchedulerBackend that is passed on to the master to request executors for the application.","title":"RequestExecutors RPC message"},{"location":"spark-standalone/#killexecutors-rpc-message","text":"KillExecutors is a reply-response message from the SparkDeploySchedulerBackend that is passed on to the master to kill executors assigned to the application.","title":"KillExecutors RPC message"},{"location":"spark-standalone/ClientApp/","text":"ClientApp \u00b6 ClientApp is...FIXME","title":"ClientApp"},{"location":"spark-standalone/ClientApp/#clientapp","text":"ClientApp is...FIXME","title":"ClientApp"},{"location":"spark-standalone/ClientEndpoint/","text":"ClientEndpoint \u00b6 ClientEndpoint is a ThreadSafeRpcEndpoint . Creating Instance \u00b6 ClientEndpoint takes the following to be created: RpcEnv ClientEndpoint is created when: StandaloneAppClient is requested to start","title":"ClientEndpoint"},{"location":"spark-standalone/ClientEndpoint/#clientendpoint","text":"ClientEndpoint is a ThreadSafeRpcEndpoint .","title":"ClientEndpoint"},{"location":"spark-standalone/ClientEndpoint/#creating-instance","text":"ClientEndpoint takes the following to be created: RpcEnv ClientEndpoint is created when: StandaloneAppClient is requested to start","title":"Creating Instance"},{"location":"spark-standalone/CreateSubmissionRequest/","text":"CreateSubmissionRequest \u00b6 CreateSubmissionRequest is a SubmitRestProtocolRequest . Demo \u00b6 Start spark-shell and use :paste -raw to paste the following snippet (in raw mode that allows bypassing private[rest] in the classes of our interest). package org.apache.spark.deploy.rest object demo { def createCreateSubmissionRequest(): CreateSubmissionRequest = { val submitRequest = new CreateSubmissionRequest submitRequest.clientSparkVersion=\"3.1.2\" submitRequest.sparkProperties=Map(\"spark.app.name\" -> \"DemoApp\") submitRequest.appResource=\"hdfs://localhost:9000/demo-app-1.0.0-jar-with-dependencies.jar\" submitRequest.appArgs=\"this is a demo\".split(\"\\\\s+\") submitRequest.environmentVariables=Map.empty submitRequest.validate submitRequest } def client(master: String = \"spark://localhost:6066\"): RestSubmissionClient = { // (1) new RestSubmissionClient(master) } } Uses the default 6066 REST port (not 7077 ) import org . apache . spark . deploy . rest . demo val submitRequest = demo . createCreateSubmissionRequest val client = demo . client () client . createSubmission ( submitRequest ) scala> println(submitRequest.toJson) { \"action\" : \"CreateSubmissionRequest\", \"appArgs\" : [ \"this\", \"is\", \"a\", \"demo\" ], \"appResource\" : \"demo.jar\", \"clientSparkVersion\" : \"3.1.2\", \"environmentVariables\" : { }, \"sparkProperties\" : { \"spark.app.name\" : \"DemoApp\" } }","title":"CreateSubmissionRequest"},{"location":"spark-standalone/CreateSubmissionRequest/#createsubmissionrequest","text":"CreateSubmissionRequest is a SubmitRestProtocolRequest .","title":"CreateSubmissionRequest"},{"location":"spark-standalone/CreateSubmissionRequest/#demo","text":"Start spark-shell and use :paste -raw to paste the following snippet (in raw mode that allows bypassing private[rest] in the classes of our interest). package org.apache.spark.deploy.rest object demo { def createCreateSubmissionRequest(): CreateSubmissionRequest = { val submitRequest = new CreateSubmissionRequest submitRequest.clientSparkVersion=\"3.1.2\" submitRequest.sparkProperties=Map(\"spark.app.name\" -> \"DemoApp\") submitRequest.appResource=\"hdfs://localhost:9000/demo-app-1.0.0-jar-with-dependencies.jar\" submitRequest.appArgs=\"this is a demo\".split(\"\\\\s+\") submitRequest.environmentVariables=Map.empty submitRequest.validate submitRequest } def client(master: String = \"spark://localhost:6066\"): RestSubmissionClient = { // (1) new RestSubmissionClient(master) } } Uses the default 6066 REST port (not 7077 ) import org . apache . spark . deploy . rest . demo val submitRequest = demo . createCreateSubmissionRequest val client = demo . client () client . createSubmission ( submitRequest ) scala> println(submitRequest.toJson) { \"action\" : \"CreateSubmissionRequest\", \"appArgs\" : [ \"this\", \"is\", \"a\", \"demo\" ], \"appResource\" : \"demo.jar\", \"clientSparkVersion\" : \"3.1.2\", \"environmentVariables\" : { }, \"sparkProperties\" : { \"spark.app.name\" : \"DemoApp\" } }","title":"Demo"},{"location":"spark-standalone/ExecutorRunner/","text":"ExecutorRunner \u00b6 Creating Instance \u00b6 ExecutorRunner takes the following to be created: Application ID Executor ID ApplicationDescription Number of CPU cores Amount of Memory RpcEndpointRef Worker ID web UI's scheme Host web UI's port Public Address Spark's Home Directory Executor's Directory Worker's URL SparkConf Local Directories of the Spark Application Executor State Map[String, ResourceInformation] (default: empty) ExecutorRunner is created when: Worker is requested to handle a LaunchExecutor message Starting ExecutorRunner \u00b6 start (): Unit start ...FIXME start is used when: Worker is requested to handle a LaunchExecutor message fetchAndRunExecutor \u00b6 fetchAndRunExecutor (): Unit fetchAndRunExecutor ...FIXME killProcess \u00b6 killProcess ( message : Option [ String ]): Unit killProcess ...FIXME killProcess is used when: ExecutorRunner is requested to fetchAndRunExecutor (and fails) and shut down","title":"ExecutorRunner"},{"location":"spark-standalone/ExecutorRunner/#executorrunner","text":"","title":"ExecutorRunner"},{"location":"spark-standalone/ExecutorRunner/#creating-instance","text":"ExecutorRunner takes the following to be created: Application ID Executor ID ApplicationDescription Number of CPU cores Amount of Memory RpcEndpointRef Worker ID web UI's scheme Host web UI's port Public Address Spark's Home Directory Executor's Directory Worker's URL SparkConf Local Directories of the Spark Application Executor State Map[String, ResourceInformation] (default: empty) ExecutorRunner is created when: Worker is requested to handle a LaunchExecutor message","title":"Creating Instance"},{"location":"spark-standalone/ExecutorRunner/#starting-executorrunner","text":"start (): Unit start ...FIXME start is used when: Worker is requested to handle a LaunchExecutor message","title":" Starting ExecutorRunner"},{"location":"spark-standalone/ExecutorRunner/#fetchandrunexecutor","text":"fetchAndRunExecutor (): Unit fetchAndRunExecutor ...FIXME","title":" fetchAndRunExecutor"},{"location":"spark-standalone/ExecutorRunner/#killprocess","text":"killProcess ( message : Option [ String ]): Unit killProcess ...FIXME killProcess is used when: ExecutorRunner is requested to fetchAndRunExecutor (and fails) and shut down","title":" killProcess"},{"location":"spark-standalone/LocalSparkCluster/","text":"LocalSparkCluster \u00b6 LocalSparkCluster is a single-JVM Spark Standalone cluster that is available as local-cluster master URL. NOTE: local-cluster master URL matches local-cluster[numWorkers,coresPerWorker,memoryPerWorker] pattern where < >, < > and < > are all numbers separated by the comma. LocalSparkCluster can be particularly useful to test distributed operation and fault recovery without spinning up a lot of processes. LocalSparkCluster is < > when SparkContext is created for local-cluster master URL (and so requested to xref:ROOT:SparkContext.md#createTaskScheduler[create the SchedulerBackend and the TaskScheduler]). [[logging]] [TIP] ==== Enable INFO logging level for org.apache.spark.deploy.LocalSparkCluster logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.LocalSparkCluster=INFO Refer to link:spark-logging.md[Logging]. \u00b6 === [[creating-instance]] Creating LocalSparkCluster Instance LocalSparkCluster takes the following when created: [[numWorkers]] Number of workers [[coresPerWorker]] CPU cores per worker [[memoryPerWorker]] Memory per worker [[conf]] xref:ROOT:SparkConf.md[SparkConf] LocalSparkCluster initializes the < >. Starting \u00b6 start (): Array [ String ] start ...FIXME start is used when...FIXME === [[stop]] Stopping LocalSparkCluster [source, scala] \u00b6 stop(): Unit \u00b6 stop ...FIXME NOTE: stop is used when...FIXME","title":"LocalSparkCluster"},{"location":"spark-standalone/LocalSparkCluster/#localsparkcluster","text":"LocalSparkCluster is a single-JVM Spark Standalone cluster that is available as local-cluster master URL. NOTE: local-cluster master URL matches local-cluster[numWorkers,coresPerWorker,memoryPerWorker] pattern where < >, < > and < > are all numbers separated by the comma. LocalSparkCluster can be particularly useful to test distributed operation and fault recovery without spinning up a lot of processes. LocalSparkCluster is < > when SparkContext is created for local-cluster master URL (and so requested to xref:ROOT:SparkContext.md#createTaskScheduler[create the SchedulerBackend and the TaskScheduler]). [[logging]] [TIP] ==== Enable INFO logging level for org.apache.spark.deploy.LocalSparkCluster logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.LocalSparkCluster=INFO","title":"LocalSparkCluster"},{"location":"spark-standalone/LocalSparkCluster/#refer-to-linkspark-loggingmdlogging","text":"=== [[creating-instance]] Creating LocalSparkCluster Instance LocalSparkCluster takes the following when created: [[numWorkers]] Number of workers [[coresPerWorker]] CPU cores per worker [[memoryPerWorker]] Memory per worker [[conf]] xref:ROOT:SparkConf.md[SparkConf] LocalSparkCluster initializes the < >.","title":"Refer to link:spark-logging.md[Logging]."},{"location":"spark-standalone/LocalSparkCluster/#starting","text":"start (): Array [ String ] start ...FIXME start is used when...FIXME === [[stop]] Stopping LocalSparkCluster","title":" Starting"},{"location":"spark-standalone/LocalSparkCluster/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-standalone/LocalSparkCluster/#stop-unit","text":"stop ...FIXME NOTE: stop is used when...FIXME","title":"stop(): Unit"},{"location":"spark-standalone/Master/","text":"Master \u00b6 Master is the manager of a Spark Standalone cluster. Master can be launched from command line . StandaloneRestServer \u00b6 Master can start StandaloneRestServer when enabled using spark.master.rest.enabled configuration property. StandaloneRestServer is requested to start in onStart and stop in onStop Master RPC Endpoint \u00b6 Master is a ThreadSafeRpcEndpoint and is registered under Master name (when launched as a command-line application and requested to start up an RPC environment ). Launching Standalone Master \u00b6 Master can be launched as a standalone application using spark-class . ./bin/spark-class org.apache.spark.deploy.master.Master main Entry Point \u00b6 main ( argStrings : Array [ String ]): Unit main is the entry point of the Master standalone application. main prints out the following INFO message to the logs: Started daemon with process name: [processName] main registers signal handlers for TERM , HUP , INT signals. main parses command-line options (using MasterArguments ) and initializes an RpcEnv . In the end, main requests the RpcEnv to be notified when terminated . Command-Line Options \u00b6 Master supports command-line options. Usage: Master [options] Options: -i HOST, --ip HOST Hostname to listen on (deprecated, please use --host or -h) -h HOST, --host HOST Hostname to listen on -p PORT, --port PORT Port to listen on (default: 7077) --webui-port PORT Port for web UI (default: 8080) --properties-file FILE Path to a custom Spark properties file. Default is conf/spark-defaults.conf. host \u00b6 ip \u00b6 port \u00b6 properties-file \u00b6 webui-port \u00b6 Creating Instance \u00b6 Master takes the following to be created: RpcEnv RpcAddress web UI's Port SecurityManager SparkConf Master is created when: Master utility is requested to start up RPC environment Starting Up RPC Environment \u00b6 startRpcEnvAndEndpoint ( host : String , port : Int , webUiPort : Int , conf : SparkConf ): ( RpcEnv , Int , Option [ Int ]) startRpcEnvAndEndpoint creates a RPC Environment with sparkMaster name (and the input arguments) and registers Master endpoint with Master name. In the end, startRpcEnvAndEndpoint sends BoundPortsResponse message (synchronously) to the Master endpoint and returns the RpcEnv with the ports of the web UI and the REST Server . startRpcEnvAndEndpoint is used when: LocalSparkCluster is requested to start Master standalone application is launched spreadOutApps \u00b6 Master uses spark.deploy.spreadOut configuration property when requested to startExecutorsOnWorkers . Scheduling Resources Among Waiting Applications \u00b6 schedule (): Unit schedule ...FIXME schedule is used when: Master is requested to schedule resources among waiting applications startExecutorsOnWorkers \u00b6 startExecutorsOnWorkers (): Unit startExecutorsOnWorkers ...FIXME WebUI \u00b6 MasterWebUI is the Web UI server for the standalone master. Master starts Web UI to listen to http://[master's hostname]:webUIPort (e.g. http://localhost:8080 ). Successfully started service 'MasterUI' on port 8080. Started MasterWebUI at http://192.168.1.4:8080 States \u00b6 Master can be in the following states: STANDBY - the initial state while Master is initializing ALIVE - start scheduling resources among applications RECOVERING COMPLETING_RECOVERY LeaderElectable \u00b6 Master is LeaderElectable . To be Reviewed \u00b6 Application ids follows the pattern app-yyyyMMddHHmmss . Master can be < > and stopped using link:spark-standalone-master-scripts.md[custom management scripts for standalone Master]. REST Server \u00b6 The standalone Master starts the REST Server service for alternative application submission that is supposed to work across Spark versions. It is enabled by default (see < >) and used by link:spark-submit.md[spark-submit] for the link:spark-standalone.md#deployment-modes[standalone cluster mode], i.e. --deploy-mode is cluster . RestSubmissionClient is the client. The server includes a JSON representation of SubmitRestProtocolResponse in the HTTP body. The following INFOs show up when the Master Endpoint starts up ( Master#onStart is called) with REST Server enabled: INFO Utils: Successfully started service on port 6066. INFO StandaloneRestServer: Started REST server for submitting applications on port 6066 Recovery Mode \u00b6 A standalone Master can run with recovery mode enabled and be able to recover state among the available swarm of masters. By default, there is no recovery, i.e. no persistence and no election. NOTE: Only a master can schedule tasks so having one always on is important for cases where you want to launch new tasks. Running tasks are unaffected by the state of the master. Master uses spark.deploy.recoveryMode to set up the recovery mode (see < >). The Recovery Mode enables < > among the masters. TIP: Check out the exercise link:exercises/spark-exercise-standalone-master-ha.md[Spark Standalone - Using ZooKeeper for High-Availability of Master]. RPC Messages \u00b6 Master communicates with drivers, executors and configures itself using RPC messages . The following message types are accepted by master (see Master#receive or Master#receiveAndReply methods): ElectedLeader for < > CompleteRecovery RevokedLeadership < > ExecutorStateChanged DriverStateChanged Heartbeat MasterChangeAcknowledged WorkerSchedulerStateResponse UnregisterApplication CheckForWorkerTimeOut RegisterWorker RequestSubmitDriver RequestKillDriver RequestDriverStatus RequestMasterState BoundPortsRequest RequestExecutors KillExecutors RegisterApplication event \u00b6 A RegisterApplication event is sent by link:spark-standalone.md#AppClient[AppClient] to the standalone Master. The event holds information about the application being deployed ( ApplicationDescription ) and the driver's endpoint reference. ApplicationDescription describes an application by its name, maximum number of cores, executor's memory, command, appUiUrl, and user with optional eventLogDir and eventLogCodec for Event Logs, and the number of cores per executor. CAUTION: FIXME Finish A standalone Master receives RegisterApplication with a ApplicationDescription and the driver's xref:rpc:RpcEndpointRef.md[RpcEndpointRef]. INFO Registering app \" + description.name Application ids in Spark Standalone are in the format of app-[yyyyMMddHHmmss]-[4-digit nextAppNumber] . Master keeps track of the number of already-scheduled applications ( nextAppNumber ). ApplicationDescription (AppClient) \u2192 ApplicationInfo (Master) - application structure enrichment ApplicationSource metrics + applicationMetricsSystem INFO Registered app \" + description.name + \" with ID \" + app.id CAUTION: FIXME persistenceEngine.addApplication(app) schedule() schedules the currently available resources among waiting apps. FIXME When is schedule() method called? It's only executed when the Master is in RecoveryState.ALIVE state. Worker in WorkerState.ALIVE state can accept applications. A driver has a state, i.e. driver.state and when it's in DriverState.RUNNING state the driver has been assigned to a worker for execution. LaunchDriver RPC message \u00b6 WARNING: It seems a dead message. Disregard it for now. A LaunchDriver message is sent by an active standalone Master to a worker to launch a driver. .Master finds a place for a driver (posts LaunchDriver) image::spark-standalone-master-worker-LaunchDriver.png[align=\"center\"] You should see the following INFO in the logs right before the message is sent out to a worker: INFO Launching driver [driver.id] on worker [worker.id] The message holds information about the id and name of the driver. A driver can be running on a single worker while a worker can have many drivers running. When a worker receives a LaunchDriver message, it prints out the following INFO: INFO Asked to launch driver [driver.id] It then creates a DriverRunner and starts it. It starts a separate JVM process. Workers' free memory and cores are considered when assigning some to waiting drivers (applications). CAUTION: FIXME Go over waitingDrivers ... Internals of org.apache.spark.deploy.master.Master \u00b6 When Master starts, it first creates the xref:ROOT:SparkConf.md#default-configuration[default SparkConf configuration] whose values it then overrides using < > and < >. A fully-configured master instance requires host , port (default: 7077 ), webUiPort (default: 8080 ) settings defined. TIP: When in troubles, consult link:spark-tips-and-tricks.md[Spark Tips and Tricks] document. It starts < > with necessary endpoints and lives until the RPC environment terminates. Worker Management \u00b6 Master uses master-forward-message-thread to schedule a thread every spark.worker.timeout to check workers' availability and remove timed-out workers. It is that Master sends CheckForWorkerTimeOut message to itself to trigger verification. When a worker hasn't responded for spark.worker.timeout , it is assumed dead and the following WARN message appears in the logs: WARN Removing [worker.id] because we got no heartbeat in [spark.worker.timeout] seconds System Environment Variables \u00b6 Master uses the following system environment variables (directly or indirectly): SPARK_LOCAL_HOSTNAME - the custom host name SPARK_LOCAL_IP - the custom IP to use when SPARK_LOCAL_HOSTNAME is not set SPARK_MASTER_HOST (not SPARK_MASTER_IP as used in start-master.sh script above!) - the master custom host SPARK_MASTER_PORT (default: 7077 ) - the master custom port SPARK_MASTER_IP (default: hostname command's output) SPARK_MASTER_WEBUI_PORT (default: 8080 ) - the port of the master's WebUI. Overriden by spark.master.ui.port if set in the properties file. SPARK_PUBLIC_DNS (default: hostname) - the custom master hostname for WebUI's http URL and master's address. SPARK_CONF_DIR (default: $SPARK_HOME/conf ) - the directory of the default properties file link:spark-properties.md#spark-defaults-conf[spark-defaults.conf] from which all properties that start with spark. prefix are loaded. Settings \u00b6 Master uses the following properties: spark.cores.max (default: 0 ) - total expected number of cores. When set, an application could get executors of different sizes (in terms of cores). spark.dead.worker.persistence (default: 15 ) spark.deploy.retainedApplications (default: 200 ) spark.deploy.retainedDrivers (default: 200 ) spark.deploy.recoveryMode (default: NONE ) - possible modes: ZOOKEEPER , FILESYSTEM , or CUSTOM . Refer to < >. spark.deploy.recoveryMode.factory - the class name of the custom StandaloneRecoveryModeFactory . spark.deploy.recoveryDirectory (default: empty) - the directory to persist recovery state link:spark-standalone.md#spark.deploy.spreadOut[spark.deploy.spreadOut] to perform link:spark-standalone.md#round-robin-scheduling[round-robin scheduling across the nodes]. spark.deploy.defaultCores (default: Int.MaxValue , i.e. unbounded) - the number of maxCores for applications that don't specify it. spark.worker.timeout (default: 60 ) - time (in seconds) when no heartbeat from a worker means it is lost. See < >.","title":"Master"},{"location":"spark-standalone/Master/#master","text":"Master is the manager of a Spark Standalone cluster. Master can be launched from command line .","title":"Master"},{"location":"spark-standalone/Master/#standalonerestserver","text":"Master can start StandaloneRestServer when enabled using spark.master.rest.enabled configuration property. StandaloneRestServer is requested to start in onStart and stop in onStop","title":" StandaloneRestServer"},{"location":"spark-standalone/Master/#master-rpc-endpoint","text":"Master is a ThreadSafeRpcEndpoint and is registered under Master name (when launched as a command-line application and requested to start up an RPC environment ).","title":" Master RPC Endpoint"},{"location":"spark-standalone/Master/#launching-standalone-master","text":"Master can be launched as a standalone application using spark-class . ./bin/spark-class org.apache.spark.deploy.master.Master","title":" Launching Standalone Master"},{"location":"spark-standalone/Master/#main-entry-point","text":"main ( argStrings : Array [ String ]): Unit main is the entry point of the Master standalone application. main prints out the following INFO message to the logs: Started daemon with process name: [processName] main registers signal handlers for TERM , HUP , INT signals. main parses command-line options (using MasterArguments ) and initializes an RpcEnv . In the end, main requests the RpcEnv to be notified when terminated .","title":" main Entry Point"},{"location":"spark-standalone/Master/#command-line-options","text":"Master supports command-line options. Usage: Master [options] Options: -i HOST, --ip HOST Hostname to listen on (deprecated, please use --host or -h) -h HOST, --host HOST Hostname to listen on -p PORT, --port PORT Port to listen on (default: 7077) --webui-port PORT Port for web UI (default: 8080) --properties-file FILE Path to a custom Spark properties file. Default is conf/spark-defaults.conf.","title":" Command-Line Options"},{"location":"spark-standalone/Master/#host","text":"","title":"host"},{"location":"spark-standalone/Master/#ip","text":"","title":"ip"},{"location":"spark-standalone/Master/#port","text":"","title":"port"},{"location":"spark-standalone/Master/#properties-file","text":"","title":"properties-file"},{"location":"spark-standalone/Master/#webui-port","text":"","title":"webui-port"},{"location":"spark-standalone/Master/#creating-instance","text":"Master takes the following to be created: RpcEnv RpcAddress web UI's Port SecurityManager SparkConf Master is created when: Master utility is requested to start up RPC environment","title":"Creating Instance"},{"location":"spark-standalone/Master/#starting-up-rpc-environment","text":"startRpcEnvAndEndpoint ( host : String , port : Int , webUiPort : Int , conf : SparkConf ): ( RpcEnv , Int , Option [ Int ]) startRpcEnvAndEndpoint creates a RPC Environment with sparkMaster name (and the input arguments) and registers Master endpoint with Master name. In the end, startRpcEnvAndEndpoint sends BoundPortsResponse message (synchronously) to the Master endpoint and returns the RpcEnv with the ports of the web UI and the REST Server . startRpcEnvAndEndpoint is used when: LocalSparkCluster is requested to start Master standalone application is launched","title":" Starting Up RPC Environment"},{"location":"spark-standalone/Master/#spreadoutapps","text":"Master uses spark.deploy.spreadOut configuration property when requested to startExecutorsOnWorkers .","title":" spreadOutApps"},{"location":"spark-standalone/Master/#scheduling-resources-among-waiting-applications","text":"schedule (): Unit schedule ...FIXME schedule is used when: Master is requested to schedule resources among waiting applications","title":" Scheduling Resources Among Waiting Applications"},{"location":"spark-standalone/Master/#startexecutorsonworkers","text":"startExecutorsOnWorkers (): Unit startExecutorsOnWorkers ...FIXME","title":" startExecutorsOnWorkers"},{"location":"spark-standalone/Master/#webui","text":"MasterWebUI is the Web UI server for the standalone master. Master starts Web UI to listen to http://[master's hostname]:webUIPort (e.g. http://localhost:8080 ). Successfully started service 'MasterUI' on port 8080. Started MasterWebUI at http://192.168.1.4:8080","title":"WebUI"},{"location":"spark-standalone/Master/#states","text":"Master can be in the following states: STANDBY - the initial state while Master is initializing ALIVE - start scheduling resources among applications RECOVERING COMPLETING_RECOVERY","title":"States"},{"location":"spark-standalone/Master/#leaderelectable","text":"Master is LeaderElectable .","title":" LeaderElectable"},{"location":"spark-standalone/Master/#to-be-reviewed","text":"Application ids follows the pattern app-yyyyMMddHHmmss . Master can be < > and stopped using link:spark-standalone-master-scripts.md[custom management scripts for standalone Master].","title":"To be Reviewed"},{"location":"spark-standalone/Master/#rest-server","text":"The standalone Master starts the REST Server service for alternative application submission that is supposed to work across Spark versions. It is enabled by default (see < >) and used by link:spark-submit.md[spark-submit] for the link:spark-standalone.md#deployment-modes[standalone cluster mode], i.e. --deploy-mode is cluster . RestSubmissionClient is the client. The server includes a JSON representation of SubmitRestProtocolResponse in the HTTP body. The following INFOs show up when the Master Endpoint starts up ( Master#onStart is called) with REST Server enabled: INFO Utils: Successfully started service on port 6066. INFO StandaloneRestServer: Started REST server for submitting applications on port 6066","title":"REST Server"},{"location":"spark-standalone/Master/#recovery-mode","text":"A standalone Master can run with recovery mode enabled and be able to recover state among the available swarm of masters. By default, there is no recovery, i.e. no persistence and no election. NOTE: Only a master can schedule tasks so having one always on is important for cases where you want to launch new tasks. Running tasks are unaffected by the state of the master. Master uses spark.deploy.recoveryMode to set up the recovery mode (see < >). The Recovery Mode enables < > among the masters. TIP: Check out the exercise link:exercises/spark-exercise-standalone-master-ha.md[Spark Standalone - Using ZooKeeper for High-Availability of Master].","title":"Recovery Mode"},{"location":"spark-standalone/Master/#rpc-messages","text":"Master communicates with drivers, executors and configures itself using RPC messages . The following message types are accepted by master (see Master#receive or Master#receiveAndReply methods): ElectedLeader for < > CompleteRecovery RevokedLeadership < > ExecutorStateChanged DriverStateChanged Heartbeat MasterChangeAcknowledged WorkerSchedulerStateResponse UnregisterApplication CheckForWorkerTimeOut RegisterWorker RequestSubmitDriver RequestKillDriver RequestDriverStatus RequestMasterState BoundPortsRequest RequestExecutors KillExecutors","title":"RPC Messages"},{"location":"spark-standalone/Master/#registerapplication-event","text":"A RegisterApplication event is sent by link:spark-standalone.md#AppClient[AppClient] to the standalone Master. The event holds information about the application being deployed ( ApplicationDescription ) and the driver's endpoint reference. ApplicationDescription describes an application by its name, maximum number of cores, executor's memory, command, appUiUrl, and user with optional eventLogDir and eventLogCodec for Event Logs, and the number of cores per executor. CAUTION: FIXME Finish A standalone Master receives RegisterApplication with a ApplicationDescription and the driver's xref:rpc:RpcEndpointRef.md[RpcEndpointRef]. INFO Registering app \" + description.name Application ids in Spark Standalone are in the format of app-[yyyyMMddHHmmss]-[4-digit nextAppNumber] . Master keeps track of the number of already-scheduled applications ( nextAppNumber ). ApplicationDescription (AppClient) \u2192 ApplicationInfo (Master) - application structure enrichment ApplicationSource metrics + applicationMetricsSystem INFO Registered app \" + description.name + \" with ID \" + app.id CAUTION: FIXME persistenceEngine.addApplication(app) schedule() schedules the currently available resources among waiting apps. FIXME When is schedule() method called? It's only executed when the Master is in RecoveryState.ALIVE state. Worker in WorkerState.ALIVE state can accept applications. A driver has a state, i.e. driver.state and when it's in DriverState.RUNNING state the driver has been assigned to a worker for execution.","title":"RegisterApplication event"},{"location":"spark-standalone/Master/#launchdriver-rpc-message","text":"WARNING: It seems a dead message. Disregard it for now. A LaunchDriver message is sent by an active standalone Master to a worker to launch a driver. .Master finds a place for a driver (posts LaunchDriver) image::spark-standalone-master-worker-LaunchDriver.png[align=\"center\"] You should see the following INFO in the logs right before the message is sent out to a worker: INFO Launching driver [driver.id] on worker [worker.id] The message holds information about the id and name of the driver. A driver can be running on a single worker while a worker can have many drivers running. When a worker receives a LaunchDriver message, it prints out the following INFO: INFO Asked to launch driver [driver.id] It then creates a DriverRunner and starts it. It starts a separate JVM process. Workers' free memory and cores are considered when assigning some to waiting drivers (applications). CAUTION: FIXME Go over waitingDrivers ...","title":"LaunchDriver RPC message"},{"location":"spark-standalone/Master/#internals-of-orgapachesparkdeploymastermaster","text":"When Master starts, it first creates the xref:ROOT:SparkConf.md#default-configuration[default SparkConf configuration] whose values it then overrides using < > and < >. A fully-configured master instance requires host , port (default: 7077 ), webUiPort (default: 8080 ) settings defined. TIP: When in troubles, consult link:spark-tips-and-tricks.md[Spark Tips and Tricks] document. It starts < > with necessary endpoints and lives until the RPC environment terminates.","title":"Internals of org.apache.spark.deploy.master.Master"},{"location":"spark-standalone/Master/#worker-management","text":"Master uses master-forward-message-thread to schedule a thread every spark.worker.timeout to check workers' availability and remove timed-out workers. It is that Master sends CheckForWorkerTimeOut message to itself to trigger verification. When a worker hasn't responded for spark.worker.timeout , it is assumed dead and the following WARN message appears in the logs: WARN Removing [worker.id] because we got no heartbeat in [spark.worker.timeout] seconds","title":"Worker Management"},{"location":"spark-standalone/Master/#system-environment-variables","text":"Master uses the following system environment variables (directly or indirectly): SPARK_LOCAL_HOSTNAME - the custom host name SPARK_LOCAL_IP - the custom IP to use when SPARK_LOCAL_HOSTNAME is not set SPARK_MASTER_HOST (not SPARK_MASTER_IP as used in start-master.sh script above!) - the master custom host SPARK_MASTER_PORT (default: 7077 ) - the master custom port SPARK_MASTER_IP (default: hostname command's output) SPARK_MASTER_WEBUI_PORT (default: 8080 ) - the port of the master's WebUI. Overriden by spark.master.ui.port if set in the properties file. SPARK_PUBLIC_DNS (default: hostname) - the custom master hostname for WebUI's http URL and master's address. SPARK_CONF_DIR (default: $SPARK_HOME/conf ) - the directory of the default properties file link:spark-properties.md#spark-defaults-conf[spark-defaults.conf] from which all properties that start with spark. prefix are loaded.","title":"System Environment Variables"},{"location":"spark-standalone/Master/#settings","text":"Master uses the following properties: spark.cores.max (default: 0 ) - total expected number of cores. When set, an application could get executors of different sizes (in terms of cores). spark.dead.worker.persistence (default: 15 ) spark.deploy.retainedApplications (default: 200 ) spark.deploy.retainedDrivers (default: 200 ) spark.deploy.recoveryMode (default: NONE ) - possible modes: ZOOKEEPER , FILESYSTEM , or CUSTOM . Refer to < >. spark.deploy.recoveryMode.factory - the class name of the custom StandaloneRecoveryModeFactory . spark.deploy.recoveryDirectory (default: empty) - the directory to persist recovery state link:spark-standalone.md#spark.deploy.spreadOut[spark.deploy.spreadOut] to perform link:spark-standalone.md#round-robin-scheduling[round-robin scheduling across the nodes]. spark.deploy.defaultCores (default: Int.MaxValue , i.e. unbounded) - the number of maxCores for applications that don't specify it. spark.worker.timeout (default: 60 ) - time (in seconds) when no heartbeat from a worker means it is lost. See < >.","title":"Settings"},{"location":"spark-standalone/RestSubmissionClient/","text":"RestSubmissionClient \u00b6 Creating Instance \u00b6 RestSubmissionClient takes the following to be created: Master URL RestSubmissionClient is created when: SparkSubmit is requested to kill and requestStatus RestSubmissionClientApp is requested to run createSubmission \u00b6 createSubmission ( request : CreateSubmissionRequest ): SubmitRestProtocolResponse createSubmission prints out the following INFO message to the logs (with the master URL ): Submitting a request to launch an application in [master]. createSubmission ...FIXME createSubmission is used when: FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.rest.RestSubmissionClient logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.rest.RestSubmissionClient=ALL Refer to Logging .","title":"RestSubmissionClient"},{"location":"spark-standalone/RestSubmissionClient/#restsubmissionclient","text":"","title":"RestSubmissionClient"},{"location":"spark-standalone/RestSubmissionClient/#creating-instance","text":"RestSubmissionClient takes the following to be created: Master URL RestSubmissionClient is created when: SparkSubmit is requested to kill and requestStatus RestSubmissionClientApp is requested to run","title":"Creating Instance"},{"location":"spark-standalone/RestSubmissionClient/#createsubmission","text":"createSubmission ( request : CreateSubmissionRequest ): SubmitRestProtocolResponse createSubmission prints out the following INFO message to the logs (with the master URL ): Submitting a request to launch an application in [master]. createSubmission ...FIXME createSubmission is used when: FIXME","title":" createSubmission"},{"location":"spark-standalone/RestSubmissionClient/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.rest.RestSubmissionClient logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.rest.RestSubmissionClient=ALL Refer to Logging .","title":"Logging"},{"location":"spark-standalone/RestSubmissionServer/","text":"RestSubmissionServer \u00b6 RestSubmissionServer is an abstraction of Application Submission Gateways that can handle submit , kill and status requests using REST API (JSON over HTTP). URLs and RestServlets \u00b6 URL Method RestServlet /v1/submissions/create/* POST SubmitRequestServlet /v1/submissions/kill/* POST KillRequestServlet /v1/submissions/status/* GET StatusRequestServlet /* (all) ErrorServlet The above URLs and RestServlet s are registered when RestSubmissionServer is requested to start . Contract \u00b6 killRequestServlet \u00b6 killRequestServlet : KillRequestServlet Used when: RestSubmissionServer is requested for the contextToServlet statusRequestServlet \u00b6 statusRequestServlet : StatusRequestServlet Used when: RestSubmissionServer is requested for the contextToServlet submitRequestServlet \u00b6 submitRequestServlet : SubmitRequestServlet Used when: RestSubmissionServer is requested for the contextToServlet Implementations \u00b6 MesosRestServer StandaloneRestServer Creating Instance \u00b6 RestSubmissionServer takes the following to be created: Host name Requested Port SparkConf Abstract Class RestSubmissionServer is an abstract class and cannot be created directly. It is created indirectly for the concrete RestSubmissionServers . Starting \u00b6 start (): Int start starts a REST service on the requested port (or any free higher). start prints out the following INFO to the logs: Started REST server for submitting applications on port [port] In the end, start returns the port of the server. start is used when: Master (Spark Standalone) is requested to onStart doStart \u00b6 doStart ( startPort : Int ): ( Server , Int ) doStart ...FIXME Logging \u00b6 RestSubmissionServer is an abstract class and logging is configured using the logger of the implementations .","title":"RestSubmissionServer"},{"location":"spark-standalone/RestSubmissionServer/#restsubmissionserver","text":"RestSubmissionServer is an abstraction of Application Submission Gateways that can handle submit , kill and status requests using REST API (JSON over HTTP).","title":"RestSubmissionServer"},{"location":"spark-standalone/RestSubmissionServer/#urls-and-restservlets","text":"URL Method RestServlet /v1/submissions/create/* POST SubmitRequestServlet /v1/submissions/kill/* POST KillRequestServlet /v1/submissions/status/* GET StatusRequestServlet /* (all) ErrorServlet The above URLs and RestServlet s are registered when RestSubmissionServer is requested to start .","title":" URLs and RestServlets"},{"location":"spark-standalone/RestSubmissionServer/#contract","text":"","title":"Contract"},{"location":"spark-standalone/RestSubmissionServer/#killrequestservlet","text":"killRequestServlet : KillRequestServlet Used when: RestSubmissionServer is requested for the contextToServlet","title":" killRequestServlet"},{"location":"spark-standalone/RestSubmissionServer/#statusrequestservlet","text":"statusRequestServlet : StatusRequestServlet Used when: RestSubmissionServer is requested for the contextToServlet","title":" statusRequestServlet"},{"location":"spark-standalone/RestSubmissionServer/#submitrequestservlet","text":"submitRequestServlet : SubmitRequestServlet Used when: RestSubmissionServer is requested for the contextToServlet","title":" submitRequestServlet"},{"location":"spark-standalone/RestSubmissionServer/#implementations","text":"MesosRestServer StandaloneRestServer","title":"Implementations"},{"location":"spark-standalone/RestSubmissionServer/#creating-instance","text":"RestSubmissionServer takes the following to be created: Host name Requested Port SparkConf Abstract Class RestSubmissionServer is an abstract class and cannot be created directly. It is created indirectly for the concrete RestSubmissionServers .","title":"Creating Instance"},{"location":"spark-standalone/RestSubmissionServer/#starting","text":"start (): Int start starts a REST service on the requested port (or any free higher). start prints out the following INFO to the logs: Started REST server for submitting applications on port [port] In the end, start returns the port of the server. start is used when: Master (Spark Standalone) is requested to onStart","title":" Starting"},{"location":"spark-standalone/RestSubmissionServer/#dostart","text":"doStart ( startPort : Int ): ( Server , Int ) doStart ...FIXME","title":" doStart"},{"location":"spark-standalone/RestSubmissionServer/#logging","text":"RestSubmissionServer is an abstract class and logging is configured using the logger of the implementations .","title":"Logging"},{"location":"spark-standalone/StandaloneAppClient/","text":"StandaloneAppClient \u00b6 Creating Instance \u00b6 StandaloneAppClient takes the following to be created: RpcEnv Master URLs ApplicationDescription StandaloneAppClientListener SparkConf StandaloneAppClient is created when: StandaloneSchedulerBackend is requested to start Starting \u00b6 start (): Unit start registers a ClientEndpoint under the name AppClient (and saves it as the RpcEndpointRef ). start is used when: StandaloneSchedulerBackend is requested to start Stopping \u00b6 stop (): Unit stop ...FIXME stop is used when: StandaloneSchedulerBackend is requested to stop","title":"StandaloneAppClient"},{"location":"spark-standalone/StandaloneAppClient/#standaloneappclient","text":"","title":"StandaloneAppClient"},{"location":"spark-standalone/StandaloneAppClient/#creating-instance","text":"StandaloneAppClient takes the following to be created: RpcEnv Master URLs ApplicationDescription StandaloneAppClientListener SparkConf StandaloneAppClient is created when: StandaloneSchedulerBackend is requested to start","title":"Creating Instance"},{"location":"spark-standalone/StandaloneAppClient/#starting","text":"start (): Unit start registers a ClientEndpoint under the name AppClient (and saves it as the RpcEndpointRef ). start is used when: StandaloneSchedulerBackend is requested to start","title":" Starting"},{"location":"spark-standalone/StandaloneAppClient/#stopping","text":"stop (): Unit stop ...FIXME stop is used when: StandaloneSchedulerBackend is requested to stop","title":" Stopping"},{"location":"spark-standalone/StandaloneAppClientListener/","text":"StandaloneAppClientListener \u00b6 StandaloneAppClientListener is an abstraction of listeners . Contract \u00b6 connected \u00b6 connected ( appId : String ): Unit Used when: ClientEndpoint is requested to handle a RegisteredApplication message dead \u00b6 dead ( reason : String ): Unit Used when: ClientEndpoint is requested to markDead disconnected \u00b6 disconnected (): Unit Used when: ClientEndpoint is requested to markDisconnected executorAdded \u00b6 executorAdded ( fullId : String , workerId : String , hostPort : String , cores : Int , memory : Int ): Unit Used when: ClientEndpoint is requested to handle a ExecutorAdded message executorDecommissioned \u00b6 executorDecommissioned ( fullId : String , decommissionInfo : ExecutorDecommissionInfo ): Unit Used when: ClientEndpoint is requested to handle a ExecutorUpdated message executorRemoved \u00b6 executorRemoved ( fullId : String , message : String , exitStatus : Option [ Int ], workerHost : Option [ String ]): Unit Used when: ClientEndpoint is requested to handle a ExecutorUpdated message workerRemoved \u00b6 workerRemoved ( workerId : String , host : String , message : String ): Unit Used when: ClientEndpoint is requested to handle a WorkerRemoved message Implementations \u00b6 StandaloneSchedulerBackend","title":"StandaloneAppClientListener"},{"location":"spark-standalone/StandaloneAppClientListener/#standaloneappclientlistener","text":"StandaloneAppClientListener is an abstraction of listeners .","title":"StandaloneAppClientListener"},{"location":"spark-standalone/StandaloneAppClientListener/#contract","text":"","title":"Contract"},{"location":"spark-standalone/StandaloneAppClientListener/#connected","text":"connected ( appId : String ): Unit Used when: ClientEndpoint is requested to handle a RegisteredApplication message","title":" connected"},{"location":"spark-standalone/StandaloneAppClientListener/#dead","text":"dead ( reason : String ): Unit Used when: ClientEndpoint is requested to markDead","title":" dead"},{"location":"spark-standalone/StandaloneAppClientListener/#disconnected","text":"disconnected (): Unit Used when: ClientEndpoint is requested to markDisconnected","title":" disconnected"},{"location":"spark-standalone/StandaloneAppClientListener/#executoradded","text":"executorAdded ( fullId : String , workerId : String , hostPort : String , cores : Int , memory : Int ): Unit Used when: ClientEndpoint is requested to handle a ExecutorAdded message","title":" executorAdded"},{"location":"spark-standalone/StandaloneAppClientListener/#executordecommissioned","text":"executorDecommissioned ( fullId : String , decommissionInfo : ExecutorDecommissionInfo ): Unit Used when: ClientEndpoint is requested to handle a ExecutorUpdated message","title":" executorDecommissioned"},{"location":"spark-standalone/StandaloneAppClientListener/#executorremoved","text":"executorRemoved ( fullId : String , message : String , exitStatus : Option [ Int ], workerHost : Option [ String ]): Unit Used when: ClientEndpoint is requested to handle a ExecutorUpdated message","title":" executorRemoved"},{"location":"spark-standalone/StandaloneAppClientListener/#workerremoved","text":"workerRemoved ( workerId : String , host : String , message : String ): Unit Used when: ClientEndpoint is requested to handle a WorkerRemoved message","title":" workerRemoved"},{"location":"spark-standalone/StandaloneAppClientListener/#implementations","text":"StandaloneSchedulerBackend","title":"Implementations"},{"location":"spark-standalone/StandaloneRestServer/","text":"StandaloneRestServer \u00b6 StandaloneRestServer is a RestSubmissionServer . Creating Instance \u00b6 StandaloneRestServer takes the following to be created: Host Name Requested Port SparkConf RpcEndpointRef of the Master URL of the Master StandaloneRestServer is created when: Master is requested to onStart (with spark.master.rest.enabled configuration property enabled) StandaloneSubmitRequestServlet \u00b6 StandaloneRestServer uses a StandaloneSubmitRequestServlet as the submitRequestServlet . StandaloneSubmitRequestServlet requires the following parameters: appResource (the jar of a Spark application) mainClass","title":"StandaloneRestServer"},{"location":"spark-standalone/StandaloneRestServer/#standalonerestserver","text":"StandaloneRestServer is a RestSubmissionServer .","title":"StandaloneRestServer"},{"location":"spark-standalone/StandaloneRestServer/#creating-instance","text":"StandaloneRestServer takes the following to be created: Host Name Requested Port SparkConf RpcEndpointRef of the Master URL of the Master StandaloneRestServer is created when: Master is requested to onStart (with spark.master.rest.enabled configuration property enabled)","title":"Creating Instance"},{"location":"spark-standalone/StandaloneRestServer/#standalonesubmitrequestservlet","text":"StandaloneRestServer uses a StandaloneSubmitRequestServlet as the submitRequestServlet . StandaloneSubmitRequestServlet requires the following parameters: appResource (the jar of a Spark application) mainClass","title":" StandaloneSubmitRequestServlet"},{"location":"spark-standalone/StandaloneSchedulerBackend/","text":"StandaloneSchedulerBackend \u00b6 StandaloneSchedulerBackend is a CoarseGrainedSchedulerBackend . Creating Instance \u00b6 StandaloneSchedulerBackend takes the following to be created: TaskSchedulerImpl SparkContext Standalone master URLs StandaloneSchedulerBackend is created when: SparkContext is requested for a SchedulerBackend and TaskScheduler (for spark:// and local-cluster master URLs) StandaloneAppClientListener \u00b6 StandaloneSchedulerBackend is a StandaloneAppClientListener . Starting SchedulerBackend \u00b6 start (): Unit start is part of the SchedulerBackend abstraction. start ...FIXME start creates a StandaloneAppClient and requests it to start . start ...FIXME executorDecommissioned \u00b6 executorDecommissioned ( fullId : String , decommissionInfo : ExecutorDecommissionInfo ): Unit executorDecommissioned is part of the StandaloneAppClientListener abstraction. executorDecommissioned ...FIXME StandaloneAppClient \u00b6 StandaloneSchedulerBackend creates a StandaloneAppClient (with itself as a StandaloneAppClientListener ) when requested to start . StandaloneAppClient is started with StandaloneSchedulerBackend . StandaloneAppClient is stopped with StandaloneSchedulerBackend . StandaloneAppClient is used for the following: doRequestTotalExecutors doKillExecutors","title":"StandaloneSchedulerBackend"},{"location":"spark-standalone/StandaloneSchedulerBackend/#standaloneschedulerbackend","text":"StandaloneSchedulerBackend is a CoarseGrainedSchedulerBackend .","title":"StandaloneSchedulerBackend"},{"location":"spark-standalone/StandaloneSchedulerBackend/#creating-instance","text":"StandaloneSchedulerBackend takes the following to be created: TaskSchedulerImpl SparkContext Standalone master URLs StandaloneSchedulerBackend is created when: SparkContext is requested for a SchedulerBackend and TaskScheduler (for spark:// and local-cluster master URLs)","title":"Creating Instance"},{"location":"spark-standalone/StandaloneSchedulerBackend/#standaloneappclientlistener","text":"StandaloneSchedulerBackend is a StandaloneAppClientListener .","title":" StandaloneAppClientListener"},{"location":"spark-standalone/StandaloneSchedulerBackend/#starting-schedulerbackend","text":"start (): Unit start is part of the SchedulerBackend abstraction. start ...FIXME start creates a StandaloneAppClient and requests it to start . start ...FIXME","title":" Starting SchedulerBackend"},{"location":"spark-standalone/StandaloneSchedulerBackend/#executordecommissioned","text":"executorDecommissioned ( fullId : String , decommissionInfo : ExecutorDecommissionInfo ): Unit executorDecommissioned is part of the StandaloneAppClientListener abstraction. executorDecommissioned ...FIXME","title":" executorDecommissioned"},{"location":"spark-standalone/StandaloneSchedulerBackend/#standaloneappclient","text":"StandaloneSchedulerBackend creates a StandaloneAppClient (with itself as a StandaloneAppClientListener ) when requested to start . StandaloneAppClient is started with StandaloneSchedulerBackend . StandaloneAppClient is stopped with StandaloneSchedulerBackend . StandaloneAppClient is used for the following: doRequestTotalExecutors doKillExecutors","title":" StandaloneAppClient"},{"location":"spark-standalone/Worker/","text":"Worker \u00b6 Worker is a logical worker node in a Spark Standalone cluster. Worker can be launched from command line . Worker RPC Endpoint \u00b6 Worker is a ThreadSafeRpcEndpoint and is registered under Worker name (when launched as a command-line application and requested to set up an RPC environment ). Launching Standalone Worker \u00b6 Worker can be launched as a standalone application using spark-class . ./bin/spark-class org.apache.spark.deploy.worker.Worker Note At least one master URL is required. main Entry Point \u00b6 main ( args : Array [ String ]): Unit main is the entry point of Worker standalone application. main prints out the following INFO message to the logs: Started daemon with process name: [processName] main registers signal handlers for TERM , HUP , INT signals. main parses command-line options (using WorkerArguments ) and initializes an RpcEnv . main asserts that: External shuffle service is not used (based on spark.shuffle.service.enabled configuration property) Number of worker instances is 1 (based on SPARK_WORKER_INSTANCES environment variable) main throws an IllegalArgumentException when the above does not hold: Starting multiple workers on one host is failed because we may launch no more than one external shuffle service on each host, please set spark.shuffle.service.enabled to false or set SPARK_WORKER_INSTANCES to 1 to resolve the conflict. In the end, main requests the RpcEnv to be notified when terminated . Command-Line Options \u00b6 Worker supports command-line options. Usage: Worker [options] <master> Master must be a URL of the form spark://hostname:port Options: -c CORES, --cores CORES Number of cores to use -m MEM, --memory MEM Amount of memory to use (e.g. 1000M, 2G) -d DIR, --work-dir DIR Directory to run apps in (default: SPARK_HOME/work) -i HOST, --ip IP Hostname to listen on (deprecated, please use --host or -h) -h HOST, --host HOST Hostname to listen on -p PORT, --port PORT Port to listen on (default: random) --webui-port PORT Port for web UI (default: 8081) --properties-file FILE Path to a custom Spark properties file. Default is conf/spark-defaults.conf. cores \u00b6 host \u00b6 ip \u00b6 Master URLs \u00b6 (required) Comma-separated standalone Master's URLs in the form: spark://host1:port1,host2:port2,... memory \u00b6 port \u00b6 properties-file \u00b6 webui-port \u00b6 work-dir \u00b6 Creating Instance \u00b6 Worker takes the following to be created: RpcEnv web UI's Port Number of CPU cores Memory RpcAddress es of Master s Endpoint Name Work Dir Path (default: null ) SparkConf SecurityManager Optional Resource File (default: (undefined)) Supplier of ExternalShuffleService (default: null ) Worker is created when: Worker utility is requested to startRpcEnvAndEndpoint ExternalShuffleService \u00b6 Worker initializes an ExternalShuffleService (directly or indirectly using a Supplier if given). ExternalShuffleService is started when Worker is requested to startExternalShuffleService . ExternalShuffleService is used as follows: Informed about an application removed when Worker handles a WorkDirCleanup message or maybeCleanupApplication Informed about an executor removed when Worker is requested to handleExecutorStateChanged ExternalShuffleService is stopped when Worker is requested to stop . Starting Up RPC Environment \u00b6 startRpcEnvAndEndpoint ( host : String , port : Int , webUiPort : Int , cores : Int , memory : Int , masterUrls : Array [ String ], workDir : String , workerNumber : Option [ Int ] = None , conf : SparkConf = new SparkConf , resourceFileOpt : Option [ String ] = None ): RpcEnv startRpcEnvAndEndpoint creates an RpcEnv with the name sparkWorker and the given host and port . startRpcEnvAndEndpoint translates the given masterUrls to RpcAddresses . startRpcEnvAndEndpoint creates a Worker and requests the RpcEnv to set it up as an RPC endpoint under the Worker name. startRpcEnvAndEndpoint is used when: LocalSparkCluster is requested to start Worker standalone application is launched onStart \u00b6 onStart (): Unit onStart is part of the RpcEndpoint abstraction. onStart ...FIXME Creating Work Directory \u00b6 createWorkDir (): Unit createWorkDir sets < > to be either < > if defined or < > with work subdirectory. In the end, createWorkDir creates < > directory (including any necessary but nonexistent parent directories). createWorkDir reports...FIXME Messages \u00b6 ApplicationFinished \u00b6 DriverStateChanged \u00b6 ExecutorStateChanged \u00b6 ExecutorStateChanged ( appId : String , execId : Int , state : ExecutorState , message : Option [ String ], exitStatus : Option [ Int ]) Message Handler: handleExecutorStateChanged Posted when: ExecutorRunner is requested to killProcess and fetchAndRunExecutor KillDriver \u00b6 KillExecutor \u00b6 LaunchDriver \u00b6 LaunchExecutor \u00b6 MasterChanged \u00b6 ReconnectWorker \u00b6 RegisterWorkerResponse \u00b6 ReregisterWithMaster \u00b6 RequestWorkerState \u00b6 SendHeartbeat \u00b6 WorkDirCleanup \u00b6 handleExecutorStateChanged \u00b6 handleExecutorStateChanged ( executorStateChanged : ExecutorStateChanged ): Unit handleExecutorStateChanged ...FIXME handleExecutorStateChanged is used when: Worker is requested to handle ExecutorStateChanged message maybeCleanupApplication \u00b6 maybeCleanupApplication ( id : String ): Unit maybeCleanupApplication ...FIXME maybeCleanupApplication is used when: Worker is requested to handle a ApplicationFinished message and handleExecutorStateChanged Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.worker.Worker logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.worker.Worker=ALL Refer to Logging .","title":"Worker"},{"location":"spark-standalone/Worker/#worker","text":"Worker is a logical worker node in a Spark Standalone cluster. Worker can be launched from command line .","title":"Worker"},{"location":"spark-standalone/Worker/#worker-rpc-endpoint","text":"Worker is a ThreadSafeRpcEndpoint and is registered under Worker name (when launched as a command-line application and requested to set up an RPC environment ).","title":" Worker RPC Endpoint"},{"location":"spark-standalone/Worker/#launching-standalone-worker","text":"Worker can be launched as a standalone application using spark-class . ./bin/spark-class org.apache.spark.deploy.worker.Worker Note At least one master URL is required.","title":" Launching Standalone Worker"},{"location":"spark-standalone/Worker/#main-entry-point","text":"main ( args : Array [ String ]): Unit main is the entry point of Worker standalone application. main prints out the following INFO message to the logs: Started daemon with process name: [processName] main registers signal handlers for TERM , HUP , INT signals. main parses command-line options (using WorkerArguments ) and initializes an RpcEnv . main asserts that: External shuffle service is not used (based on spark.shuffle.service.enabled configuration property) Number of worker instances is 1 (based on SPARK_WORKER_INSTANCES environment variable) main throws an IllegalArgumentException when the above does not hold: Starting multiple workers on one host is failed because we may launch no more than one external shuffle service on each host, please set spark.shuffle.service.enabled to false or set SPARK_WORKER_INSTANCES to 1 to resolve the conflict. In the end, main requests the RpcEnv to be notified when terminated .","title":" main Entry Point"},{"location":"spark-standalone/Worker/#command-line-options","text":"Worker supports command-line options. Usage: Worker [options] <master> Master must be a URL of the form spark://hostname:port Options: -c CORES, --cores CORES Number of cores to use -m MEM, --memory MEM Amount of memory to use (e.g. 1000M, 2G) -d DIR, --work-dir DIR Directory to run apps in (default: SPARK_HOME/work) -i HOST, --ip IP Hostname to listen on (deprecated, please use --host or -h) -h HOST, --host HOST Hostname to listen on -p PORT, --port PORT Port to listen on (default: random) --webui-port PORT Port for web UI (default: 8081) --properties-file FILE Path to a custom Spark properties file. Default is conf/spark-defaults.conf.","title":" Command-Line Options"},{"location":"spark-standalone/Worker/#cores","text":"","title":"cores"},{"location":"spark-standalone/Worker/#host","text":"","title":"host"},{"location":"spark-standalone/Worker/#ip","text":"","title":"ip"},{"location":"spark-standalone/Worker/#master-urls","text":"(required) Comma-separated standalone Master's URLs in the form: spark://host1:port1,host2:port2,...","title":" Master URLs"},{"location":"spark-standalone/Worker/#memory","text":"","title":"memory"},{"location":"spark-standalone/Worker/#port","text":"","title":"port"},{"location":"spark-standalone/Worker/#properties-file","text":"","title":"properties-file"},{"location":"spark-standalone/Worker/#webui-port","text":"","title":"webui-port"},{"location":"spark-standalone/Worker/#work-dir","text":"","title":"work-dir"},{"location":"spark-standalone/Worker/#creating-instance","text":"Worker takes the following to be created: RpcEnv web UI's Port Number of CPU cores Memory RpcAddress es of Master s Endpoint Name Work Dir Path (default: null ) SparkConf SecurityManager Optional Resource File (default: (undefined)) Supplier of ExternalShuffleService (default: null ) Worker is created when: Worker utility is requested to startRpcEnvAndEndpoint","title":"Creating Instance"},{"location":"spark-standalone/Worker/#externalshuffleservice","text":"Worker initializes an ExternalShuffleService (directly or indirectly using a Supplier if given). ExternalShuffleService is started when Worker is requested to startExternalShuffleService . ExternalShuffleService is used as follows: Informed about an application removed when Worker handles a WorkDirCleanup message or maybeCleanupApplication Informed about an executor removed when Worker is requested to handleExecutorStateChanged ExternalShuffleService is stopped when Worker is requested to stop .","title":" ExternalShuffleService"},{"location":"spark-standalone/Worker/#starting-up-rpc-environment","text":"startRpcEnvAndEndpoint ( host : String , port : Int , webUiPort : Int , cores : Int , memory : Int , masterUrls : Array [ String ], workDir : String , workerNumber : Option [ Int ] = None , conf : SparkConf = new SparkConf , resourceFileOpt : Option [ String ] = None ): RpcEnv startRpcEnvAndEndpoint creates an RpcEnv with the name sparkWorker and the given host and port . startRpcEnvAndEndpoint translates the given masterUrls to RpcAddresses . startRpcEnvAndEndpoint creates a Worker and requests the RpcEnv to set it up as an RPC endpoint under the Worker name. startRpcEnvAndEndpoint is used when: LocalSparkCluster is requested to start Worker standalone application is launched","title":" Starting Up RPC Environment"},{"location":"spark-standalone/Worker/#onstart","text":"onStart (): Unit onStart is part of the RpcEndpoint abstraction. onStart ...FIXME","title":" onStart"},{"location":"spark-standalone/Worker/#creating-work-directory","text":"createWorkDir (): Unit createWorkDir sets < > to be either < > if defined or < > with work subdirectory. In the end, createWorkDir creates < > directory (including any necessary but nonexistent parent directories). createWorkDir reports...FIXME","title":" Creating Work Directory"},{"location":"spark-standalone/Worker/#messages","text":"","title":"Messages"},{"location":"spark-standalone/Worker/#applicationfinished","text":"","title":" ApplicationFinished"},{"location":"spark-standalone/Worker/#driverstatechanged","text":"","title":" DriverStateChanged"},{"location":"spark-standalone/Worker/#executorstatechanged","text":"ExecutorStateChanged ( appId : String , execId : Int , state : ExecutorState , message : Option [ String ], exitStatus : Option [ Int ]) Message Handler: handleExecutorStateChanged Posted when: ExecutorRunner is requested to killProcess and fetchAndRunExecutor","title":" ExecutorStateChanged"},{"location":"spark-standalone/Worker/#killdriver","text":"","title":" KillDriver"},{"location":"spark-standalone/Worker/#killexecutor","text":"","title":" KillExecutor"},{"location":"spark-standalone/Worker/#launchdriver","text":"","title":" LaunchDriver"},{"location":"spark-standalone/Worker/#launchexecutor","text":"","title":" LaunchExecutor"},{"location":"spark-standalone/Worker/#masterchanged","text":"","title":" MasterChanged"},{"location":"spark-standalone/Worker/#reconnectworker","text":"","title":" ReconnectWorker"},{"location":"spark-standalone/Worker/#registerworkerresponse","text":"","title":" RegisterWorkerResponse"},{"location":"spark-standalone/Worker/#reregisterwithmaster","text":"","title":" ReregisterWithMaster"},{"location":"spark-standalone/Worker/#requestworkerstate","text":"","title":" RequestWorkerState"},{"location":"spark-standalone/Worker/#sendheartbeat","text":"","title":" SendHeartbeat"},{"location":"spark-standalone/Worker/#workdircleanup","text":"","title":" WorkDirCleanup"},{"location":"spark-standalone/Worker/#handleexecutorstatechanged","text":"handleExecutorStateChanged ( executorStateChanged : ExecutorStateChanged ): Unit handleExecutorStateChanged ...FIXME handleExecutorStateChanged is used when: Worker is requested to handle ExecutorStateChanged message","title":" handleExecutorStateChanged"},{"location":"spark-standalone/Worker/#maybecleanupapplication","text":"maybeCleanupApplication ( id : String ): Unit maybeCleanupApplication ...FIXME maybeCleanupApplication is used when: Worker is requested to handle a ApplicationFinished message and handleExecutorStateChanged","title":" maybeCleanupApplication"},{"location":"spark-standalone/Worker/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.worker.Worker logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.worker.Worker=ALL Refer to Logging .","title":"Logging"},{"location":"spark-standalone/configuration-properties/","text":"Configuration Properties of Spark Standalone \u00b6 spark.deploy.spreadOut \u00b6 Controls whether standalone Master should perform round-robin scheduling across worker nodes (spreading out each app among all the nodes) instead of trying to consolidate each app onto a small number of nodes Default: true spark.master.rest.enabled \u00b6 Default: false Used when: Master standalone application is requested to onStart spark.master.rest.port \u00b6 Default: 6066 Used when: Master standalone application is requested to onStart StandaloneSubmitRequestServlet is requested to buildDriverDescription spark.worker.resourcesFile \u00b6 (internal) Path to a file containing the resources allocated to the worker. The file should be formatted as a JSON array of ResourceAllocation objects. Only used internally in standalone mode. Default: (undefined) Used when: LocalSparkCluster is requested to start Worker standalone application is launched","title":"Configuration Properties"},{"location":"spark-standalone/configuration-properties/#configuration-properties-of-spark-standalone","text":"","title":"Configuration Properties of Spark Standalone"},{"location":"spark-standalone/configuration-properties/#sparkdeployspreadout","text":"Controls whether standalone Master should perform round-robin scheduling across worker nodes (spreading out each app among all the nodes) instead of trying to consolidate each app onto a small number of nodes Default: true","title":" spark.deploy.spreadOut"},{"location":"spark-standalone/configuration-properties/#sparkmasterrestenabled","text":"Default: false Used when: Master standalone application is requested to onStart","title":" spark.master.rest.enabled"},{"location":"spark-standalone/configuration-properties/#sparkmasterrestport","text":"Default: 6066 Used when: Master standalone application is requested to onStart StandaloneSubmitRequestServlet is requested to buildDriverDescription","title":" spark.master.rest.port"},{"location":"spark-standalone/configuration-properties/#sparkworkerresourcesfile","text":"(internal) Path to a file containing the resources allocated to the worker. The file should be formatted as a JSON array of ResourceAllocation objects. Only used internally in standalone mode. Default: (undefined) Used when: LocalSparkCluster is requested to start Worker standalone application is launched","title":" spark.worker.resourcesFile"},{"location":"stage-level-scheduling/","text":"Stage-Level Scheduling \u00b6 Stage-Level Scheduling is a work in progress under SPARK-27495: SPIP: Support Stage level resource configuration and scheduling . Main abstractions: ResourceProfile","title":"Stage-Level Scheduling"},{"location":"stage-level-scheduling/#stage-level-scheduling","text":"Stage-Level Scheduling is a work in progress under SPARK-27495: SPIP: Support Stage level resource configuration and scheduling . Main abstractions: ResourceProfile","title":"Stage-Level Scheduling"},{"location":"stage-level-scheduling/ResourceProfile/","text":"ResourceProfile \u00b6 ResourceProfile is a resource profile (with executor and task requirements) for Stage Level Scheduling . ResourceProfile is a Java Serializable . Creating Instance \u00b6 ResourceProfile takes the following to be created: Executor Resources ( Map[String, ExecutorResourceRequest] ) Task Resources ( Map[String, TaskResourceRequest] ) ResourceProfile is created (directly or using getOrCreateDefaultProfile ) when: DriverEndpoint is requested to handle a RetrieveSparkAppConfig message ResourceProfileBuilder utility is requested to build getOrCreateDefaultProfile Utility \u00b6 getOrCreateDefaultProfile ( conf : SparkConf ): ResourceProfile getOrCreateDefaultProfile ...FIXME getOrCreateDefaultProfile is used when: DriverEndpoint is requested to handle a RetrieveSparkAppConfig message","title":"ResourceProfile"},{"location":"stage-level-scheduling/ResourceProfile/#resourceprofile","text":"ResourceProfile is a resource profile (with executor and task requirements) for Stage Level Scheduling . ResourceProfile is a Java Serializable .","title":"ResourceProfile"},{"location":"stage-level-scheduling/ResourceProfile/#creating-instance","text":"ResourceProfile takes the following to be created: Executor Resources ( Map[String, ExecutorResourceRequest] ) Task Resources ( Map[String, TaskResourceRequest] ) ResourceProfile is created (directly or using getOrCreateDefaultProfile ) when: DriverEndpoint is requested to handle a RetrieveSparkAppConfig message ResourceProfileBuilder utility is requested to build","title":"Creating Instance"},{"location":"stage-level-scheduling/ResourceProfile/#getorcreatedefaultprofile-utility","text":"getOrCreateDefaultProfile ( conf : SparkConf ): ResourceProfile getOrCreateDefaultProfile ...FIXME getOrCreateDefaultProfile is used when: DriverEndpoint is requested to handle a RetrieveSparkAppConfig message","title":" getOrCreateDefaultProfile Utility"},{"location":"stage-level-scheduling/ResourceProfileBuilder/","text":"ResourceProfileBuilder \u00b6 Building ResourceProfile \u00b6 build : ResourceProfile build creates a ResourceProfile with the executorResources and taskResources . Executor Resources \u00b6 executorResources : Map [ String , ExecutorResourceRequest ] executorResources ...FIXME Task Resources \u00b6 taskResources : Map [ String , TaskResourceRequest ] taskResources ...FIXME","title":"ResourceProfileBuilder"},{"location":"stage-level-scheduling/ResourceProfileBuilder/#resourceprofilebuilder","text":"","title":"ResourceProfileBuilder"},{"location":"stage-level-scheduling/ResourceProfileBuilder/#building-resourceprofile","text":"build : ResourceProfile build creates a ResourceProfile with the executorResources and taskResources .","title":" Building ResourceProfile"},{"location":"stage-level-scheduling/ResourceProfileBuilder/#executor-resources","text":"executorResources : Map [ String , ExecutorResourceRequest ] executorResources ...FIXME","title":" Executor Resources"},{"location":"stage-level-scheduling/ResourceProfileBuilder/#task-resources","text":"taskResources : Map [ String , TaskResourceRequest ] taskResources ...FIXME","title":" Task Resources"},{"location":"status/","text":"Status \u00b6 Status system uses AppStatusListener to write the state of a Spark application to AppStatusStore for reporting and monitoring: web UI REST API Spark History Server Metrics","title":"Status"},{"location":"status/#status","text":"Status system uses AppStatusListener to write the state of a Spark application to AppStatusStore for reporting and monitoring: web UI REST API Spark History Server Metrics","title":"Status"},{"location":"status/AppStatusListener/","text":"AppStatusListener \u00b6 AppStatusListener is a SparkListener that writes application state information to a data store . Event Handlers \u00b6 Event Handler LiveEntities onJobStart LiveJob LiveStage RDDOperationGraph onStageSubmitted Creating Instance \u00b6 AppStatusListener takes the following to be created: ElementTrackingStore SparkConf live flag AppStatusSource (default: None ) Last Update Time (default: None ) AppStatusListener is created when: AppStatusStore is requested for a in-memory store for a running Spark application (with the live flag enabled) FsHistoryProvider is requested to rebuildAppStore (with the live flag disabled) ElementTrackingStore \u00b6 AppStatusListener is given an ElementTrackingStore when created . AppStatusListener registers triggers to clean up state in the store: cleanupExecutors cleanupJobs cleanupStages ElementTrackingStore is used to write and...FIXME live Flag \u00b6 AppStatusListener is given a live flag when created . live flag indicates whether AppStatusListener is created for the following: true when created for a active ( live ) Spark application (for AppStatusStore ) false when created for Spark History Server (for FsHistoryProvider ) Updating ElementTrackingStore for Active Spark Application \u00b6 liveUpdate ( entity : LiveEntity , now : Long ): Unit liveUpdate update the ElementTrackingStore when the live flag is enabled. Updating ElementTrackingStore \u00b6 update ( entity : LiveEntity , now : Long , last : Boolean = false ): Unit update requests the given LiveEntity to write (with the ElementTrackingStore and checkTriggers flag being the given last flag). getOrCreateExecutor \u00b6 getOrCreateExecutor ( executorId : String , addTime : Long ): LiveExecutor getOrCreateExecutor ...FIXME getOrCreateExecutor is used when: AppStatusListener is requested to onExecutorAdded and onBlockManagerAdded getOrCreateStage \u00b6 getOrCreateStage ( info : StageInfo ): LiveStage getOrCreateStage ...FIXME getOrCreateStage is used when: AppStatusListener is requested to onJobStart and onStageSubmitted","title":"AppStatusListener"},{"location":"status/AppStatusListener/#appstatuslistener","text":"AppStatusListener is a SparkListener that writes application state information to a data store .","title":"AppStatusListener"},{"location":"status/AppStatusListener/#event-handlers","text":"Event Handler LiveEntities onJobStart LiveJob LiveStage RDDOperationGraph onStageSubmitted","title":"Event Handlers"},{"location":"status/AppStatusListener/#creating-instance","text":"AppStatusListener takes the following to be created: ElementTrackingStore SparkConf live flag AppStatusSource (default: None ) Last Update Time (default: None ) AppStatusListener is created when: AppStatusStore is requested for a in-memory store for a running Spark application (with the live flag enabled) FsHistoryProvider is requested to rebuildAppStore (with the live flag disabled)","title":"Creating Instance"},{"location":"status/AppStatusListener/#elementtrackingstore","text":"AppStatusListener is given an ElementTrackingStore when created . AppStatusListener registers triggers to clean up state in the store: cleanupExecutors cleanupJobs cleanupStages ElementTrackingStore is used to write and...FIXME","title":" ElementTrackingStore"},{"location":"status/AppStatusListener/#live-flag","text":"AppStatusListener is given a live flag when created . live flag indicates whether AppStatusListener is created for the following: true when created for a active ( live ) Spark application (for AppStatusStore ) false when created for Spark History Server (for FsHistoryProvider )","title":" live Flag"},{"location":"status/AppStatusListener/#updating-elementtrackingstore-for-active-spark-application","text":"liveUpdate ( entity : LiveEntity , now : Long ): Unit liveUpdate update the ElementTrackingStore when the live flag is enabled.","title":" Updating ElementTrackingStore for Active Spark Application"},{"location":"status/AppStatusListener/#updating-elementtrackingstore","text":"update ( entity : LiveEntity , now : Long , last : Boolean = false ): Unit update requests the given LiveEntity to write (with the ElementTrackingStore and checkTriggers flag being the given last flag).","title":" Updating ElementTrackingStore"},{"location":"status/AppStatusListener/#getorcreateexecutor","text":"getOrCreateExecutor ( executorId : String , addTime : Long ): LiveExecutor getOrCreateExecutor ...FIXME getOrCreateExecutor is used when: AppStatusListener is requested to onExecutorAdded and onBlockManagerAdded","title":" getOrCreateExecutor"},{"location":"status/AppStatusListener/#getorcreatestage","text":"getOrCreateStage ( info : StageInfo ): LiveStage getOrCreateStage ...FIXME getOrCreateStage is used when: AppStatusListener is requested to onJobStart and onStageSubmitted","title":" getOrCreateStage"},{"location":"status/AppStatusSource/","text":"AppStatusSource \u00b6 AppStatusSource is...FIXME","title":"AppStatusSource"},{"location":"status/AppStatusSource/#appstatussource","text":"AppStatusSource is...FIXME","title":"AppStatusSource"},{"location":"status/AppStatusStore/","text":"AppStatusStore \u00b6 AppStatusStore stores the state of a Spark application in a data store (listening to state changes using AppStatusListener ). Creating Instance \u00b6 AppStatusStore takes the following to be created: KVStore AppStatusListener AppStatusStore is created using createLiveStore utility. Creating In-Memory Store for Live Spark Application \u00b6 createLiveStore ( conf : SparkConf , appStatusSource : Option [ AppStatusSource ] = None ): AppStatusStore createLiveStore creates an ElementTrackingStore (with InMemoryStore and the SparkConf ). createLiveStore creates an AppStatusListener (with the ElementTrackingStore , live flag on and the AppStatusSource ). In the end, creates an AppStatusStore (with the ElementTrackingStore and AppStatusListener ). createLiveStore is used when: SparkContext is created Accessing AppStatusStore \u00b6 AppStatusStore is available using SparkContext . SparkStatusTracker \u00b6 AppStatusStore is used to create SparkStatusTracker . SparkUI \u00b6 AppStatusStore is used to create SparkUI . RDDs \u00b6 rddList ( cachedOnly : Boolean = true ): Seq [ v1 . RDDStorageInfo ] rddList requests the KVStore for (a view over) RDDStorageInfo s (cached or not based on the given cachedOnly flag). rddList is used when: AbstractApplicationResource is requested for the RDDs StageTableBase is created (and renders a stage table for AllStagesPage , JobPage and PoolPage ) StoragePage is requested to render Streaming Blocks \u00b6 streamBlocksList (): Seq [ StreamBlockData ] streamBlocksList requests the KVStore for (a view over) StreamBlockData s. streamBlocksList is used when: StoragePage is requested to render Stages \u00b6 stageList ( statuses : JList [ v1 . StageStatus ]): Seq [ v1 . StageData ] stageList requests the KVStore for (a view over) StageData s. stageList is used when: SparkStatusTracker is requested for active stage IDs StagesResource is requested for stages AllStagesPage is requested to render Jobs \u00b6 jobsList ( statuses : JList [ JobExecutionStatus ]): Seq [ v1 . JobData ] jobsList requests the KVStore for (a view over) JobData s. jobsList is used when: SparkStatusTracker is requested for getJobIdsForGroup and getActiveJobIds AbstractApplicationResource is requested for jobs AllJobsPage is requested to render Executors \u00b6 executorList ( activeOnly : Boolean ): Seq [ v1 . ExecutorSummary ] executorList requests the KVStore for (a view over) ExecutorSummary s. executorList is used when: FIXME Application Summary \u00b6 appSummary (): AppSummary appSummary requests the KVStore to read the AppSummary . appSummary is used when: AllJobsPage is requested to render AllStagesPage is requested to render","title":"AppStatusStore"},{"location":"status/AppStatusStore/#appstatusstore","text":"AppStatusStore stores the state of a Spark application in a data store (listening to state changes using AppStatusListener ).","title":"AppStatusStore"},{"location":"status/AppStatusStore/#creating-instance","text":"AppStatusStore takes the following to be created: KVStore AppStatusListener AppStatusStore is created using createLiveStore utility.","title":"Creating Instance"},{"location":"status/AppStatusStore/#creating-in-memory-store-for-live-spark-application","text":"createLiveStore ( conf : SparkConf , appStatusSource : Option [ AppStatusSource ] = None ): AppStatusStore createLiveStore creates an ElementTrackingStore (with InMemoryStore and the SparkConf ). createLiveStore creates an AppStatusListener (with the ElementTrackingStore , live flag on and the AppStatusSource ). In the end, creates an AppStatusStore (with the ElementTrackingStore and AppStatusListener ). createLiveStore is used when: SparkContext is created","title":" Creating In-Memory Store for Live Spark Application"},{"location":"status/AppStatusStore/#accessing-appstatusstore","text":"AppStatusStore is available using SparkContext .","title":" Accessing AppStatusStore"},{"location":"status/AppStatusStore/#sparkstatustracker","text":"AppStatusStore is used to create SparkStatusTracker .","title":" SparkStatusTracker"},{"location":"status/AppStatusStore/#sparkui","text":"AppStatusStore is used to create SparkUI .","title":" SparkUI"},{"location":"status/AppStatusStore/#rdds","text":"rddList ( cachedOnly : Boolean = true ): Seq [ v1 . RDDStorageInfo ] rddList requests the KVStore for (a view over) RDDStorageInfo s (cached or not based on the given cachedOnly flag). rddList is used when: AbstractApplicationResource is requested for the RDDs StageTableBase is created (and renders a stage table for AllStagesPage , JobPage and PoolPage ) StoragePage is requested to render","title":" RDDs"},{"location":"status/AppStatusStore/#streaming-blocks","text":"streamBlocksList (): Seq [ StreamBlockData ] streamBlocksList requests the KVStore for (a view over) StreamBlockData s. streamBlocksList is used when: StoragePage is requested to render","title":" Streaming Blocks"},{"location":"status/AppStatusStore/#stages","text":"stageList ( statuses : JList [ v1 . StageStatus ]): Seq [ v1 . StageData ] stageList requests the KVStore for (a view over) StageData s. stageList is used when: SparkStatusTracker is requested for active stage IDs StagesResource is requested for stages AllStagesPage is requested to render","title":" Stages"},{"location":"status/AppStatusStore/#jobs","text":"jobsList ( statuses : JList [ JobExecutionStatus ]): Seq [ v1 . JobData ] jobsList requests the KVStore for (a view over) JobData s. jobsList is used when: SparkStatusTracker is requested for getJobIdsForGroup and getActiveJobIds AbstractApplicationResource is requested for jobs AllJobsPage is requested to render","title":" Jobs"},{"location":"status/AppStatusStore/#executors","text":"executorList ( activeOnly : Boolean ): Seq [ v1 . ExecutorSummary ] executorList requests the KVStore for (a view over) ExecutorSummary s. executorList is used when: FIXME","title":" Executors"},{"location":"status/AppStatusStore/#application-summary","text":"appSummary (): AppSummary appSummary requests the KVStore to read the AppSummary . appSummary is used when: AllJobsPage is requested to render AllStagesPage is requested to render","title":" Application Summary"},{"location":"status/ElementTrackingStore/","text":"ElementTrackingStore \u00b6 ElementTrackingStore is a KVStore that tracks the number of entities ( elements ) of specific types in a store and triggers actions once they reach a threshold. Creating Instance \u00b6 ElementTrackingStore takes the following to be created: KVStore SparkConf ElementTrackingStore is created when: AppStatusStore is requested to createLiveStore FsHistoryProvider is requested to rebuildAppStore Writing Value to Store \u00b6 write ( value : Any ): Unit write is part of the KVStore abstraction. write requests the KVStore to write the value Writing Value to Store and Checking Triggers \u00b6 write ( value : Any , checkTriggers : Boolean ): WriteQueueResult write writes the value . write ...FIXME write is used when: LiveEntity is requested to write StreamingQueryStatusListener (Spark Structured Streaming) is requested to onQueryStarted and onQueryTerminated Creating View of Specific Entities \u00b6 view [ T ]( klass : Class [ T ]): KVStoreView [ T ] view is part of the KVStore abstraction. view requests the KVStore for a view of klass entities. Registering Trigger \u00b6 addTrigger ( klass : Class [ _ ], threshold : Long )( action : Long => Unit ): Unit addTrigger ...FIXME addTrigger is used when: AppStatusListener is created HiveThriftServer2Listener (Spark Thrift Server) is created SQLAppStatusListener (Spark SQL) is created StreamingQueryStatusListener (Spark Structured Streaming) is created","title":"ElementTrackingStore"},{"location":"status/ElementTrackingStore/#elementtrackingstore","text":"ElementTrackingStore is a KVStore that tracks the number of entities ( elements ) of specific types in a store and triggers actions once they reach a threshold.","title":"ElementTrackingStore"},{"location":"status/ElementTrackingStore/#creating-instance","text":"ElementTrackingStore takes the following to be created: KVStore SparkConf ElementTrackingStore is created when: AppStatusStore is requested to createLiveStore FsHistoryProvider is requested to rebuildAppStore","title":"Creating Instance"},{"location":"status/ElementTrackingStore/#writing-value-to-store","text":"write ( value : Any ): Unit write is part of the KVStore abstraction. write requests the KVStore to write the value","title":" Writing Value to Store"},{"location":"status/ElementTrackingStore/#writing-value-to-store-and-checking-triggers","text":"write ( value : Any , checkTriggers : Boolean ): WriteQueueResult write writes the value . write ...FIXME write is used when: LiveEntity is requested to write StreamingQueryStatusListener (Spark Structured Streaming) is requested to onQueryStarted and onQueryTerminated","title":" Writing Value to Store and Checking Triggers"},{"location":"status/ElementTrackingStore/#creating-view-of-specific-entities","text":"view [ T ]( klass : Class [ T ]): KVStoreView [ T ] view is part of the KVStore abstraction. view requests the KVStore for a view of klass entities.","title":" Creating View of Specific Entities"},{"location":"status/ElementTrackingStore/#registering-trigger","text":"addTrigger ( klass : Class [ _ ], threshold : Long )( action : Long => Unit ): Unit addTrigger ...FIXME addTrigger is used when: AppStatusListener is created HiveThriftServer2Listener (Spark Thrift Server) is created SQLAppStatusListener (Spark SQL) is created StreamingQueryStatusListener (Spark Structured Streaming) is created","title":" Registering Trigger"},{"location":"status/LiveEntity/","text":"LiveEntity \u00b6 LiveEntity is an abstraction of entities of a running ( live ) Spark application. Contract \u00b6 doUpdate \u00b6 doUpdate (): Any Updated view of this entity's data Used when: LiveEntity is requested to write out to the store Implementations \u00b6 LiveExecutionData (Spark SQL) LiveExecutionData (Spark Thrift Server) LiveExecutor LiveExecutorStageSummary LiveJob LiveRDD LiveResourceProfile LiveSessionData LiveStage LiveTask SchedulerPool Writing Out to Store \u00b6 write ( store : ElementTrackingStore , now : Long , checkTriggers : Boolean = false ): Unit write ...FIXME write is used when: AppStatusListener is requested to update HiveThriftServer2Listener (Spark Thrift Server) is requested to updateStoreWithTriggerEnabled and updateLiveStore SQLAppStatusListener (Spark SQL) is requested to update","title":"LiveEntity"},{"location":"status/LiveEntity/#liveentity","text":"LiveEntity is an abstraction of entities of a running ( live ) Spark application.","title":"LiveEntity"},{"location":"status/LiveEntity/#contract","text":"","title":"Contract"},{"location":"status/LiveEntity/#doupdate","text":"doUpdate (): Any Updated view of this entity's data Used when: LiveEntity is requested to write out to the store","title":" doUpdate"},{"location":"status/LiveEntity/#implementations","text":"LiveExecutionData (Spark SQL) LiveExecutionData (Spark Thrift Server) LiveExecutor LiveExecutorStageSummary LiveJob LiveRDD LiveResourceProfile LiveSessionData LiveStage LiveTask SchedulerPool","title":"Implementations"},{"location":"status/LiveEntity/#writing-out-to-store","text":"write ( store : ElementTrackingStore , now : Long , checkTriggers : Boolean = false ): Unit write ...FIXME write is used when: AppStatusListener is requested to update HiveThriftServer2Listener (Spark Thrift Server) is requested to updateStoreWithTriggerEnabled and updateLiveStore SQLAppStatusListener (Spark SQL) is requested to update","title":" Writing Out to Store"},{"location":"storage/BlockData/","text":"= BlockData BlockData is...FIXME","title":"BlockData"},{"location":"storage/BlockDataManager/","text":"BlockDataManager \u00b6 BlockDataManager is an abstraction of block data managers that manage storage for blocks of data (aka block storage management API ). BlockDataManager uses BlockId to uniquely identify blocks of data and ManagedBuffer to represent them. BlockDataManager is used to initialize a BlockTransferService . BlockDataManager is used to create a NettyBlockRpcServer . Contract \u00b6 getHostLocalShuffleData \u00b6 getHostLocalShuffleData ( blockId : BlockId , dirs : Array [ String ]): ManagedBuffer Used when: ShuffleBlockFetcherIterator is requested to fetchHostLocalBlock getLocalBlockData \u00b6 getLocalBlockData ( blockId : BlockId ): ManagedBuffer Used when: NettyBlockRpcServer is requested to receive a request ( OpenBlocks and FetchShuffleBlocks ) getLocalDiskDirs \u00b6 getLocalDiskDirs : Array [ String ] Used when: NettyBlockRpcServer is requested to receive a GetLocalDirsForExecutors request putBlockData \u00b6 putBlockData ( blockId : BlockId , data : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Boolean Stores ( puts ) a block data (as a ManagedBuffer ) for the given BlockId . Returns true when completed successfully or false when failed. Used when: NettyBlockRpcServer is requested to receive a UploadBlock request putBlockDataAsStream \u00b6 putBlockDataAsStream ( blockId : BlockId , level : StorageLevel , classTag : ClassTag [ _ ]): StreamCallbackWithID Used when: NettyBlockRpcServer is requested to receiveStream releaseLock \u00b6 releaseLock ( blockId : BlockId , taskContext : Option [ TaskContext ]): Unit Used when: TorrentBroadcast is requested to releaseBlockManagerLock BlockManager is requested to handleLocalReadFailure , getLocalValues , getOrElseUpdate , doPut , releaseLockAndDispose Implementations \u00b6 BlockManager","title":"BlockDataManager"},{"location":"storage/BlockDataManager/#blockdatamanager","text":"BlockDataManager is an abstraction of block data managers that manage storage for blocks of data (aka block storage management API ). BlockDataManager uses BlockId to uniquely identify blocks of data and ManagedBuffer to represent them. BlockDataManager is used to initialize a BlockTransferService . BlockDataManager is used to create a NettyBlockRpcServer .","title":"BlockDataManager"},{"location":"storage/BlockDataManager/#contract","text":"","title":"Contract"},{"location":"storage/BlockDataManager/#gethostlocalshuffledata","text":"getHostLocalShuffleData ( blockId : BlockId , dirs : Array [ String ]): ManagedBuffer Used when: ShuffleBlockFetcherIterator is requested to fetchHostLocalBlock","title":" getHostLocalShuffleData"},{"location":"storage/BlockDataManager/#getlocalblockdata","text":"getLocalBlockData ( blockId : BlockId ): ManagedBuffer Used when: NettyBlockRpcServer is requested to receive a request ( OpenBlocks and FetchShuffleBlocks )","title":" getLocalBlockData"},{"location":"storage/BlockDataManager/#getlocaldiskdirs","text":"getLocalDiskDirs : Array [ String ] Used when: NettyBlockRpcServer is requested to receive a GetLocalDirsForExecutors request","title":" getLocalDiskDirs"},{"location":"storage/BlockDataManager/#putblockdata","text":"putBlockData ( blockId : BlockId , data : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Boolean Stores ( puts ) a block data (as a ManagedBuffer ) for the given BlockId . Returns true when completed successfully or false when failed. Used when: NettyBlockRpcServer is requested to receive a UploadBlock request","title":" putBlockData"},{"location":"storage/BlockDataManager/#putblockdataasstream","text":"putBlockDataAsStream ( blockId : BlockId , level : StorageLevel , classTag : ClassTag [ _ ]): StreamCallbackWithID Used when: NettyBlockRpcServer is requested to receiveStream","title":" putBlockDataAsStream"},{"location":"storage/BlockDataManager/#releaselock","text":"releaseLock ( blockId : BlockId , taskContext : Option [ TaskContext ]): Unit Used when: TorrentBroadcast is requested to releaseBlockManagerLock BlockManager is requested to handleLocalReadFailure , getLocalValues , getOrElseUpdate , doPut , releaseLockAndDispose","title":" releaseLock"},{"location":"storage/BlockDataManager/#implementations","text":"BlockManager","title":"Implementations"},{"location":"storage/BlockEvictionHandler/","text":"= BlockEvictionHandler BlockEvictionHandler is an < > of < > that can < >. == [[contract]] Contract === [[dropFromMemory]] dropFromMemory Method [source,scala] \u00b6 dropFromMemory T: ClassTag : StorageLevel Used when MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace]. == [[implementations]] Available BlockEvictionHandlers storage:BlockManager.md[] is the default and only known BlockEvictionHandler in Apache Spark.","title":"BlockEvictionHandler"},{"location":"storage/BlockEvictionHandler/#sourcescala","text":"dropFromMemory T: ClassTag : StorageLevel Used when MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace]. == [[implementations]] Available BlockEvictionHandlers storage:BlockManager.md[] is the default and only known BlockEvictionHandler in Apache Spark.","title":"[source,scala]"},{"location":"storage/BlockId/","text":"BlockId \u00b6 BlockId is an abstraction of data block identifiers based on an unique name . Contract \u00b6 Name \u00b6 name : String A globally unique identifier of this Block Used when: BlockManager is requested to putBlockDataAsStream and readDiskBlockFromSameHostExecutor UpdateBlockInfo is requested to writeExternal DiskBlockManager is requested to getFile and containsBlock DiskStore is requested to getBytes , remove , moveFileToBlock , contains Implementations \u00b6 Sealed Abstract Class BlockId is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). BroadcastBlockId \u00b6 BlockId for Broadcast blocks: broadcastId identifier Optional field name (default: empty ) Uses broadcast_ prefix for the name Used when: TorrentBroadcast is created , requested to store a broadcast and the blocks in a local BlockManager , and read blocks BlockManager is requested to remove all the blocks of a broadcast variable SerializerManager is requested to shouldCompress AppStatusListener is requested to onBlockUpdated RDDBlockId \u00b6 BlockId for RDD partitions: rddId identifier splitIndex identifier Uses rdd_ prefix for the name Used when: StorageStatus is requested to register the status of a data block , get the status of a data block , updateStorageInfo LocalRDDCheckpointData is requested to doCheckpoint RDD is requested to getOrCompute DAGScheduler is requested for the BlockManagers (executors) for cached RDD partitions BlockManagerMasterEndpoint is requested to removeRdd AppStatusListener is requested to updateRDDBlock (when onBlockUpdated for an RDDBlockId ) Compressed when spark.rdd.compress configuration property is enabled ShuffleBlockBatchId \u00b6 ShuffleBlockId \u00b6 BlockId for shuffle blocks: shuffleId identifier mapId identifier reduceId identifier Uses shuffle_ prefix for the name Used when: ShuffleBlockFetcherIterator is requested to throwFetchFailedException MapOutputTracker utility is requested to convertMapStatuses NettyBlockRpcServer is requested to handle a FetchShuffleBlocks message ExternalSorter is requested to writePartitionedMapOutput ShuffleBlockFetcherIterator is requested to mergeContinuousShuffleBlockIdsIfNeeded IndexShuffleBlockResolver is requested to getBlockData Compressed when spark.shuffle.compress configuration property is enabled ShuffleDataBlockId \u00b6 ShuffleIndexBlockId \u00b6 StreamBlockId \u00b6 BlockId for ...FIXME: streamId uniqueId Uses the following name : input-[streamId]-[uniqueId] Used in Spark Streaming TaskResultBlockId \u00b6 TempLocalBlockId \u00b6 TempShuffleBlockId \u00b6 TestBlockId \u00b6 Creating BlockId by Name \u00b6 apply ( name : String ): BlockId apply creates one of the available BlockId s by the given name (that uses a prefix to differentiate between different BlockId s). apply is used when: NettyBlockRpcServer is requested to handle OpenBlocks , UploadBlock messages and receiveStream UpdateBlockInfo is requested to deserialize ( readExternal ) DiskBlockManager is requested for all the blocks (from files stored on disk) ShuffleBlockFetcherIterator is requested to sendRequest JsonProtocol utility is used to accumValueFromJson , taskMetricsFromJson and blockUpdatedInfoFromJson","title":"BlockId"},{"location":"storage/BlockId/#blockid","text":"BlockId is an abstraction of data block identifiers based on an unique name .","title":"BlockId"},{"location":"storage/BlockId/#contract","text":"","title":"Contract"},{"location":"storage/BlockId/#name","text":"name : String A globally unique identifier of this Block Used when: BlockManager is requested to putBlockDataAsStream and readDiskBlockFromSameHostExecutor UpdateBlockInfo is requested to writeExternal DiskBlockManager is requested to getFile and containsBlock DiskStore is requested to getBytes , remove , moveFileToBlock , contains","title":" Name"},{"location":"storage/BlockId/#implementations","text":"Sealed Abstract Class BlockId is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file).","title":"Implementations"},{"location":"storage/BlockId/#broadcastblockid","text":"BlockId for Broadcast blocks: broadcastId identifier Optional field name (default: empty ) Uses broadcast_ prefix for the name Used when: TorrentBroadcast is created , requested to store a broadcast and the blocks in a local BlockManager , and read blocks BlockManager is requested to remove all the blocks of a broadcast variable SerializerManager is requested to shouldCompress AppStatusListener is requested to onBlockUpdated","title":" BroadcastBlockId"},{"location":"storage/BlockId/#rddblockid","text":"BlockId for RDD partitions: rddId identifier splitIndex identifier Uses rdd_ prefix for the name Used when: StorageStatus is requested to register the status of a data block , get the status of a data block , updateStorageInfo LocalRDDCheckpointData is requested to doCheckpoint RDD is requested to getOrCompute DAGScheduler is requested for the BlockManagers (executors) for cached RDD partitions BlockManagerMasterEndpoint is requested to removeRdd AppStatusListener is requested to updateRDDBlock (when onBlockUpdated for an RDDBlockId ) Compressed when spark.rdd.compress configuration property is enabled","title":" RDDBlockId"},{"location":"storage/BlockId/#shuffleblockbatchid","text":"","title":" ShuffleBlockBatchId"},{"location":"storage/BlockId/#shuffleblockid","text":"BlockId for shuffle blocks: shuffleId identifier mapId identifier reduceId identifier Uses shuffle_ prefix for the name Used when: ShuffleBlockFetcherIterator is requested to throwFetchFailedException MapOutputTracker utility is requested to convertMapStatuses NettyBlockRpcServer is requested to handle a FetchShuffleBlocks message ExternalSorter is requested to writePartitionedMapOutput ShuffleBlockFetcherIterator is requested to mergeContinuousShuffleBlockIdsIfNeeded IndexShuffleBlockResolver is requested to getBlockData Compressed when spark.shuffle.compress configuration property is enabled","title":" ShuffleBlockId"},{"location":"storage/BlockId/#shuffledatablockid","text":"","title":" ShuffleDataBlockId"},{"location":"storage/BlockId/#shuffleindexblockid","text":"","title":" ShuffleIndexBlockId"},{"location":"storage/BlockId/#streamblockid","text":"BlockId for ...FIXME: streamId uniqueId Uses the following name : input-[streamId]-[uniqueId] Used in Spark Streaming","title":" StreamBlockId"},{"location":"storage/BlockId/#taskresultblockid","text":"","title":" TaskResultBlockId"},{"location":"storage/BlockId/#templocalblockid","text":"","title":" TempLocalBlockId"},{"location":"storage/BlockId/#tempshuffleblockid","text":"","title":" TempShuffleBlockId"},{"location":"storage/BlockId/#testblockid","text":"","title":" TestBlockId"},{"location":"storage/BlockId/#creating-blockid-by-name","text":"apply ( name : String ): BlockId apply creates one of the available BlockId s by the given name (that uses a prefix to differentiate between different BlockId s). apply is used when: NettyBlockRpcServer is requested to handle OpenBlocks , UploadBlock messages and receiveStream UpdateBlockInfo is requested to deserialize ( readExternal ) DiskBlockManager is requested for all the blocks (from files stored on disk) ShuffleBlockFetcherIterator is requested to sendRequest JsonProtocol utility is used to accumValueFromJson , taskMetricsFromJson and blockUpdatedInfoFromJson","title":" Creating BlockId by Name"},{"location":"storage/BlockInfo/","text":"= BlockInfo BlockInfo is a metadata of storage:BlockId.md[memory block] -- the memory block's < >, the < > and the < >. BlockInfo has a storage:StorageLevel.md[], ClassTag and tellMaster flag. == [[size]] Size size attribute is the size of the memory block. It starts with 0 . It represents the number of bytes that storage:BlockManager.md#putBytes[ BlockManager saved] or storage:BlockManager.md#doPutIterator[BlockManager.doPutIterator]. == [[readerCount]] Reader Count -- readerCount Counter readerCount counter is the number of readers of the memory block, i.e. the number of read locks. It starts with 0 . readerCount is incremented when a storage:BlockInfoManager.md#lockForReading[read lock is acquired] and decreases when the following happens: The storage:BlockManager.md#unlock[memory block is unlocked] storage:BlockInfoManager.md#releaseAllLocksForTask[All locks for the memory block obtained by a task are released]. storage:BlockInfoManager.md#removeBlock[memory block is removed] storage:BlockInfoManager.md#clear[Clearing the current state of BlockInfoManager ]. == [[writerTask]] Writer Task -- writerTask Attribute writerTask attribute is the task that owns the write lock for the memory block. A writer task can be one of the three possible identifiers: [[NO_WRITER]] NO_WRITER (i.e. -1 ) to denote no writers and hence no write lock in use. [[NON_TASK_WRITER]] NON_TASK_WRITER (i.e. -1024 ) for non-task threads, e.g. by a driver thread or by unit test code. the task attempt id of the task which currently holds the write lock for this block. The writer task is assigned in the following scenarios: A storage:BlockInfoManager.md#lockForWriting[write lock is requested for a memory block (with no writer and readers)] A storage:BlockInfoManager.md#unlock[memory block is unlocked] storage:BlockInfoManager.md#releaseAllLocksForTask[All locks obtained by a task are released] A storage:BlockInfoManager.md#removeBlock[memory block is removed] storage:BlockInfoManager.md#clear[Clearing the current state of BlockInfoManager ].","title":"BlockInfo"},{"location":"storage/BlockInfoManager/","text":"BlockInfoManager \u00b6 BlockInfoManager is used by BlockManager (and MemoryStore ) to manage metadata of memory blocks and control concurrent access by locks for reading and writing . BlockInfoManager is used to create a MemoryStore and a BlockManagerManagedBuffer . Creating Instance \u00b6 BlockInfoManager takes no arguments to be created. BlockInfoManager is created for BlockManager Block Metadata \u00b6 infos : HashMap [ BlockId , BlockInfo ] BlockInfoManager uses a registry of block metadata s per block . Locks \u00b6 Locks are the mechanism to control concurrent access to data and prevent destructive interaction between operations that use the same resource. BlockInfoManager uses read and write locks by task attempts. Read Locks \u00b6 readLocksByTask : HashMap [ TaskAttemptId , ConcurrentHashMultiset [ BlockId ]] BlockInfoManager uses readLocksByTask registry to track tasks (by TaskAttemptId ) and the blocks they locked for reading (as BlockId s). A new entry is added when BlockInfoManager is requested to register a task (attempt). A new BlockId is added to an existing task attempt in lockForReading . Write Locks \u00b6 Tracks tasks (by TaskAttemptId ) and the blocks they locked for writing (as BlockId.md[]). Registering Task (Execution Attempt) \u00b6 registerTask ( taskAttemptId : Long ): Unit registerTask registers a new \"empty\" entry for the given task (by the task attempt ID) to the readLocksByTask internal registry. registerTask is used when: BlockInfoManager is created BlockManager is requested to registerTask Downgrading Exclusive Write Lock to Shared Read Lock \u00b6 downgradeLock ( blockId : BlockId ): Unit downgradeLock prints out the following TRACE message to the logs: Task [currentTaskAttemptId] downgrading write lock for [blockId] downgradeLock ...FIXME downgradeLock is used when: BlockManager is requested to doPut and downgradeLock Obtaining Read Lock for Block \u00b6 lockForReading ( blockId : BlockId , blocking : Boolean = true ): Option [ BlockInfo ] lockForReading locks a given memory block for reading when the block was registered earlier and no writer tasks use it. When executed, lockForReading prints out the following TRACE message to the logs: Task [currentTaskAttemptId] trying to acquire read lock for [blockId] lockForReading looks up the metadata of the blockId block (in the infos registry). If no metadata could be found, lockForReading returns None which means that the block does not exist or was removed (and anybody could acquire a write lock). Otherwise, when the metadata was found (i.e. registered) lockForReading checks so-called writerTask . Only when the block has no writer tasks , a read lock can be acquired. If so, the readerCount of the block metadata is incremented and the block is recorded (in the internal readLocksByTask registry). lockForReading prints out the following TRACE message to the logs: Task [currentTaskAttemptId] acquired read lock for [blockId] The BlockInfo for the blockId block is returned. Note -1024 is a special taskAttemptId ( NON_TASK_WRITER ) used to mark a non-task thread, e.g. by a driver thread or by unit test code. For blocks with writerTask other than NO_WRITER , when blocking is enabled, lockForReading waits (until another thread invokes the Object.notify method or the Object.notifyAll methods for this object). With blocking enabled, it will repeat the waiting-for-read-lock sequence until either None or the lock is obtained. When blocking is disabled and the lock could not be obtained, None is returned immediately. Note lockForReading is a synchronized method, i.e. no two objects can use this and other instance methods. lockForReading is used when: BlockInfoManager is requested to downgradeLock and lockNewBlockForWriting BlockManager is requested to getLocalValues , getLocalBytes and replicateBlock BlockManagerManagedBuffer is requested to retain Obtaining Write Lock for Block \u00b6 lockForWriting ( blockId : BlockId , blocking : Boolean = true ): Option [ BlockInfo ] lockForWriting prints out the following TRACE message to the logs: Task [currentTaskAttemptId] trying to acquire write lock for [blockId] lockForWriting finds the blockId (in the infos registry). When no BlockInfo could be found, None is returned. Otherwise, blockId block is checked for writerTask to be BlockInfo.NO_WRITER with no readers (i.e. readerCount is 0 ) and only then the lock is returned. When the write lock can be returned, BlockInfo.writerTask is set to currentTaskAttemptId and a new binding is added to the internal writeLocksByTask registry. lockForWriting prints out the following TRACE message to the logs: Task [currentTaskAttemptId] acquired write lock for [blockId] If, for some reason, BlockInfo.md#writerTask[ blockId has a writer] or the number of readers is positive (i.e. BlockInfo.readerCount is greater than 0 ), the method will wait (based on the input blocking flag) and attempt the write lock acquisition process until it finishes with a write lock. NOTE: (deadlock possible) The method is synchronized and can block, i.e. wait that causes the current thread to wait until another thread invokes Object.notify or Object.notifyAll methods for this object. lockForWriting returns None for no blockId in the internal infos registry or when blocking flag is disabled and the write lock could not be acquired. lockForWriting is used when: BlockInfoManager is requested to lockNewBlockForWriting BlockManager is requested to removeBlock MemoryStore is requested to evictBlocksToFreeSpace Obtaining Write Lock for New Block \u00b6 lockNewBlockForWriting ( blockId : BlockId , newBlockInfo : BlockInfo ): Boolean lockNewBlockForWriting obtains a write lock for blockId but only when the method could register the block. Note lockNewBlockForWriting is similar to lockForWriting method but for brand new blocks. When executed, lockNewBlockForWriting prints out the following TRACE message to the logs: Task [currentTaskAttemptId] trying to put [blockId] If some other thread has already created the block , lockNewBlockForWriting finishes returning false . Otherwise, when the block does not exist, newBlockInfo is recorded in the infos internal registry and the block is locked for this client for writing . lockNewBlockForWriting then returns true . Note lockNewBlockForWriting executes itself in synchronized block so once the BlockInfoManager is locked the other internal registries should be available for the current thread only. lockNewBlockForWriting is used when: BlockManager is requested to doPut Releasing Lock on Block \u00b6 unlock ( blockId : BlockId , taskAttemptId : Option [ TaskAttemptId ] = None ): Unit unlock prints out the following TRACE message to the logs: Task [currentTaskAttemptId] releasing lock for [blockId] unlock gets the metadata for blockId (and throws an IllegalStateException if the block was not found). If the writer task for the block is not NO_WRITER , it becomes so and the blockId block is removed from the internal writeLocksByTask registry for the current task attempt . Otherwise, if the writer task is indeed NO_WRITER , the block is assumed locked for reading . The readerCount counter is decremented for the blockId block and the read lock removed from the internal readLocksByTask registry for the task attempt. In the end, unlock wakes up all the threads waiting for the BlockInfoManager . unlock is used when: BlockInfoManager is requested to downgradeLock BlockManager is requested to releaseLock and doPut BlockManagerManagedBuffer is requested to release MemoryStore is requested to evictBlocksToFreeSpace Logging \u00b6 Enable ALL logging level for org.apache.spark.storage.BlockInfoManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.BlockInfoManager=ALL Refer to Logging .","title":"BlockInfoManager"},{"location":"storage/BlockInfoManager/#blockinfomanager","text":"BlockInfoManager is used by BlockManager (and MemoryStore ) to manage metadata of memory blocks and control concurrent access by locks for reading and writing . BlockInfoManager is used to create a MemoryStore and a BlockManagerManagedBuffer .","title":"BlockInfoManager"},{"location":"storage/BlockInfoManager/#creating-instance","text":"BlockInfoManager takes no arguments to be created. BlockInfoManager is created for BlockManager","title":"Creating Instance"},{"location":"storage/BlockInfoManager/#block-metadata","text":"infos : HashMap [ BlockId , BlockInfo ] BlockInfoManager uses a registry of block metadata s per block .","title":" Block Metadata"},{"location":"storage/BlockInfoManager/#locks","text":"Locks are the mechanism to control concurrent access to data and prevent destructive interaction between operations that use the same resource. BlockInfoManager uses read and write locks by task attempts.","title":"Locks"},{"location":"storage/BlockInfoManager/#read-locks","text":"readLocksByTask : HashMap [ TaskAttemptId , ConcurrentHashMultiset [ BlockId ]] BlockInfoManager uses readLocksByTask registry to track tasks (by TaskAttemptId ) and the blocks they locked for reading (as BlockId s). A new entry is added when BlockInfoManager is requested to register a task (attempt). A new BlockId is added to an existing task attempt in lockForReading .","title":" Read Locks"},{"location":"storage/BlockInfoManager/#write-locks","text":"Tracks tasks (by TaskAttemptId ) and the blocks they locked for writing (as BlockId.md[]).","title":" Write Locks"},{"location":"storage/BlockInfoManager/#registering-task-execution-attempt","text":"registerTask ( taskAttemptId : Long ): Unit registerTask registers a new \"empty\" entry for the given task (by the task attempt ID) to the readLocksByTask internal registry. registerTask is used when: BlockInfoManager is created BlockManager is requested to registerTask","title":" Registering Task (Execution Attempt)"},{"location":"storage/BlockInfoManager/#downgrading-exclusive-write-lock-to-shared-read-lock","text":"downgradeLock ( blockId : BlockId ): Unit downgradeLock prints out the following TRACE message to the logs: Task [currentTaskAttemptId] downgrading write lock for [blockId] downgradeLock ...FIXME downgradeLock is used when: BlockManager is requested to doPut and downgradeLock","title":" Downgrading Exclusive Write Lock to Shared Read Lock"},{"location":"storage/BlockInfoManager/#obtaining-read-lock-for-block","text":"lockForReading ( blockId : BlockId , blocking : Boolean = true ): Option [ BlockInfo ] lockForReading locks a given memory block for reading when the block was registered earlier and no writer tasks use it. When executed, lockForReading prints out the following TRACE message to the logs: Task [currentTaskAttemptId] trying to acquire read lock for [blockId] lockForReading looks up the metadata of the blockId block (in the infos registry). If no metadata could be found, lockForReading returns None which means that the block does not exist or was removed (and anybody could acquire a write lock). Otherwise, when the metadata was found (i.e. registered) lockForReading checks so-called writerTask . Only when the block has no writer tasks , a read lock can be acquired. If so, the readerCount of the block metadata is incremented and the block is recorded (in the internal readLocksByTask registry). lockForReading prints out the following TRACE message to the logs: Task [currentTaskAttemptId] acquired read lock for [blockId] The BlockInfo for the blockId block is returned. Note -1024 is a special taskAttemptId ( NON_TASK_WRITER ) used to mark a non-task thread, e.g. by a driver thread or by unit test code. For blocks with writerTask other than NO_WRITER , when blocking is enabled, lockForReading waits (until another thread invokes the Object.notify method or the Object.notifyAll methods for this object). With blocking enabled, it will repeat the waiting-for-read-lock sequence until either None or the lock is obtained. When blocking is disabled and the lock could not be obtained, None is returned immediately. Note lockForReading is a synchronized method, i.e. no two objects can use this and other instance methods. lockForReading is used when: BlockInfoManager is requested to downgradeLock and lockNewBlockForWriting BlockManager is requested to getLocalValues , getLocalBytes and replicateBlock BlockManagerManagedBuffer is requested to retain","title":" Obtaining Read Lock for Block"},{"location":"storage/BlockInfoManager/#obtaining-write-lock-for-block","text":"lockForWriting ( blockId : BlockId , blocking : Boolean = true ): Option [ BlockInfo ] lockForWriting prints out the following TRACE message to the logs: Task [currentTaskAttemptId] trying to acquire write lock for [blockId] lockForWriting finds the blockId (in the infos registry). When no BlockInfo could be found, None is returned. Otherwise, blockId block is checked for writerTask to be BlockInfo.NO_WRITER with no readers (i.e. readerCount is 0 ) and only then the lock is returned. When the write lock can be returned, BlockInfo.writerTask is set to currentTaskAttemptId and a new binding is added to the internal writeLocksByTask registry. lockForWriting prints out the following TRACE message to the logs: Task [currentTaskAttemptId] acquired write lock for [blockId] If, for some reason, BlockInfo.md#writerTask[ blockId has a writer] or the number of readers is positive (i.e. BlockInfo.readerCount is greater than 0 ), the method will wait (based on the input blocking flag) and attempt the write lock acquisition process until it finishes with a write lock. NOTE: (deadlock possible) The method is synchronized and can block, i.e. wait that causes the current thread to wait until another thread invokes Object.notify or Object.notifyAll methods for this object. lockForWriting returns None for no blockId in the internal infos registry or when blocking flag is disabled and the write lock could not be acquired. lockForWriting is used when: BlockInfoManager is requested to lockNewBlockForWriting BlockManager is requested to removeBlock MemoryStore is requested to evictBlocksToFreeSpace","title":" Obtaining Write Lock for Block"},{"location":"storage/BlockInfoManager/#obtaining-write-lock-for-new-block","text":"lockNewBlockForWriting ( blockId : BlockId , newBlockInfo : BlockInfo ): Boolean lockNewBlockForWriting obtains a write lock for blockId but only when the method could register the block. Note lockNewBlockForWriting is similar to lockForWriting method but for brand new blocks. When executed, lockNewBlockForWriting prints out the following TRACE message to the logs: Task [currentTaskAttemptId] trying to put [blockId] If some other thread has already created the block , lockNewBlockForWriting finishes returning false . Otherwise, when the block does not exist, newBlockInfo is recorded in the infos internal registry and the block is locked for this client for writing . lockNewBlockForWriting then returns true . Note lockNewBlockForWriting executes itself in synchronized block so once the BlockInfoManager is locked the other internal registries should be available for the current thread only. lockNewBlockForWriting is used when: BlockManager is requested to doPut","title":" Obtaining Write Lock for New Block"},{"location":"storage/BlockInfoManager/#releasing-lock-on-block","text":"unlock ( blockId : BlockId , taskAttemptId : Option [ TaskAttemptId ] = None ): Unit unlock prints out the following TRACE message to the logs: Task [currentTaskAttemptId] releasing lock for [blockId] unlock gets the metadata for blockId (and throws an IllegalStateException if the block was not found). If the writer task for the block is not NO_WRITER , it becomes so and the blockId block is removed from the internal writeLocksByTask registry for the current task attempt . Otherwise, if the writer task is indeed NO_WRITER , the block is assumed locked for reading . The readerCount counter is decremented for the blockId block and the read lock removed from the internal readLocksByTask registry for the task attempt. In the end, unlock wakes up all the threads waiting for the BlockInfoManager . unlock is used when: BlockInfoManager is requested to downgradeLock BlockManager is requested to releaseLock and doPut BlockManagerManagedBuffer is requested to release MemoryStore is requested to evictBlocksToFreeSpace","title":" Releasing Lock on Block"},{"location":"storage/BlockInfoManager/#logging","text":"Enable ALL logging level for org.apache.spark.storage.BlockInfoManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.BlockInfoManager=ALL Refer to Logging .","title":"Logging"},{"location":"storage/BlockManager/","text":"BlockManager \u00b6 BlockManager manages the storage for blocks ( chunks of data ) that can be stored in memory and on disk . BlockManager runs as part of the driver and executor processes. BlockManager provides interface for uploading and fetching blocks both locally and remotely using various stores (i.e. memory, disk, and off-heap). Cached blocks are blocks with non-zero sum of memory and disk sizes. Tip Use Web UI (esp. Storage and Executors tabs) to monitor the memory used. Tip Use spark-submit 's command-line options (i.e. --driver-memory for the driver and --executor-memory for executors) or their equivalents as Spark properties (i.e. spark.executor.memory and spark.driver.memory ) to control the memory for storage memory. When External Shuffle Service is enabled , BlockManager uses ExternalShuffleClient to read shuffle files (of other executors). Creating Instance \u00b6 BlockManager takes the following to be created: Executor ID RpcEnv BlockManagerMaster SerializerManager SparkConf MemoryManager MapOutputTracker ShuffleManager BlockTransferService SecurityManager Optional ExternalBlockStoreClient When created, BlockManager sets externalShuffleServiceEnabled internal flag based on spark.shuffle.service.enabled configuration property. BlockManager then creates an instance of DiskBlockManager (requesting deleteFilesOnStop when an external shuffle service is not in use). BlockManager creates block-manager-future daemon cached thread pool with 128 threads maximum (as futureExecutionContext ). BlockManager calculates the maximum memory to use (as maxMemory ) by requesting the maximum on-heap and off-heap storage memory from the assigned MemoryManager . BlockManager calculates the port used by the external shuffle service (as externalShuffleServicePort ). BlockManager creates a client to read other executors' shuffle files (as shuffleClient ). If the external shuffle service is used...FIXME BlockManager sets the maximum number of failures before this block manager refreshes the block locations from the driver (as maxFailuresBeforeLocationRefresh ). BlockManager registers a BlockManagerSlaveEndpoint with the input RpcEnv , itself, and MapOutputTracker (as slaveEndpoint ). BlockManager is created when SparkEnv is created (for the driver and executors) when a Spark application starts. MigratableResolver \u00b6 migratableResolver : MigratableResolver BlockManager creates a reference to a MigratableResolver by requesting the ShuffleManager for the ShuffleBlockResolver (that is assumed a MigratableResolver ). Lazy Value migratableResolver is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards. private[storage] migratableResolver is a private[storage] so it is available to others in the org.apache.spark.storage package. migratableResolver is used when: BlockManager is requested to putBlockDataAsStream ShuffleMigrationRunnable is requested to run BlockManagerDecommissioner is requested to refreshOffloadingShuffleBlocks FallbackStorage is requested to copy Initializing BlockManager \u00b6 initialize ( appId : String ): Unit initialize requests the BlockTransferService to initialize . initialize requests the ExternalBlockStoreClient to initialize (if given). initialize determines the BlockReplicationPolicy based on spark.storage.replication.policy configuration property and prints out the following INFO message to the logs: Using [priorityClass] for block replication policy initialize creates a BlockManagerId and requests the BlockManagerMaster to registerBlockManager (with the BlockManagerId , the local directories of the DiskBlockManager , the maxOnHeapMemory , the maxOffHeapMemory and the slaveEndpoint ). initialize sets the internal BlockManagerId to be the response from the BlockManagerMaster (if available) or the BlockManagerId just created. initialize initializes the External Shuffle Server's Address when enabled and prints out the following INFO message to the logs (with the externalShuffleServicePort ): external shuffle service port = [externalShuffleServicePort] (only for executors and External Shuffle Service enabled ) initialize registers with the External Shuffle Server . initialize determines the hostLocalDirManager . With spark.shuffle.readHostLocalDisk configuration property enabled and spark.shuffle.useOldFetchProtocol disabled, initialize uses the ExternalBlockStoreClient to create a HostLocalDirManager (with spark.storage.localDiskByExecutors.cacheSize configuration property). In the end, initialize prints out the following INFO message to the logs (with the blockManagerId ): Initialized BlockManager: [blockManagerId] initialize is used when: SparkContext is created (on the driver) Executor is created (with isLocal flag disabled) Registering Executor's BlockManager with External Shuffle Server \u00b6 registerWithExternalShuffleServer (): Unit registerWithExternalShuffleServer registers the BlockManager (for an executor) with External Shuffle Service . registerWithExternalShuffleServer prints out the following INFO message to the logs: Registering executor with local external shuffle service. registerWithExternalShuffleServer creates an ExecutorShuffleInfo (with the localDirs and subDirsPerLocalDir of the DiskBlockManager , and the class name of the ShuffleManager ). registerWithExternalShuffleServer uses spark.shuffle.registration.maxAttempts configuration property and 5 sleep time when requesting the ExternalBlockStoreClient to registerWithShuffleServer (using the BlockManagerId and the ExecutorShuffleInfo ). In case of any exception that happen below the maximum number of attempts, registerWithExternalShuffleServer prints out the following ERROR message to the logs and sleeps 5 seconds: Failed to connect to external shuffle server, will retry [attempts] more times after waiting 5 seconds... BlockManagerId \u00b6 BlockManager uses a BlockManagerId for...FIXME HostLocalDirManager \u00b6 BlockManager can use a HostLocalDirManager . Default: (undefined) BlockReplicationPolicy \u00b6 BlockManager uses a BlockReplicationPolicy for...FIXME External Shuffle Service's Port \u00b6 BlockManager determines the port of an external shuffle service when created . The port is used to create the shuffleServerId and a HostLocalDirManager . The port is also used for preferExecutors . spark.diskStore.subDirectories Configuration Property \u00b6 BlockManager uses spark.diskStore.subDirectories configuration property to initialize a subDirsPerLocalDir local value. subDirsPerLocalDir is used when: IndexShuffleBlockResolver is requested to getDataFile and getIndexFile BlockManager is requested to readDiskBlockFromSameHostExecutor Fetching Block or Computing (and Storing) it \u00b6 getOrElseUpdate [ T ]( blockId : BlockId , level : StorageLevel , classTag : ClassTag [ T ], makeIterator : () => Iterator [ T ]): Either [ BlockResult , Iterator [ T ]] Map.getOrElseUpdate I think it is fair to say that getOrElseUpdate is like getOrElseUpdate of scala.collection.mutable.Map in Scala. getOrElseUpdate ( key : K , op : \u21d2 V ): V Quoting the official scaladoc: If given key K is already in this map, getOrElseUpdate returns the associated value V . Otherwise, getOrElseUpdate computes a value V from given expression op , stores with the key K in the map and returns that value. Since BlockManager is a key-value store of blocks of data identified by a block ID that seems to fit so well. getOrElseUpdate first attempts to get the block by the BlockId (from the local block manager first and, if unavailable, requesting remote peers). getOrElseUpdate gives the BlockResult of the block if found. If however the block was not found (in any block manager in a Spark cluster), getOrElseUpdate doPutIterator (for the input BlockId , the makeIterator function and the StorageLevel ). getOrElseUpdate branches off per the result. For None , getOrElseUpdate getLocalValues for the BlockId and eventually returns the BlockResult (unless terminated by a SparkException due to some internal error). For Some(iter) , getOrElseUpdate returns an iterator of T values. getOrElseUpdate is used when: RDD is requested to get or compute an RDD partition (for an RDDBlockId with the RDD's id and partition index). Fetching Block from Local or Remote Block Managers \u00b6 get [ T : ClassTag ]( blockId : BlockId ): Option [ BlockResult ] get attempts to get the blockId block from a local block manager first before requesting it from remote block managers. Internally, get tries to get the block from the local BlockManager . If the block was found, you should see the following INFO message in the logs and get returns the local BlockResult . Found block [blockId] locally If however the block was not found locally, get tries to get the block from remote block managers . If retrieved from a remote block manager, you should see the following INFO message in the logs and get returns the remote BlockResult . Found block [blockId] remotely In the end, get returns \"nothing\" (i.e. NONE ) when the blockId block was not found either in the local BlockManager or any remote BlockManager. get is used when: BlockManager is requested to getOrElseUpdate getRemoteValues \u00b6 getRemoteValues [ T : ClassTag ]( blockId : BlockId ): Option [ BlockResult ] getRemoteValues getRemoteBlock with the bufferTransformer function that takes a ManagedBuffer and does the following: Requests the SerializerManager to deserialize values from an input stream from the ManagedBuffer Creates a BlockResult with the values (and their total size, and Network read method) Fetching Block Bytes From Remote Block Managers \u00b6 getRemoteBytes ( blockId : BlockId ): Option [ ChunkedByteBuffer ] getRemoteBytes getRemoteBlock with the bufferTransformer function that takes a ManagedBuffer and creates a ChunkedByteBuffer . getRemoteBytes is used when: TorrentBroadcast is requested to readBlocks TaskResultGetter is requested to enqueueSuccessfulTask Fetching Remote Block \u00b6 getRemoteBlock [ T ]( blockId : BlockId , bufferTransformer : ManagedBuffer => T ): Option [ T ] getRemoteBlock is used for getRemoteValues and getRemoteBytes . getRemoteBlock prints out the following DEBUG message to the logs: Getting remote block [blockId] getRemoteBlock requests the BlockManagerMaster for locations and status of the input BlockId (with the host of BlockManagerId ). With some locations, getRemoteBlock determines the size of the block (max of diskSize and memSize ). getRemoteBlock tries to read the block from the local directories of another executor on the same host . getRemoteBlock prints out the following INFO message to the logs: Read [blockId] from the disk of a same host executor is [successful|failed]. When a data block could not be found in any of the local directories, getRemoteBlock fetchRemoteManagedBuffer . For no locations from the BlockManagerMaster , getRemoteBlock prints out the following DEBUG message to the logs: readDiskBlockFromSameHostExecutor \u00b6 readDiskBlockFromSameHostExecutor ( blockId : BlockId , localDirs : Array [ String ], blockSize : Long ): Option [ ManagedBuffer ] readDiskBlockFromSameHostExecutor ...FIXME fetchRemoteManagedBuffer \u00b6 fetchRemoteManagedBuffer ( blockId : BlockId , blockSize : Long , locationsAndStatus : BlockManagerMessages . BlockLocationsAndStatus ): Option [ ManagedBuffer ] fetchRemoteManagedBuffer ...FIXME sortLocations \u00b6 sortLocations ( locations : Seq [ BlockManagerId ]): Seq [ BlockManagerId ] sortLocations ...FIXME preferExecutors \u00b6 preferExecutors ( locations : Seq [ BlockManagerId ]): Seq [ BlockManagerId ] preferExecutors ...FIXME readDiskBlockFromSameHostExecutor \u00b6 readDiskBlockFromSameHostExecutor ( blockId : BlockId , localDirs : Array [ String ], blockSize : Long ): Option [ ManagedBuffer ] readDiskBlockFromSameHostExecutor ...FIXME ExecutionContextExecutorService \u00b6 BlockManager uses a Scala ExecutionContextExecutorService to execute FIXME asynchronously (on a thread pool with block-manager-future prefix and maximum of 128 threads). BlockEvictionHandler \u00b6 BlockManager is a BlockEvictionHandler that can drop a block from memory (and store it on a disk when necessary). ShuffleClient and External Shuffle Service \u00b6 Danger FIXME ShuffleClient and ExternalShuffleClient are dead. Long live BlockStoreClient and ExternalBlockStoreClient . BlockManager manages the lifecycle of a ShuffleClient : Creates when created Inits (and possibly registers with an external shuffle server ) when requested to initialize Closes when requested to stop The ShuffleClient can be an ExternalShuffleClient or the given BlockTransferService based on spark.shuffle.service.enabled configuration property. When enabled, BlockManager uses the ExternalShuffleClient . The ShuffleClient is available to other Spark services (using shuffleClient value) and is used when BlockStoreShuffleReader is requested to read combined key-value records for a reduce task . When requested for shuffle metrics , BlockManager simply requests them from the ShuffleClient . BlockManager and RpcEnv \u00b6 BlockManager is given a RpcEnv when created . The RpcEnv is used to set up a BlockManagerSlaveEndpoint . BlockInfoManager \u00b6 BlockManager creates a BlockInfoManager when created . BlockManager requests the BlockInfoManager to clear when requested to stop . BlockManager uses the BlockInfoManager to create a MemoryStore . BlockManager uses the BlockInfoManager when requested for the following: reportAllBlocks getStatus getMatchingBlockIds getLocalValues and getLocalBytes doPut replicateBlock dropFromMemory removeRdd , removeBroadcast , removeBlock , removeBlockInternal downgradeLock , releaseLock , registerTask , releaseAllLocksForTask BlockManager and BlockManagerMaster \u00b6 BlockManager is given a BlockManagerMaster when created . BlockManager as BlockDataManager \u00b6 BlockManager is a BlockDataManager . BlockManager and MapOutputTracker \u00b6 BlockManager is given a MapOutputTracker when created . Executor ID \u00b6 BlockManager is given an Executor ID when created . The Executor ID is one of the following: driver ( SparkContext.DRIVER_IDENTIFIER ) for the driver Value of --executor-id command-line argument for CoarseGrainedExecutorBackend executors BlockManagerEndpoint RPC Endpoint \u00b6 BlockManager requests the RpcEnv to register a BlockManagerSlaveEndpoint under the name BlockManagerEndpoint[ID] . The RPC endpoint is used when BlockManager is requested to initialize and reregister (to register the BlockManager on an executor with the BlockManagerMaster on the driver). The endpoint is stopped (by requesting the RpcEnv to stop the reference ) when BlockManager is requested to stop . Accessing BlockManager \u00b6 BlockManager is available using SparkEnv on the driver and executors. import org.apache.spark.SparkEnv val bm = SparkEnv.get.blockManager scala> :type bm org.apache.spark.storage.BlockManager BlockStoreClient \u00b6 BlockManager uses a BlockStoreClient to read other executors' blocks. This is an ExternalBlockStoreClient (when given and an external shuffle service is used) or a BlockTransferService (to directly connect to other executors). This BlockStoreClient is used when: BlockStoreShuffleReader is requested to read combined key-values for a reduce task Create the HostLocalDirManager (when BlockManager is initialized ) As the shuffleMetricsSource registerWithExternalShuffleServer (when an external shuffle server is used and the ExternalBlockStoreClient defined) BlockTransferService \u00b6 BlockManager is given a BlockTransferService when created . Note There is only one concrete BlockTransferService that is NettyBlockTransferService and there seem to be no way to reconfigure Apache Spark to use a different implementation (if there were any). BlockTransferService is used when BlockManager is requested to fetch a block from and replicate a block to remote block managers. BlockTransferService is used as the BlockStoreClient (unless an ExternalBlockStoreClient is specified). BlockTransferService is initialized with this BlockManager . BlockTransferService is closed when BlockManager is requested to stop . MemoryManager \u00b6 BlockManager is given a MemoryManager when created . BlockManager uses the MemoryManager for the following: Create the MemoryStore (that is then assigned to MemoryManager as a \"circular dependency\") Initialize maxOnHeapMemory and maxOffHeapMemory (for reporting) ShuffleManager \u00b6 BlockManager is given a ShuffleManager when created . BlockManager uses the ShuffleManager for the following: Retrieving a block data (for shuffle blocks) Retrieving a non-shuffle block data (for shuffle blocks anyway) Registering an executor with a local external shuffle service (when initialized on an executor with externalShuffleServiceEnabled ) DiskBlockManager \u00b6 BlockManager creates a DiskBlockManager when created . BlockManager uses the BlockManager for the following: Creating a DiskStore Registering an executor with a local external shuffle service (when initialized on an executor with externalShuffleServiceEnabled ) The BlockManager is available as diskBlockManager reference to other Spark systems. import org . apache . spark . SparkEnv SparkEnv . get . blockManager . diskBlockManager MemoryStore \u00b6 BlockManager creates a MemoryStore when created (with the BlockInfoManager , the SerializerManager , the MemoryManager and itself as a BlockEvictionHandler ). BlockManager requests the MemoryManager to use the MemoryStore . BlockManager uses the MemoryStore for the following: getStatus and getCurrentBlockStatus getLocalValues doGetLocalBytes doPutBytes and doPutIterator maybeCacheDiskBytesInMemory and maybeCacheDiskValuesInMemory dropFromMemory removeBlockInternal The MemoryStore is requested to clear when BlockManager is requested to stop . The MemoryStore is available as memoryStore private reference to other Spark services. import org . apache . spark . SparkEnv SparkEnv . get . blockManager . memoryStore The MemoryStore is used (via SparkEnv.get.blockManager.memoryStore reference) when Task is requested to run (that has finished and requests the MemoryStore to releaseUnrollMemoryForThisTask ). DiskStore \u00b6 BlockManager creates a DiskStore (with the DiskBlockManager ) when created . BlockManager uses the DiskStore when requested to getStatus , getCurrentBlockStatus , getLocalValues , doGetLocalBytes , doPutBytes , doPutIterator , dropFromMemory , removeBlockInternal . Performance Metrics \u00b6 BlockManager uses BlockManagerSource to report metrics under the name BlockManager . getPeers \u00b6 getPeers ( forceFetch : Boolean ): Seq [ BlockManagerId ] getPeers ...FIXME getPeers is used when BlockManager is requested to replicateBlock and replicate . Releasing All Locks For Task \u00b6 releaseAllLocksForTask ( taskAttemptId : Long ): Seq [ BlockId ] releaseAllLocksForTask ...FIXME releaseAllLocksForTask is used when TaskRunner is requested to run (at the end of a task). Stopping BlockManager \u00b6 stop (): Unit stop ...FIXME stop is used when SparkEnv is requested to stop . Getting IDs of Existing Blocks (For a Given Filter) \u00b6 getMatchingBlockIds ( filter : BlockId => Boolean ): Seq [ BlockId ] getMatchingBlockIds ...FIXME getMatchingBlockIds is used when BlockManagerSlaveEndpoint is requested to handle a GetMatchingBlockIds message . Getting Local Block \u00b6 getLocalValues ( blockId : BlockId ): Option [ BlockResult ] getLocalValues prints out the following DEBUG message to the logs: Getting local block [blockId] getLocalValues obtains a read lock for blockId . When no blockId block was found, you should see the following DEBUG message in the logs and getLocalValues returns \"nothing\" (i.e. NONE ). Block [blockId] was not found When the blockId block was found, you should see the following DEBUG message in the logs: Level for block [blockId] is [level] If blockId block has memory level and is registered in MemoryStore , getLocalValues returns a BlockResult as Memory read method and with a CompletionIterator for an interator: Values iterator from MemoryStore for blockId for \"deserialized\" persistence levels. Iterator from SerializerManager after the data stream has been deserialized for the blockId block and the bytes for blockId block for \"serialized\" persistence levels. getLocalValues is used when: TorrentBroadcast is requested to readBroadcastBlock BlockManager is requested to get and getOrElseUpdate maybeCacheDiskValuesInMemory \u00b6 maybeCacheDiskValuesInMemory [ T ]( blockInfo : BlockInfo , blockId : BlockId , level : StorageLevel , diskIterator : Iterator [ T ]): Iterator [ T ] maybeCacheDiskValuesInMemory ...FIXME maybeCacheDiskValuesInMemory is used when BlockManager is requested to getLocalValues . Retrieving Block Data \u00b6 getBlockData ( blockId : BlockId ): ManagedBuffer getBlockData is part of the BlockDataManager abstraction. For a BlockId.md[] of a shuffle (a ShuffleBlockId), getBlockData requests the < > for the shuffle:ShuffleManager.md#shuffleBlockResolver[ShuffleBlockResolver] that is then requested for shuffle:ShuffleBlockResolver.md#getBlockData[getBlockData]. Otherwise, getBlockData < > for the given BlockId. If found, getBlockData creates a new BlockManagerManagedBuffer (with the < >, the input BlockId, the retrieved BlockData and the dispose flag enabled). If not found, getBlockData < > that the block could not be found (and that the master should no longer assume the block is available on this executor) and throws a BlockNotFoundException. NOTE: getBlockData is executed for shuffle blocks or local blocks that the BlockManagerMaster knows this executor really has (unless BlockManagerMaster is outdated). Retrieving Non-Shuffle Local Block Data \u00b6 getLocalBytes ( blockId : BlockId ): Option [ BlockData ] getLocalBytes ...FIXME getLocalBytes is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#readBlocks[readBlocks] BlockManager is requested for the < > (of a non-shuffle block) removeBlockInternal \u00b6 removeBlockInternal ( blockId : BlockId , tellMaster : Boolean ): Unit removeBlockInternal ...FIXME removeBlockInternal is used when BlockManager is requested to < > and < >. Stores \u00b6 A Store is the place where blocks are held. There are the following possible stores: MemoryStore.md[MemoryStore] for memory storage level. DiskStore.md[DiskStore] for disk storage level. ExternalBlockStore for OFF_HEAP storage level. Storing Block Data Locally \u00b6 putBlockData ( blockId : BlockId , data : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Boolean putBlockData is part of the BlockDataManager abstraction. putBlockData putBytes with Java NIO's ByteBuffer of the given ManagedBuffer . Storing Block Bytes Locally \u00b6 putBytes ( blockId : BlockId , bytes : ChunkedByteBuffer , level : StorageLevel , tellMaster : Boolean = true ): Boolean putBytes creates a ByteBufferBlockStoreUpdater and requests it to store the bytes . putBytes is used when: BlockManager is requested to puts a block data locally TaskRunner is requested to run (and the result size is above maxDirectResultSize ) TorrentBroadcast is requested to writeBlocks and readBlocks doPutBytes \u00b6 doPutBytes [ T ]( blockId : BlockId , bytes : ChunkedByteBuffer , level : StorageLevel , classTag : ClassTag [ T ], tellMaster : Boolean = true , keepReadLock : Boolean = false ): Boolean doPutBytes calls the internal helper < > with a function that accepts a BlockInfo and does the uploading. Inside the function, if the StorageLevel.md[storage level ]'s replication is greater than 1, it immediately starts < > of the blockId block on a separate thread (from futureExecutionContext thread pool). The replication uses the input bytes and level storage level. For a memory storage level, the function checks whether the storage level is deserialized or not. For a deserialized storage level , BlockManager 's serializer:SerializerManager.md#dataDeserializeStream[ SerializerManager deserializes bytes into an iterator of values] that MemoryStore.md#putIteratorAsValues[ MemoryStore stores]. If however the storage level is not deserialized, the function requests MemoryStore.md#putBytes[ MemoryStore to store the bytes] If the put did not succeed and the storage level is to use disk, you should see the following WARN message in the logs: Persisting block [blockId] to disk instead. And DiskStore.md#putBytes[ DiskStore stores the bytes]. NOTE: DiskStore.md[DiskStore] is requested to store the bytes of a block with memory and disk storage level only when MemoryStore.md[MemoryStore] has failed. If the storage level is to use disk only, DiskStore.md#putBytes[ DiskStore stores the bytes]. doPutBytes requests < > and if the block was successfully stored, and the driver should know about it ( tellMaster ), the function < >. The executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the updated block status] (only when executed inside a task where TaskContext is available). You should see the following DEBUG message in the logs: Put block [blockId] locally took [time] ms The function waits till the earlier asynchronous replication finishes for a block with replication level greater than 1 . The final result of doPutBytes is the result of storing the block successful or not (as computed earlier). NOTE: doPutBytes is used exclusively when BlockManager is requested to < >. doPut \u00b6 doPut [ T ]( blockId : BlockId , level : StorageLevel , classTag : ClassTag [ _ ], tellMaster : Boolean , keepReadLock : Boolean )( putBody : BlockInfo => Option [ T ]): Option [ T ] doPut executes the input putBody function with a BlockInfo.md[] being a new BlockInfo object (with level storage level) that BlockInfoManager.md#lockNewBlockForWriting[ BlockInfoManager managed to create a write lock for]. If the block has already been created (and BlockInfoManager.md#lockNewBlockForWriting[ BlockInfoManager did not manage to create a write lock for]), the following WARN message is printed out to the logs: Block [blockId] already exists on this machine; not re-adding it doPut < > when keepReadLock flag is disabled and returns None immediately. If however the write lock has been given, doPut executes putBody . If the result of putBody is None the block is considered saved successfully. For successful save and keepReadLock enabled, BlockInfoManager.md#downgradeLock[ BlockInfoManager is requested to downgrade an exclusive write lock for blockId to a shared read lock]. For successful save and keepReadLock disabled, BlockInfoManager.md#unlock[ BlockInfoManager is requested to release lock on blockId ]. For unsuccessful save, < > and the following WARN message is printed out to the logs: Putting block [blockId] failed In the end, doPut prints out the following DEBUG message to the logs: Putting block [blockId] [withOrWithout] replication took [usedTime] ms doPut is used when BlockManager is requested to < > and < >. Removing Block From Memory and Disk \u00b6 removeBlock ( blockId : BlockId , tellMaster : Boolean = true ): Unit removeBlock removes the blockId block from the MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore]. When executed, it prints out the following DEBUG message to the logs: Removing block [blockId] It requests BlockInfoManager.md[] for lock for writing for the blockId block. If it receives none, it prints out the following WARN message to the logs and quits. Asked to remove block [blockId], which does not exist Otherwise, with a write lock for the block, the block is removed from MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] (see MemoryStore.md#remove[Removing Block in MemoryStore ] and DiskStore.md#remove[Removing Block in DiskStore ]). If both removals fail, it prints out the following WARN message: Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store The block is removed from BlockInfoManager.md[]. removeBlock then < > that is used to < > (if the input tellMaster and the info's tellMaster are both enabled, i.e. true ) and the executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. removeBlock is used when: BlockManager is requested to < >, < > and < > BlockManagerSlaveEndpoint is requested to handle a BlockManagerSlaveEndpoint.md#RemoveBlock[RemoveBlock] message Removing RDD Blocks \u00b6 removeRdd ( rddId : Int ): Int removeRdd removes all the blocks that belong to the rddId RDD. It prints out the following INFO message to the logs: Removing RDD [rddId] It then requests RDD blocks from BlockInfoManager.md[] and < > (without informing the driver). The number of blocks removed is the final result. NOTE: It is used by BlockManagerSlaveEndpoint.md#RemoveRdd[ BlockManagerSlaveEndpoint while handling RemoveRdd messages]. Removing All Blocks of Broadcast Variable \u00b6 removeBroadcast ( broadcastId : Long , tellMaster : Boolean ): Int removeBroadcast removes all the blocks of the input broadcastId broadcast. Internally, it starts by printing out the following DEBUG message to the logs: Removing broadcast [broadcastId] It then requests all the BlockId.md#BroadcastBlockId[BroadcastBlockId] objects that belong to the broadcastId broadcast from BlockInfoManager.md[] and < >. The number of blocks removed is the final result. NOTE: It is used by BlockManagerSlaveEndpoint.md#RemoveBroadcast[ BlockManagerSlaveEndpoint while handling RemoveBroadcast messages]. External Shuffle Server's Address \u00b6 shuffleServerId : BlockManagerId When requested to initialize , BlockManager records the location ( BlockManagerId ) of External Shuffle Service if enabled or simply uses the non-external-shuffle-service BlockManagerId . The BlockManagerId is used to register an executor with a local external shuffle service . The BlockManagerId is used as the location of a shuffle map output when: BypassMergeSortShuffleWriter is requested to write partition records to a shuffle file UnsafeShuffleWriter is requested to close and write output SortShuffleWriter is requested to write output getStatus \u00b6 getStatus ( blockId : BlockId ): Option [ BlockStatus ] getStatus ...FIXME getStatus is used when BlockManagerSlaveEndpoint is requested to handle GetBlockStatus message. Re-registering BlockManager with Driver and Reporting Blocks \u00b6 reregister (): Unit When executed, reregister prints the following INFO message to the logs: BlockManager [blockManagerId] re-registering with master reregister then BlockManagerMaster.md#registerBlockManager[registers itself to the driver's BlockManagerMaster ] (just as it was when BlockManager was initializing ). It passes the BlockManagerId.md[], the maximum memory (as maxMemory ), and the BlockManagerSlaveEndpoint.md[]. reregister will then report all the local blocks to the BlockManagerMaster.md[BlockManagerMaster]. You should see the following INFO message in the logs: Reporting [blockInfoManager.size] blocks to the master. For each block metadata (in BlockInfoManager.md[]) it gets block current status and tries to send it to the BlockManagerMaster . If there is an issue communicating to the BlockManagerMaster.md[BlockManagerMaster], you should see the following ERROR message in the logs: Failed to report [blockId] to master; giving up. After the ERROR message, reregister stops reporting. reregister is used when an Executor was informed to re-register while sending heartbeats . reportAllBlocks \u00b6 reportAllBlocks (): Unit reportAllBlocks ...FIXME Calculate Current Block Status \u00b6 getCurrentBlockStatus ( blockId : BlockId , info : BlockInfo ): BlockStatus getCurrentBlockStatus gives the current BlockStatus of the BlockId block (with the block's current StorageLevel.md[StorageLevel], memory and disk sizes). It uses MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] for size and other information. NOTE: Most of the information to build BlockStatus is already in BlockInfo except that it may not necessarily reflect the current state per MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore]. Internally, it uses the input BlockInfo.md[] to know about the block's storage level. If the storage level is not set (i.e. null ), the returned BlockStatus assumes the StorageLevel.md[default NONE storage level] and the memory and disk sizes being 0 . If however the storage level is set, getCurrentBlockStatus uses MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their getSize or assume 0 ). NOTE: It is acceptable that the BlockInfo says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status. getCurrentBlockStatus is used when < >, < > or < > or < >. Reporting Current Storage Status of Block to Driver \u00b6 reportBlockStatus ( blockId : BlockId , info : BlockInfo , status : BlockStatus , droppedMemorySize : Long = 0L ): Unit reportBlockStatus is an for < > and if told to re-register it prints out the following INFO message to the logs: Got told to re-register updating block [blockId] It does asynchronous reregistration (using asyncReregister ). In either case, it prints out the following DEBUG message to the logs: Told master about block [blockId] reportBlockStatus is used when BlockManager is requested to getBlockData , doPutBytes , doPutIterator , dropFromMemory and removeBlockInternal . Reporting Block Status Update to Driver \u00b6 def tryToReportBlockStatus ( blockId : BlockId , info : BlockInfo , status : BlockStatus , droppedMemorySize : Long = 0L ): Boolean tryToReportBlockStatus reports block status update to BlockManagerMaster and returns its response. tryToReportBlockStatus is used when BlockManager is requested to reportAllBlocks or reportBlockStatus . Execution Context \u00b6 block-manager-future is the execution context for...FIXME ByteBuffer \u00b6 The underlying abstraction for blocks in Spark is a ByteBuffer that limits the size of a block to 2GB ( Integer.MAX_VALUE - see Why does FileChannel.map take up to Integer.MAX_VALUE of data? and SPARK-1476 2GB limit in spark for blocks ). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for long ), ser-deser via byte array-backed output streams. BlockResult \u00b6 BlockResult is a description of a fetched block with the readMethod and bytes . Registering Task \u00b6 registerTask ( taskAttemptId : Long ): Unit registerTask requests the BlockInfoManager to register a given task . registerTask is used when Task is requested to run (at the start of a task). Creating DiskBlockObjectWriter \u00b6 getDiskWriter ( blockId : BlockId , file : File , serializerInstance : SerializerInstance , bufferSize : Int , writeMetrics : ShuffleWriteMetrics ): DiskBlockObjectWriter getDiskWriter creates a DiskBlockObjectWriter (with spark.shuffle.sync configuration property for syncWrites argument). getDiskWriter uses the SerializerManager . getDiskWriter is used when: BypassMergeSortShuffleWriter is requested to write records (of a partition) ShuffleExternalSorter is requested to writeSortedFile ExternalAppendOnlyMap is requested to spillMemoryIteratorToDisk ExternalSorter is requested to spillMemoryIteratorToDisk and writePartitionedFile UnsafeSorterSpillWriter is created Recording Updated BlockStatus In Current Task's TaskMetrics \u00b6 addUpdatedBlockStatusToTaskMetrics ( blockId : BlockId , status : BlockStatus ): Unit addUpdatedBlockStatusToTaskMetrics takes an active TaskContext (if available) and records updated BlockStatus for Block (in the task's TaskMetrics ). addUpdatedBlockStatusToTaskMetrics is used when BlockManager doPutBytes (for a block that was successfully stored), doPut , doPutIterator , removes blocks from memory (possibly spilling it to disk) and removes block from memory and disk . Shuffle Metrics Source \u00b6 shuffleMetricsSource : Source shuffleMetricsSource creates a ShuffleMetricsSource with the shuffleMetrics (of the BlockStoreClient ) and the source name as follows: ExternalShuffle when ExternalBlockStoreClient is specified NettyBlockTransfer otherwise shuffleMetricsSource is available using SparkEnv : env . blockManager . shuffleMetricsSource shuffleMetricsSource is used when: Executor is created (for non-local / cluster modes) Replicating Block To Peers \u00b6 replicate ( blockId : BlockId , data : BlockData , level : StorageLevel , classTag : ClassTag [ _ ], existingReplicas : Set [ BlockManagerId ] = Set . empty ): Unit replicate ...FIXME replicate is used when BlockManager is requested to doPutBytes , doPutIterator and replicateBlock . replicateBlock \u00b6 replicateBlock ( blockId : BlockId , existingReplicas : Set [ BlockManagerId ], maxReplicas : Int ): Unit replicateBlock ...FIXME replicateBlock is used when BlockManagerSlaveEndpoint is requested to handle a ReplicateBlock message . putIterator \u00b6 putIterator [ T : ClassTag ]( blockId : BlockId , values : Iterator [ T ], level : StorageLevel , tellMaster : Boolean = true ): Boolean putIterator ...FIXME putIterator is used when: BlockManager is requested to putSingle putSingle Method \u00b6 putSingle [ T : ClassTag ]( blockId : BlockId , value : T , level : StorageLevel , tellMaster : Boolean = true ): Boolean putSingle ...FIXME putSingle is used when TorrentBroadcast is requested to write the blocks and readBroadcastBlock . doPutIterator \u00b6 doPutIterator [ T ]( blockId : BlockId , iterator : () => Iterator [ T ], level : StorageLevel , classTag : ClassTag [ T ], tellMaster : Boolean = true , keepReadLock : Boolean = false ): Option [ PartiallyUnrolledIterator [ T ]] doPutIterator simply < > with the putBody function that accepts a BlockInfo and does the following: . putBody branches off per whether the StorageLevel indicates to use a StorageLevel.md#useMemory[memory] or simply a StorageLevel.md#useDisk[disk], i.e. When the input StorageLevel indicates to StorageLevel.md#useMemory[use a memory] for storage in StorageLevel.md#deserialized[deserialized] format, putBody requests < > to MemoryStore.md#putIteratorAsValues[putIteratorAsValues] (for the BlockId and with the iterator factory function). + If the < > returned a correct value, the internal size is set to the value. + If however the < > failed to give a correct value, FIXME When the input StorageLevel indicates to StorageLevel.md#useMemory[use memory] for storage in StorageLevel.md#deserialized[serialized] format, putBody ...FIXME When the input StorageLevel does not indicate to use memory for storage but StorageLevel.md#useDisk[disk] instead, putBody ...FIXME . putBody requests the < > . Only when the block was successfully stored in either the memory or disk store: putBody < > to the < > when the input tellMaster flag (default: enabled) and the tellMaster flag of the block info are both enabled. putBody < > (with the BlockId and BlockStatus ) putBody prints out the following DEBUG message to the logs: + Put block [blockId] locally took [time] ms When the input StorageLevel indicates to use StorageLevel.md#replication[replication], putBody < > followed by < > (with the input BlockId and the StorageLevel as well as the BlockData to replicate) With a successful replication, putBody prints out the following DEBUG message to the logs: + Put block [blockId] remotely took [time] ms . In the end, putBody may or may not give a PartiallyUnrolledIterator if...FIXME NOTE: doPutIterator is used when BlockManager is requested to < > and < >. Dropping Block from Memory \u00b6 dropFromMemory ( blockId : BlockId , data : () => Either [ Array [ T ], ChunkedByteBuffer ]): StorageLevel dropFromMemory prints out the following INFO message to the logs: Dropping block [blockId] from memory dropFromMemory then asserts that the given block is BlockInfoManager.md#assertBlockIsLockedForWriting[locked for writing]. If the block's StorageLevel.md[StorageLevel] uses disks and the internal DiskStore.md[DiskStore] object ( diskStore ) does not contain the block, it is saved then. You should see the following INFO message in the logs: Writing block [blockId] to disk CAUTION: FIXME Describe the case with saving a block to disk. The block's memory size is fetched and recorded (using MemoryStore.getSize ). The block is MemoryStore.md#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs: Block [blockId] could not be dropped from memory as it does not exist It then < > and < >. It only happens when info.tellMaster . CAUTION: FIXME When would info.tellMaster be true ? A block is considered updated when it was written to disk or removed from memory or both. If either happened, the executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. In the end, dropFromMemory returns the current storage level of the block. dropFromMemory is part of the BlockEvictionHandler abstraction. releaseLock Method \u00b6 releaseLock ( blockId : BlockId , taskAttemptId : Option [ Long ] = None ): Unit releaseLock requests the BlockInfoManager to unlock the given block . releaseLock is part of the BlockDataManager abstraction. putBlockDataAsStream \u00b6 putBlockDataAsStream ( blockId : BlockId , level : StorageLevel , classTag : ClassTag [ _ ]): StreamCallbackWithID putBlockDataAsStream is part of the BlockDataManager abstraction. putBlockDataAsStream ...FIXME Maximum Memory \u00b6 Total maximum value that BlockManager can ever possibly use (that depends on MemoryManager and may vary over time). Total available on-heap and off-heap memory for storage (in bytes) Maximum Off-Heap Memory \u00b6 Maximum On-Heap Memory \u00b6 decommissionSelf \u00b6 decommissionSelf (): Unit decommissionSelf ...FIXME decommissionSelf is used when: BlockManagerStorageEndpoint is requested to handle a DecommissionBlockManager message decommissionBlockManager \u00b6 decommissionBlockManager (): Unit decommissionBlockManager sends a DecommissionBlockManager message to the BlockManagerStorageEndpoint . decommissionBlockManager is used when: CoarseGrainedExecutorBackend is requested to decommissionSelf BlockManagerStorageEndpoint \u00b6 storageEndpoint : RpcEndpointRef BlockManager sets up a RpcEndpointRef (within the RpcEnv ) under the name BlockManagerEndpoint[ID] with a BlockManagerStorageEndpoint message handler. BlockManagerDecommissioner \u00b6 decommissioner : Option [ BlockManagerDecommissioner ] BlockManager defines decommissioner internal registry for a BlockManagerDecommissioner . decommissioner is undefined ( None ) by default. BlockManager creates and starts a BlockManagerDecommissioner when requested to decommissionSelf . decommissioner is used for isDecommissioning and lastMigrationInfo . BlockManager requests the BlockManagerDecommissioner to stop when stopped . Logging \u00b6 Enable ALL logging level for org.apache.spark.storage.BlockManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.BlockManager=ALL Refer to Logging .","title":"BlockManager"},{"location":"storage/BlockManager/#blockmanager","text":"BlockManager manages the storage for blocks ( chunks of data ) that can be stored in memory and on disk . BlockManager runs as part of the driver and executor processes. BlockManager provides interface for uploading and fetching blocks both locally and remotely using various stores (i.e. memory, disk, and off-heap). Cached blocks are blocks with non-zero sum of memory and disk sizes. Tip Use Web UI (esp. Storage and Executors tabs) to monitor the memory used. Tip Use spark-submit 's command-line options (i.e. --driver-memory for the driver and --executor-memory for executors) or their equivalents as Spark properties (i.e. spark.executor.memory and spark.driver.memory ) to control the memory for storage memory. When External Shuffle Service is enabled , BlockManager uses ExternalShuffleClient to read shuffle files (of other executors).","title":"BlockManager"},{"location":"storage/BlockManager/#creating-instance","text":"BlockManager takes the following to be created: Executor ID RpcEnv BlockManagerMaster SerializerManager SparkConf MemoryManager MapOutputTracker ShuffleManager BlockTransferService SecurityManager Optional ExternalBlockStoreClient When created, BlockManager sets externalShuffleServiceEnabled internal flag based on spark.shuffle.service.enabled configuration property. BlockManager then creates an instance of DiskBlockManager (requesting deleteFilesOnStop when an external shuffle service is not in use). BlockManager creates block-manager-future daemon cached thread pool with 128 threads maximum (as futureExecutionContext ). BlockManager calculates the maximum memory to use (as maxMemory ) by requesting the maximum on-heap and off-heap storage memory from the assigned MemoryManager . BlockManager calculates the port used by the external shuffle service (as externalShuffleServicePort ). BlockManager creates a client to read other executors' shuffle files (as shuffleClient ). If the external shuffle service is used...FIXME BlockManager sets the maximum number of failures before this block manager refreshes the block locations from the driver (as maxFailuresBeforeLocationRefresh ). BlockManager registers a BlockManagerSlaveEndpoint with the input RpcEnv , itself, and MapOutputTracker (as slaveEndpoint ). BlockManager is created when SparkEnv is created (for the driver and executors) when a Spark application starts.","title":"Creating Instance"},{"location":"storage/BlockManager/#migratableresolver","text":"migratableResolver : MigratableResolver BlockManager creates a reference to a MigratableResolver by requesting the ShuffleManager for the ShuffleBlockResolver (that is assumed a MigratableResolver ). Lazy Value migratableResolver is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards. private[storage] migratableResolver is a private[storage] so it is available to others in the org.apache.spark.storage package. migratableResolver is used when: BlockManager is requested to putBlockDataAsStream ShuffleMigrationRunnable is requested to run BlockManagerDecommissioner is requested to refreshOffloadingShuffleBlocks FallbackStorage is requested to copy","title":" MigratableResolver"},{"location":"storage/BlockManager/#initializing-blockmanager","text":"initialize ( appId : String ): Unit initialize requests the BlockTransferService to initialize . initialize requests the ExternalBlockStoreClient to initialize (if given). initialize determines the BlockReplicationPolicy based on spark.storage.replication.policy configuration property and prints out the following INFO message to the logs: Using [priorityClass] for block replication policy initialize creates a BlockManagerId and requests the BlockManagerMaster to registerBlockManager (with the BlockManagerId , the local directories of the DiskBlockManager , the maxOnHeapMemory , the maxOffHeapMemory and the slaveEndpoint ). initialize sets the internal BlockManagerId to be the response from the BlockManagerMaster (if available) or the BlockManagerId just created. initialize initializes the External Shuffle Server's Address when enabled and prints out the following INFO message to the logs (with the externalShuffleServicePort ): external shuffle service port = [externalShuffleServicePort] (only for executors and External Shuffle Service enabled ) initialize registers with the External Shuffle Server . initialize determines the hostLocalDirManager . With spark.shuffle.readHostLocalDisk configuration property enabled and spark.shuffle.useOldFetchProtocol disabled, initialize uses the ExternalBlockStoreClient to create a HostLocalDirManager (with spark.storage.localDiskByExecutors.cacheSize configuration property). In the end, initialize prints out the following INFO message to the logs (with the blockManagerId ): Initialized BlockManager: [blockManagerId] initialize is used when: SparkContext is created (on the driver) Executor is created (with isLocal flag disabled)","title":" Initializing BlockManager"},{"location":"storage/BlockManager/#registering-executors-blockmanager-with-external-shuffle-server","text":"registerWithExternalShuffleServer (): Unit registerWithExternalShuffleServer registers the BlockManager (for an executor) with External Shuffle Service . registerWithExternalShuffleServer prints out the following INFO message to the logs: Registering executor with local external shuffle service. registerWithExternalShuffleServer creates an ExecutorShuffleInfo (with the localDirs and subDirsPerLocalDir of the DiskBlockManager , and the class name of the ShuffleManager ). registerWithExternalShuffleServer uses spark.shuffle.registration.maxAttempts configuration property and 5 sleep time when requesting the ExternalBlockStoreClient to registerWithShuffleServer (using the BlockManagerId and the ExecutorShuffleInfo ). In case of any exception that happen below the maximum number of attempts, registerWithExternalShuffleServer prints out the following ERROR message to the logs and sleeps 5 seconds: Failed to connect to external shuffle server, will retry [attempts] more times after waiting 5 seconds...","title":" Registering Executor's BlockManager with External Shuffle Server"},{"location":"storage/BlockManager/#blockmanagerid","text":"BlockManager uses a BlockManagerId for...FIXME","title":" BlockManagerId"},{"location":"storage/BlockManager/#hostlocaldirmanager","text":"BlockManager can use a HostLocalDirManager . Default: (undefined)","title":" HostLocalDirManager"},{"location":"storage/BlockManager/#blockreplicationpolicy","text":"BlockManager uses a BlockReplicationPolicy for...FIXME","title":" BlockReplicationPolicy"},{"location":"storage/BlockManager/#external-shuffle-services-port","text":"BlockManager determines the port of an external shuffle service when created . The port is used to create the shuffleServerId and a HostLocalDirManager . The port is also used for preferExecutors .","title":" External Shuffle Service's Port"},{"location":"storage/BlockManager/#sparkdiskstoresubdirectories-configuration-property","text":"BlockManager uses spark.diskStore.subDirectories configuration property to initialize a subDirsPerLocalDir local value. subDirsPerLocalDir is used when: IndexShuffleBlockResolver is requested to getDataFile and getIndexFile BlockManager is requested to readDiskBlockFromSameHostExecutor","title":" spark.diskStore.subDirectories Configuration Property"},{"location":"storage/BlockManager/#fetching-block-or-computing-and-storing-it","text":"getOrElseUpdate [ T ]( blockId : BlockId , level : StorageLevel , classTag : ClassTag [ T ], makeIterator : () => Iterator [ T ]): Either [ BlockResult , Iterator [ T ]] Map.getOrElseUpdate I think it is fair to say that getOrElseUpdate is like getOrElseUpdate of scala.collection.mutable.Map in Scala. getOrElseUpdate ( key : K , op : \u21d2 V ): V Quoting the official scaladoc: If given key K is already in this map, getOrElseUpdate returns the associated value V . Otherwise, getOrElseUpdate computes a value V from given expression op , stores with the key K in the map and returns that value. Since BlockManager is a key-value store of blocks of data identified by a block ID that seems to fit so well. getOrElseUpdate first attempts to get the block by the BlockId (from the local block manager first and, if unavailable, requesting remote peers). getOrElseUpdate gives the BlockResult of the block if found. If however the block was not found (in any block manager in a Spark cluster), getOrElseUpdate doPutIterator (for the input BlockId , the makeIterator function and the StorageLevel ). getOrElseUpdate branches off per the result. For None , getOrElseUpdate getLocalValues for the BlockId and eventually returns the BlockResult (unless terminated by a SparkException due to some internal error). For Some(iter) , getOrElseUpdate returns an iterator of T values. getOrElseUpdate is used when: RDD is requested to get or compute an RDD partition (for an RDDBlockId with the RDD's id and partition index).","title":" Fetching Block or Computing (and Storing) it"},{"location":"storage/BlockManager/#fetching-block-from-local-or-remote-block-managers","text":"get [ T : ClassTag ]( blockId : BlockId ): Option [ BlockResult ] get attempts to get the blockId block from a local block manager first before requesting it from remote block managers. Internally, get tries to get the block from the local BlockManager . If the block was found, you should see the following INFO message in the logs and get returns the local BlockResult . Found block [blockId] locally If however the block was not found locally, get tries to get the block from remote block managers . If retrieved from a remote block manager, you should see the following INFO message in the logs and get returns the remote BlockResult . Found block [blockId] remotely In the end, get returns \"nothing\" (i.e. NONE ) when the blockId block was not found either in the local BlockManager or any remote BlockManager. get is used when: BlockManager is requested to getOrElseUpdate","title":" Fetching Block from Local or Remote Block Managers"},{"location":"storage/BlockManager/#getremotevalues","text":"getRemoteValues [ T : ClassTag ]( blockId : BlockId ): Option [ BlockResult ] getRemoteValues getRemoteBlock with the bufferTransformer function that takes a ManagedBuffer and does the following: Requests the SerializerManager to deserialize values from an input stream from the ManagedBuffer Creates a BlockResult with the values (and their total size, and Network read method)","title":" getRemoteValues"},{"location":"storage/BlockManager/#fetching-block-bytes-from-remote-block-managers","text":"getRemoteBytes ( blockId : BlockId ): Option [ ChunkedByteBuffer ] getRemoteBytes getRemoteBlock with the bufferTransformer function that takes a ManagedBuffer and creates a ChunkedByteBuffer . getRemoteBytes is used when: TorrentBroadcast is requested to readBlocks TaskResultGetter is requested to enqueueSuccessfulTask","title":" Fetching Block Bytes From Remote Block Managers"},{"location":"storage/BlockManager/#fetching-remote-block","text":"getRemoteBlock [ T ]( blockId : BlockId , bufferTransformer : ManagedBuffer => T ): Option [ T ] getRemoteBlock is used for getRemoteValues and getRemoteBytes . getRemoteBlock prints out the following DEBUG message to the logs: Getting remote block [blockId] getRemoteBlock requests the BlockManagerMaster for locations and status of the input BlockId (with the host of BlockManagerId ). With some locations, getRemoteBlock determines the size of the block (max of diskSize and memSize ). getRemoteBlock tries to read the block from the local directories of another executor on the same host . getRemoteBlock prints out the following INFO message to the logs: Read [blockId] from the disk of a same host executor is [successful|failed]. When a data block could not be found in any of the local directories, getRemoteBlock fetchRemoteManagedBuffer . For no locations from the BlockManagerMaster , getRemoteBlock prints out the following DEBUG message to the logs:","title":" Fetching Remote Block"},{"location":"storage/BlockManager/#readdiskblockfromsamehostexecutor","text":"readDiskBlockFromSameHostExecutor ( blockId : BlockId , localDirs : Array [ String ], blockSize : Long ): Option [ ManagedBuffer ] readDiskBlockFromSameHostExecutor ...FIXME","title":" readDiskBlockFromSameHostExecutor"},{"location":"storage/BlockManager/#fetchremotemanagedbuffer","text":"fetchRemoteManagedBuffer ( blockId : BlockId , blockSize : Long , locationsAndStatus : BlockManagerMessages . BlockLocationsAndStatus ): Option [ ManagedBuffer ] fetchRemoteManagedBuffer ...FIXME","title":" fetchRemoteManagedBuffer"},{"location":"storage/BlockManager/#sortlocations","text":"sortLocations ( locations : Seq [ BlockManagerId ]): Seq [ BlockManagerId ] sortLocations ...FIXME","title":" sortLocations"},{"location":"storage/BlockManager/#preferexecutors","text":"preferExecutors ( locations : Seq [ BlockManagerId ]): Seq [ BlockManagerId ] preferExecutors ...FIXME","title":" preferExecutors"},{"location":"storage/BlockManager/#readdiskblockfromsamehostexecutor_1","text":"readDiskBlockFromSameHostExecutor ( blockId : BlockId , localDirs : Array [ String ], blockSize : Long ): Option [ ManagedBuffer ] readDiskBlockFromSameHostExecutor ...FIXME","title":" readDiskBlockFromSameHostExecutor"},{"location":"storage/BlockManager/#executioncontextexecutorservice","text":"BlockManager uses a Scala ExecutionContextExecutorService to execute FIXME asynchronously (on a thread pool with block-manager-future prefix and maximum of 128 threads).","title":" ExecutionContextExecutorService"},{"location":"storage/BlockManager/#blockevictionhandler","text":"BlockManager is a BlockEvictionHandler that can drop a block from memory (and store it on a disk when necessary).","title":" BlockEvictionHandler"},{"location":"storage/BlockManager/#shuffleclient-and-external-shuffle-service","text":"Danger FIXME ShuffleClient and ExternalShuffleClient are dead. Long live BlockStoreClient and ExternalBlockStoreClient . BlockManager manages the lifecycle of a ShuffleClient : Creates when created Inits (and possibly registers with an external shuffle server ) when requested to initialize Closes when requested to stop The ShuffleClient can be an ExternalShuffleClient or the given BlockTransferService based on spark.shuffle.service.enabled configuration property. When enabled, BlockManager uses the ExternalShuffleClient . The ShuffleClient is available to other Spark services (using shuffleClient value) and is used when BlockStoreShuffleReader is requested to read combined key-value records for a reduce task . When requested for shuffle metrics , BlockManager simply requests them from the ShuffleClient .","title":" ShuffleClient and External Shuffle Service"},{"location":"storage/BlockManager/#blockmanager-and-rpcenv","text":"BlockManager is given a RpcEnv when created . The RpcEnv is used to set up a BlockManagerSlaveEndpoint .","title":" BlockManager and RpcEnv"},{"location":"storage/BlockManager/#blockinfomanager","text":"BlockManager creates a BlockInfoManager when created . BlockManager requests the BlockInfoManager to clear when requested to stop . BlockManager uses the BlockInfoManager to create a MemoryStore . BlockManager uses the BlockInfoManager when requested for the following: reportAllBlocks getStatus getMatchingBlockIds getLocalValues and getLocalBytes doPut replicateBlock dropFromMemory removeRdd , removeBroadcast , removeBlock , removeBlockInternal downgradeLock , releaseLock , registerTask , releaseAllLocksForTask","title":" BlockInfoManager"},{"location":"storage/BlockManager/#blockmanager-and-blockmanagermaster","text":"BlockManager is given a BlockManagerMaster when created .","title":" BlockManager and BlockManagerMaster"},{"location":"storage/BlockManager/#blockmanager-as-blockdatamanager","text":"BlockManager is a BlockDataManager .","title":" BlockManager as BlockDataManager"},{"location":"storage/BlockManager/#blockmanager-and-mapoutputtracker","text":"BlockManager is given a MapOutputTracker when created .","title":" BlockManager and MapOutputTracker"},{"location":"storage/BlockManager/#executor-id","text":"BlockManager is given an Executor ID when created . The Executor ID is one of the following: driver ( SparkContext.DRIVER_IDENTIFIER ) for the driver Value of --executor-id command-line argument for CoarseGrainedExecutorBackend executors","title":" Executor ID"},{"location":"storage/BlockManager/#blockmanagerendpoint-rpc-endpoint","text":"BlockManager requests the RpcEnv to register a BlockManagerSlaveEndpoint under the name BlockManagerEndpoint[ID] . The RPC endpoint is used when BlockManager is requested to initialize and reregister (to register the BlockManager on an executor with the BlockManagerMaster on the driver). The endpoint is stopped (by requesting the RpcEnv to stop the reference ) when BlockManager is requested to stop .","title":" BlockManagerEndpoint RPC Endpoint"},{"location":"storage/BlockManager/#accessing-blockmanager","text":"BlockManager is available using SparkEnv on the driver and executors. import org.apache.spark.SparkEnv val bm = SparkEnv.get.blockManager scala> :type bm org.apache.spark.storage.BlockManager","title":" Accessing BlockManager"},{"location":"storage/BlockManager/#blockstoreclient","text":"BlockManager uses a BlockStoreClient to read other executors' blocks. This is an ExternalBlockStoreClient (when given and an external shuffle service is used) or a BlockTransferService (to directly connect to other executors). This BlockStoreClient is used when: BlockStoreShuffleReader is requested to read combined key-values for a reduce task Create the HostLocalDirManager (when BlockManager is initialized ) As the shuffleMetricsSource registerWithExternalShuffleServer (when an external shuffle server is used and the ExternalBlockStoreClient defined)","title":" BlockStoreClient"},{"location":"storage/BlockManager/#blocktransferservice","text":"BlockManager is given a BlockTransferService when created . Note There is only one concrete BlockTransferService that is NettyBlockTransferService and there seem to be no way to reconfigure Apache Spark to use a different implementation (if there were any). BlockTransferService is used when BlockManager is requested to fetch a block from and replicate a block to remote block managers. BlockTransferService is used as the BlockStoreClient (unless an ExternalBlockStoreClient is specified). BlockTransferService is initialized with this BlockManager . BlockTransferService is closed when BlockManager is requested to stop .","title":" BlockTransferService"},{"location":"storage/BlockManager/#memorymanager","text":"BlockManager is given a MemoryManager when created . BlockManager uses the MemoryManager for the following: Create the MemoryStore (that is then assigned to MemoryManager as a \"circular dependency\") Initialize maxOnHeapMemory and maxOffHeapMemory (for reporting)","title":" MemoryManager"},{"location":"storage/BlockManager/#shufflemanager","text":"BlockManager is given a ShuffleManager when created . BlockManager uses the ShuffleManager for the following: Retrieving a block data (for shuffle blocks) Retrieving a non-shuffle block data (for shuffle blocks anyway) Registering an executor with a local external shuffle service (when initialized on an executor with externalShuffleServiceEnabled )","title":" ShuffleManager"},{"location":"storage/BlockManager/#diskblockmanager","text":"BlockManager creates a DiskBlockManager when created . BlockManager uses the BlockManager for the following: Creating a DiskStore Registering an executor with a local external shuffle service (when initialized on an executor with externalShuffleServiceEnabled ) The BlockManager is available as diskBlockManager reference to other Spark systems. import org . apache . spark . SparkEnv SparkEnv . get . blockManager . diskBlockManager","title":" DiskBlockManager"},{"location":"storage/BlockManager/#memorystore","text":"BlockManager creates a MemoryStore when created (with the BlockInfoManager , the SerializerManager , the MemoryManager and itself as a BlockEvictionHandler ). BlockManager requests the MemoryManager to use the MemoryStore . BlockManager uses the MemoryStore for the following: getStatus and getCurrentBlockStatus getLocalValues doGetLocalBytes doPutBytes and doPutIterator maybeCacheDiskBytesInMemory and maybeCacheDiskValuesInMemory dropFromMemory removeBlockInternal The MemoryStore is requested to clear when BlockManager is requested to stop . The MemoryStore is available as memoryStore private reference to other Spark services. import org . apache . spark . SparkEnv SparkEnv . get . blockManager . memoryStore The MemoryStore is used (via SparkEnv.get.blockManager.memoryStore reference) when Task is requested to run (that has finished and requests the MemoryStore to releaseUnrollMemoryForThisTask ).","title":" MemoryStore"},{"location":"storage/BlockManager/#diskstore","text":"BlockManager creates a DiskStore (with the DiskBlockManager ) when created . BlockManager uses the DiskStore when requested to getStatus , getCurrentBlockStatus , getLocalValues , doGetLocalBytes , doPutBytes , doPutIterator , dropFromMemory , removeBlockInternal .","title":" DiskStore"},{"location":"storage/BlockManager/#performance-metrics","text":"BlockManager uses BlockManagerSource to report metrics under the name BlockManager .","title":" Performance Metrics"},{"location":"storage/BlockManager/#getpeers","text":"getPeers ( forceFetch : Boolean ): Seq [ BlockManagerId ] getPeers ...FIXME getPeers is used when BlockManager is requested to replicateBlock and replicate .","title":" getPeers"},{"location":"storage/BlockManager/#releasing-all-locks-for-task","text":"releaseAllLocksForTask ( taskAttemptId : Long ): Seq [ BlockId ] releaseAllLocksForTask ...FIXME releaseAllLocksForTask is used when TaskRunner is requested to run (at the end of a task).","title":" Releasing All Locks For Task"},{"location":"storage/BlockManager/#stopping-blockmanager","text":"stop (): Unit stop ...FIXME stop is used when SparkEnv is requested to stop .","title":" Stopping BlockManager"},{"location":"storage/BlockManager/#getting-ids-of-existing-blocks-for-a-given-filter","text":"getMatchingBlockIds ( filter : BlockId => Boolean ): Seq [ BlockId ] getMatchingBlockIds ...FIXME getMatchingBlockIds is used when BlockManagerSlaveEndpoint is requested to handle a GetMatchingBlockIds message .","title":" Getting IDs of Existing Blocks (For a Given Filter)"},{"location":"storage/BlockManager/#getting-local-block","text":"getLocalValues ( blockId : BlockId ): Option [ BlockResult ] getLocalValues prints out the following DEBUG message to the logs: Getting local block [blockId] getLocalValues obtains a read lock for blockId . When no blockId block was found, you should see the following DEBUG message in the logs and getLocalValues returns \"nothing\" (i.e. NONE ). Block [blockId] was not found When the blockId block was found, you should see the following DEBUG message in the logs: Level for block [blockId] is [level] If blockId block has memory level and is registered in MemoryStore , getLocalValues returns a BlockResult as Memory read method and with a CompletionIterator for an interator: Values iterator from MemoryStore for blockId for \"deserialized\" persistence levels. Iterator from SerializerManager after the data stream has been deserialized for the blockId block and the bytes for blockId block for \"serialized\" persistence levels. getLocalValues is used when: TorrentBroadcast is requested to readBroadcastBlock BlockManager is requested to get and getOrElseUpdate","title":" Getting Local Block"},{"location":"storage/BlockManager/#maybecachediskvaluesinmemory","text":"maybeCacheDiskValuesInMemory [ T ]( blockInfo : BlockInfo , blockId : BlockId , level : StorageLevel , diskIterator : Iterator [ T ]): Iterator [ T ] maybeCacheDiskValuesInMemory ...FIXME maybeCacheDiskValuesInMemory is used when BlockManager is requested to getLocalValues .","title":" maybeCacheDiskValuesInMemory"},{"location":"storage/BlockManager/#retrieving-block-data","text":"getBlockData ( blockId : BlockId ): ManagedBuffer getBlockData is part of the BlockDataManager abstraction. For a BlockId.md[] of a shuffle (a ShuffleBlockId), getBlockData requests the < > for the shuffle:ShuffleManager.md#shuffleBlockResolver[ShuffleBlockResolver] that is then requested for shuffle:ShuffleBlockResolver.md#getBlockData[getBlockData]. Otherwise, getBlockData < > for the given BlockId. If found, getBlockData creates a new BlockManagerManagedBuffer (with the < >, the input BlockId, the retrieved BlockData and the dispose flag enabled). If not found, getBlockData < > that the block could not be found (and that the master should no longer assume the block is available on this executor) and throws a BlockNotFoundException. NOTE: getBlockData is executed for shuffle blocks or local blocks that the BlockManagerMaster knows this executor really has (unless BlockManagerMaster is outdated).","title":" Retrieving Block Data"},{"location":"storage/BlockManager/#retrieving-non-shuffle-local-block-data","text":"getLocalBytes ( blockId : BlockId ): Option [ BlockData ] getLocalBytes ...FIXME getLocalBytes is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#readBlocks[readBlocks] BlockManager is requested for the < > (of a non-shuffle block)","title":" Retrieving Non-Shuffle Local Block Data"},{"location":"storage/BlockManager/#removeblockinternal","text":"removeBlockInternal ( blockId : BlockId , tellMaster : Boolean ): Unit removeBlockInternal ...FIXME removeBlockInternal is used when BlockManager is requested to < > and < >.","title":" removeBlockInternal"},{"location":"storage/BlockManager/#stores","text":"A Store is the place where blocks are held. There are the following possible stores: MemoryStore.md[MemoryStore] for memory storage level. DiskStore.md[DiskStore] for disk storage level. ExternalBlockStore for OFF_HEAP storage level.","title":" Stores"},{"location":"storage/BlockManager/#storing-block-data-locally","text":"putBlockData ( blockId : BlockId , data : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Boolean putBlockData is part of the BlockDataManager abstraction. putBlockData putBytes with Java NIO's ByteBuffer of the given ManagedBuffer .","title":" Storing Block Data Locally"},{"location":"storage/BlockManager/#storing-block-bytes-locally","text":"putBytes ( blockId : BlockId , bytes : ChunkedByteBuffer , level : StorageLevel , tellMaster : Boolean = true ): Boolean putBytes creates a ByteBufferBlockStoreUpdater and requests it to store the bytes . putBytes is used when: BlockManager is requested to puts a block data locally TaskRunner is requested to run (and the result size is above maxDirectResultSize ) TorrentBroadcast is requested to writeBlocks and readBlocks","title":" Storing Block Bytes Locally"},{"location":"storage/BlockManager/#doputbytes","text":"doPutBytes [ T ]( blockId : BlockId , bytes : ChunkedByteBuffer , level : StorageLevel , classTag : ClassTag [ T ], tellMaster : Boolean = true , keepReadLock : Boolean = false ): Boolean doPutBytes calls the internal helper < > with a function that accepts a BlockInfo and does the uploading. Inside the function, if the StorageLevel.md[storage level ]'s replication is greater than 1, it immediately starts < > of the blockId block on a separate thread (from futureExecutionContext thread pool). The replication uses the input bytes and level storage level. For a memory storage level, the function checks whether the storage level is deserialized or not. For a deserialized storage level , BlockManager 's serializer:SerializerManager.md#dataDeserializeStream[ SerializerManager deserializes bytes into an iterator of values] that MemoryStore.md#putIteratorAsValues[ MemoryStore stores]. If however the storage level is not deserialized, the function requests MemoryStore.md#putBytes[ MemoryStore to store the bytes] If the put did not succeed and the storage level is to use disk, you should see the following WARN message in the logs: Persisting block [blockId] to disk instead. And DiskStore.md#putBytes[ DiskStore stores the bytes]. NOTE: DiskStore.md[DiskStore] is requested to store the bytes of a block with memory and disk storage level only when MemoryStore.md[MemoryStore] has failed. If the storage level is to use disk only, DiskStore.md#putBytes[ DiskStore stores the bytes]. doPutBytes requests < > and if the block was successfully stored, and the driver should know about it ( tellMaster ), the function < >. The executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the updated block status] (only when executed inside a task where TaskContext is available). You should see the following DEBUG message in the logs: Put block [blockId] locally took [time] ms The function waits till the earlier asynchronous replication finishes for a block with replication level greater than 1 . The final result of doPutBytes is the result of storing the block successful or not (as computed earlier). NOTE: doPutBytes is used exclusively when BlockManager is requested to < >.","title":" doPutBytes"},{"location":"storage/BlockManager/#doput","text":"doPut [ T ]( blockId : BlockId , level : StorageLevel , classTag : ClassTag [ _ ], tellMaster : Boolean , keepReadLock : Boolean )( putBody : BlockInfo => Option [ T ]): Option [ T ] doPut executes the input putBody function with a BlockInfo.md[] being a new BlockInfo object (with level storage level) that BlockInfoManager.md#lockNewBlockForWriting[ BlockInfoManager managed to create a write lock for]. If the block has already been created (and BlockInfoManager.md#lockNewBlockForWriting[ BlockInfoManager did not manage to create a write lock for]), the following WARN message is printed out to the logs: Block [blockId] already exists on this machine; not re-adding it doPut < > when keepReadLock flag is disabled and returns None immediately. If however the write lock has been given, doPut executes putBody . If the result of putBody is None the block is considered saved successfully. For successful save and keepReadLock enabled, BlockInfoManager.md#downgradeLock[ BlockInfoManager is requested to downgrade an exclusive write lock for blockId to a shared read lock]. For successful save and keepReadLock disabled, BlockInfoManager.md#unlock[ BlockInfoManager is requested to release lock on blockId ]. For unsuccessful save, < > and the following WARN message is printed out to the logs: Putting block [blockId] failed In the end, doPut prints out the following DEBUG message to the logs: Putting block [blockId] [withOrWithout] replication took [usedTime] ms doPut is used when BlockManager is requested to < > and < >.","title":" doPut"},{"location":"storage/BlockManager/#removing-block-from-memory-and-disk","text":"removeBlock ( blockId : BlockId , tellMaster : Boolean = true ): Unit removeBlock removes the blockId block from the MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore]. When executed, it prints out the following DEBUG message to the logs: Removing block [blockId] It requests BlockInfoManager.md[] for lock for writing for the blockId block. If it receives none, it prints out the following WARN message to the logs and quits. Asked to remove block [blockId], which does not exist Otherwise, with a write lock for the block, the block is removed from MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] (see MemoryStore.md#remove[Removing Block in MemoryStore ] and DiskStore.md#remove[Removing Block in DiskStore ]). If both removals fail, it prints out the following WARN message: Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store The block is removed from BlockInfoManager.md[]. removeBlock then < > that is used to < > (if the input tellMaster and the info's tellMaster are both enabled, i.e. true ) and the executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. removeBlock is used when: BlockManager is requested to < >, < > and < > BlockManagerSlaveEndpoint is requested to handle a BlockManagerSlaveEndpoint.md#RemoveBlock[RemoveBlock] message","title":" Removing Block From Memory and Disk"},{"location":"storage/BlockManager/#removing-rdd-blocks","text":"removeRdd ( rddId : Int ): Int removeRdd removes all the blocks that belong to the rddId RDD. It prints out the following INFO message to the logs: Removing RDD [rddId] It then requests RDD blocks from BlockInfoManager.md[] and < > (without informing the driver). The number of blocks removed is the final result. NOTE: It is used by BlockManagerSlaveEndpoint.md#RemoveRdd[ BlockManagerSlaveEndpoint while handling RemoveRdd messages].","title":" Removing RDD Blocks"},{"location":"storage/BlockManager/#removing-all-blocks-of-broadcast-variable","text":"removeBroadcast ( broadcastId : Long , tellMaster : Boolean ): Int removeBroadcast removes all the blocks of the input broadcastId broadcast. Internally, it starts by printing out the following DEBUG message to the logs: Removing broadcast [broadcastId] It then requests all the BlockId.md#BroadcastBlockId[BroadcastBlockId] objects that belong to the broadcastId broadcast from BlockInfoManager.md[] and < >. The number of blocks removed is the final result. NOTE: It is used by BlockManagerSlaveEndpoint.md#RemoveBroadcast[ BlockManagerSlaveEndpoint while handling RemoveBroadcast messages].","title":" Removing All Blocks of Broadcast Variable"},{"location":"storage/BlockManager/#external-shuffle-servers-address","text":"shuffleServerId : BlockManagerId When requested to initialize , BlockManager records the location ( BlockManagerId ) of External Shuffle Service if enabled or simply uses the non-external-shuffle-service BlockManagerId . The BlockManagerId is used to register an executor with a local external shuffle service . The BlockManagerId is used as the location of a shuffle map output when: BypassMergeSortShuffleWriter is requested to write partition records to a shuffle file UnsafeShuffleWriter is requested to close and write output SortShuffleWriter is requested to write output","title":" External Shuffle Server's Address"},{"location":"storage/BlockManager/#getstatus","text":"getStatus ( blockId : BlockId ): Option [ BlockStatus ] getStatus ...FIXME getStatus is used when BlockManagerSlaveEndpoint is requested to handle GetBlockStatus message.","title":" getStatus"},{"location":"storage/BlockManager/#re-registering-blockmanager-with-driver-and-reporting-blocks","text":"reregister (): Unit When executed, reregister prints the following INFO message to the logs: BlockManager [blockManagerId] re-registering with master reregister then BlockManagerMaster.md#registerBlockManager[registers itself to the driver's BlockManagerMaster ] (just as it was when BlockManager was initializing ). It passes the BlockManagerId.md[], the maximum memory (as maxMemory ), and the BlockManagerSlaveEndpoint.md[]. reregister will then report all the local blocks to the BlockManagerMaster.md[BlockManagerMaster]. You should see the following INFO message in the logs: Reporting [blockInfoManager.size] blocks to the master. For each block metadata (in BlockInfoManager.md[]) it gets block current status and tries to send it to the BlockManagerMaster . If there is an issue communicating to the BlockManagerMaster.md[BlockManagerMaster], you should see the following ERROR message in the logs: Failed to report [blockId] to master; giving up. After the ERROR message, reregister stops reporting. reregister is used when an Executor was informed to re-register while sending heartbeats .","title":" Re-registering BlockManager with Driver and Reporting Blocks"},{"location":"storage/BlockManager/#reportallblocks","text":"reportAllBlocks (): Unit reportAllBlocks ...FIXME","title":" reportAllBlocks"},{"location":"storage/BlockManager/#calculate-current-block-status","text":"getCurrentBlockStatus ( blockId : BlockId , info : BlockInfo ): BlockStatus getCurrentBlockStatus gives the current BlockStatus of the BlockId block (with the block's current StorageLevel.md[StorageLevel], memory and disk sizes). It uses MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] for size and other information. NOTE: Most of the information to build BlockStatus is already in BlockInfo except that it may not necessarily reflect the current state per MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore]. Internally, it uses the input BlockInfo.md[] to know about the block's storage level. If the storage level is not set (i.e. null ), the returned BlockStatus assumes the StorageLevel.md[default NONE storage level] and the memory and disk sizes being 0 . If however the storage level is set, getCurrentBlockStatus uses MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their getSize or assume 0 ). NOTE: It is acceptable that the BlockInfo says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status. getCurrentBlockStatus is used when < >, < > or < > or < >.","title":" Calculate Current Block Status"},{"location":"storage/BlockManager/#reporting-current-storage-status-of-block-to-driver","text":"reportBlockStatus ( blockId : BlockId , info : BlockInfo , status : BlockStatus , droppedMemorySize : Long = 0L ): Unit reportBlockStatus is an for < > and if told to re-register it prints out the following INFO message to the logs: Got told to re-register updating block [blockId] It does asynchronous reregistration (using asyncReregister ). In either case, it prints out the following DEBUG message to the logs: Told master about block [blockId] reportBlockStatus is used when BlockManager is requested to getBlockData , doPutBytes , doPutIterator , dropFromMemory and removeBlockInternal .","title":" Reporting Current Storage Status of Block to Driver"},{"location":"storage/BlockManager/#reporting-block-status-update-to-driver","text":"def tryToReportBlockStatus ( blockId : BlockId , info : BlockInfo , status : BlockStatus , droppedMemorySize : Long = 0L ): Boolean tryToReportBlockStatus reports block status update to BlockManagerMaster and returns its response. tryToReportBlockStatus is used when BlockManager is requested to reportAllBlocks or reportBlockStatus .","title":" Reporting Block Status Update to Driver"},{"location":"storage/BlockManager/#execution-context","text":"block-manager-future is the execution context for...FIXME","title":" Execution Context"},{"location":"storage/BlockManager/#bytebuffer","text":"The underlying abstraction for blocks in Spark is a ByteBuffer that limits the size of a block to 2GB ( Integer.MAX_VALUE - see Why does FileChannel.map take up to Integer.MAX_VALUE of data? and SPARK-1476 2GB limit in spark for blocks ). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for long ), ser-deser via byte array-backed output streams.","title":" ByteBuffer"},{"location":"storage/BlockManager/#blockresult","text":"BlockResult is a description of a fetched block with the readMethod and bytes .","title":" BlockResult"},{"location":"storage/BlockManager/#registering-task","text":"registerTask ( taskAttemptId : Long ): Unit registerTask requests the BlockInfoManager to register a given task . registerTask is used when Task is requested to run (at the start of a task).","title":" Registering Task"},{"location":"storage/BlockManager/#creating-diskblockobjectwriter","text":"getDiskWriter ( blockId : BlockId , file : File , serializerInstance : SerializerInstance , bufferSize : Int , writeMetrics : ShuffleWriteMetrics ): DiskBlockObjectWriter getDiskWriter creates a DiskBlockObjectWriter (with spark.shuffle.sync configuration property for syncWrites argument). getDiskWriter uses the SerializerManager . getDiskWriter is used when: BypassMergeSortShuffleWriter is requested to write records (of a partition) ShuffleExternalSorter is requested to writeSortedFile ExternalAppendOnlyMap is requested to spillMemoryIteratorToDisk ExternalSorter is requested to spillMemoryIteratorToDisk and writePartitionedFile UnsafeSorterSpillWriter is created","title":" Creating DiskBlockObjectWriter"},{"location":"storage/BlockManager/#recording-updated-blockstatus-in-current-tasks-taskmetrics","text":"addUpdatedBlockStatusToTaskMetrics ( blockId : BlockId , status : BlockStatus ): Unit addUpdatedBlockStatusToTaskMetrics takes an active TaskContext (if available) and records updated BlockStatus for Block (in the task's TaskMetrics ). addUpdatedBlockStatusToTaskMetrics is used when BlockManager doPutBytes (for a block that was successfully stored), doPut , doPutIterator , removes blocks from memory (possibly spilling it to disk) and removes block from memory and disk .","title":" Recording Updated BlockStatus In Current Task's TaskMetrics"},{"location":"storage/BlockManager/#shuffle-metrics-source","text":"shuffleMetricsSource : Source shuffleMetricsSource creates a ShuffleMetricsSource with the shuffleMetrics (of the BlockStoreClient ) and the source name as follows: ExternalShuffle when ExternalBlockStoreClient is specified NettyBlockTransfer otherwise shuffleMetricsSource is available using SparkEnv : env . blockManager . shuffleMetricsSource shuffleMetricsSource is used when: Executor is created (for non-local / cluster modes)","title":" Shuffle Metrics Source"},{"location":"storage/BlockManager/#replicating-block-to-peers","text":"replicate ( blockId : BlockId , data : BlockData , level : StorageLevel , classTag : ClassTag [ _ ], existingReplicas : Set [ BlockManagerId ] = Set . empty ): Unit replicate ...FIXME replicate is used when BlockManager is requested to doPutBytes , doPutIterator and replicateBlock .","title":" Replicating Block To Peers"},{"location":"storage/BlockManager/#replicateblock","text":"replicateBlock ( blockId : BlockId , existingReplicas : Set [ BlockManagerId ], maxReplicas : Int ): Unit replicateBlock ...FIXME replicateBlock is used when BlockManagerSlaveEndpoint is requested to handle a ReplicateBlock message .","title":" replicateBlock"},{"location":"storage/BlockManager/#putiterator","text":"putIterator [ T : ClassTag ]( blockId : BlockId , values : Iterator [ T ], level : StorageLevel , tellMaster : Boolean = true ): Boolean putIterator ...FIXME putIterator is used when: BlockManager is requested to putSingle","title":" putIterator"},{"location":"storage/BlockManager/#putsingle-method","text":"putSingle [ T : ClassTag ]( blockId : BlockId , value : T , level : StorageLevel , tellMaster : Boolean = true ): Boolean putSingle ...FIXME putSingle is used when TorrentBroadcast is requested to write the blocks and readBroadcastBlock .","title":" putSingle Method"},{"location":"storage/BlockManager/#doputiterator","text":"doPutIterator [ T ]( blockId : BlockId , iterator : () => Iterator [ T ], level : StorageLevel , classTag : ClassTag [ T ], tellMaster : Boolean = true , keepReadLock : Boolean = false ): Option [ PartiallyUnrolledIterator [ T ]] doPutIterator simply < > with the putBody function that accepts a BlockInfo and does the following: . putBody branches off per whether the StorageLevel indicates to use a StorageLevel.md#useMemory[memory] or simply a StorageLevel.md#useDisk[disk], i.e. When the input StorageLevel indicates to StorageLevel.md#useMemory[use a memory] for storage in StorageLevel.md#deserialized[deserialized] format, putBody requests < > to MemoryStore.md#putIteratorAsValues[putIteratorAsValues] (for the BlockId and with the iterator factory function). + If the < > returned a correct value, the internal size is set to the value. + If however the < > failed to give a correct value, FIXME When the input StorageLevel indicates to StorageLevel.md#useMemory[use memory] for storage in StorageLevel.md#deserialized[serialized] format, putBody ...FIXME When the input StorageLevel does not indicate to use memory for storage but StorageLevel.md#useDisk[disk] instead, putBody ...FIXME . putBody requests the < > . Only when the block was successfully stored in either the memory or disk store: putBody < > to the < > when the input tellMaster flag (default: enabled) and the tellMaster flag of the block info are both enabled. putBody < > (with the BlockId and BlockStatus ) putBody prints out the following DEBUG message to the logs: + Put block [blockId] locally took [time] ms When the input StorageLevel indicates to use StorageLevel.md#replication[replication], putBody < > followed by < > (with the input BlockId and the StorageLevel as well as the BlockData to replicate) With a successful replication, putBody prints out the following DEBUG message to the logs: + Put block [blockId] remotely took [time] ms . In the end, putBody may or may not give a PartiallyUnrolledIterator if...FIXME NOTE: doPutIterator is used when BlockManager is requested to < > and < >.","title":" doPutIterator"},{"location":"storage/BlockManager/#dropping-block-from-memory","text":"dropFromMemory ( blockId : BlockId , data : () => Either [ Array [ T ], ChunkedByteBuffer ]): StorageLevel dropFromMemory prints out the following INFO message to the logs: Dropping block [blockId] from memory dropFromMemory then asserts that the given block is BlockInfoManager.md#assertBlockIsLockedForWriting[locked for writing]. If the block's StorageLevel.md[StorageLevel] uses disks and the internal DiskStore.md[DiskStore] object ( diskStore ) does not contain the block, it is saved then. You should see the following INFO message in the logs: Writing block [blockId] to disk CAUTION: FIXME Describe the case with saving a block to disk. The block's memory size is fetched and recorded (using MemoryStore.getSize ). The block is MemoryStore.md#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs: Block [blockId] could not be dropped from memory as it does not exist It then < > and < >. It only happens when info.tellMaster . CAUTION: FIXME When would info.tellMaster be true ? A block is considered updated when it was written to disk or removed from memory or both. If either happened, the executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. In the end, dropFromMemory returns the current storage level of the block. dropFromMemory is part of the BlockEvictionHandler abstraction.","title":" Dropping Block from Memory"},{"location":"storage/BlockManager/#releaselock-method","text":"releaseLock ( blockId : BlockId , taskAttemptId : Option [ Long ] = None ): Unit releaseLock requests the BlockInfoManager to unlock the given block . releaseLock is part of the BlockDataManager abstraction.","title":" releaseLock Method"},{"location":"storage/BlockManager/#putblockdataasstream","text":"putBlockDataAsStream ( blockId : BlockId , level : StorageLevel , classTag : ClassTag [ _ ]): StreamCallbackWithID putBlockDataAsStream is part of the BlockDataManager abstraction. putBlockDataAsStream ...FIXME","title":" putBlockDataAsStream"},{"location":"storage/BlockManager/#maximum-memory","text":"Total maximum value that BlockManager can ever possibly use (that depends on MemoryManager and may vary over time). Total available on-heap and off-heap memory for storage (in bytes)","title":" Maximum Memory"},{"location":"storage/BlockManager/#maximum-off-heap-memory","text":"","title":" Maximum Off-Heap Memory"},{"location":"storage/BlockManager/#maximum-on-heap-memory","text":"","title":" Maximum On-Heap Memory"},{"location":"storage/BlockManager/#decommissionself","text":"decommissionSelf (): Unit decommissionSelf ...FIXME decommissionSelf is used when: BlockManagerStorageEndpoint is requested to handle a DecommissionBlockManager message","title":" decommissionSelf"},{"location":"storage/BlockManager/#decommissionblockmanager","text":"decommissionBlockManager (): Unit decommissionBlockManager sends a DecommissionBlockManager message to the BlockManagerStorageEndpoint . decommissionBlockManager is used when: CoarseGrainedExecutorBackend is requested to decommissionSelf","title":" decommissionBlockManager"},{"location":"storage/BlockManager/#blockmanagerstorageendpoint","text":"storageEndpoint : RpcEndpointRef BlockManager sets up a RpcEndpointRef (within the RpcEnv ) under the name BlockManagerEndpoint[ID] with a BlockManagerStorageEndpoint message handler.","title":" BlockManagerStorageEndpoint"},{"location":"storage/BlockManager/#blockmanagerdecommissioner","text":"decommissioner : Option [ BlockManagerDecommissioner ] BlockManager defines decommissioner internal registry for a BlockManagerDecommissioner . decommissioner is undefined ( None ) by default. BlockManager creates and starts a BlockManagerDecommissioner when requested to decommissionSelf . decommissioner is used for isDecommissioning and lastMigrationInfo . BlockManager requests the BlockManagerDecommissioner to stop when stopped .","title":" BlockManagerDecommissioner"},{"location":"storage/BlockManager/#logging","text":"Enable ALL logging level for org.apache.spark.storage.BlockManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.BlockManager=ALL Refer to Logging .","title":"Logging"},{"location":"storage/BlockManagerDecommissioner/","text":"BlockManagerDecommissioner \u00b6 BlockManagerDecommissioner is a decommissioning process used by BlockManager . Creating Instance \u00b6 BlockManagerDecommissioner takes the following to be created: SparkConf BlockManager BlockManagerDecommissioner is created when: BlockManager is requested to decommissionSelf","title":"BlockManagerDecommissioner"},{"location":"storage/BlockManagerDecommissioner/#blockmanagerdecommissioner","text":"BlockManagerDecommissioner is a decommissioning process used by BlockManager .","title":"BlockManagerDecommissioner"},{"location":"storage/BlockManagerDecommissioner/#creating-instance","text":"BlockManagerDecommissioner takes the following to be created: SparkConf BlockManager BlockManagerDecommissioner is created when: BlockManager is requested to decommissionSelf","title":"Creating Instance"},{"location":"storage/BlockManagerId/","text":"BlockManagerId \u00b6 BlockManagerId is a unique identifier ( address ) of a BlockManager .","title":"BlockManagerId"},{"location":"storage/BlockManagerId/#blockmanagerid","text":"BlockManagerId is a unique identifier ( address ) of a BlockManager .","title":"BlockManagerId"},{"location":"storage/BlockManagerInfo/","text":"= BlockManagerInfo BlockManagerInfo is...FIXME","title":"BlockManagerInfo"},{"location":"storage/BlockManagerMaster/","text":"BlockManagerMaster \u00b6 BlockManagerMaster runs on the driver and executors to exchange block metadata (status and locations) in a Spark application. BlockManagerMaster uses BlockManagerMasterEndpoint (registered as BlockManagerMaster RPC endpoint on the driver with the endpoint references on executors) for executors to send block status updates and so let the driver keep track of block status and locations. Creating Instance \u00b6 BlockManagerMaster takes the following to be created: Driver Endpoint Heartbeat Endpoint SparkConf isDriver flag (whether it is created for the driver or executors) BlockManagerMaster is created when: SparkEnv utility is used to create a SparkEnv (and create a BlockManager ) Driver Endpoint \u00b6 BlockManagerMaster is given a RpcEndpointRef of the BlockManagerMaster RPC Endpoint (on the driver) when created . Heartbeat Endpoint \u00b6 BlockManagerMaster is given a RpcEndpointRef of the BlockManagerMasterHeartbeat RPC Endpoint (on the driver) when created . The endpoint is used (mainly) when: DAGScheduler is requested to executorHeartbeatReceived Registering BlockManager (on Executor) with Driver \u00b6 registerBlockManager ( id : BlockManagerId , localDirs : Array [ String ], maxOnHeapMemSize : Long , maxOffHeapMemSize : Long , storageEndpoint : RpcEndpointRef ): BlockManagerId registerBlockManager prints out the following INFO message to the logs (with the given BlockManagerId ): Registering BlockManager [id] registerBlockManager notifies the driver (using the BlockManagerMaster RPC endpoint ) that the BlockManagerId wants to register (and sends a blocking RegisterBlockManager message). Note The input maxMemSize is the total available on-heap and off-heap memory for storage on the BlockManager . registerBlockManager waits until a confirmation comes (as a possibly-updated BlockManagerId ). In the end, registerBlockManager prints out the following INFO message to the logs and returns the BlockManagerId received. Registered BlockManager [updatedId] registerBlockManager is used when: BlockManager is requested to initialize and reregister FallbackStorage utility is used to registerBlockManagerIfNeeded Finding Block Locations for Single Block \u00b6 getLocations ( blockId : BlockId ): Seq [ BlockManagerId ] getLocations requests the driver (using the BlockManagerMaster RPC endpoint ) for BlockManagerId s of the given BlockId (and sends a blocking GetLocations message). getLocations is used when: BlockManager is requested to fetchRemoteManagedBuffer BlockManagerMaster is requested to contains a BlockId Finding Block Locations for Multiple Blocks \u00b6 getLocations ( blockIds : Array [ BlockId ]): IndexedSeq [ Seq [ BlockManagerId ]] getLocations requests the driver (using the BlockManagerMaster RPC endpoint ) for BlockManagerId s of the given BlockId s (and sends a blocking GetLocationsMultipleBlockIds message). getLocations is used when: DAGScheduler is requested for BlockManagers (executors) for cached RDD partitions BlockManager is requested to getLocationBlockIds BlockManager utility is used to blockIdsToLocations contains \u00b6 contains ( blockId : BlockId ): Boolean contains is positive ( true ) when there is at least one executor with the given BlockId . contains is used when: LocalRDDCheckpointData is requested to doCheckpoint Logging \u00b6 Enable ALL logging level for org.apache.spark.storage.BlockManagerMaster logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.BlockManagerMaster=ALL Refer to Logging .","title":"BlockManagerMaster"},{"location":"storage/BlockManagerMaster/#blockmanagermaster","text":"BlockManagerMaster runs on the driver and executors to exchange block metadata (status and locations) in a Spark application. BlockManagerMaster uses BlockManagerMasterEndpoint (registered as BlockManagerMaster RPC endpoint on the driver with the endpoint references on executors) for executors to send block status updates and so let the driver keep track of block status and locations.","title":"BlockManagerMaster"},{"location":"storage/BlockManagerMaster/#creating-instance","text":"BlockManagerMaster takes the following to be created: Driver Endpoint Heartbeat Endpoint SparkConf isDriver flag (whether it is created for the driver or executors) BlockManagerMaster is created when: SparkEnv utility is used to create a SparkEnv (and create a BlockManager )","title":"Creating Instance"},{"location":"storage/BlockManagerMaster/#driver-endpoint","text":"BlockManagerMaster is given a RpcEndpointRef of the BlockManagerMaster RPC Endpoint (on the driver) when created .","title":" Driver Endpoint"},{"location":"storage/BlockManagerMaster/#heartbeat-endpoint","text":"BlockManagerMaster is given a RpcEndpointRef of the BlockManagerMasterHeartbeat RPC Endpoint (on the driver) when created . The endpoint is used (mainly) when: DAGScheduler is requested to executorHeartbeatReceived","title":" Heartbeat Endpoint"},{"location":"storage/BlockManagerMaster/#registering-blockmanager-on-executor-with-driver","text":"registerBlockManager ( id : BlockManagerId , localDirs : Array [ String ], maxOnHeapMemSize : Long , maxOffHeapMemSize : Long , storageEndpoint : RpcEndpointRef ): BlockManagerId registerBlockManager prints out the following INFO message to the logs (with the given BlockManagerId ): Registering BlockManager [id] registerBlockManager notifies the driver (using the BlockManagerMaster RPC endpoint ) that the BlockManagerId wants to register (and sends a blocking RegisterBlockManager message). Note The input maxMemSize is the total available on-heap and off-heap memory for storage on the BlockManager . registerBlockManager waits until a confirmation comes (as a possibly-updated BlockManagerId ). In the end, registerBlockManager prints out the following INFO message to the logs and returns the BlockManagerId received. Registered BlockManager [updatedId] registerBlockManager is used when: BlockManager is requested to initialize and reregister FallbackStorage utility is used to registerBlockManagerIfNeeded","title":" Registering BlockManager (on Executor) with Driver"},{"location":"storage/BlockManagerMaster/#finding-block-locations-for-single-block","text":"getLocations ( blockId : BlockId ): Seq [ BlockManagerId ] getLocations requests the driver (using the BlockManagerMaster RPC endpoint ) for BlockManagerId s of the given BlockId (and sends a blocking GetLocations message). getLocations is used when: BlockManager is requested to fetchRemoteManagedBuffer BlockManagerMaster is requested to contains a BlockId","title":" Finding Block Locations for Single Block"},{"location":"storage/BlockManagerMaster/#finding-block-locations-for-multiple-blocks","text":"getLocations ( blockIds : Array [ BlockId ]): IndexedSeq [ Seq [ BlockManagerId ]] getLocations requests the driver (using the BlockManagerMaster RPC endpoint ) for BlockManagerId s of the given BlockId s (and sends a blocking GetLocationsMultipleBlockIds message). getLocations is used when: DAGScheduler is requested for BlockManagers (executors) for cached RDD partitions BlockManager is requested to getLocationBlockIds BlockManager utility is used to blockIdsToLocations","title":" Finding Block Locations for Multiple Blocks"},{"location":"storage/BlockManagerMaster/#contains","text":"contains ( blockId : BlockId ): Boolean contains is positive ( true ) when there is at least one executor with the given BlockId . contains is used when: LocalRDDCheckpointData is requested to doCheckpoint","title":" contains"},{"location":"storage/BlockManagerMaster/#logging","text":"Enable ALL logging level for org.apache.spark.storage.BlockManagerMaster logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.BlockManagerMaster=ALL Refer to Logging .","title":"Logging"},{"location":"storage/BlockManagerMasterEndpoint/","text":"BlockManagerMasterEndpoint \u00b6 BlockManagerMasterEndpoint is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] for storage:BlockManagerMaster.md[BlockManagerMaster]. BlockManagerMasterEndpoint is registered under BlockManagerMaster name. BlockManagerMasterEndpoint tracks status of the storage:BlockManager.md[BlockManagers] (on the executors) in a Spark application. == [[creating-instance]] Creating Instance BlockManagerMasterEndpoint takes the following to be created: [[rpcEnv]] rpc:RpcEnv.md[] [[isLocal]] Flag whether BlockManagerMasterEndpoint works in local or cluster mode [[conf]] SparkConf.md[] [[listenerBus]] scheduler:LiveListenerBus.md[] BlockManagerMasterEndpoint is created for the core:SparkEnv.md#create[SparkEnv] on the driver (to create a storage:BlockManagerMaster.md[] for a storage:BlockManager.md#master[BlockManager]). When created, BlockManagerMasterEndpoint prints out the following INFO message to the logs: [source,plaintext] \u00b6 BlockManagerMasterEndpoint up \u00b6 == [[messages]][[receiveAndReply]] Messages As an rpc:RpcEndpoint.md[], BlockManagerMasterEndpoint handles RPC messages. === [[BlockManagerHeartbeat]] BlockManagerHeartbeat [source, scala] \u00b6 BlockManagerHeartbeat( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocations]] GetLocations [source, scala] \u00b6 GetLocations( blockId: BlockId) When received, BlockManagerMasterEndpoint replies with the < > of blockId . Posted when BlockManagerMaster.md#getLocations-block[ BlockManagerMaster requests the block locations of a single block]. === [[GetLocationsAndStatus]] GetLocationsAndStatus [source, scala] \u00b6 GetLocationsAndStatus( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocationsMultipleBlockIds]] GetLocationsMultipleBlockIds [source, scala] \u00b6 GetLocationsMultipleBlockIds( blockIds: Array[BlockId]) When received, BlockManagerMasterEndpoint replies with the < > for the given storage:BlockId.md[]. Posted when BlockManagerMaster.md#getLocations[ BlockManagerMaster requests the block locations for multiple blocks]. === [[GetPeers]] GetPeers [source, scala] \u00b6 GetPeers( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint replies with the < > of blockManagerId . Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. Posted when BlockManagerMaster.md#getPeers[ BlockManagerMaster requests the peers of a BlockManager ]. === [[GetExecutorEndpointRef]] GetExecutorEndpointRef [source, scala] \u00b6 GetExecutorEndpointRef( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetMemoryStatus]] GetMemoryStatus [source, scala] \u00b6 GetMemoryStatus \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetStorageStatus]] GetStorageStatus [source, scala] \u00b6 GetStorageStatus \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetBlockStatus]] GetBlockStatus [source, scala] \u00b6 GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint is requested to < >. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds [source, scala] \u00b6 GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[HasCachedBlocks]] HasCachedBlocks [source, scala] \u00b6 HasCachedBlocks( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RegisterBlockManager]] RegisterBlockManager [source,scala] \u00b6 RegisterBlockManager( blockManagerId: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, sender: RpcEndpointRef) When received, BlockManagerMasterEndpoint is requested to < > (by the given storage:BlockManagerId.md[]). Posted when BlockManagerMaster is requested to storage:BlockManagerMaster.md#registerBlockManager[register a BlockManager] === [[RemoveRdd]] RemoveRdd [source, scala] \u00b6 RemoveRdd( rddId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveShuffle]] RemoveShuffle [source, scala] \u00b6 RemoveShuffle( shuffleId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBroadcast]] RemoveBroadcast [source, scala] \u00b6 RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBlock]] RemoveBlock [source, scala] \u00b6 RemoveBlock( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveExecutor]] RemoveExecutor [source, scala] \u00b6 RemoveExecutor( execId: String) When received, BlockManagerMasterEndpoint < execId is removed>> and the response true sent back. Posted when BlockManagerMaster.md#removeExecutor[ BlockManagerMaster removes an executor]. === [[StopBlockManagerMaster]] StopBlockManagerMaster [source, scala] \u00b6 StopBlockManagerMaster \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[UpdateBlockInfo]] UpdateBlockInfo [source, scala] \u00b6 UpdateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long) When received, BlockManagerMasterEndpoint...FIXME Posted when BlockManagerMaster is requested to storage:BlockManagerMaster.md#updateBlockInfo[handle a block status update (from BlockManager on an executor)]. == [[storageStatus]] storageStatus Internal Method [source,scala] \u00b6 storageStatus: Array[StorageStatus] \u00b6 storageStatus...FIXME storageStatus is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getLocationsMultipleBlockIds]] getLocationsMultipleBlockIds Internal Method [source,scala] \u00b6 getLocationsMultipleBlockIds( blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]] getLocationsMultipleBlockIds...FIXME getLocationsMultipleBlockIds is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeShuffle]] removeShuffle Internal Method [source,scala] \u00b6 removeShuffle( shuffleId: Int): Future[Seq[Boolean]] removeShuffle...FIXME removeShuffle is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getPeers]] getPeers Internal Method [source, scala] \u00b6 getPeers( blockManagerId: BlockManagerId): Seq[BlockManagerId] getPeers finds all the registered BlockManagers (using < > internal registry) and checks if the input blockManagerId is amongst them. If the input blockManagerId is registered, getPeers returns all the registered BlockManagers but the one on the driver and blockManagerId . Otherwise, getPeers returns no BlockManagers . NOTE: Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. getPeers is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[register]] register Internal Method [source, scala] \u00b6 register( idWithoutTopologyInfo: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId register registers a storage:BlockManager.md[] (based on the given storage:BlockManagerId.md[]) in the < > and < > registries and posts a SparkListenerBlockManagerAdded message (to the < >). NOTE: The input maxMemSize is the storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. NOTE: Registering a BlockManager can only happen once for an executor (identified by BlockManagerId.executorId in < > internal registry). If another BlockManager has earlier been registered for the executor, you should see the following ERROR message in the logs: [source,plaintext] \u00b6 Got two different block manager registrations on same executor - will replace old one [oldId] with new one [id] \u00b6 And then < >. register prints out the following INFO message to the logs: [source,plaintext] \u00b6 Registering block manager [hostPort] with [bytes] RAM, [id] \u00b6 The BlockManager is recorded in the internal registries: < > < > In the end, register requests the < > to scheduler:LiveListenerBus.md#post[post] a SparkListener.md#SparkListenerBlockManagerAdded[SparkListenerBlockManagerAdded] message. register is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeExecutor]] removeExecutor Internal Method [source, scala] \u00b6 removeExecutor( execId: String): Unit removeExecutor prints the following INFO message to the logs: [source,plaintext] \u00b6 Trying to remove executor [execId] from BlockManagerMaster. \u00b6 If the execId executor is registered (in the internal < > internal registry), removeExecutor < BlockManager >>. removeExecutor is used when BlockManagerMasterEndpoint is requested to handle < > or < > messages. == [[removeBlockManager]] removeBlockManager Internal Method [source, scala] \u00b6 removeBlockManager( blockManagerId: BlockManagerId): Unit removeBlockManager looks up blockManagerId and removes the executor it was working on from the internal registries: < > < > It then goes over all the blocks for the BlockManager , and removes the executor for each block from blockLocations registry. SparkListener.md#SparkListenerBlockManagerRemoved[SparkListenerBlockManagerRemoved(System.currentTimeMillis(), blockManagerId)] is posted to SparkContext.md#listenerBus[listenerBus]. You should then see the following INFO message in the logs: [source,plaintext] \u00b6 Removing block manager [blockManagerId] \u00b6 removeBlockManager is used when BlockManagerMasterEndpoint is requested to < > (to handle < > or < > messages). == [[getLocations]] getLocations Internal Method [source, scala] \u00b6 getLocations( blockId: BlockId): Seq[BlockManagerId] getLocations looks up the given storage:BlockId.md[] in the blockLocations internal registry and returns the locations (as a collection of BlockManagerId ) or an empty collection. getLocations is used when BlockManagerMasterEndpoint is requested to handle < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockManagerMasterEndpoint=ALL \u00b6 Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[blockManagerIdByExecutor]] blockManagerIdByExecutor Lookup Table [source,scala] \u00b6 blockManagerIdByExecutor: Map[String, BlockManagerId] \u00b6 Lookup table of storage:BlockManagerId.md[]s by executor ID A new executor is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). An executor is removed when BlockManagerMasterEndpoint is requested to handle a < > and a < > messages (via < >) Used when BlockManagerMasterEndpoint is requested to handle < > message, < >, < > and < >. === [[blockManagerInfo]] blockManagerInfo Lookup Table [source,scala] \u00b6 blockManagerIdByExecutor: Map[String, BlockManagerId] \u00b6 Lookup table of storage:BlockManagerInfo.md[] by storage:BlockManagerId.md[] A new BlockManagerInfo is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). A BlockManagerInfo is removed when BlockManagerMasterEndpoint is requested to < > (to handle < > and < > messages). === [[blockLocations]] blockLocations [source,scala] \u00b6 blockLocations: Map[BlockId, Set[BlockManagerId]] \u00b6 Collection of storage:BlockId.md[] and their locations (as BlockManagerId ). Used in removeRdd to remove blocks for a RDD, removeBlockManager to remove blocks after a BlockManager gets removed, removeBlockFromWorkers , updateBlockInfo , and < >.","title":"BlockManagerMasterEndpoint"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanagermasterendpoint","text":"BlockManagerMasterEndpoint is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] for storage:BlockManagerMaster.md[BlockManagerMaster]. BlockManagerMasterEndpoint is registered under BlockManagerMaster name. BlockManagerMasterEndpoint tracks status of the storage:BlockManager.md[BlockManagers] (on the executors) in a Spark application. == [[creating-instance]] Creating Instance BlockManagerMasterEndpoint takes the following to be created: [[rpcEnv]] rpc:RpcEnv.md[] [[isLocal]] Flag whether BlockManagerMasterEndpoint works in local or cluster mode [[conf]] SparkConf.md[] [[listenerBus]] scheduler:LiveListenerBus.md[] BlockManagerMasterEndpoint is created for the core:SparkEnv.md#create[SparkEnv] on the driver (to create a storage:BlockManagerMaster.md[] for a storage:BlockManager.md#master[BlockManager]). When created, BlockManagerMasterEndpoint prints out the following INFO message to the logs:","title":"BlockManagerMasterEndpoint"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanagermasterendpoint-up","text":"== [[messages]][[receiveAndReply]] Messages As an rpc:RpcEndpoint.md[], BlockManagerMasterEndpoint handles RPC messages. === [[BlockManagerHeartbeat]] BlockManagerHeartbeat","title":"BlockManagerMasterEndpoint up"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala","text":"BlockManagerHeartbeat( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocations]] GetLocations","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_1","text":"GetLocations( blockId: BlockId) When received, BlockManagerMasterEndpoint replies with the < > of blockId . Posted when BlockManagerMaster.md#getLocations-block[ BlockManagerMaster requests the block locations of a single block]. === [[GetLocationsAndStatus]] GetLocationsAndStatus","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_2","text":"GetLocationsAndStatus( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocationsMultipleBlockIds]] GetLocationsMultipleBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_3","text":"GetLocationsMultipleBlockIds( blockIds: Array[BlockId]) When received, BlockManagerMasterEndpoint replies with the < > for the given storage:BlockId.md[]. Posted when BlockManagerMaster.md#getLocations[ BlockManagerMaster requests the block locations for multiple blocks]. === [[GetPeers]] GetPeers","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_4","text":"GetPeers( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint replies with the < > of blockManagerId . Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. Posted when BlockManagerMaster.md#getPeers[ BlockManagerMaster requests the peers of a BlockManager ]. === [[GetExecutorEndpointRef]] GetExecutorEndpointRef","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_5","text":"GetExecutorEndpointRef( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetMemoryStatus]] GetMemoryStatus","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#getmemorystatus","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetStorageStatus]] GetStorageStatus","title":"GetMemoryStatus"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_7","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#getstoragestatus","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetBlockStatus]] GetBlockStatus","title":"GetStorageStatus"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_8","text":"GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint is requested to < >. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_9","text":"GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[HasCachedBlocks]] HasCachedBlocks","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_10","text":"HasCachedBlocks( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RegisterBlockManager]] RegisterBlockManager","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala","text":"RegisterBlockManager( blockManagerId: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, sender: RpcEndpointRef) When received, BlockManagerMasterEndpoint is requested to < > (by the given storage:BlockManagerId.md[]). Posted when BlockManagerMaster is requested to storage:BlockManagerMaster.md#registerBlockManager[register a BlockManager] === [[RemoveRdd]] RemoveRdd","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_11","text":"RemoveRdd( rddId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveShuffle]] RemoveShuffle","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_12","text":"RemoveShuffle( shuffleId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBroadcast]] RemoveBroadcast","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_13","text":"RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBlock]] RemoveBlock","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_14","text":"RemoveBlock( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveExecutor]] RemoveExecutor","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_15","text":"RemoveExecutor( execId: String) When received, BlockManagerMasterEndpoint < execId is removed>> and the response true sent back. Posted when BlockManagerMaster.md#removeExecutor[ BlockManagerMaster removes an executor]. === [[StopBlockManagerMaster]] StopBlockManagerMaster","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#stopblockmanagermaster","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[UpdateBlockInfo]] UpdateBlockInfo","title":"StopBlockManagerMaster"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_17","text":"UpdateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long) When received, BlockManagerMasterEndpoint...FIXME Posted when BlockManagerMaster is requested to storage:BlockManagerMaster.md#updateBlockInfo[handle a block status update (from BlockManager on an executor)]. == [[storageStatus]] storageStatus Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#storagestatus-arraystoragestatus","text":"storageStatus...FIXME storageStatus is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getLocationsMultipleBlockIds]] getLocationsMultipleBlockIds Internal Method","title":"storageStatus: Array[StorageStatus]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_2","text":"getLocationsMultipleBlockIds( blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]] getLocationsMultipleBlockIds...FIXME getLocationsMultipleBlockIds is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeShuffle]] removeShuffle Internal Method","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_3","text":"removeShuffle( shuffleId: Int): Future[Seq[Boolean]] removeShuffle...FIXME removeShuffle is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getPeers]] getPeers Internal Method","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_18","text":"getPeers( blockManagerId: BlockManagerId): Seq[BlockManagerId] getPeers finds all the registered BlockManagers (using < > internal registry) and checks if the input blockManagerId is amongst them. If the input blockManagerId is registered, getPeers returns all the registered BlockManagers but the one on the driver and blockManagerId . Otherwise, getPeers returns no BlockManagers . NOTE: Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. getPeers is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[register]] register Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_19","text":"register( idWithoutTopologyInfo: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId register registers a storage:BlockManager.md[] (based on the given storage:BlockManagerId.md[]) in the < > and < > registries and posts a SparkListenerBlockManagerAdded message (to the < >). NOTE: The input maxMemSize is the storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. NOTE: Registering a BlockManager can only happen once for an executor (identified by BlockManagerId.executorId in < > internal registry). If another BlockManager has earlier been registered for the executor, you should see the following ERROR message in the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#got-two-different-block-manager-registrations-on-same-executor-will-replace-old-one-oldid-with-new-one-id","text":"And then < >. register prints out the following INFO message to the logs:","title":"Got two different block manager registrations on same executor - will replace old one [oldId] with new one [id]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#registering-block-manager-hostport-with-bytes-ram-id","text":"The BlockManager is recorded in the internal registries: < > < > In the end, register requests the < > to scheduler:LiveListenerBus.md#post[post] a SparkListener.md#SparkListenerBlockManagerAdded[SparkListenerBlockManagerAdded] message. register is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeExecutor]] removeExecutor Internal Method","title":"Registering block manager [hostPort] with [bytes] RAM, [id]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_20","text":"removeExecutor( execId: String): Unit removeExecutor prints the following INFO message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#trying-to-remove-executor-execid-from-blockmanagermaster","text":"If the execId executor is registered (in the internal < > internal registry), removeExecutor < BlockManager >>. removeExecutor is used when BlockManagerMasterEndpoint is requested to handle < > or < > messages. == [[removeBlockManager]] removeBlockManager Internal Method","title":"Trying to remove executor [execId] from BlockManagerMaster."},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_21","text":"removeBlockManager( blockManagerId: BlockManagerId): Unit removeBlockManager looks up blockManagerId and removes the executor it was working on from the internal registries: < > < > It then goes over all the blocks for the BlockManager , and removes the executor for each block from blockLocations registry. SparkListener.md#SparkListenerBlockManagerRemoved[SparkListenerBlockManagerRemoved(System.currentTimeMillis(), blockManagerId)] is posted to SparkContext.md#listenerBus[listenerBus]. You should then see the following INFO message in the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#removing-block-manager-blockmanagerid","text":"removeBlockManager is used when BlockManagerMasterEndpoint is requested to < > (to handle < > or < > messages). == [[getLocations]] getLocations Internal Method","title":"Removing block manager [blockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_22","text":"getLocations( blockId: BlockId): Seq[BlockManagerId] getLocations looks up the given storage:BlockId.md[] in the blockLocations internal registry and returns the locations (as a collection of BlockManagerId ) or an empty collection. getLocations is used when BlockManagerMasterEndpoint is requested to handle < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source","text":"","title":"[source]"},{"location":"storage/BlockManagerMasterEndpoint/#log4jloggerorgapachesparkstorageblockmanagermasterendpointall","text":"Refer to spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[blockManagerIdByExecutor]] blockManagerIdByExecutor Lookup Table","title":"log4j.logger.org.apache.spark.storage.BlockManagerMasterEndpoint=ALL"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_4","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanageridbyexecutor-mapstring-blockmanagerid","text":"Lookup table of storage:BlockManagerId.md[]s by executor ID A new executor is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). An executor is removed when BlockManagerMasterEndpoint is requested to handle a < > and a < > messages (via < >) Used when BlockManagerMasterEndpoint is requested to handle < > message, < >, < > and < >. === [[blockManagerInfo]] blockManagerInfo Lookup Table","title":"blockManagerIdByExecutor: Map[String, BlockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_5","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanageridbyexecutor-mapstring-blockmanagerid_1","text":"Lookup table of storage:BlockManagerInfo.md[] by storage:BlockManagerId.md[] A new BlockManagerInfo is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). A BlockManagerInfo is removed when BlockManagerMasterEndpoint is requested to < > (to handle < > and < > messages). === [[blockLocations]] blockLocations","title":"blockManagerIdByExecutor: Map[String, BlockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_6","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blocklocations-mapblockid-setblockmanagerid","text":"Collection of storage:BlockId.md[] and their locations (as BlockManagerId ). Used in removeRdd to remove blocks for a RDD, removeBlockManager to remove blocks after a BlockManager gets removed, removeBlockFromWorkers , updateBlockInfo , and < >.","title":"blockLocations: Map[BlockId, Set[BlockManagerId]]"},{"location":"storage/BlockManagerMasterHeartbeatEndpoint/","text":"BlockManagerMasterHeartbeatEndpoint \u00b6 BlockManagerMasterHeartbeatEndpoint is...FIXME","title":"BlockManagerMasterHeartbeatEndpoint"},{"location":"storage/BlockManagerMasterHeartbeatEndpoint/#blockmanagermasterheartbeatendpoint","text":"BlockManagerMasterHeartbeatEndpoint is...FIXME","title":"BlockManagerMasterHeartbeatEndpoint"},{"location":"storage/BlockManagerSlaveEndpoint/","text":"BlockManagerSlaveEndpoint \u00b6 BlockManagerSlaveEndpoint is a ThreadSafeRpcEndpoint for BlockManager . Creating Instance \u00b6 BlockManagerSlaveEndpoint takes the following to be created: [[rpcEnv]] rpc:RpcEnv.md[] [[blockManager]] Parent BlockManager.md[] [[mapOutputTracker]] scheduler:MapOutputTracker.md[] BlockManagerSlaveEndpoint is created for BlockManager.md#slaveEndpoint[BlockManager] (and registered under the name BlockManagerEndpoint[ID] ). == [[messages]] Messages === [[GetBlockStatus]] GetBlockStatus [source, scala] \u00b6 GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > for the BlockManager.md#getStatus[status of a given block] (by BlockId.md[]) and sends it back to a sender. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds [source, scala] \u00b6 GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > to storage:BlockManager.md#getMatchingBlockIds[find IDs of existing blocks for a given filter] and sends them back to a sender. Posted when...FIXME === [[RemoveBlock]] RemoveBlock [source, scala] \u00b6 RemoveBlock( blockId: BlockId) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 removing block [blockId] \u00b6 BlockManagerSlaveEndpoint then < blockId block>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing block [blockId], response is [response] And true response is sent back. You should see the following DEBUG in the logs: Sent response: true to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing block [blockId] === [[RemoveBroadcast]] RemoveBroadcast [source, scala] \u00b6 RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 removing broadcast [broadcastId] \u00b6 It then calls < broadcastId broadcast>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing broadcast [broadcastId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing broadcast [broadcastId] === [[RemoveRdd]] RemoveRdd [source, scala] \u00b6 RemoveRdd( rddId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing RDD [rddId] It then calls < rddId RDD>>. NOTE: Handling RemoveRdd messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing RDD [rddId], response is [response] And the number of blocks removed is sent back. You should see the following DEBUG in the logs: Sent response: [#blocks] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing RDD [rddId] === [[RemoveShuffle]] RemoveShuffle [source, scala] \u00b6 RemoveShuffle( shuffleId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing shuffle [shuffleId] If scheduler:MapOutputTracker.md[MapOutputTracker] was given (when the RPC endpoint was created), it calls scheduler:MapOutputTracker.md#unregisterShuffle[MapOutputTracker to unregister the shuffleId shuffle]. It then calls shuffle:ShuffleManager.md#unregisterShuffle[ShuffleManager to unregister the shuffleId shuffle]. NOTE: Handling RemoveShuffle messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing shuffle [shuffleId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing shuffle [shuffleId] Posted when BlockManagerMaster.md#removeShuffle[BlockManagerMaster] and storage:BlockManagerMasterEndpoint.md#removeShuffle[BlockManagerMasterEndpoint] are requested to remove all blocks of a shuffle. === [[ReplicateBlock]] ReplicateBlock [source, scala] \u00b6 ReplicateBlock( blockId: BlockId, replicas: Seq[BlockManagerId], maxReplicas: Int) When received, BlockManagerSlaveEndpoint...FIXME Posted when...FIXME === [[TriggerThreadDump]] TriggerThreadDump When received, BlockManagerSlaveEndpoint is requested for the thread info for all live threads with stack trace and synchronization information. == [[asyncThreadPool]][[asyncExecutionContext]] block-manager-slave-async-thread-pool Thread Pool BlockManagerSlaveEndpoint creates a thread pool of maximum 100 daemon threads with block-manager-slave-async-thread-pool thread prefix (using {java-javadoc-url}/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor]). BlockManagerSlaveEndpoint uses the thread pool (as a Scala implicit value) when requested to < > to communicate in a non-blocking, asynchronous way. The thread pool is shut down when BlockManagerSlaveEndpoint is requested to < >. The reason for the async thread pool is that the block-related operations might take quite some time and to release the main RPC thread other threads are spawned to talk to the external services and pass responses on to the clients. == [[doAsync]] doAsync Internal Method [source,scala] \u00b6 doAsync T ( body: => T) doAsync creates a Scala Future to execute the following asynchronously (i.e. on a separate thread from the < >): . Prints out the given actionMessage as a DEBUG message to the logs . Executes the given body When completed successfully, doAsync prints out the following DEBUG messages to the logs and requests the given RpcCallContext to reply the response to the sender. [source,plaintext] \u00b6 Done [actionMessage], response is [response] Sent response: [response] to [senderAddress] In case of a failure, doAsync prints out the following ERROR message to the logs and requests the given RpcCallContext to send the failure to the sender. [source,plaintext] \u00b6 Error in [actionMessage] \u00b6 doAsync is used when BlockManagerSlaveEndpoint is requested to handle < >, < >, < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerSlaveEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=ALL \u00b6 Refer to spark-logging.md[Logging].","title":"BlockManagerSlaveEndpoint"},{"location":"storage/BlockManagerSlaveEndpoint/#blockmanagerslaveendpoint","text":"BlockManagerSlaveEndpoint is a ThreadSafeRpcEndpoint for BlockManager .","title":"BlockManagerSlaveEndpoint"},{"location":"storage/BlockManagerSlaveEndpoint/#creating-instance","text":"BlockManagerSlaveEndpoint takes the following to be created: [[rpcEnv]] rpc:RpcEnv.md[] [[blockManager]] Parent BlockManager.md[] [[mapOutputTracker]] scheduler:MapOutputTracker.md[] BlockManagerSlaveEndpoint is created for BlockManager.md#slaveEndpoint[BlockManager] (and registered under the name BlockManagerEndpoint[ID] ). == [[messages]] Messages === [[GetBlockStatus]] GetBlockStatus","title":"Creating Instance"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala","text":"GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > for the BlockManager.md#getStatus[status of a given block] (by BlockId.md[]) and sends it back to a sender. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_1","text":"GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > to storage:BlockManager.md#getMatchingBlockIds[find IDs of existing blocks for a given filter] and sends them back to a sender. Posted when...FIXME === [[RemoveBlock]] RemoveBlock","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_2","text":"RemoveBlock( blockId: BlockId) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#removing-block-blockid","text":"BlockManagerSlaveEndpoint then < blockId block>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing block [blockId], response is [response] And true response is sent back. You should see the following DEBUG in the logs: Sent response: true to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing block [blockId] === [[RemoveBroadcast]] RemoveBroadcast","title":"removing block [blockId]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_3","text":"RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#removing-broadcast-broadcastid","text":"It then calls < broadcastId broadcast>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing broadcast [broadcastId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing broadcast [broadcastId] === [[RemoveRdd]] RemoveRdd","title":"removing broadcast [broadcastId]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_4","text":"RemoveRdd( rddId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing RDD [rddId] It then calls < rddId RDD>>. NOTE: Handling RemoveRdd messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing RDD [rddId], response is [response] And the number of blocks removed is sent back. You should see the following DEBUG in the logs: Sent response: [#blocks] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing RDD [rddId] === [[RemoveShuffle]] RemoveShuffle","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_5","text":"RemoveShuffle( shuffleId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing shuffle [shuffleId] If scheduler:MapOutputTracker.md[MapOutputTracker] was given (when the RPC endpoint was created), it calls scheduler:MapOutputTracker.md#unregisterShuffle[MapOutputTracker to unregister the shuffleId shuffle]. It then calls shuffle:ShuffleManager.md#unregisterShuffle[ShuffleManager to unregister the shuffleId shuffle]. NOTE: Handling RemoveShuffle messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing shuffle [shuffleId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing shuffle [shuffleId] Posted when BlockManagerMaster.md#removeShuffle[BlockManagerMaster] and storage:BlockManagerMasterEndpoint.md#removeShuffle[BlockManagerMasterEndpoint] are requested to remove all blocks of a shuffle. === [[ReplicateBlock]] ReplicateBlock","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_6","text":"ReplicateBlock( blockId: BlockId, replicas: Seq[BlockManagerId], maxReplicas: Int) When received, BlockManagerSlaveEndpoint...FIXME Posted when...FIXME === [[TriggerThreadDump]] TriggerThreadDump When received, BlockManagerSlaveEndpoint is requested for the thread info for all live threads with stack trace and synchronization information. == [[asyncThreadPool]][[asyncExecutionContext]] block-manager-slave-async-thread-pool Thread Pool BlockManagerSlaveEndpoint creates a thread pool of maximum 100 daemon threads with block-manager-slave-async-thread-pool thread prefix (using {java-javadoc-url}/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor]). BlockManagerSlaveEndpoint uses the thread pool (as a Scala implicit value) when requested to < > to communicate in a non-blocking, asynchronous way. The thread pool is shut down when BlockManagerSlaveEndpoint is requested to < >. The reason for the async thread pool is that the block-related operations might take quite some time and to release the main RPC thread other threads are spawned to talk to the external services and pass responses on to the clients. == [[doAsync]] doAsync Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourcescala","text":"doAsync T ( body: => T) doAsync creates a Scala Future to execute the following asynchronously (i.e. on a separate thread from the < >): . Prints out the given actionMessage as a DEBUG message to the logs . Executes the given body When completed successfully, doAsync prints out the following DEBUG messages to the logs and requests the given RpcCallContext to reply the response to the sender.","title":"[source,scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_2","text":"Done [actionMessage], response is [response] Sent response: [response] to [senderAddress] In case of a failure, doAsync prints out the following ERROR message to the logs and requests the given RpcCallContext to send the failure to the sender.","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#error-in-actionmessage","text":"doAsync is used when BlockManagerSlaveEndpoint is requested to handle < >, < >, < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerSlaveEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Error in [actionMessage]"},{"location":"storage/BlockManagerSlaveEndpoint/#source","text":"","title":"[source]"},{"location":"storage/BlockManagerSlaveEndpoint/#log4jloggerorgapachesparkstorageblockmanagerslaveendpointall","text":"Refer to spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=ALL"},{"location":"storage/BlockManagerSource/","text":"BlockManagerSource -- Metrics Source for BlockManager \u00b6 BlockManagerSource is the spark-metrics-Source.md[metrics source] of a storage:BlockManager.md[BlockManager]. [[sourceName]] BlockManagerSource is registered under the name BlockManager (when SparkContext is created). [[metrics]] .BlockManagerSource's Gauge Metrics (in alphabetical order) [width=\"100%\",cols=\"1,1,2\",options=\"header\"] |=== | Name | Type | Description | disk.diskSpaceUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their disk space used ( diskUsed ). | memory.maxMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their maximum memory limit ( maxMem ). | memory.maxOffHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory remaining ( maxOffHeapMem ). | memory.maxOnHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory remaining ( maxOnHeapMem ). | memory.memUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their memory used ( memUsed ). | memory.offHeapMemUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory used ( offHeapMemUsed ). | memory.onHeapMemUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory used ( onHeapMemUsed ). | memory.remainingMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their memory remaining ( memRemaining ). | memory.remainingOffHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory remaining ( offHeapMemRemaining ). | memory.remainingOnHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory remaining ( onHeapMemRemaining ). |=== You can access the BlockManagerSource < > using the web UI's port (as spark-webui-properties.md#spark.ui.port[spark.ui.port] configuration property). $ http --follow http://localhost:4040/metrics/json \\ | jq '.gauges | keys | .[] | select(test(\".driver.BlockManager\"; \"g\"))' \"local-1528725411625.driver.BlockManager.disk.diskSpaceUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.maxMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOnHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.memUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.offHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.onHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOnHeapMem_MB\" [[creating-instance]] [[blockManager]] BlockManagerSource takes a storage:BlockManager.md[BlockManager] when created. BlockManagerSource is created when SparkContext is created.","title":"BlockManagerSource"},{"location":"storage/BlockManagerSource/#blockmanagersource-metrics-source-for-blockmanager","text":"BlockManagerSource is the spark-metrics-Source.md[metrics source] of a storage:BlockManager.md[BlockManager]. [[sourceName]] BlockManagerSource is registered under the name BlockManager (when SparkContext is created). [[metrics]] .BlockManagerSource's Gauge Metrics (in alphabetical order) [width=\"100%\",cols=\"1,1,2\",options=\"header\"] |=== | Name | Type | Description | disk.diskSpaceUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their disk space used ( diskUsed ). | memory.maxMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their maximum memory limit ( maxMem ). | memory.maxOffHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory remaining ( maxOffHeapMem ). | memory.maxOnHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory remaining ( maxOnHeapMem ). | memory.memUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their memory used ( memUsed ). | memory.offHeapMemUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory used ( offHeapMemUsed ). | memory.onHeapMemUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory used ( onHeapMemUsed ). | memory.remainingMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their memory remaining ( memRemaining ). | memory.remainingOffHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory remaining ( offHeapMemRemaining ). | memory.remainingOnHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory remaining ( onHeapMemRemaining ). |=== You can access the BlockManagerSource < > using the web UI's port (as spark-webui-properties.md#spark.ui.port[spark.ui.port] configuration property). $ http --follow http://localhost:4040/metrics/json \\ | jq '.gauges | keys | .[] | select(test(\".driver.BlockManager\"; \"g\"))' \"local-1528725411625.driver.BlockManager.disk.diskSpaceUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.maxMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOnHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.memUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.offHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.onHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOnHeapMem_MB\" [[creating-instance]] [[blockManager]] BlockManagerSource takes a storage:BlockManager.md[BlockManager] when created. BlockManagerSource is created when SparkContext is created.","title":"BlockManagerSource -- Metrics Source for BlockManager"},{"location":"storage/BlockManagerStorageEndpoint/","text":"BlockManagerStorageEndpoint \u00b6 BlockManagerStorageEndpoint is an IsolatedRpcEndpoint . Creating Instance \u00b6 BlockManagerStorageEndpoint takes the following to be created: RpcEnv BlockManager MapOutputTracker BlockManagerStorageEndpoint is created when: BlockManager is created Messages \u00b6 DecommissionBlockManager \u00b6 When received, receiveAndReply requests the BlockManager to decommissionSelf . DecommissionBlockManager is sent out when BlockManager is requested to decommissionBlockManager .","title":"BlockManagerStorageEndpoint"},{"location":"storage/BlockManagerStorageEndpoint/#blockmanagerstorageendpoint","text":"BlockManagerStorageEndpoint is an IsolatedRpcEndpoint .","title":"BlockManagerStorageEndpoint"},{"location":"storage/BlockManagerStorageEndpoint/#creating-instance","text":"BlockManagerStorageEndpoint takes the following to be created: RpcEnv BlockManager MapOutputTracker BlockManagerStorageEndpoint is created when: BlockManager is created","title":"Creating Instance"},{"location":"storage/BlockManagerStorageEndpoint/#messages","text":"","title":"Messages"},{"location":"storage/BlockManagerStorageEndpoint/#decommissionblockmanager","text":"When received, receiveAndReply requests the BlockManager to decommissionSelf . DecommissionBlockManager is sent out when BlockManager is requested to decommissionBlockManager .","title":" DecommissionBlockManager"},{"location":"storage/BlockReplicationPolicy/","text":"BlockReplicationPolicy \u00b6 BlockReplicationPolicy is...FIXME","title":"BlockReplicationPolicy"},{"location":"storage/BlockReplicationPolicy/#blockreplicationpolicy","text":"BlockReplicationPolicy is...FIXME","title":"BlockReplicationPolicy"},{"location":"storage/BlockStoreClient/","text":"BlockStoreClient \u00b6 BlockStoreClient is an abstraction of block clients that can fetch blocks from a remote node (an executor or an external service). BlockStoreClient is a Java Closeable . Note BlockStoreClient was known previously as ShuffleClient ( SPARK-28593 ). Contract \u00b6 Fetching Blocks \u00b6 void fetchBlocks ( String host , int port , String execId , String [] blockIds , BlockFetchingListener listener , DownloadFileManager downloadFileManager ) Fetches blocks from a remote node (using DownloadFileManager ) Used when: BlockTransferService is requested to fetchBlockSync ShuffleBlockFetcherIterator is requested to sendRequest Shuffle Metrics \u00b6 MetricSet shuffleMetrics () Shuffle MetricsSet Default: (empty) Used when: BlockManager is requested for the Shuffle Metrics Source Implementations \u00b6 BlockTransferService ExternalBlockStoreClient","title":"BlockStoreClient"},{"location":"storage/BlockStoreClient/#blockstoreclient","text":"BlockStoreClient is an abstraction of block clients that can fetch blocks from a remote node (an executor or an external service). BlockStoreClient is a Java Closeable . Note BlockStoreClient was known previously as ShuffleClient ( SPARK-28593 ).","title":"BlockStoreClient"},{"location":"storage/BlockStoreClient/#contract","text":"","title":"Contract"},{"location":"storage/BlockStoreClient/#fetching-blocks","text":"void fetchBlocks ( String host , int port , String execId , String [] blockIds , BlockFetchingListener listener , DownloadFileManager downloadFileManager ) Fetches blocks from a remote node (using DownloadFileManager ) Used when: BlockTransferService is requested to fetchBlockSync ShuffleBlockFetcherIterator is requested to sendRequest","title":" Fetching Blocks"},{"location":"storage/BlockStoreClient/#shuffle-metrics","text":"MetricSet shuffleMetrics () Shuffle MetricsSet Default: (empty) Used when: BlockManager is requested for the Shuffle Metrics Source","title":" Shuffle Metrics"},{"location":"storage/BlockStoreClient/#implementations","text":"BlockTransferService ExternalBlockStoreClient","title":"Implementations"},{"location":"storage/BlockStoreUpdater/","text":"BlockStoreUpdater \u00b6 BlockStoreUpdater is an abstraction of block store updaters that store blocks (from bytes, whether they start in memory or on disk). BlockStoreUpdater is an internal class of BlockManager . Contract \u00b6 blockData \u00b6 blockData (): BlockData BlockData Used when: BlockStoreUpdater is requested to save TempFileBasedBlockStoreUpdater is requested to readToByteBuffer readToByteBuffer \u00b6 readToByteBuffer (): ChunkedByteBuffer Used when: BlockStoreUpdater is requested to save saveToDiskStore \u00b6 saveToDiskStore (): Unit Used when: BlockStoreUpdater is requested to save Implementations \u00b6 ByteBufferBlockStoreUpdater TempFileBasedBlockStoreUpdater Creating Instance \u00b6 BlockStoreUpdater takes the following to be created: Block Size BlockId StorageLevel Scala's ClassTag tellMaster flag keepReadLock flag Abstract Class BlockStoreUpdater is an abstract class and cannot be created directly. It is created indirectly for the concrete BlockStoreUpdaters . Storing Block Data \u00b6 save (): Boolean save ...FIXME save is used when: BlockManager is requested to putBlockDataAsStream and putBytes","title":"BlockStoreUpdater"},{"location":"storage/BlockStoreUpdater/#blockstoreupdater","text":"BlockStoreUpdater is an abstraction of block store updaters that store blocks (from bytes, whether they start in memory or on disk). BlockStoreUpdater is an internal class of BlockManager .","title":"BlockStoreUpdater"},{"location":"storage/BlockStoreUpdater/#contract","text":"","title":"Contract"},{"location":"storage/BlockStoreUpdater/#blockdata","text":"blockData (): BlockData BlockData Used when: BlockStoreUpdater is requested to save TempFileBasedBlockStoreUpdater is requested to readToByteBuffer","title":" blockData"},{"location":"storage/BlockStoreUpdater/#readtobytebuffer","text":"readToByteBuffer (): ChunkedByteBuffer Used when: BlockStoreUpdater is requested to save","title":" readToByteBuffer"},{"location":"storage/BlockStoreUpdater/#savetodiskstore","text":"saveToDiskStore (): Unit Used when: BlockStoreUpdater is requested to save","title":" saveToDiskStore"},{"location":"storage/BlockStoreUpdater/#implementations","text":"ByteBufferBlockStoreUpdater TempFileBasedBlockStoreUpdater","title":"Implementations"},{"location":"storage/BlockStoreUpdater/#creating-instance","text":"BlockStoreUpdater takes the following to be created: Block Size BlockId StorageLevel Scala's ClassTag tellMaster flag keepReadLock flag Abstract Class BlockStoreUpdater is an abstract class and cannot be created directly. It is created indirectly for the concrete BlockStoreUpdaters .","title":"Creating Instance"},{"location":"storage/BlockStoreUpdater/#storing-block-data","text":"save (): Boolean save ...FIXME save is used when: BlockManager is requested to putBlockDataAsStream and putBytes","title":" Storing Block Data"},{"location":"storage/BlockTransferService/","text":"BlockTransferService \u00b6 BlockTransferService is an extension of the BlockStoreClient abstraction for shuffle clients that can fetch and upload blocks of data (synchronously or asynchronously). BlockTransferService is a network service available by a host name and a port . BlockTransferService was introduced in SPARK-3019 Pluggable block transfer interface (BlockTransferService) . Contract \u00b6 Host Name \u00b6 hostName : String Host name this service is listening on Used when: BlockManager is requested to initialize Initializing \u00b6 init ( blockDataManager : BlockDataManager ): Unit Used when: BlockManager is requested to initialize Port \u00b6 port : Int Used when: BlockManager is requested to initialize Uploading Block Asynchronously \u00b6 uploadBlock ( hostname : String , port : Int , execId : String , blockId : BlockId , blockData : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Future [ Unit ] Used when: BlockTransferService is requested to uploadBlockSync Implementations \u00b6 NettyBlockTransferService Uploading Block Synchronously \u00b6 uploadBlockSync ( hostname : String , port : Int , execId : String , blockId : BlockId , blockData : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Unit uploadBlockSync uploadBlock and waits till it finishes. uploadBlockSync is used when: BlockManager is requested to replicate ShuffleMigrationRunnable is requested to run","title":"BlockTransferService"},{"location":"storage/BlockTransferService/#blocktransferservice","text":"BlockTransferService is an extension of the BlockStoreClient abstraction for shuffle clients that can fetch and upload blocks of data (synchronously or asynchronously). BlockTransferService is a network service available by a host name and a port . BlockTransferService was introduced in SPARK-3019 Pluggable block transfer interface (BlockTransferService) .","title":"BlockTransferService"},{"location":"storage/BlockTransferService/#contract","text":"","title":"Contract"},{"location":"storage/BlockTransferService/#host-name","text":"hostName : String Host name this service is listening on Used when: BlockManager is requested to initialize","title":" Host Name"},{"location":"storage/BlockTransferService/#initializing","text":"init ( blockDataManager : BlockDataManager ): Unit Used when: BlockManager is requested to initialize","title":" Initializing"},{"location":"storage/BlockTransferService/#port","text":"port : Int Used when: BlockManager is requested to initialize","title":" Port"},{"location":"storage/BlockTransferService/#uploading-block-asynchronously","text":"uploadBlock ( hostname : String , port : Int , execId : String , blockId : BlockId , blockData : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Future [ Unit ] Used when: BlockTransferService is requested to uploadBlockSync","title":" Uploading Block Asynchronously"},{"location":"storage/BlockTransferService/#implementations","text":"NettyBlockTransferService","title":"Implementations"},{"location":"storage/BlockTransferService/#uploading-block-synchronously","text":"uploadBlockSync ( hostname : String , port : Int , execId : String , blockId : BlockId , blockData : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Unit uploadBlockSync uploadBlock and waits till it finishes. uploadBlockSync is used when: BlockManager is requested to replicate ShuffleMigrationRunnable is requested to run","title":" Uploading Block Synchronously"},{"location":"storage/DiskBlockManager/","text":"DiskBlockManager \u00b6 DiskBlockManager manages a logical mapping of logical blocks and their physical on-disk locations for a BlockManager . By default, one block is mapped to one file with a name given by its BlockId . It is however possible to have a block map to only a segment of a file. Block files are hashed among the local directories . DiskBlockManager is used to create a DiskStore . Tip Consult Demo: DiskBlockManager and Block Data . Creating Instance \u00b6 DiskBlockManager takes the following to be created: SparkConf deleteFilesOnStop flag When created, DiskBlockManager creates one or many local directories to store block data and initializes the internal subDirs collection of locks for every local directory. In the end, DiskBlockManager registers a shutdown hook to clean up the local directories for blocks. DiskBlockManager is created for BlockManager . Local Directories for Blocks \u00b6 localDirs : Array [ File ] While being created, DiskBlockManager creates local directories for block data. DiskBlockManager expects at least one local directory or prints out the following ERROR message to the logs and exits the JVM (with exit code 53): Failed to create any local dir. localDirs is used when: DiskBlockManager is requested to getFile , initialize the subDirs internal registry, and to doStop BlockManager is requested to register with an external shuffle server Creating Local Directories \u00b6 createLocalDirs ( conf : SparkConf ): Array [ File ] createLocalDirs creates blockmgr-[random UUID] directory under local directories to store block data. Internally, createLocalDirs finds the configured local directories where Spark can write files and creates a subdirectory blockmgr-[UUID] under every configured parent directory. For every local directory, createLocalDirs prints out the following INFO message to the logs: Created local directory at [localDir] In case of an exception, createLocalDirs prints out the following ERROR message to the logs and skips the directory: Failed to create local dir in [rootDir]. Ignoring this directory. File Locks for Local Block Store Directories \u00b6 subDirs : Array [ Array [ File ]] subDirs is a lookup table for file locks of every local block directory (with the first dimension for local directories and the second for locks). The number of block subdirectories is controlled by spark.diskStore.subDirectories configuration property. subDirs(dirId)(subDirId) is used to access subDirId subdirectory in dirId local directory. subDirs is used when DiskBlockManager is requested for a block file or all block files . Finding Block File (and Creating Parent Directories) \u00b6 getFile ( blockId : BlockId ): File getFile ( filename : String ): File getFile computes a hash of the file name of the input BlockId that is used for the name of the parent directory and subdirectory. getFile creates the subdirectory unless it already exists. getFile is used when: DiskBlockManager is requested to containsBlock , createTempLocalBlock , createTempShuffleBlock DiskStore is requested to getBytes , remove , contains , and put IndexShuffleBlockResolver is requested to getDataFile and getIndexFile createTempShuffleBlock \u00b6 createTempShuffleBlock (): ( TempShuffleBlockId , File ) createTempShuffleBlock creates a temporary TempShuffleBlockId block. createTempShuffleBlock ...FIXME Registering Shutdown Hook \u00b6 addShutdownHook (): AnyRef addShutdownHook registers a shutdown hook to execute doStop at shutdown. When executed, you should see the following DEBUG message in the logs: Adding shutdown hook addShutdownHook adds the shutdown hook so it prints the following INFO message and executes doStop : Shutdown hook called Getting Local Directories for Spark to Write Files \u00b6 getConfiguredLocalDirs ( conf : SparkConf ): Array [ String ] getConfiguredLocalDirs returns the local directories where Spark can write files. Internally, getConfiguredLocalDirs uses conf SparkConf to know if External Shuffle Service is enabled (based on spark.shuffle.service.enabled configuration property). getConfiguredLocalDirs checks if Spark runs on YARN and if so, returns LOCAL_DIRS -controlled local directories. In non-YARN mode (or for the driver in yarn-client mode), getConfiguredLocalDirs checks the following environment variables (in the order) and returns the value of the first met: SPARK_EXECUTOR_DIRS environment variable SPARK_LOCAL_DIRS environment variable MESOS_DIRECTORY environment variable (only when External Shuffle Service is not used) In the end, when no earlier environment variables were found, getConfiguredLocalDirs uses spark.local.dir configuration property or falls back to java.io.tmpdir System property. getConfiguredLocalDirs is used when: DiskBlockManager is requested to createLocalDirs Utils helper is requested to getLocalDir and getOrCreateLocalRootDirsImpl Getting Writable Directories in YARN \u00b6 getYarnLocalDirs ( conf : SparkConf ): String getYarnLocalDirs uses conf SparkConf to read LOCAL_DIRS environment variable with comma-separated local directories (that have already been created and secured so that only the user has access to them). getYarnLocalDirs throws an Exception when LOCAL_DIRS environment variable was not set: Yarn Local dirs can't be empty Checking Whether Spark Runs on YARN \u00b6 isRunningInYarnContainer ( conf : SparkConf ): Boolean isRunningInYarnContainer uses conf SparkConf to read Hadoop YARN's CONTAINER_ID environment variable to find out if Spark runs in a YARN container (that is exported by YARN NodeManager). Getting All Blocks (From Files Stored On Disk) \u00b6 getAllBlocks (): Seq [ BlockId ] getAllBlocks gets all the blocks stored on disk. Internally, getAllBlocks takes the block files and returns their names (as BlockId ). getAllBlocks is used when BlockManager is requested to find IDs of existing blocks for a given filter . All Block Files \u00b6 getAllFiles (): Seq [ File ] getAllFiles ...FIXME Stopping \u00b6 stop (): Unit stop ...FIXME stop is used when BlockManager is requested to stop . Stopping DiskBlockManager (Removing Local Directories for Blocks) \u00b6 doStop (): Unit doStop deletes the local directories recursively (only when the constructor's deleteFilesOnStop is enabled and the parent directories are not registered to be removed at shutdown). doStop is used when DiskBlockManager is requested to shut down or stop . Logging \u00b6 Enable ALL logging level for org.apache.spark.storage.DiskBlockManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL Refer to Logging .","title":"DiskBlockManager"},{"location":"storage/DiskBlockManager/#diskblockmanager","text":"DiskBlockManager manages a logical mapping of logical blocks and their physical on-disk locations for a BlockManager . By default, one block is mapped to one file with a name given by its BlockId . It is however possible to have a block map to only a segment of a file. Block files are hashed among the local directories . DiskBlockManager is used to create a DiskStore . Tip Consult Demo: DiskBlockManager and Block Data .","title":"DiskBlockManager"},{"location":"storage/DiskBlockManager/#creating-instance","text":"DiskBlockManager takes the following to be created: SparkConf deleteFilesOnStop flag When created, DiskBlockManager creates one or many local directories to store block data and initializes the internal subDirs collection of locks for every local directory. In the end, DiskBlockManager registers a shutdown hook to clean up the local directories for blocks. DiskBlockManager is created for BlockManager .","title":"Creating Instance"},{"location":"storage/DiskBlockManager/#local-directories-for-blocks","text":"localDirs : Array [ File ] While being created, DiskBlockManager creates local directories for block data. DiskBlockManager expects at least one local directory or prints out the following ERROR message to the logs and exits the JVM (with exit code 53): Failed to create any local dir. localDirs is used when: DiskBlockManager is requested to getFile , initialize the subDirs internal registry, and to doStop BlockManager is requested to register with an external shuffle server","title":" Local Directories for Blocks"},{"location":"storage/DiskBlockManager/#creating-local-directories","text":"createLocalDirs ( conf : SparkConf ): Array [ File ] createLocalDirs creates blockmgr-[random UUID] directory under local directories to store block data. Internally, createLocalDirs finds the configured local directories where Spark can write files and creates a subdirectory blockmgr-[UUID] under every configured parent directory. For every local directory, createLocalDirs prints out the following INFO message to the logs: Created local directory at [localDir] In case of an exception, createLocalDirs prints out the following ERROR message to the logs and skips the directory: Failed to create local dir in [rootDir]. Ignoring this directory.","title":" Creating Local Directories"},{"location":"storage/DiskBlockManager/#file-locks-for-local-block-store-directories","text":"subDirs : Array [ Array [ File ]] subDirs is a lookup table for file locks of every local block directory (with the first dimension for local directories and the second for locks). The number of block subdirectories is controlled by spark.diskStore.subDirectories configuration property. subDirs(dirId)(subDirId) is used to access subDirId subdirectory in dirId local directory. subDirs is used when DiskBlockManager is requested for a block file or all block files .","title":" File Locks for Local Block Store Directories"},{"location":"storage/DiskBlockManager/#finding-block-file-and-creating-parent-directories","text":"getFile ( blockId : BlockId ): File getFile ( filename : String ): File getFile computes a hash of the file name of the input BlockId that is used for the name of the parent directory and subdirectory. getFile creates the subdirectory unless it already exists. getFile is used when: DiskBlockManager is requested to containsBlock , createTempLocalBlock , createTempShuffleBlock DiskStore is requested to getBytes , remove , contains , and put IndexShuffleBlockResolver is requested to getDataFile and getIndexFile","title":" Finding Block File (and Creating Parent Directories)"},{"location":"storage/DiskBlockManager/#createtempshuffleblock","text":"createTempShuffleBlock (): ( TempShuffleBlockId , File ) createTempShuffleBlock creates a temporary TempShuffleBlockId block. createTempShuffleBlock ...FIXME","title":" createTempShuffleBlock"},{"location":"storage/DiskBlockManager/#registering-shutdown-hook","text":"addShutdownHook (): AnyRef addShutdownHook registers a shutdown hook to execute doStop at shutdown. When executed, you should see the following DEBUG message in the logs: Adding shutdown hook addShutdownHook adds the shutdown hook so it prints the following INFO message and executes doStop : Shutdown hook called","title":" Registering Shutdown Hook"},{"location":"storage/DiskBlockManager/#getting-local-directories-for-spark-to-write-files","text":"getConfiguredLocalDirs ( conf : SparkConf ): Array [ String ] getConfiguredLocalDirs returns the local directories where Spark can write files. Internally, getConfiguredLocalDirs uses conf SparkConf to know if External Shuffle Service is enabled (based on spark.shuffle.service.enabled configuration property). getConfiguredLocalDirs checks if Spark runs on YARN and if so, returns LOCAL_DIRS -controlled local directories. In non-YARN mode (or for the driver in yarn-client mode), getConfiguredLocalDirs checks the following environment variables (in the order) and returns the value of the first met: SPARK_EXECUTOR_DIRS environment variable SPARK_LOCAL_DIRS environment variable MESOS_DIRECTORY environment variable (only when External Shuffle Service is not used) In the end, when no earlier environment variables were found, getConfiguredLocalDirs uses spark.local.dir configuration property or falls back to java.io.tmpdir System property. getConfiguredLocalDirs is used when: DiskBlockManager is requested to createLocalDirs Utils helper is requested to getLocalDir and getOrCreateLocalRootDirsImpl","title":" Getting Local Directories for Spark to Write Files"},{"location":"storage/DiskBlockManager/#getting-writable-directories-in-yarn","text":"getYarnLocalDirs ( conf : SparkConf ): String getYarnLocalDirs uses conf SparkConf to read LOCAL_DIRS environment variable with comma-separated local directories (that have already been created and secured so that only the user has access to them). getYarnLocalDirs throws an Exception when LOCAL_DIRS environment variable was not set: Yarn Local dirs can't be empty","title":" Getting Writable Directories in YARN"},{"location":"storage/DiskBlockManager/#checking-whether-spark-runs-on-yarn","text":"isRunningInYarnContainer ( conf : SparkConf ): Boolean isRunningInYarnContainer uses conf SparkConf to read Hadoop YARN's CONTAINER_ID environment variable to find out if Spark runs in a YARN container (that is exported by YARN NodeManager).","title":" Checking Whether Spark Runs on YARN"},{"location":"storage/DiskBlockManager/#getting-all-blocks-from-files-stored-on-disk","text":"getAllBlocks (): Seq [ BlockId ] getAllBlocks gets all the blocks stored on disk. Internally, getAllBlocks takes the block files and returns their names (as BlockId ). getAllBlocks is used when BlockManager is requested to find IDs of existing blocks for a given filter .","title":" Getting All Blocks (From Files Stored On Disk)"},{"location":"storage/DiskBlockManager/#all-block-files","text":"getAllFiles (): Seq [ File ] getAllFiles ...FIXME","title":" All Block Files"},{"location":"storage/DiskBlockManager/#stopping","text":"stop (): Unit stop ...FIXME stop is used when BlockManager is requested to stop .","title":" Stopping"},{"location":"storage/DiskBlockManager/#stopping-diskblockmanager-removing-local-directories-for-blocks","text":"doStop (): Unit doStop deletes the local directories recursively (only when the constructor's deleteFilesOnStop is enabled and the parent directories are not registered to be removed at shutdown). doStop is used when DiskBlockManager is requested to shut down or stop .","title":" Stopping DiskBlockManager (Removing Local Directories for Blocks)"},{"location":"storage/DiskBlockManager/#logging","text":"Enable ALL logging level for org.apache.spark.storage.DiskBlockManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL Refer to Logging .","title":"Logging"},{"location":"storage/DiskBlockObjectWriter/","text":"DiskBlockObjectWriter \u00b6 DiskBlockObjectWriter is a custom java.io.OutputStream that BlockManager offers for writing data blocks to disk . DiskBlockObjectWriter is used when: BypassMergeSortShuffleWriter is requested for partition writers UnsafeSorterSpillWriter is requested for a partition writer ShuffleExternalSorter is requested to writeSortedFile ExternalSorter is requested to spillMemoryIteratorToDisk Creating Instance \u00b6 DiskBlockObjectWriter takes the following to be created: Java File SerializerManager SerializerInstance Buffer size syncWrites flag (based on spark.shuffle.sync configuration property) ShuffleWriteMetricsReporter BlockId (default: null ) DiskBlockObjectWriter is created when: BlockManager is requested for one SerializationStream \u00b6 DiskBlockObjectWriter manages a SerializationStream for writing a key-value record : Opens it when requested to open Closes it when requested to commitAndGet Dereferences it ( null s it) when closeResources States \u00b6 DiskBlockObjectWriter can be in one of the following states (that match the state of the underlying output streams): Initialized Open Closed Writing Out Record \u00b6 write ( key : Any , value : Any ): Unit write opens the underlying stream unless open already. write requests the SerializationStream to write the key and then the value . In the end, write updates the write metrics . write is used when: BypassMergeSortShuffleWriter is requested to write records of a partition ExternalAppendOnlyMap is requested to spillMemoryIteratorToDisk ExternalSorter is requested to write all records into a partitioned file SpillableIterator is requested to spill WritablePartitionedPairCollection is requested for a destructiveSortedWritablePartitionedIterator commitAndGet \u00b6 commitAndGet (): FileSegment commitAndGet ...FIXME commitAndGet is used when...FIXME Committing Writes and Closing Resources \u00b6 close (): Unit close ...FIXME close is used when...FIXME revertPartialWritesAndClose \u00b6 revertPartialWritesAndClose (): File revertPartialWritesAndClose ...FIXME revertPartialWritesAndClose is used when...FIXME Writing Bytes (From Byte Array Starting From Offset) \u00b6 write ( kvBytes : Array [ Byte ], offs : Int , len : Int ): Unit write ...FIXME write is used when...FIXME Opening DiskBlockObjectWriter \u00b6 open (): DiskBlockObjectWriter open opens the DiskBlockObjectWriter , i.e. initializes and re-sets bs and objOut internal output streams. Internally, open makes sure that DiskBlockObjectWriter is not closed ( hasBeenClosed flag is disabled). If it was, open throws a IllegalStateException : Writer already closed. Cannot be reopened. Unless DiskBlockObjectWriter has already been initialized ( initialized flag is enabled), open initializes it (and turns initialized flag on). Regardless of whether DiskBlockObjectWriter was already initialized or not, open requests SerializerManager to wrap mcs output stream for encryption and compression (for blockId ) and sets it as bs . open requests the SerializerInstance to serialize bs output stream and sets it as objOut . Note open uses the SerializerInstance that was used to create the DiskBlockObjectWriter . In the end, open turns streamOpen flag on. open is used when DiskBlockObjectWriter writes out a record or bytes from a specified byte array and the stream is not open yet . Internal Properties \u00b6 initialized Flag \u00b6 hasBeenClosed Flag \u00b6 mcs \u00b6 bs \u00b6","title":"DiskBlockObjectWriter"},{"location":"storage/DiskBlockObjectWriter/#diskblockobjectwriter","text":"DiskBlockObjectWriter is a custom java.io.OutputStream that BlockManager offers for writing data blocks to disk . DiskBlockObjectWriter is used when: BypassMergeSortShuffleWriter is requested for partition writers UnsafeSorterSpillWriter is requested for a partition writer ShuffleExternalSorter is requested to writeSortedFile ExternalSorter is requested to spillMemoryIteratorToDisk","title":"DiskBlockObjectWriter"},{"location":"storage/DiskBlockObjectWriter/#creating-instance","text":"DiskBlockObjectWriter takes the following to be created: Java File SerializerManager SerializerInstance Buffer size syncWrites flag (based on spark.shuffle.sync configuration property) ShuffleWriteMetricsReporter BlockId (default: null ) DiskBlockObjectWriter is created when: BlockManager is requested for one","title":"Creating Instance"},{"location":"storage/DiskBlockObjectWriter/#serializationstream","text":"DiskBlockObjectWriter manages a SerializationStream for writing a key-value record : Opens it when requested to open Closes it when requested to commitAndGet Dereferences it ( null s it) when closeResources","title":" SerializationStream"},{"location":"storage/DiskBlockObjectWriter/#states","text":"DiskBlockObjectWriter can be in one of the following states (that match the state of the underlying output streams): Initialized Open Closed","title":" States"},{"location":"storage/DiskBlockObjectWriter/#writing-out-record","text":"write ( key : Any , value : Any ): Unit write opens the underlying stream unless open already. write requests the SerializationStream to write the key and then the value . In the end, write updates the write metrics . write is used when: BypassMergeSortShuffleWriter is requested to write records of a partition ExternalAppendOnlyMap is requested to spillMemoryIteratorToDisk ExternalSorter is requested to write all records into a partitioned file SpillableIterator is requested to spill WritablePartitionedPairCollection is requested for a destructiveSortedWritablePartitionedIterator","title":" Writing Out Record"},{"location":"storage/DiskBlockObjectWriter/#commitandget","text":"commitAndGet (): FileSegment commitAndGet ...FIXME commitAndGet is used when...FIXME","title":" commitAndGet"},{"location":"storage/DiskBlockObjectWriter/#committing-writes-and-closing-resources","text":"close (): Unit close ...FIXME close is used when...FIXME","title":" Committing Writes and Closing Resources"},{"location":"storage/DiskBlockObjectWriter/#revertpartialwritesandclose","text":"revertPartialWritesAndClose (): File revertPartialWritesAndClose ...FIXME revertPartialWritesAndClose is used when...FIXME","title":" revertPartialWritesAndClose"},{"location":"storage/DiskBlockObjectWriter/#writing-bytes-from-byte-array-starting-from-offset","text":"write ( kvBytes : Array [ Byte ], offs : Int , len : Int ): Unit write ...FIXME write is used when...FIXME","title":" Writing Bytes (From Byte Array Starting From Offset)"},{"location":"storage/DiskBlockObjectWriter/#opening-diskblockobjectwriter","text":"open (): DiskBlockObjectWriter open opens the DiskBlockObjectWriter , i.e. initializes and re-sets bs and objOut internal output streams. Internally, open makes sure that DiskBlockObjectWriter is not closed ( hasBeenClosed flag is disabled). If it was, open throws a IllegalStateException : Writer already closed. Cannot be reopened. Unless DiskBlockObjectWriter has already been initialized ( initialized flag is enabled), open initializes it (and turns initialized flag on). Regardless of whether DiskBlockObjectWriter was already initialized or not, open requests SerializerManager to wrap mcs output stream for encryption and compression (for blockId ) and sets it as bs . open requests the SerializerInstance to serialize bs output stream and sets it as objOut . Note open uses the SerializerInstance that was used to create the DiskBlockObjectWriter . In the end, open turns streamOpen flag on. open is used when DiskBlockObjectWriter writes out a record or bytes from a specified byte array and the stream is not open yet .","title":" Opening DiskBlockObjectWriter"},{"location":"storage/DiskBlockObjectWriter/#internal-properties","text":"","title":"Internal Properties"},{"location":"storage/DiskBlockObjectWriter/#initialized-flag","text":"","title":" initialized Flag"},{"location":"storage/DiskBlockObjectWriter/#hasbeenclosed-flag","text":"","title":" hasBeenClosed Flag"},{"location":"storage/DiskBlockObjectWriter/#mcs","text":"","title":" mcs"},{"location":"storage/DiskBlockObjectWriter/#bs","text":"","title":" bs"},{"location":"storage/DiskStore/","text":"= DiskStore DiskStore manages data blocks on disk for storage:BlockManager.md#diskStore[BlockManager]. .DiskStore and BlockManager image::DiskStore-BlockManager.png[align=\"center\"] == [[creating-instance]] Creating Instance DiskStore takes the following to be created: [[conf]] SparkConf.md[] [[diskManager]] storage:DiskBlockManager.md[] [[securityManager]] SecurityManager == [[getBytes]] getBytes Method [source,scala] \u00b6 getBytes( blockId: BlockId): BlockData getBytes...FIXME getBytes is used when BlockManager is requested to storage:BlockManager.md#getLocalValues[getLocalValues] and storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes]. == [[blockSizes]] blockSizes Internal Registry [source, scala] \u00b6 blockSizes: ConcurrentHashMap[BlockId, Long] \u00b6 blockSizes is a Java {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] that DiskStore uses to track storage:BlockId.md[]s by their size on disk. == [[contains]] Checking if Block File Exists [source, scala] \u00b6 contains( blockId: BlockId): Boolean contains requests the < > for the storage:DiskBlockManager.md#getFile[block file] by (the name of) the input storage:BlockId.md[] and check whether the file actually exists or not. contains is used when: BlockManager is requested to storage:BlockManager.md#getStatus[getStatus], storage:BlockManager.md#getCurrentBlockStatus[getCurrentBlockStatus], storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes], storage:BlockManager.md#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[put]] Writing Block to Disk [source, scala] \u00b6 put( blockId: BlockId)( writeFunc: WritableByteChannel => Unit): Unit put prints out the following DEBUG message to the logs: Attempting to put block [blockId] put requests the < > for the storage:DiskBlockManager.md#getFile[block file] for the input storage:BlockId.md[]. put < > (wrapped into a CountingWritableChannel to count the bytes written). put executes the given writeFunc function with the WritableByteChannel of the block file and registers the bytes written to the < > internal registry. In the end, put prints out the following DEBUG message to the logs: Block [fileName] stored as [size] file on disk in [time] ms In case of any exception, put < >. put throws an IllegalStateException when the BlockId is already < >: Block [blockId] is already present in the disk store put is used when: BlockManager is requested to storage:BlockManager.md#doPutIterator[doPutIterator] and storage:BlockManager.md#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[putBytes]] putBytes Method [source, scala] \u00b6 putBytes( blockId: BlockId, bytes: ChunkedByteBuffer): Unit putBytes ...FIXME putBytes is used when BlockManager is requested to storage:BlockManager.md#doPutBytes[doPutBytes] and storage:BlockManager.md#dropFromMemory[dropFromMemory]. == [[remove]] Removing Block [source, scala] \u00b6 remove( blockId: BlockId): Boolean remove ...FIXME remove is used when: BlockManager is requested to storage:BlockManager.md#removeBlockInternal[removeBlockInternal] DiskStore is requested to < > (when an exception was thrown) == [[openForWrite]] Opening Block File For Writing [source, scala] \u00b6 openForWrite( file: File): WritableByteChannel openForWrite ...FIXME openForWrite is used when DiskStore is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskStore logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.DiskStore=ALL \u00b6 Refer to spark-logging.md[Logging].","title":"DiskStore"},{"location":"storage/DiskStore/#sourcescala","text":"getBytes( blockId: BlockId): BlockData getBytes...FIXME getBytes is used when BlockManager is requested to storage:BlockManager.md#getLocalValues[getLocalValues] and storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes]. == [[blockSizes]] blockSizes Internal Registry","title":"[source,scala]"},{"location":"storage/DiskStore/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/DiskStore/#blocksizes-concurrenthashmapblockid-long","text":"blockSizes is a Java {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] that DiskStore uses to track storage:BlockId.md[]s by their size on disk. == [[contains]] Checking if Block File Exists","title":"blockSizes: ConcurrentHashMap[BlockId, Long]"},{"location":"storage/DiskStore/#source-scala_1","text":"contains( blockId: BlockId): Boolean contains requests the < > for the storage:DiskBlockManager.md#getFile[block file] by (the name of) the input storage:BlockId.md[] and check whether the file actually exists or not. contains is used when: BlockManager is requested to storage:BlockManager.md#getStatus[getStatus], storage:BlockManager.md#getCurrentBlockStatus[getCurrentBlockStatus], storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes], storage:BlockManager.md#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[put]] Writing Block to Disk","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_2","text":"put( blockId: BlockId)( writeFunc: WritableByteChannel => Unit): Unit put prints out the following DEBUG message to the logs: Attempting to put block [blockId] put requests the < > for the storage:DiskBlockManager.md#getFile[block file] for the input storage:BlockId.md[]. put < > (wrapped into a CountingWritableChannel to count the bytes written). put executes the given writeFunc function with the WritableByteChannel of the block file and registers the bytes written to the < > internal registry. In the end, put prints out the following DEBUG message to the logs: Block [fileName] stored as [size] file on disk in [time] ms In case of any exception, put < >. put throws an IllegalStateException when the BlockId is already < >: Block [blockId] is already present in the disk store put is used when: BlockManager is requested to storage:BlockManager.md#doPutIterator[doPutIterator] and storage:BlockManager.md#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[putBytes]] putBytes Method","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_3","text":"putBytes( blockId: BlockId, bytes: ChunkedByteBuffer): Unit putBytes ...FIXME putBytes is used when BlockManager is requested to storage:BlockManager.md#doPutBytes[doPutBytes] and storage:BlockManager.md#dropFromMemory[dropFromMemory]. == [[remove]] Removing Block","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_4","text":"remove( blockId: BlockId): Boolean remove ...FIXME remove is used when: BlockManager is requested to storage:BlockManager.md#removeBlockInternal[removeBlockInternal] DiskStore is requested to < > (when an exception was thrown) == [[openForWrite]] Opening Block File For Writing","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_5","text":"openForWrite( file: File): WritableByteChannel openForWrite ...FIXME openForWrite is used when DiskStore is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskStore logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/DiskStore/#source","text":"","title":"[source]"},{"location":"storage/DiskStore/#log4jloggerorgapachesparkstoragediskstoreall","text":"Refer to spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.storage.DiskStore=ALL"},{"location":"storage/ExternalBlockStoreClient/","text":"ExternalBlockStoreClient \u00b6 ExternalBlockStoreClient is a BlockStoreClient that the driver and executors use when spark.shuffle.service.enabled configuration property is enabled. Creating Instance \u00b6 ExternalBlockStoreClient takes the following to be created: TransportConf SecretKeyHolder authEnabled flag registrationTimeoutMs ExternalBlockStoreClient is created when: SparkEnv utility is requested to create a SparkEnv (for the driver and executors) with spark.shuffle.service.enabled configuration property enabled","title":"ExternalBlockStoreClient"},{"location":"storage/ExternalBlockStoreClient/#externalblockstoreclient","text":"ExternalBlockStoreClient is a BlockStoreClient that the driver and executors use when spark.shuffle.service.enabled configuration property is enabled.","title":"ExternalBlockStoreClient"},{"location":"storage/ExternalBlockStoreClient/#creating-instance","text":"ExternalBlockStoreClient takes the following to be created: TransportConf SecretKeyHolder authEnabled flag registrationTimeoutMs ExternalBlockStoreClient is created when: SparkEnv utility is requested to create a SparkEnv (for the driver and executors) with spark.shuffle.service.enabled configuration property enabled","title":"Creating Instance"},{"location":"storage/FallbackStorage/","text":"FallbackStorage \u00b6 FallbackStorage is...FIXME","title":"FallbackStorage"},{"location":"storage/FallbackStorage/#fallbackstorage","text":"FallbackStorage is...FIXME","title":"FallbackStorage"},{"location":"storage/MemoryStore/","text":"MemoryStore \u00b6 MemoryStore manages blocks of data in memory for BlockManager . Creating Instance \u00b6 MemoryStore takes the following to be created: SparkConf BlockInfoManager SerializerManager MemoryManager BlockEvictionHandler MemoryStore is created when: BlockManager is created Blocks \u00b6 entries : LinkedHashMap [ BlockId , MemoryEntry [ _ ]] MemoryStore creates a LinkedHashMap ( Java ) of blocks (as MemoryEntries per BlockId ) when created . entries uses access-order ordering mode where the order of iteration is the order in which the entries were last accessed (from least-recently accessed to most-recently). That gives LRU cache behaviour when MemoryStore is requested to evict blocks . MemoryEntries are added in putBytes and putIterator . MemoryEntries are removed in remove , clear , and while evicting blocks to free up memory . DeserializedMemoryEntry \u00b6 DeserializedMemoryEntry is a MemoryEntry for block values with the following: Array[T] (for the values) size ON_HEAP memory mode SerializedMemoryEntry \u00b6 SerializedMemoryEntry is a MemoryEntry for block bytes with the following: ChunkedByteBuffer (for the serialized values) size MemoryMode Evicting Blocks \u00b6 evictBlocksToFreeSpace ( blockId : Option [ BlockId ], space : Long , memoryMode : MemoryMode ): Long evictBlocksToFreeSpace finds blocks to evict in the entries registry (based on least-recently accessed order and until the required space to free up is met or there are no more blocks). Once done, evictBlocksToFreeSpace returns the memory freed up. When there is enough blocks to drop to free up memory, evictBlocksToFreeSpace prints out the following INFO message to the logs: [n] blocks selected for dropping ([freedMemory]) bytes) evictBlocksToFreeSpace drops the blocks one by one. evictBlocksToFreeSpace prints out the following INFO message to the logs: After dropping [n] blocks, free memory is [memory] When there is not enough blocks to drop to make room for the given block (if any), evictBlocksToFreeSpace prints out the following INFO message to the logs: Will not store [blockId] evictBlocksToFreeSpace is used when: StorageMemoryPool is requested to acquireMemory and freeSpaceToShrinkPool Dropping Block \u00b6 dropBlock [ T ]( blockId : BlockId , entry : MemoryEntry [ T ]): Unit dropBlock requests the BlockEvictionHandler to drop the block from memory . If the block is no longer available in any other store, dropBlock requests the BlockInfoManager to remove the block (info) . BlockInfoManager \u00b6 MemoryStore is given a BlockInfoManager when created . MemoryStore uses the BlockInfoManager when requested to evict blocks . Accessing MemoryStore \u00b6 MemoryStore is available to other Spark services using BlockManager.memoryStore . import org . apache . spark . SparkEnv SparkEnv . get . blockManager . memoryStore Serialized Block Bytes \u00b6 getBytes ( blockId : BlockId ): Option [ ChunkedByteBuffer ] getBytes returns the bytes of the SerializedMemoryEntry of a block (if found in the entries registry). getBytes is used (for blocks with a serialized and in-memory storage level ) when: BlockManager is requested for the serialized bytes of a block (from a local block manager) , getLocalValues , maybeCacheDiskBytesInMemory Deserialized Block Values \u00b6 getValues ( blockId : BlockId ): Option [ Iterator [ _ ]] getValues returns the values of the DeserializedMemoryEntry of a block (if found in the entries registry). getValues is used (for blocks with a deserialized and in-memory storage level ) when: BlockManager is requested for the serialized bytes of a block (from a local block manager) , getLocalValues , maybeCacheDiskBytesInMemory putIteratorAsValues \u00b6 putIteratorAsValues [ T ]( blockId : BlockId , values : Iterator [ T ], classTag : ClassTag [ T ]): Either [ PartiallyUnrolledIterator [ T ], Long ] putIteratorAsValues ...FIXME putIteratorAsValues is used when: BlockStoreUpdater is requested to saveDeserializedValuesToMemoryStore BlockManager is requested to doPutIterator and maybeCacheDiskValuesInMemory putIteratorAsBytes \u00b6 putIteratorAsBytes [ T ]( blockId : BlockId , values : Iterator [ T ], classTag : ClassTag [ T ], memoryMode : MemoryMode ): Either [ PartiallySerializedBlock [ T ], Long ] putIteratorAsBytes ...FIXME putIteratorAsBytes is used when: BlockManager is requested to doPutIterator putIterator \u00b6 putIterator [ T ]( blockId : BlockId , values : Iterator [ T ], classTag : ClassTag [ T ], memoryMode : MemoryMode , valuesHolder : ValuesHolder [ T ]): Either [ Long , Long ] putIterator ...FIXME putIterator is used when: MemoryStore is requested to putIteratorAsValues and putIteratorAsBytes putBytes \u00b6 putBytes [ T : ClassTag ]( blockId : BlockId , size : Long , memoryMode : MemoryMode , _bytes : () => ChunkedByteBuffer ): Boolean putBytes ...FIXME putBytes is used when: BlockStoreUpdater is requested to saveSerializedValuesToMemoryStore BlockManager is requested to maybeCacheDiskBytesInMemory Logging \u00b6 Enable ALL logging level for org.apache.spark.storage.memory.MemoryStore logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.memory.MemoryStore=ALL Refer to Logging . Review Me \u00b6 == [[unrollMemoryThreshold]][[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Configuration Property MemoryStore uses configuration-properties.md#spark.storage.unrollMemoryThreshold[spark.storage.unrollMemoryThreshold] configuration property for < > and < >. == [[releaseUnrollMemoryForThisTask]] releaseUnrollMemoryForThisTask Method [source, scala] \u00b6 releaseUnrollMemoryForThisTask( memoryMode: MemoryMode, memory: Long = Long.MaxValue): Unit releaseUnrollMemoryForThisTask...FIXME releaseUnrollMemoryForThisTask is used when: Task is requested to scheduler:Task.md#run[run] (and cleans up after itself) MemoryStore is requested to < > PartiallyUnrolledIterator is requested to releaseUnrollMemory PartiallySerializedBlock is requested to discard and finishWritingToStream == [[remove]] Dropping Block from Memory [source, scala] \u00b6 remove( blockId: BlockId): Boolean remove removes the given BlockId.md[] from the < > internal registry and branches off based on whether the < > or < >. === [[remove-block-removed]] Block Removed When found and removed, remove requests the < > to memory:MemoryManager.md#releaseStorageMemory[releaseStorageMemory] and prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Block [blockId] of size [size] dropped from memory (free [memory]) \u00b6 remove returns true . === [[remove-no-block]] No Block Removed If no BlockId was registered and removed, remove returns false . === [[remove-usage]] Usage remove is used when BlockManager is requested to BlockManager.md#dropFromMemory[dropFromMemory] and BlockManager.md#removeBlockInternal[removeBlockInternal]. == [[putBytes]] Acquiring Storage Memory for Blocks [source, scala] \u00b6 putBytes T: ClassTag : Boolean putBytes requests memory:MemoryManager.md#acquireStorageMemory[storage memory for blockId from MemoryManager ] and registers the block in < > internal registry. Internally, putBytes first makes sure that blockId block has not been registered already in < > internal registry. putBytes then requests memory:MemoryManager.md#acquireStorageMemory[ size memory for the blockId block in a given memoryMode from the current MemoryManager ]. [NOTE] \u00b6 memoryMode can be ON_HEAP or OFF_HEAP and is a property of a StorageLevel.md[StorageLevel]. import org.apache.spark.storage.StorageLevel._ scala> MEMORY_AND_DISK.useOffHeap res0: Boolean = false scala> OFF_HEAP.useOffHeap res1: Boolean = true \u00b6 If successful, putBytes \"materializes\" _bytes byte buffer and makes sure that the size is exactly size . It then registers a SerializedMemoryEntry (for the bytes and memoryMode ) for blockId in the internal < > registry. You should see the following INFO message in the logs: Block [blockId] stored as bytes in memory (estimated size [size], free [bytes]) putBytes returns true only after blockId was successfully registered in the internal < > registry. putBytes is used when BlockManager is requested to BlockManager.md#doPutBytes[doPutBytes] and BlockManager.md#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[contains]] Checking Whether Block Exists In MemoryStore [source, scala] \u00b6 contains( blockId: BlockId): Boolean contains is positive ( true ) when the < > internal registry contains blockId key. contains is used when...FIXME == [[reserveUnrollMemoryForThisTask]] reserveUnrollMemoryForThisTask Method [source, scala] \u00b6 reserveUnrollMemoryForThisTask( blockId: BlockId, memory: Long, memoryMode: MemoryMode): Boolean reserveUnrollMemoryForThisTask acquires a lock on < > and requests it to memory:MemoryManager.md#acquireUnrollMemory[acquireUnrollMemory]. NOTE: reserveUnrollMemoryForThisTask is used when MemoryStore is requested to < > and < >. == [[maxMemory]] Total Amount Of Memory Available For Storage [source, scala] \u00b6 maxMemory: Long \u00b6 maxMemory requests the < > for the current memory:MemoryManager.md#maxOnHeapStorageMemory[maxOnHeapStorageMemory] and memory:MemoryManager.md#maxOffHeapStorageMemory[maxOffHeapStorageMemory], and simply returns their sum. [TIP] \u00b6 Enable INFO < > to find the maxMemory in the logs when MemoryStore is < >: MemoryStore started with capacity [maxMemory] MB \u00b6 NOTE: maxMemory is used for < > purposes only. == [[logUnrollFailureMessage]] logUnrollFailureMessage Internal Method [source, scala] \u00b6 logUnrollFailureMessage( blockId: BlockId, finalVectorSize: Long): Unit logUnrollFailureMessage...FIXME logUnrollFailureMessage is used when MemoryStore is requested to < >. == [[logMemoryUsage]] logMemoryUsage Internal Method [source, scala] \u00b6 logMemoryUsage(): Unit \u00b6 logMemoryUsage...FIXME logMemoryUsage is used when MemoryStore is requested to < >. == [[memoryUsed]] Total Memory Used [source, scala] \u00b6 memoryUsed: Long \u00b6 memoryUsed requests the < > for the memory:MemoryManager.md#storageMemoryUsed[storageMemoryUsed]. memoryUsed is used when MemoryStore is requested for < > and to < >. == [[blocksMemoryUsed]] Memory Used for Caching Blocks [source, scala] \u00b6 blocksMemoryUsed: Long \u00b6 blocksMemoryUsed is the < > without the < >. blocksMemoryUsed is used for logging purposes when MemoryStore is requested to < >, < >, < >, < > and < >.","title":"MemoryStore"},{"location":"storage/MemoryStore/#memorystore","text":"MemoryStore manages blocks of data in memory for BlockManager .","title":"MemoryStore"},{"location":"storage/MemoryStore/#creating-instance","text":"MemoryStore takes the following to be created: SparkConf BlockInfoManager SerializerManager MemoryManager BlockEvictionHandler MemoryStore is created when: BlockManager is created","title":"Creating Instance"},{"location":"storage/MemoryStore/#blocks","text":"entries : LinkedHashMap [ BlockId , MemoryEntry [ _ ]] MemoryStore creates a LinkedHashMap ( Java ) of blocks (as MemoryEntries per BlockId ) when created . entries uses access-order ordering mode where the order of iteration is the order in which the entries were last accessed (from least-recently accessed to most-recently). That gives LRU cache behaviour when MemoryStore is requested to evict blocks . MemoryEntries are added in putBytes and putIterator . MemoryEntries are removed in remove , clear , and while evicting blocks to free up memory .","title":" Blocks"},{"location":"storage/MemoryStore/#deserializedmemoryentry","text":"DeserializedMemoryEntry is a MemoryEntry for block values with the following: Array[T] (for the values) size ON_HEAP memory mode","title":" DeserializedMemoryEntry"},{"location":"storage/MemoryStore/#serializedmemoryentry","text":"SerializedMemoryEntry is a MemoryEntry for block bytes with the following: ChunkedByteBuffer (for the serialized values) size MemoryMode","title":" SerializedMemoryEntry"},{"location":"storage/MemoryStore/#evicting-blocks","text":"evictBlocksToFreeSpace ( blockId : Option [ BlockId ], space : Long , memoryMode : MemoryMode ): Long evictBlocksToFreeSpace finds blocks to evict in the entries registry (based on least-recently accessed order and until the required space to free up is met or there are no more blocks). Once done, evictBlocksToFreeSpace returns the memory freed up. When there is enough blocks to drop to free up memory, evictBlocksToFreeSpace prints out the following INFO message to the logs: [n] blocks selected for dropping ([freedMemory]) bytes) evictBlocksToFreeSpace drops the blocks one by one. evictBlocksToFreeSpace prints out the following INFO message to the logs: After dropping [n] blocks, free memory is [memory] When there is not enough blocks to drop to make room for the given block (if any), evictBlocksToFreeSpace prints out the following INFO message to the logs: Will not store [blockId] evictBlocksToFreeSpace is used when: StorageMemoryPool is requested to acquireMemory and freeSpaceToShrinkPool","title":" Evicting Blocks"},{"location":"storage/MemoryStore/#dropping-block","text":"dropBlock [ T ]( blockId : BlockId , entry : MemoryEntry [ T ]): Unit dropBlock requests the BlockEvictionHandler to drop the block from memory . If the block is no longer available in any other store, dropBlock requests the BlockInfoManager to remove the block (info) .","title":" Dropping Block"},{"location":"storage/MemoryStore/#blockinfomanager","text":"MemoryStore is given a BlockInfoManager when created . MemoryStore uses the BlockInfoManager when requested to evict blocks .","title":" BlockInfoManager"},{"location":"storage/MemoryStore/#accessing-memorystore","text":"MemoryStore is available to other Spark services using BlockManager.memoryStore . import org . apache . spark . SparkEnv SparkEnv . get . blockManager . memoryStore","title":"Accessing MemoryStore"},{"location":"storage/MemoryStore/#serialized-block-bytes","text":"getBytes ( blockId : BlockId ): Option [ ChunkedByteBuffer ] getBytes returns the bytes of the SerializedMemoryEntry of a block (if found in the entries registry). getBytes is used (for blocks with a serialized and in-memory storage level ) when: BlockManager is requested for the serialized bytes of a block (from a local block manager) , getLocalValues , maybeCacheDiskBytesInMemory","title":" Serialized Block Bytes"},{"location":"storage/MemoryStore/#deserialized-block-values","text":"getValues ( blockId : BlockId ): Option [ Iterator [ _ ]] getValues returns the values of the DeserializedMemoryEntry of a block (if found in the entries registry). getValues is used (for blocks with a deserialized and in-memory storage level ) when: BlockManager is requested for the serialized bytes of a block (from a local block manager) , getLocalValues , maybeCacheDiskBytesInMemory","title":" Deserialized Block Values"},{"location":"storage/MemoryStore/#putiteratorasvalues","text":"putIteratorAsValues [ T ]( blockId : BlockId , values : Iterator [ T ], classTag : ClassTag [ T ]): Either [ PartiallyUnrolledIterator [ T ], Long ] putIteratorAsValues ...FIXME putIteratorAsValues is used when: BlockStoreUpdater is requested to saveDeserializedValuesToMemoryStore BlockManager is requested to doPutIterator and maybeCacheDiskValuesInMemory","title":" putIteratorAsValues"},{"location":"storage/MemoryStore/#putiteratorasbytes","text":"putIteratorAsBytes [ T ]( blockId : BlockId , values : Iterator [ T ], classTag : ClassTag [ T ], memoryMode : MemoryMode ): Either [ PartiallySerializedBlock [ T ], Long ] putIteratorAsBytes ...FIXME putIteratorAsBytes is used when: BlockManager is requested to doPutIterator","title":" putIteratorAsBytes"},{"location":"storage/MemoryStore/#putiterator","text":"putIterator [ T ]( blockId : BlockId , values : Iterator [ T ], classTag : ClassTag [ T ], memoryMode : MemoryMode , valuesHolder : ValuesHolder [ T ]): Either [ Long , Long ] putIterator ...FIXME putIterator is used when: MemoryStore is requested to putIteratorAsValues and putIteratorAsBytes","title":" putIterator"},{"location":"storage/MemoryStore/#putbytes","text":"putBytes [ T : ClassTag ]( blockId : BlockId , size : Long , memoryMode : MemoryMode , _bytes : () => ChunkedByteBuffer ): Boolean putBytes ...FIXME putBytes is used when: BlockStoreUpdater is requested to saveSerializedValuesToMemoryStore BlockManager is requested to maybeCacheDiskBytesInMemory","title":" putBytes"},{"location":"storage/MemoryStore/#logging","text":"Enable ALL logging level for org.apache.spark.storage.memory.MemoryStore logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.memory.MemoryStore=ALL Refer to Logging .","title":"Logging"},{"location":"storage/MemoryStore/#review-me","text":"== [[unrollMemoryThreshold]][[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Configuration Property MemoryStore uses configuration-properties.md#spark.storage.unrollMemoryThreshold[spark.storage.unrollMemoryThreshold] configuration property for < > and < >. == [[releaseUnrollMemoryForThisTask]] releaseUnrollMemoryForThisTask Method","title":"Review Me"},{"location":"storage/MemoryStore/#source-scala","text":"releaseUnrollMemoryForThisTask( memoryMode: MemoryMode, memory: Long = Long.MaxValue): Unit releaseUnrollMemoryForThisTask...FIXME releaseUnrollMemoryForThisTask is used when: Task is requested to scheduler:Task.md#run[run] (and cleans up after itself) MemoryStore is requested to < > PartiallyUnrolledIterator is requested to releaseUnrollMemory PartiallySerializedBlock is requested to discard and finishWritingToStream == [[remove]] Dropping Block from Memory","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_1","text":"remove( blockId: BlockId): Boolean remove removes the given BlockId.md[] from the < > internal registry and branches off based on whether the < > or < >. === [[remove-block-removed]] Block Removed When found and removed, remove requests the < > to memory:MemoryManager.md#releaseStorageMemory[releaseStorageMemory] and prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/MemoryStore/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/MemoryStore/#block-blockid-of-size-size-dropped-from-memory-free-memory","text":"remove returns true . === [[remove-no-block]] No Block Removed If no BlockId was registered and removed, remove returns false . === [[remove-usage]] Usage remove is used when BlockManager is requested to BlockManager.md#dropFromMemory[dropFromMemory] and BlockManager.md#removeBlockInternal[removeBlockInternal]. == [[putBytes]] Acquiring Storage Memory for Blocks","title":"Block [blockId] of size [size] dropped from memory (free [memory])"},{"location":"storage/MemoryStore/#source-scala_2","text":"putBytes T: ClassTag : Boolean putBytes requests memory:MemoryManager.md#acquireStorageMemory[storage memory for blockId from MemoryManager ] and registers the block in < > internal registry. Internally, putBytes first makes sure that blockId block has not been registered already in < > internal registry. putBytes then requests memory:MemoryManager.md#acquireStorageMemory[ size memory for the blockId block in a given memoryMode from the current MemoryManager ].","title":"[source, scala]"},{"location":"storage/MemoryStore/#note","text":"memoryMode can be ON_HEAP or OFF_HEAP and is a property of a StorageLevel.md[StorageLevel].","title":"[NOTE]"},{"location":"storage/MemoryStore/#import-orgapachesparkstoragestoragelevel_-scala-memory_and_diskuseoffheap-res0-boolean-false-scala-off_heapuseoffheap-res1-boolean-true","text":"If successful, putBytes \"materializes\" _bytes byte buffer and makes sure that the size is exactly size . It then registers a SerializedMemoryEntry (for the bytes and memoryMode ) for blockId in the internal < > registry. You should see the following INFO message in the logs: Block [blockId] stored as bytes in memory (estimated size [size], free [bytes]) putBytes returns true only after blockId was successfully registered in the internal < > registry. putBytes is used when BlockManager is requested to BlockManager.md#doPutBytes[doPutBytes] and BlockManager.md#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[contains]] Checking Whether Block Exists In MemoryStore","title":"import org.apache.spark.storage.StorageLevel._\nscala&gt; MEMORY_AND_DISK.useOffHeap\nres0: Boolean = false\n\nscala&gt; OFF_HEAP.useOffHeap\nres1: Boolean = true\n"},{"location":"storage/MemoryStore/#source-scala_3","text":"contains( blockId: BlockId): Boolean contains is positive ( true ) when the < > internal registry contains blockId key. contains is used when...FIXME == [[reserveUnrollMemoryForThisTask]] reserveUnrollMemoryForThisTask Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_4","text":"reserveUnrollMemoryForThisTask( blockId: BlockId, memory: Long, memoryMode: MemoryMode): Boolean reserveUnrollMemoryForThisTask acquires a lock on < > and requests it to memory:MemoryManager.md#acquireUnrollMemory[acquireUnrollMemory]. NOTE: reserveUnrollMemoryForThisTask is used when MemoryStore is requested to < > and < >. == [[maxMemory]] Total Amount Of Memory Available For Storage","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#maxmemory-long","text":"maxMemory requests the < > for the current memory:MemoryManager.md#maxOnHeapStorageMemory[maxOnHeapStorageMemory] and memory:MemoryManager.md#maxOffHeapStorageMemory[maxOffHeapStorageMemory], and simply returns their sum.","title":"maxMemory: Long"},{"location":"storage/MemoryStore/#tip","text":"Enable INFO < > to find the maxMemory in the logs when MemoryStore is < >:","title":"[TIP]"},{"location":"storage/MemoryStore/#memorystore-started-with-capacity-maxmemory-mb","text":"NOTE: maxMemory is used for < > purposes only. == [[logUnrollFailureMessage]] logUnrollFailureMessage Internal Method","title":"MemoryStore started with capacity [maxMemory] MB\n"},{"location":"storage/MemoryStore/#source-scala_6","text":"logUnrollFailureMessage( blockId: BlockId, finalVectorSize: Long): Unit logUnrollFailureMessage...FIXME logUnrollFailureMessage is used when MemoryStore is requested to < >. == [[logMemoryUsage]] logMemoryUsage Internal Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_7","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#logmemoryusage-unit","text":"logMemoryUsage...FIXME logMemoryUsage is used when MemoryStore is requested to < >. == [[memoryUsed]] Total Memory Used","title":"logMemoryUsage(): Unit"},{"location":"storage/MemoryStore/#source-scala_8","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#memoryused-long","text":"memoryUsed requests the < > for the memory:MemoryManager.md#storageMemoryUsed[storageMemoryUsed]. memoryUsed is used when MemoryStore is requested for < > and to < >. == [[blocksMemoryUsed]] Memory Used for Caching Blocks","title":"memoryUsed: Long"},{"location":"storage/MemoryStore/#source-scala_9","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#blocksmemoryused-long","text":"blocksMemoryUsed is the < > without the < >. blocksMemoryUsed is used for logging purposes when MemoryStore is requested to < >, < >, < >, < > and < >.","title":"blocksMemoryUsed: Long"},{"location":"storage/NettyBlockRpcServer/","text":"NettyBlockRpcServer \u00b6 NettyBlockRpcServer is a RpcHandler to handle messages for NettyBlockTransferService . Creating Instance \u00b6 NettyBlockRpcServer takes the following to be created: Application ID Serializer BlockDataManager NettyBlockRpcServer is created when: NettyBlockTransferService is requested to initialize OneForOneStreamManager \u00b6 NettyBlockRpcServer uses a OneForOneStreamManager . Receiving RPC Messages \u00b6 receive ( client : TransportClient , rpcMessage : ByteBuffer , responseContext : RpcResponseCallback ): Unit receive is part of the RpcHandler abstraction. receive deserializes the incoming RPC message (from ByteBuffer to BlockTransferMessage ) and prints out the following TRACE message to the logs: Received request: [message] receive handles the message. FetchShuffleBlocks \u00b6 FetchShuffleBlocks carries the following: Application ID Executor ID Shuffle ID Map IDs ( long[] ) Reduce IDs ( long[][] ) batchFetchEnabled flag When received, receive ...FIXME receive prints out the following TRACE message in the logs: Registered streamId [streamId] with [numBlockIds] buffers In the end, receive responds with a StreamHandle (with the streamId and the number of blocks). The response is serialized to a ByteBuffer . FetchShuffleBlocks is posted when: OneForOneBlockFetcher is requested to createFetchShuffleBlocksMsgAndBuildBlockIds GetLocalDirsForExecutors \u00b6 OpenBlocks \u00b6 OpenBlocks carries the following: Application ID Executor ID Block IDs When received, receive ...FIXME receive prints out the following TRACE message in the logs: Registered streamId [streamId] with [blocksNum] buffers In the end, receive responds with a StreamHandle (with the streamId and the number of blocks). The response is serialized to a ByteBuffer . OpenBlocks is posted when: OneForOneBlockFetcher is requested to start UploadBlock \u00b6 UploadBlock carries the following: Application ID Executor ID Block ID Metadata ( byte[] ) Block Data ( byte[] ) When received, receive deserializes the metadata to get the StorageLevel and ClassTag of the block being uploaded. receive ...FIXME UploadBlock is posted when: NettyBlockTransferService is requested to upload a block receiveStream \u00b6 receiveStream ( client : TransportClient , messageHeader : ByteBuffer , responseContext : RpcResponseCallback ): StreamCallbackWithID receiveStream is part of the RpcHandler abstraction. receiveStream ...FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.network.netty.NettyBlockRpcServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.netty.NettyBlockRpcServer=ALL Refer to Logging .","title":"NettyBlockRpcServer"},{"location":"storage/NettyBlockRpcServer/#nettyblockrpcserver","text":"NettyBlockRpcServer is a RpcHandler to handle messages for NettyBlockTransferService .","title":"NettyBlockRpcServer"},{"location":"storage/NettyBlockRpcServer/#creating-instance","text":"NettyBlockRpcServer takes the following to be created: Application ID Serializer BlockDataManager NettyBlockRpcServer is created when: NettyBlockTransferService is requested to initialize","title":"Creating Instance"},{"location":"storage/NettyBlockRpcServer/#oneforonestreammanager","text":"NettyBlockRpcServer uses a OneForOneStreamManager .","title":" OneForOneStreamManager"},{"location":"storage/NettyBlockRpcServer/#receiving-rpc-messages","text":"receive ( client : TransportClient , rpcMessage : ByteBuffer , responseContext : RpcResponseCallback ): Unit receive is part of the RpcHandler abstraction. receive deserializes the incoming RPC message (from ByteBuffer to BlockTransferMessage ) and prints out the following TRACE message to the logs: Received request: [message] receive handles the message.","title":" Receiving RPC Messages"},{"location":"storage/NettyBlockRpcServer/#fetchshuffleblocks","text":"FetchShuffleBlocks carries the following: Application ID Executor ID Shuffle ID Map IDs ( long[] ) Reduce IDs ( long[][] ) batchFetchEnabled flag When received, receive ...FIXME receive prints out the following TRACE message in the logs: Registered streamId [streamId] with [numBlockIds] buffers In the end, receive responds with a StreamHandle (with the streamId and the number of blocks). The response is serialized to a ByteBuffer . FetchShuffleBlocks is posted when: OneForOneBlockFetcher is requested to createFetchShuffleBlocksMsgAndBuildBlockIds","title":" FetchShuffleBlocks"},{"location":"storage/NettyBlockRpcServer/#getlocaldirsforexecutors","text":"","title":" GetLocalDirsForExecutors"},{"location":"storage/NettyBlockRpcServer/#openblocks","text":"OpenBlocks carries the following: Application ID Executor ID Block IDs When received, receive ...FIXME receive prints out the following TRACE message in the logs: Registered streamId [streamId] with [blocksNum] buffers In the end, receive responds with a StreamHandle (with the streamId and the number of blocks). The response is serialized to a ByteBuffer . OpenBlocks is posted when: OneForOneBlockFetcher is requested to start","title":" OpenBlocks"},{"location":"storage/NettyBlockRpcServer/#uploadblock","text":"UploadBlock carries the following: Application ID Executor ID Block ID Metadata ( byte[] ) Block Data ( byte[] ) When received, receive deserializes the metadata to get the StorageLevel and ClassTag of the block being uploaded. receive ...FIXME UploadBlock is posted when: NettyBlockTransferService is requested to upload a block","title":" UploadBlock"},{"location":"storage/NettyBlockRpcServer/#receivestream","text":"receiveStream ( client : TransportClient , messageHeader : ByteBuffer , responseContext : RpcResponseCallback ): StreamCallbackWithID receiveStream is part of the RpcHandler abstraction. receiveStream ...FIXME","title":" receiveStream"},{"location":"storage/NettyBlockRpcServer/#logging","text":"Enable ALL logging level for org.apache.spark.network.netty.NettyBlockRpcServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.netty.NettyBlockRpcServer=ALL Refer to Logging .","title":"Logging"},{"location":"storage/NettyBlockTransferService/","text":"NettyBlockTransferService \u00b6 NettyBlockTransferService is a BlockTransferService that uses Netty for uploading and fetching blocks of data. Creating Instance \u00b6 NettyBlockTransferService takes the following to be created: SparkConf SecurityManager Bind Address Host Name Port Number of CPU Cores Driver RpcEndpointRef NettyBlockTransferService is created when: SparkEnv utility is used to create a SparkEnv (for the driver and executors and creates a BlockManager ) Initializing \u00b6 init ( blockDataManager : BlockDataManager ): Unit init is part of the BlockTransferService abstraction. init creates a NettyBlockRpcServer (with the application ID , a JavaSerializer and the given BlockDataManager ). init creates a TransportContext (with the NettyBlockRpcServer just created) and requests it for a TransportClientFactory . init createServer . In the end, init prints out the following INFO message to the logs: Server created on [hostName]:[port] Fetching Blocks \u00b6 fetchBlocks ( host : String , port : Int , execId : String , blockIds : Array [ String ], listener : BlockFetchingListener , tempFileManager : DownloadFileManager ): Unit fetchBlocks is part of the BlockStoreClient abstraction. fetchBlocks prints out the following TRACE message to the logs: Fetch blocks from [host]:[port] (executor id [execId]) fetchBlocks creates a BlockFetchStarter . fetchBlocks requests the TransportConf for the maxIORetries . With the maxIORetries above zero, fetchBlocks creates a RetryingBlockFetcher (with the BlockFetchStarter , the blockIds and the BlockFetchingListener ) and starts it . Otherwise, fetchBlocks requests the BlockFetchStarter to createAndStart (with the blockIds and the BlockFetchingListener ). In case of any Exception , fetchBlocks prints out the following ERROR message to the logs and the given BlockFetchingListener gets notified . Exception while beginning fetchBlocks Uploading Block \u00b6 uploadBlock ( hostname : String , port : Int , execId : String , blockId : BlockId , blockData : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Future [ Unit ] uploadBlock is part of the BlockTransferService abstraction. uploadBlock creates a TransportClient (with the given hostname and port ). uploadBlock serializes the given StorageLevel and ClassTag (using a JavaSerializer ). uploadBlock uses a stream to transfer shuffle blocks when one of the following holds: The size of the block data ( ManagedBuffer ) is above spark.network.maxRemoteBlockSizeFetchToMem configuration property The given BlockId is a shuffle block For stream transfer uploadBlock requests the TransportClient to uploadStream . Otherwise, uploadBlock requests the TransportClient to sendRpc a UploadBlock message. Note UploadBlock message is processed by NettyBlockRpcServer . With the upload successful, uploadBlock prints out the following TRACE message to the logs: Successfully uploaded block [blockId] [as stream] With the upload failed, uploadBlock prints out the following ERROR message to the logs: Error while uploading block [blockId] [as stream] Logging \u00b6 Enable ALL logging level for org.apache.spark.network.netty.NettyBlockTransferService logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.netty.NettyBlockTransferService=ALL Refer to Logging .","title":"NettyBlockTransferService"},{"location":"storage/NettyBlockTransferService/#nettyblocktransferservice","text":"NettyBlockTransferService is a BlockTransferService that uses Netty for uploading and fetching blocks of data.","title":"NettyBlockTransferService"},{"location":"storage/NettyBlockTransferService/#creating-instance","text":"NettyBlockTransferService takes the following to be created: SparkConf SecurityManager Bind Address Host Name Port Number of CPU Cores Driver RpcEndpointRef NettyBlockTransferService is created when: SparkEnv utility is used to create a SparkEnv (for the driver and executors and creates a BlockManager )","title":"Creating Instance"},{"location":"storage/NettyBlockTransferService/#initializing","text":"init ( blockDataManager : BlockDataManager ): Unit init is part of the BlockTransferService abstraction. init creates a NettyBlockRpcServer (with the application ID , a JavaSerializer and the given BlockDataManager ). init creates a TransportContext (with the NettyBlockRpcServer just created) and requests it for a TransportClientFactory . init createServer . In the end, init prints out the following INFO message to the logs: Server created on [hostName]:[port]","title":" Initializing"},{"location":"storage/NettyBlockTransferService/#fetching-blocks","text":"fetchBlocks ( host : String , port : Int , execId : String , blockIds : Array [ String ], listener : BlockFetchingListener , tempFileManager : DownloadFileManager ): Unit fetchBlocks is part of the BlockStoreClient abstraction. fetchBlocks prints out the following TRACE message to the logs: Fetch blocks from [host]:[port] (executor id [execId]) fetchBlocks creates a BlockFetchStarter . fetchBlocks requests the TransportConf for the maxIORetries . With the maxIORetries above zero, fetchBlocks creates a RetryingBlockFetcher (with the BlockFetchStarter , the blockIds and the BlockFetchingListener ) and starts it . Otherwise, fetchBlocks requests the BlockFetchStarter to createAndStart (with the blockIds and the BlockFetchingListener ). In case of any Exception , fetchBlocks prints out the following ERROR message to the logs and the given BlockFetchingListener gets notified . Exception while beginning fetchBlocks","title":" Fetching Blocks"},{"location":"storage/NettyBlockTransferService/#uploading-block","text":"uploadBlock ( hostname : String , port : Int , execId : String , blockId : BlockId , blockData : ManagedBuffer , level : StorageLevel , classTag : ClassTag [ _ ]): Future [ Unit ] uploadBlock is part of the BlockTransferService abstraction. uploadBlock creates a TransportClient (with the given hostname and port ). uploadBlock serializes the given StorageLevel and ClassTag (using a JavaSerializer ). uploadBlock uses a stream to transfer shuffle blocks when one of the following holds: The size of the block data ( ManagedBuffer ) is above spark.network.maxRemoteBlockSizeFetchToMem configuration property The given BlockId is a shuffle block For stream transfer uploadBlock requests the TransportClient to uploadStream . Otherwise, uploadBlock requests the TransportClient to sendRpc a UploadBlock message. Note UploadBlock message is processed by NettyBlockRpcServer . With the upload successful, uploadBlock prints out the following TRACE message to the logs: Successfully uploaded block [blockId] [as stream] With the upload failed, uploadBlock prints out the following ERROR message to the logs: Error while uploading block [blockId] [as stream]","title":" Uploading Block"},{"location":"storage/NettyBlockTransferService/#logging","text":"Enable ALL logging level for org.apache.spark.network.netty.NettyBlockTransferService logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.netty.NettyBlockTransferService=ALL Refer to Logging .","title":"Logging"},{"location":"storage/OneForOneBlockFetcher/","text":"OneForOneBlockFetcher \u00b6 Creating Instance \u00b6 OneForOneBlockFetcher takes the following to be created: TransportClient Application ID Executor ID Block IDs ( String[] ) BlockFetchingListener TransportConf DownloadFileManager OneForOneBlockFetcher is created when: NettyBlockTransferService is requested to fetch blocks ExternalBlockStoreClient is requested to fetch blocks createFetchShuffleBlocksMsg \u00b6 FetchShuffleBlocks createFetchShuffleBlocksMsg ( String appId , String execId , String [] blockIds ) createFetchShuffleBlocksMsg ...FIXME Starting \u00b6 void start () start requests the TransportClient to sendRpc the BlockTransferMessage start ...FIXME start is used when: ExternalBlockStoreClient is requested to fetchBlocks NettyBlockTransferService is requested to fetchBlocks Logging \u00b6 Enable ALL logging level for org.apache.spark.network.shuffle.OneForOneBlockFetcher logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=ALL Refer to Logging .","title":"OneForOneBlockFetcher"},{"location":"storage/OneForOneBlockFetcher/#oneforoneblockfetcher","text":"","title":"OneForOneBlockFetcher"},{"location":"storage/OneForOneBlockFetcher/#creating-instance","text":"OneForOneBlockFetcher takes the following to be created: TransportClient Application ID Executor ID Block IDs ( String[] ) BlockFetchingListener TransportConf DownloadFileManager OneForOneBlockFetcher is created when: NettyBlockTransferService is requested to fetch blocks ExternalBlockStoreClient is requested to fetch blocks","title":"Creating Instance"},{"location":"storage/OneForOneBlockFetcher/#createfetchshuffleblocksmsg","text":"FetchShuffleBlocks createFetchShuffleBlocksMsg ( String appId , String execId , String [] blockIds ) createFetchShuffleBlocksMsg ...FIXME","title":" createFetchShuffleBlocksMsg"},{"location":"storage/OneForOneBlockFetcher/#starting","text":"void start () start requests the TransportClient to sendRpc the BlockTransferMessage start ...FIXME start is used when: ExternalBlockStoreClient is requested to fetchBlocks NettyBlockTransferService is requested to fetchBlocks","title":" Starting"},{"location":"storage/OneForOneBlockFetcher/#logging","text":"Enable ALL logging level for org.apache.spark.network.shuffle.OneForOneBlockFetcher logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=ALL Refer to Logging .","title":"Logging"},{"location":"storage/RDDInfo/","text":"RDDInfo \u00b6 RDDInfo is...FIXME","title":"RDDInfo"},{"location":"storage/RDDInfo/#rddinfo","text":"RDDInfo is...FIXME","title":"RDDInfo"},{"location":"storage/RandomBlockReplicationPolicy/","text":"RandomBlockReplicationPolicy \u00b6 RandomBlockReplicationPolicy is...FIXME","title":"RandomBlockReplicationPolicy"},{"location":"storage/RandomBlockReplicationPolicy/#randomblockreplicationpolicy","text":"RandomBlockReplicationPolicy is...FIXME","title":"RandomBlockReplicationPolicy"},{"location":"storage/ShuffleBlockFetcherIterator/","text":"ShuffleBlockFetcherIterator \u00b6 ShuffleBlockFetcherIterator is an Iterator[(BlockId, InputStream)] ( Scala ) that fetches shuffle blocks from local or remote BlockManager s (and makes them available as an InputStream ). ShuffleBlockFetcherIterator allows for a synchronous iteration over shuffle blocks so a caller can handle them in a pipelined fashion as they are received. ShuffleBlockFetcherIterator is exhausted (and can provide no elements ) when the number of blocks already processed is at least the total number of blocks to fetch . ShuffleBlockFetcherIterator throttles the remote fetches to avoid consuming too much memory. Creating Instance \u00b6 ShuffleBlockFetcherIterator takes the following to be created: TaskContext BlockStoreClient BlockManager Block s to Fetch by Address ( Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])] ) Stream Wrapper Function ( (BlockId, InputStream) => InputStream ) spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.network.maxRemoteBlockSizeFetchToMem spark.shuffle.detectCorrupt spark.shuffle.detectCorrupt.useExtraMemory ShuffleReadMetricsReporter doBatchFetch flag While being created, ShuffleBlockFetcherIterator initializes itself . ShuffleBlockFetcherIterator is created when: BlockStoreShuffleReader is requested to read combined key-value records for a reduce task Initializing \u00b6 initialize (): Unit initialize registers a task cleanup and fetches shuffle blocks from remote and local storage:BlockManager.md[BlockManagers]. Internally, initialize uses the TaskContext to register the ShuffleFetchCompletionListener (to cleanup ). initialize partitionBlocksByFetchMode . initialize ...FIXME partitionBlocksByFetchMode \u00b6 partitionBlocksByFetchMode (): ArrayBuffer [ FetchRequest ] partitionBlocksByFetchMode ...FIXME collectFetchRequests \u00b6 collectFetchRequests ( address : BlockManagerId , blockInfos : Seq [( BlockId , Long , Int )], collectedRemoteRequests : ArrayBuffer [ FetchRequest ]): Unit collectFetchRequests ...FIXME createFetchRequests \u00b6 createFetchRequests ( curBlocks : Seq [ FetchBlockInfo ], address : BlockManagerId , isLast : Boolean , collectedRemoteRequests : ArrayBuffer [ FetchRequest ]): Seq [ FetchBlockInfo ] createFetchRequests ...FIXME fetchUpToMaxBytes \u00b6 fetchUpToMaxBytes (): Unit fetchUpToMaxBytes ...FIXME fetchUpToMaxBytes is used when: ShuffleBlockFetcherIterator is requested to initialize and next Sending Remote Shuffle Block Fetch Request \u00b6 sendRequest ( req : FetchRequest ): Unit sendRequest prints out the following DEBUG message to the logs: Sending request for [n] blocks ([size]) from [hostPort] sendRequest add the size of the blocks in the FetchRequest to bytesInFlight and increments the reqsInFlight internal counters. sendRequest requests the ShuffleClient to fetch the blocks with a new BlockFetchingListener (and this ShuffleBlockFetcherIterator when the size of the blocks in the FetchRequest is higher than the maxReqSizeShuffleToMem ). sendRequest is used when: ShuffleBlockFetcherIterator is requested to fetch remote shuffle blocks BlockFetchingListener \u00b6 sendRequest creates a new BlockFetchingListener to be notified about successes or failures of shuffle block fetch requests. onBlockFetchSuccess \u00b6 On onBlockFetchSuccess the BlockFetchingListener adds a SuccessFetchResult to the results registry and prints out the following DEBUG message to the logs (when not a zombie ): remainingBlocks: [remainingBlocks] In the end, onBlockFetchSuccess prints out the following TRACE message to the logs: Got remote block [blockId] after [time] onBlockFetchFailure \u00b6 On onBlockFetchFailure the BlockFetchingListener adds a FailureFetchResult to the results registry and prints out the following ERROR message to the logs: Failed to get block(s) from [host]:[port] FetchResults \u00b6 results : LinkedBlockingQueue [ FetchResult ] ShuffleBlockFetcherIterator uses an internal FIFO blocking queue ( Java ) of FetchResult s. results is used for fetching the next element . For remote blocks, FetchResult s are added in sendRequest : SuccessFetchResult s after a BlockFetchingListener is notified about onBlockFetchSuccess FailureFetchResult s after a BlockFetchingListener is notified about onBlockFetchFailure For local blocks, FetchResult s are added in fetchLocalBlocks : SuccessFetchResult s after the BlockManager has successfully getLocalBlockData FailureFetchResult s otherwise For local blocks, FetchResult s are added in fetchHostLocalBlock : SuccessFetchResult s after the BlockManager has successfully getHostLocalShuffleData FailureFetchResult s otherwise FailureFetchResult s can also be added in fetchHostLocalBlocks . Cleaned up in cleanup hasNext \u00b6 hasNext : Boolean hasNext is part of the Iterator ( Scala ) abstraction (to test whether this iterator can provide another element). hasNext is true when numBlocksProcessed is below numBlocksToFetch . Retrieving Next Element \u00b6 next (): ( BlockId , InputStream ) next is part of the Iterator ( Scala ) abstraction (to produce the next element of this iterator). next ...FIXME numBlocksProcessed \u00b6 The number of blocks fetched and consumed numBlocksToFetch \u00b6 Total number of blocks to fetch and consume ShuffleBlockFetcherIterator can produce up to numBlocksToFetch elements. numBlocksToFetch is increased every time ShuffleBlockFetcherIterator is requested to partitionBlocksByFetchMode that prints it out as the INFO message to the logs: Getting [numBlocksToFetch] non-empty blocks out of [totalBlocks] blocks releaseCurrentResultBuffer \u00b6 releaseCurrentResultBuffer (): Unit releaseCurrentResultBuffer ...FIXME releaseCurrentResultBuffer is used when: ShuffleBlockFetcherIterator is requested to cleanup BufferReleasingInputStream is requested to close ShuffleFetchCompletionListener \u00b6 ShuffleBlockFetcherIterator creates a ShuffleFetchCompletionListener when created . ShuffleFetchCompletionListener is used when initialize and toCompletionIterator . Cleaning Up \u00b6 cleanup (): Unit cleanup marks this ShuffleBlockFetcherIterator a zombie . cleanup releases the current result buffer . cleanup iterates over results internal queue and for every SuccessFetchResult , increments remote bytes read and blocks fetched shuffle task metrics, and eventually releases the managed buffer. bytesInFlight \u00b6 The bytes of fetched remote shuffle blocks in flight Starts at 0 when ShuffleBlockFetcherIterator is created Incremented every sendRequest and decremented every next . ShuffleBlockFetcherIterator makes sure that the invariant of bytesInFlight is below maxBytesInFlight every remote shuffle block fetch . reqsInFlight \u00b6 The number of remote shuffle block fetch requests in flight. Starts at 0 when ShuffleBlockFetcherIterator is created Incremented every sendRequest and decremented every next . ShuffleBlockFetcherIterator makes sure that the invariant of reqsInFlight is below maxReqsInFlight every remote shuffle block fetch . isZombie \u00b6 Controls whether ShuffleBlockFetcherIterator is still active and records SuccessFetchResult s on successful shuffle block fetches . Starts false when ShuffleBlockFetcherIterator is created Enabled ( true ) in cleanup . When enabled, registerTempFileToClean is a noop. DownloadFileManager \u00b6 ShuffleBlockFetcherIterator is a DownloadFileManager . Logging \u00b6 Enable ALL logging level for org.apache.spark.storage.ShuffleBlockFetcherIterator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=ALL Refer to Logging .","title":"ShuffleBlockFetcherIterator"},{"location":"storage/ShuffleBlockFetcherIterator/#shuffleblockfetcheriterator","text":"ShuffleBlockFetcherIterator is an Iterator[(BlockId, InputStream)] ( Scala ) that fetches shuffle blocks from local or remote BlockManager s (and makes them available as an InputStream ). ShuffleBlockFetcherIterator allows for a synchronous iteration over shuffle blocks so a caller can handle them in a pipelined fashion as they are received. ShuffleBlockFetcherIterator is exhausted (and can provide no elements ) when the number of blocks already processed is at least the total number of blocks to fetch . ShuffleBlockFetcherIterator throttles the remote fetches to avoid consuming too much memory.","title":"ShuffleBlockFetcherIterator"},{"location":"storage/ShuffleBlockFetcherIterator/#creating-instance","text":"ShuffleBlockFetcherIterator takes the following to be created: TaskContext BlockStoreClient BlockManager Block s to Fetch by Address ( Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])] ) Stream Wrapper Function ( (BlockId, InputStream) => InputStream ) spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.network.maxRemoteBlockSizeFetchToMem spark.shuffle.detectCorrupt spark.shuffle.detectCorrupt.useExtraMemory ShuffleReadMetricsReporter doBatchFetch flag While being created, ShuffleBlockFetcherIterator initializes itself . ShuffleBlockFetcherIterator is created when: BlockStoreShuffleReader is requested to read combined key-value records for a reduce task","title":"Creating Instance"},{"location":"storage/ShuffleBlockFetcherIterator/#initializing","text":"initialize (): Unit initialize registers a task cleanup and fetches shuffle blocks from remote and local storage:BlockManager.md[BlockManagers]. Internally, initialize uses the TaskContext to register the ShuffleFetchCompletionListener (to cleanup ). initialize partitionBlocksByFetchMode . initialize ...FIXME","title":" Initializing"},{"location":"storage/ShuffleBlockFetcherIterator/#partitionblocksbyfetchmode","text":"partitionBlocksByFetchMode (): ArrayBuffer [ FetchRequest ] partitionBlocksByFetchMode ...FIXME","title":" partitionBlocksByFetchMode"},{"location":"storage/ShuffleBlockFetcherIterator/#collectfetchrequests","text":"collectFetchRequests ( address : BlockManagerId , blockInfos : Seq [( BlockId , Long , Int )], collectedRemoteRequests : ArrayBuffer [ FetchRequest ]): Unit collectFetchRequests ...FIXME","title":" collectFetchRequests"},{"location":"storage/ShuffleBlockFetcherIterator/#createfetchrequests","text":"createFetchRequests ( curBlocks : Seq [ FetchBlockInfo ], address : BlockManagerId , isLast : Boolean , collectedRemoteRequests : ArrayBuffer [ FetchRequest ]): Seq [ FetchBlockInfo ] createFetchRequests ...FIXME","title":" createFetchRequests"},{"location":"storage/ShuffleBlockFetcherIterator/#fetchuptomaxbytes","text":"fetchUpToMaxBytes (): Unit fetchUpToMaxBytes ...FIXME fetchUpToMaxBytes is used when: ShuffleBlockFetcherIterator is requested to initialize and next","title":" fetchUpToMaxBytes"},{"location":"storage/ShuffleBlockFetcherIterator/#sending-remote-shuffle-block-fetch-request","text":"sendRequest ( req : FetchRequest ): Unit sendRequest prints out the following DEBUG message to the logs: Sending request for [n] blocks ([size]) from [hostPort] sendRequest add the size of the blocks in the FetchRequest to bytesInFlight and increments the reqsInFlight internal counters. sendRequest requests the ShuffleClient to fetch the blocks with a new BlockFetchingListener (and this ShuffleBlockFetcherIterator when the size of the blocks in the FetchRequest is higher than the maxReqSizeShuffleToMem ). sendRequest is used when: ShuffleBlockFetcherIterator is requested to fetch remote shuffle blocks","title":" Sending Remote Shuffle Block Fetch Request"},{"location":"storage/ShuffleBlockFetcherIterator/#blockfetchinglistener","text":"sendRequest creates a new BlockFetchingListener to be notified about successes or failures of shuffle block fetch requests.","title":" BlockFetchingListener"},{"location":"storage/ShuffleBlockFetcherIterator/#onblockfetchsuccess","text":"On onBlockFetchSuccess the BlockFetchingListener adds a SuccessFetchResult to the results registry and prints out the following DEBUG message to the logs (when not a zombie ): remainingBlocks: [remainingBlocks] In the end, onBlockFetchSuccess prints out the following TRACE message to the logs: Got remote block [blockId] after [time]","title":" onBlockFetchSuccess"},{"location":"storage/ShuffleBlockFetcherIterator/#onblockfetchfailure","text":"On onBlockFetchFailure the BlockFetchingListener adds a FailureFetchResult to the results registry and prints out the following ERROR message to the logs: Failed to get block(s) from [host]:[port]","title":" onBlockFetchFailure"},{"location":"storage/ShuffleBlockFetcherIterator/#fetchresults","text":"results : LinkedBlockingQueue [ FetchResult ] ShuffleBlockFetcherIterator uses an internal FIFO blocking queue ( Java ) of FetchResult s. results is used for fetching the next element . For remote blocks, FetchResult s are added in sendRequest : SuccessFetchResult s after a BlockFetchingListener is notified about onBlockFetchSuccess FailureFetchResult s after a BlockFetchingListener is notified about onBlockFetchFailure For local blocks, FetchResult s are added in fetchLocalBlocks : SuccessFetchResult s after the BlockManager has successfully getLocalBlockData FailureFetchResult s otherwise For local blocks, FetchResult s are added in fetchHostLocalBlock : SuccessFetchResult s after the BlockManager has successfully getHostLocalShuffleData FailureFetchResult s otherwise FailureFetchResult s can also be added in fetchHostLocalBlocks . Cleaned up in cleanup","title":" FetchResults"},{"location":"storage/ShuffleBlockFetcherIterator/#hasnext","text":"hasNext : Boolean hasNext is part of the Iterator ( Scala ) abstraction (to test whether this iterator can provide another element). hasNext is true when numBlocksProcessed is below numBlocksToFetch .","title":" hasNext"},{"location":"storage/ShuffleBlockFetcherIterator/#retrieving-next-element","text":"next (): ( BlockId , InputStream ) next is part of the Iterator ( Scala ) abstraction (to produce the next element of this iterator). next ...FIXME","title":" Retrieving Next Element"},{"location":"storage/ShuffleBlockFetcherIterator/#numblocksprocessed","text":"The number of blocks fetched and consumed","title":" numBlocksProcessed"},{"location":"storage/ShuffleBlockFetcherIterator/#numblockstofetch","text":"Total number of blocks to fetch and consume ShuffleBlockFetcherIterator can produce up to numBlocksToFetch elements. numBlocksToFetch is increased every time ShuffleBlockFetcherIterator is requested to partitionBlocksByFetchMode that prints it out as the INFO message to the logs: Getting [numBlocksToFetch] non-empty blocks out of [totalBlocks] blocks","title":" numBlocksToFetch"},{"location":"storage/ShuffleBlockFetcherIterator/#releasecurrentresultbuffer","text":"releaseCurrentResultBuffer (): Unit releaseCurrentResultBuffer ...FIXME releaseCurrentResultBuffer is used when: ShuffleBlockFetcherIterator is requested to cleanup BufferReleasingInputStream is requested to close","title":" releaseCurrentResultBuffer"},{"location":"storage/ShuffleBlockFetcherIterator/#shufflefetchcompletionlistener","text":"ShuffleBlockFetcherIterator creates a ShuffleFetchCompletionListener when created . ShuffleFetchCompletionListener is used when initialize and toCompletionIterator .","title":" ShuffleFetchCompletionListener"},{"location":"storage/ShuffleBlockFetcherIterator/#cleaning-up","text":"cleanup (): Unit cleanup marks this ShuffleBlockFetcherIterator a zombie . cleanup releases the current result buffer . cleanup iterates over results internal queue and for every SuccessFetchResult , increments remote bytes read and blocks fetched shuffle task metrics, and eventually releases the managed buffer.","title":" Cleaning Up"},{"location":"storage/ShuffleBlockFetcherIterator/#bytesinflight","text":"The bytes of fetched remote shuffle blocks in flight Starts at 0 when ShuffleBlockFetcherIterator is created Incremented every sendRequest and decremented every next . ShuffleBlockFetcherIterator makes sure that the invariant of bytesInFlight is below maxBytesInFlight every remote shuffle block fetch .","title":" bytesInFlight"},{"location":"storage/ShuffleBlockFetcherIterator/#reqsinflight","text":"The number of remote shuffle block fetch requests in flight. Starts at 0 when ShuffleBlockFetcherIterator is created Incremented every sendRequest and decremented every next . ShuffleBlockFetcherIterator makes sure that the invariant of reqsInFlight is below maxReqsInFlight every remote shuffle block fetch .","title":" reqsInFlight"},{"location":"storage/ShuffleBlockFetcherIterator/#iszombie","text":"Controls whether ShuffleBlockFetcherIterator is still active and records SuccessFetchResult s on successful shuffle block fetches . Starts false when ShuffleBlockFetcherIterator is created Enabled ( true ) in cleanup . When enabled, registerTempFileToClean is a noop.","title":" isZombie"},{"location":"storage/ShuffleBlockFetcherIterator/#downloadfilemanager","text":"ShuffleBlockFetcherIterator is a DownloadFileManager .","title":" DownloadFileManager"},{"location":"storage/ShuffleBlockFetcherIterator/#logging","text":"Enable ALL logging level for org.apache.spark.storage.ShuffleBlockFetcherIterator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=ALL Refer to Logging .","title":"Logging"},{"location":"storage/ShuffleFetchCompletionListener/","text":"ShuffleFetchCompletionListener \u00b6 ShuffleFetchCompletionListener is a TaskCompletionListener (that ShuffleBlockFetcherIterator uses to clean up after the owning task is completed). Creating Instance \u00b6 ShuffleFetchCompletionListener takes the following to be created: ShuffleBlockFetcherIterator ShuffleFetchCompletionListener is created when: ShuffleBlockFetcherIterator is created onTaskCompletion \u00b6 onTaskCompletion ( context : TaskContext ): Unit onTaskCompletion is part of the TaskCompletionListener abstraction. onTaskCompletion requests the ShuffleBlockFetcherIterator (if available) to cleanup . In the end, onTaskCompletion nulls out the reference to the ShuffleBlockFetcherIterator (to make it available for garbage collection).","title":"ShuffleFetchCompletionListener"},{"location":"storage/ShuffleFetchCompletionListener/#shufflefetchcompletionlistener","text":"ShuffleFetchCompletionListener is a TaskCompletionListener (that ShuffleBlockFetcherIterator uses to clean up after the owning task is completed).","title":"ShuffleFetchCompletionListener"},{"location":"storage/ShuffleFetchCompletionListener/#creating-instance","text":"ShuffleFetchCompletionListener takes the following to be created: ShuffleBlockFetcherIterator ShuffleFetchCompletionListener is created when: ShuffleBlockFetcherIterator is created","title":"Creating Instance"},{"location":"storage/ShuffleFetchCompletionListener/#ontaskcompletion","text":"onTaskCompletion ( context : TaskContext ): Unit onTaskCompletion is part of the TaskCompletionListener abstraction. onTaskCompletion requests the ShuffleBlockFetcherIterator (if available) to cleanup . In the end, onTaskCompletion nulls out the reference to the ShuffleBlockFetcherIterator (to make it available for garbage collection).","title":" onTaskCompletion"},{"location":"storage/ShuffleMetricsSource/","text":"= ShuffleMetricsSource ShuffleMetricsSource is the metrics:spark-metrics-Source.md[metrics source] of a storage:BlockManager.md[] for < >. ShuffleMetricsSource lives on a Spark executor and is executor:Executor.md#creating-instance-BlockManager-shuffleMetricsSource[registered only when a Spark application runs in a non-local / cluster mode]. .Registering ShuffleMetricsSource with \"executor\" MetricsSystem image::ShuffleMetricsSource.png[align=\"center\"] == [[creating-instance]] Creating Instance ShuffleMetricsSource takes the following to be created: < > < > ShuffleMetricsSource is created when BlockManager is requested for the storage:BlockManager.md#shuffleMetricsSource[shuffle metrics source]. == [[sourceName]] Source Name ShuffleMetricsSource is given a name when < > that is one of the following: NettyBlockTransfer when spark.shuffle.service.enabled configuration property is off ( false ) ExternalShuffle when spark.shuffle.service.enabled configuration property is on ( true )","title":"ShuffleMetricsSource"},{"location":"storage/ShuffleMigrationRunnable/","text":"ShuffleMigrationRunnable \u00b6 ShuffleMigrationRunnable is...FIXME","title":"ShuffleMigrationRunnable"},{"location":"storage/ShuffleMigrationRunnable/#shufflemigrationrunnable","text":"ShuffleMigrationRunnable is...FIXME","title":"ShuffleMigrationRunnable"},{"location":"storage/StorageLevel/","text":"StorageLevel \u00b6 StorageLevel is the following flags for controlling the storage of an RDD . Flag Default Value useDisk false useMemory true useOffHeap false deserialized false replication 1 Restrictions \u00b6 The replication is restricted to be less than 40 (for calculating the hash code) Off-heap storage level does not support deserialized storage Validation \u00b6 isValid : Boolean StorageLevel is considered valid when the following all hold: Uses memory or disk Replication is non-zero positive number (between the default 1 and 40 ) Externalizable \u00b6 DirectTaskResult is an Externalizable ( Java ). writeExternal \u00b6 writeExternal ( out : ObjectOutput ): Unit writeExternal is part of the Externalizable ( Java ) abstraction. writeExternal writes the bitwise representation out followed by the replication of this StorageLevel . Bitwise Integer Representation \u00b6 toInt : Int toInt converts this StorageLevel to numeric (binary) representation by turning the corresponding bits on for the following (if used and in that order): deserialized useOffHeap useMemory useDisk In other words, the following number in bitwise representation says the StorageLevel is deserialized and useMemory : import org . apache . spark . storage . StorageLevel . MEMORY_ONLY assert ( MEMORY_ONLY . toInt == ( 0 | 1 | 4 )) scala > println ( MEMORY_ONLY . toInt . toBinaryString ) 101 toInt is used when: StorageLevel is requested to writeExternal and hashCode","title":"StorageLevel"},{"location":"storage/StorageLevel/#storagelevel","text":"StorageLevel is the following flags for controlling the storage of an RDD . Flag Default Value useDisk false useMemory true useOffHeap false deserialized false replication 1","title":"StorageLevel"},{"location":"storage/StorageLevel/#restrictions","text":"The replication is restricted to be less than 40 (for calculating the hash code) Off-heap storage level does not support deserialized storage","title":"Restrictions"},{"location":"storage/StorageLevel/#validation","text":"isValid : Boolean StorageLevel is considered valid when the following all hold: Uses memory or disk Replication is non-zero positive number (between the default 1 and 40 )","title":" Validation"},{"location":"storage/StorageLevel/#externalizable","text":"DirectTaskResult is an Externalizable ( Java ).","title":" Externalizable"},{"location":"storage/StorageLevel/#writeexternal","text":"writeExternal ( out : ObjectOutput ): Unit writeExternal is part of the Externalizable ( Java ) abstraction. writeExternal writes the bitwise representation out followed by the replication of this StorageLevel .","title":" writeExternal"},{"location":"storage/StorageLevel/#bitwise-integer-representation","text":"toInt : Int toInt converts this StorageLevel to numeric (binary) representation by turning the corresponding bits on for the following (if used and in that order): deserialized useOffHeap useMemory useDisk In other words, the following number in bitwise representation says the StorageLevel is deserialized and useMemory : import org . apache . spark . storage . StorageLevel . MEMORY_ONLY assert ( MEMORY_ONLY . toInt == ( 0 | 1 | 4 )) scala > println ( MEMORY_ONLY . toInt . toBinaryString ) 101 toInt is used when: StorageLevel is requested to writeExternal and hashCode","title":" Bitwise Integer Representation"},{"location":"storage/StorageStatus/","text":"== [[StorageStatus]] StorageStatus StorageStatus is a developer API that Spark uses to pass \"just enough\" information about registered storage:BlockManager.md[BlockManagers] in a Spark application between Spark services (mostly for monitoring purposes like spark-webui.md[web UI] or SparkListener.md[]s). [NOTE] \u00b6 There are two ways to access StorageStatus about all the known BlockManagers in a Spark application: SparkContext.md#getExecutorStorageStatus[SparkContext.getExecutorStorageStatus] * Being a SparkListener.md[] and intercepting SparkListener.md#onBlockManagerAdded[onBlockManagerAdded] and SparkListener.md#onBlockManagerRemoved[onBlockManagerRemoved] events \u00b6 StorageStatus is < > when: BlockManagerMasterEndpoint storage:BlockManagerMasterEndpoint.md#storageStatus[is requested for storage status] (of every storage:BlockManager.md[BlockManager] in a Spark application) [[internal-registries]] .StorageStatus's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[_nonRddBlocks]] _nonRddBlocks | Lookup table of BlockIds per BlockId . Used when...FIXME | [[_rddBlocks]] _rddBlocks | Lookup table of BlockIds with BlockStatus per RDD id. Used when...FIXME |=== === [[updateStorageInfo]] updateStorageInfo Internal Method [source, scala] \u00b6 updateStorageInfo( blockId: BlockId, newBlockStatus: BlockStatus): Unit updateStorageInfo ...FIXME NOTE: updateStorageInfo is used when...FIXME === [[creating-instance]] Creating StorageStatus Instance StorageStatus takes the following when created: [[blockManagerId]] storage:BlockManagerId.md[] [[maxMem]] Maximum memory -- storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on the BlockManager ] StorageStatus initializes the < >. === [[rddBlocksById]] Getting RDD Blocks For RDD -- rddBlocksById Method [source, scala] \u00b6 rddBlocksById(rddId: Int): Map[BlockId, BlockStatus] \u00b6 rddBlocksById gives the blocks (as BlockId with their status as BlockStatus ) that belong to rddId RDD. === [[removeBlock]] Removing Block (From Internal Registries) -- removeBlock Internal Method [source, scala] \u00b6 removeBlock(blockId: BlockId): Option[BlockStatus] \u00b6 removeBlock removes blockId from <<_rddBlocks, _rddBlocks>> registry and returns it. Internally, removeBlock < > of blockId (to be empty, i.e. removed). removeBlock branches off per the type of storage:BlockId.md[], i.e. RDDBlockId or not. For a RDDBlockId , removeBlock finds the RDD in <<_rddBlocks, _rddBlocks>> and removes the blockId . removeBlock removes the RDD (from <<_rddBlocks, _rddBlocks>>) completely, if there are no more blocks registered. For a non- RDDBlockId , removeBlock removes blockId from <<_nonRddBlocks, _nonRddBlocks>> registry. === [[addBlock]] Registering Status of Data Block -- addBlock Method [source, scala] \u00b6 addBlock( blockId: BlockId, blockStatus: BlockStatus): Unit addBlock ...FIXME NOTE: addBlock is used when...FIXME === [[getBlock]] getBlock Method [source, scala] \u00b6 getBlock(blockId: BlockId): Option[BlockStatus] \u00b6 getBlock ...FIXME NOTE: getBlock is used when...FIXME","title":"StorageStatus"},{"location":"storage/StorageStatus/#note","text":"There are two ways to access StorageStatus about all the known BlockManagers in a Spark application: SparkContext.md#getExecutorStorageStatus[SparkContext.getExecutorStorageStatus]","title":"[NOTE]"},{"location":"storage/StorageStatus/#being-a-sparklistenermd-and-intercepting-sparklistenermdonblockmanageraddedonblockmanageradded-and-sparklistenermdonblockmanagerremovedonblockmanagerremoved-events","text":"StorageStatus is < > when: BlockManagerMasterEndpoint storage:BlockManagerMasterEndpoint.md#storageStatus[is requested for storage status] (of every storage:BlockManager.md[BlockManager] in a Spark application) [[internal-registries]] .StorageStatus's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[_nonRddBlocks]] _nonRddBlocks | Lookup table of BlockIds per BlockId . Used when...FIXME | [[_rddBlocks]] _rddBlocks | Lookup table of BlockIds with BlockStatus per RDD id. Used when...FIXME |=== === [[updateStorageInfo]] updateStorageInfo Internal Method","title":"* Being a SparkListener.md[] and intercepting SparkListener.md#onBlockManagerAdded[onBlockManagerAdded] and SparkListener.md#onBlockManagerRemoved[onBlockManagerRemoved] events"},{"location":"storage/StorageStatus/#source-scala","text":"updateStorageInfo( blockId: BlockId, newBlockStatus: BlockStatus): Unit updateStorageInfo ...FIXME NOTE: updateStorageInfo is used when...FIXME === [[creating-instance]] Creating StorageStatus Instance StorageStatus takes the following when created: [[blockManagerId]] storage:BlockManagerId.md[] [[maxMem]] Maximum memory -- storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on the BlockManager ] StorageStatus initializes the < >. === [[rddBlocksById]] Getting RDD Blocks For RDD -- rddBlocksById Method","title":"[source, scala]"},{"location":"storage/StorageStatus/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#rddblocksbyidrddid-int-mapblockid-blockstatus","text":"rddBlocksById gives the blocks (as BlockId with their status as BlockStatus ) that belong to rddId RDD. === [[removeBlock]] Removing Block (From Internal Registries) -- removeBlock Internal Method","title":"rddBlocksById(rddId: Int): Map[BlockId, BlockStatus]"},{"location":"storage/StorageStatus/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#removeblockblockid-blockid-optionblockstatus","text":"removeBlock removes blockId from <<_rddBlocks, _rddBlocks>> registry and returns it. Internally, removeBlock < > of blockId (to be empty, i.e. removed). removeBlock branches off per the type of storage:BlockId.md[], i.e. RDDBlockId or not. For a RDDBlockId , removeBlock finds the RDD in <<_rddBlocks, _rddBlocks>> and removes the blockId . removeBlock removes the RDD (from <<_rddBlocks, _rddBlocks>>) completely, if there are no more blocks registered. For a non- RDDBlockId , removeBlock removes blockId from <<_nonRddBlocks, _nonRddBlocks>> registry. === [[addBlock]] Registering Status of Data Block -- addBlock Method","title":"removeBlock(blockId: BlockId): Option[BlockStatus]"},{"location":"storage/StorageStatus/#source-scala_3","text":"addBlock( blockId: BlockId, blockStatus: BlockStatus): Unit addBlock ...FIXME NOTE: addBlock is used when...FIXME === [[getBlock]] getBlock Method","title":"[source, scala]"},{"location":"storage/StorageStatus/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#getblockblockid-blockid-optionblockstatus","text":"getBlock ...FIXME NOTE: getBlock is used when...FIXME","title":"getBlock(blockId: BlockId): Option[BlockStatus]"},{"location":"storage/StorageUtils/","text":"StorageUtils \u00b6 Port of External Shuffle Service \u00b6 externalShuffleServicePort ( conf : SparkConf ): Int externalShuffleServicePort ...FIXME externalShuffleServicePort is used when: BlockManager is created BlockManagerMasterEndpoint is created","title":"StorageUtils"},{"location":"storage/StorageUtils/#storageutils","text":"","title":"StorageUtils"},{"location":"storage/StorageUtils/#port-of-external-shuffle-service","text":"externalShuffleServicePort ( conf : SparkConf ): Int externalShuffleServicePort ...FIXME externalShuffleServicePort is used when: BlockManager is created BlockManagerMasterEndpoint is created","title":" Port of External Shuffle Service"},{"location":"tools/AbstractCommandBuilder/","text":"AbstractCommandBuilder \u00b6 AbstractCommandBuilder is an abstraction of command builders . Contract \u00b6 buildCommand \u00b6 List < String > buildCommand ( Map < String , String > env ) Used when: Main is requested to buildCommand WorkerCommandBuilder is requested to buildCommand Implementations \u00b6 SparkClassCommandBuilder SparkSubmitCommandBuilder WorkerCommandBuilder buildJavaCommand \u00b6 List < String > buildJavaCommand ( String extraClassPath ) buildJavaCommand builds the Java command for a Spark application (which is a collection of elements with the path to java executable, JVM options from java-opts file, and a class path). If javaHome is set, buildJavaCommand adds [javaHome]/bin/java to the result Java command. Otherwise, it uses JAVA_HOME or, when no earlier checks succeeded, falls through to java.home Java's system property. CAUTION: FIXME Who sets javaHome internal property and when? buildJavaCommand loads extra Java options from the java-opts file in configuration directory if the file exists and adds them to the result Java command. Eventually, buildJavaCommand builds the class path (with the extra class path if non-empty) and adds it as -cp to the result Java command. buildClassPath \u00b6 List < String > buildClassPath ( String appClassPath ) buildClassPath builds the classpath for a Spark application. Note Directories always end up with the OS-specific file separator at the end of their paths. buildClassPath adds the following in that order: SPARK_CLASSPATH environment variable The input appClassPath The configuration directory (only with SPARK_PREPEND_CLASSES set or SPARK_TESTING being 1 ) Locally compiled Spark classes in classes , test-classes and Core's jars. + CAUTION: FIXME Elaborate on \"locally compiled Spark classes\". (only with SPARK_SQL_TESTING being 1 ) ... + CAUTION: FIXME Elaborate on the SQL testing case HADOOP_CONF_DIR environment variable YARN_CONF_DIR environment variable SPARK_DIST_CLASSPATH environment variable NOTE: childEnv is queried first before System properties. It is always empty for AbstractCommandBuilder (and SparkSubmitCommandBuilder , too). Loading Properties File \u00b6 Properties loadPropertiesFile () loadPropertiesFile loads Spark settings from a properties file (when specified on the command line) or spark-defaults.conf in the configuration directory . loadPropertiesFile loads the settings from the following files starting from the first and checking every location until the first properties file is found: propertiesFile (if specified using --properties-file command-line option or set by AbstractCommandBuilder.setPropertiesFile ). [SPARK_CONF_DIR]/spark-defaults.conf [SPARK_HOME]/conf/spark-defaults.conf Spark's Configuration Directory \u00b6 AbstractCommandBuilder uses getConfDir to compute the current configuration directory of a Spark application. It uses SPARK_CONF_DIR (from childEnv which is always empty anyway or as a environment variable) and falls through to [SPARK_HOME]/conf (with SPARK_HOME from getSparkHome ). Spark's Home Directory \u00b6 AbstractCommandBuilder uses getSparkHome to compute Spark's home directory for a Spark application. It uses SPARK_HOME (from childEnv which is always empty anyway or as a environment variable). If SPARK_HOME is not set, Spark throws a IllegalStateException : Spark home not found; set it explicitly or use the SPARK_HOME environment variable.","title":"AbstractCommandBuilder"},{"location":"tools/AbstractCommandBuilder/#abstractcommandbuilder","text":"AbstractCommandBuilder is an abstraction of command builders .","title":"AbstractCommandBuilder"},{"location":"tools/AbstractCommandBuilder/#contract","text":"","title":"Contract"},{"location":"tools/AbstractCommandBuilder/#buildcommand","text":"List < String > buildCommand ( Map < String , String > env ) Used when: Main is requested to buildCommand WorkerCommandBuilder is requested to buildCommand","title":" buildCommand"},{"location":"tools/AbstractCommandBuilder/#implementations","text":"SparkClassCommandBuilder SparkSubmitCommandBuilder WorkerCommandBuilder","title":"Implementations"},{"location":"tools/AbstractCommandBuilder/#buildjavacommand","text":"List < String > buildJavaCommand ( String extraClassPath ) buildJavaCommand builds the Java command for a Spark application (which is a collection of elements with the path to java executable, JVM options from java-opts file, and a class path). If javaHome is set, buildJavaCommand adds [javaHome]/bin/java to the result Java command. Otherwise, it uses JAVA_HOME or, when no earlier checks succeeded, falls through to java.home Java's system property. CAUTION: FIXME Who sets javaHome internal property and when? buildJavaCommand loads extra Java options from the java-opts file in configuration directory if the file exists and adds them to the result Java command. Eventually, buildJavaCommand builds the class path (with the extra class path if non-empty) and adds it as -cp to the result Java command.","title":" buildJavaCommand"},{"location":"tools/AbstractCommandBuilder/#buildclasspath","text":"List < String > buildClassPath ( String appClassPath ) buildClassPath builds the classpath for a Spark application. Note Directories always end up with the OS-specific file separator at the end of their paths. buildClassPath adds the following in that order: SPARK_CLASSPATH environment variable The input appClassPath The configuration directory (only with SPARK_PREPEND_CLASSES set or SPARK_TESTING being 1 ) Locally compiled Spark classes in classes , test-classes and Core's jars. + CAUTION: FIXME Elaborate on \"locally compiled Spark classes\". (only with SPARK_SQL_TESTING being 1 ) ... + CAUTION: FIXME Elaborate on the SQL testing case HADOOP_CONF_DIR environment variable YARN_CONF_DIR environment variable SPARK_DIST_CLASSPATH environment variable NOTE: childEnv is queried first before System properties. It is always empty for AbstractCommandBuilder (and SparkSubmitCommandBuilder , too).","title":" buildClassPath"},{"location":"tools/AbstractCommandBuilder/#loading-properties-file","text":"Properties loadPropertiesFile () loadPropertiesFile loads Spark settings from a properties file (when specified on the command line) or spark-defaults.conf in the configuration directory . loadPropertiesFile loads the settings from the following files starting from the first and checking every location until the first properties file is found: propertiesFile (if specified using --properties-file command-line option or set by AbstractCommandBuilder.setPropertiesFile ). [SPARK_CONF_DIR]/spark-defaults.conf [SPARK_HOME]/conf/spark-defaults.conf","title":" Loading Properties File"},{"location":"tools/AbstractCommandBuilder/#sparks-configuration-directory","text":"AbstractCommandBuilder uses getConfDir to compute the current configuration directory of a Spark application. It uses SPARK_CONF_DIR (from childEnv which is always empty anyway or as a environment variable) and falls through to [SPARK_HOME]/conf (with SPARK_HOME from getSparkHome ).","title":" Spark's Configuration Directory"},{"location":"tools/AbstractCommandBuilder/#sparks-home-directory","text":"AbstractCommandBuilder uses getSparkHome to compute Spark's home directory for a Spark application. It uses SPARK_HOME (from childEnv which is always empty anyway or as a environment variable). If SPARK_HOME is not set, Spark throws a IllegalStateException : Spark home not found; set it explicitly or use the SPARK_HOME environment variable.","title":" Spark's Home Directory"},{"location":"tools/DependencyUtils/","text":"DependencyUtils Utilities \u00b6 resolveGlobPaths \u00b6 resolveGlobPaths ( paths : String , hadoopConf : Configuration ): String resolveGlobPaths ...FIXME resolveGlobPaths is used when: SparkSubmit is requested to prepareSubmitEnvironment DependencyUtils is used to resolveAndDownloadJars downloadFile \u00b6 downloadFile ( path : String , targetDir : File , sparkConf : SparkConf , hadoopConf : Configuration , secMgr : SecurityManager ): String downloadFile resolves the path to a well-formed URI and branches off based on the scheme: For file and local schemes, downloadFile returns the input path For other schemes, downloadFile ...FIXME downloadFile is used when: SparkSubmit is requested to prepareSubmitEnvironment DependencyUtils is used to downloadFileList downloadFileList \u00b6 downloadFileList ( fileList : String , targetDir : File , sparkConf : SparkConf , hadoopConf : Configuration , secMgr : SecurityManager ): String downloadFileList ...FIXME downloadFileList is used when: SparkSubmit is requested to prepareSubmitEnvironment DependencyUtils is used to resolveAndDownloadJars resolveMavenDependencies \u00b6 resolveMavenDependencies ( packagesExclusions : String , packages : String , repositories : String , ivyRepoPath : String , ivySettingsPath : Option [ String ]): String resolveMavenDependencies ...FIXME resolveMavenDependencies is used when: SparkSubmit is requested to prepareSubmitEnvironment (for all resource managers but Spark Standalone and Apache Mesos) Adding Local Jars to ClassLoader \u00b6 addJarToClasspath ( localJar : String , loader : MutableURLClassLoader ): Unit addJarToClasspath adds file and local jars (as localJar ) to the loader classloader. addJarToClasspath resolves the URI of localJar . If the URI is file or local and the file denoted by localJar exists, localJar is added to loader . Otherwise, the following warning is printed out to the logs: Warning: Local jar /path/to/fake.jar does not exist, skipping. For all other URIs, the following warning is printed out to the logs: Warning: Skip remote jar hdfs://fake.jar. Note addJarToClasspath assumes file URI when localJar has no URI specified, e.g. /path/to/local.jar . resolveAndDownloadJars \u00b6 resolveAndDownloadJars ( jars : String , userJar : String , sparkConf : SparkConf , hadoopConf : Configuration , secMgr : SecurityManager ): String resolveAndDownloadJars ...FIXME resolveAndDownloadJars is used when: DriverWrapper is requested to setupDependencies (Spark Standalone cluster mode)","title":"DependencyUtils"},{"location":"tools/DependencyUtils/#dependencyutils-utilities","text":"","title":"DependencyUtils Utilities"},{"location":"tools/DependencyUtils/#resolveglobpaths","text":"resolveGlobPaths ( paths : String , hadoopConf : Configuration ): String resolveGlobPaths ...FIXME resolveGlobPaths is used when: SparkSubmit is requested to prepareSubmitEnvironment DependencyUtils is used to resolveAndDownloadJars","title":" resolveGlobPaths"},{"location":"tools/DependencyUtils/#downloadfile","text":"downloadFile ( path : String , targetDir : File , sparkConf : SparkConf , hadoopConf : Configuration , secMgr : SecurityManager ): String downloadFile resolves the path to a well-formed URI and branches off based on the scheme: For file and local schemes, downloadFile returns the input path For other schemes, downloadFile ...FIXME downloadFile is used when: SparkSubmit is requested to prepareSubmitEnvironment DependencyUtils is used to downloadFileList","title":" downloadFile"},{"location":"tools/DependencyUtils/#downloadfilelist","text":"downloadFileList ( fileList : String , targetDir : File , sparkConf : SparkConf , hadoopConf : Configuration , secMgr : SecurityManager ): String downloadFileList ...FIXME downloadFileList is used when: SparkSubmit is requested to prepareSubmitEnvironment DependencyUtils is used to resolveAndDownloadJars","title":" downloadFileList"},{"location":"tools/DependencyUtils/#resolvemavendependencies","text":"resolveMavenDependencies ( packagesExclusions : String , packages : String , repositories : String , ivyRepoPath : String , ivySettingsPath : Option [ String ]): String resolveMavenDependencies ...FIXME resolveMavenDependencies is used when: SparkSubmit is requested to prepareSubmitEnvironment (for all resource managers but Spark Standalone and Apache Mesos)","title":" resolveMavenDependencies"},{"location":"tools/DependencyUtils/#adding-local-jars-to-classloader","text":"addJarToClasspath ( localJar : String , loader : MutableURLClassLoader ): Unit addJarToClasspath adds file and local jars (as localJar ) to the loader classloader. addJarToClasspath resolves the URI of localJar . If the URI is file or local and the file denoted by localJar exists, localJar is added to loader . Otherwise, the following warning is printed out to the logs: Warning: Local jar /path/to/fake.jar does not exist, skipping. For all other URIs, the following warning is printed out to the logs: Warning: Skip remote jar hdfs://fake.jar. Note addJarToClasspath assumes file URI when localJar has no URI specified, e.g. /path/to/local.jar .","title":" Adding Local Jars to ClassLoader"},{"location":"tools/DependencyUtils/#resolveanddownloadjars","text":"resolveAndDownloadJars ( jars : String , userJar : String , sparkConf : SparkConf , hadoopConf : Configuration , secMgr : SecurityManager ): String resolveAndDownloadJars ...FIXME resolveAndDownloadJars is used when: DriverWrapper is requested to setupDependencies (Spark Standalone cluster mode)","title":" resolveAndDownloadJars"},{"location":"tools/JavaMainApplication/","text":"JavaMainApplication \u00b6 JavaMainApplication is...FIXME","title":"JavaMainApplication"},{"location":"tools/JavaMainApplication/#javamainapplication","text":"JavaMainApplication is...FIXME","title":"JavaMainApplication"},{"location":"tools/Main/","text":"Main \u00b6 Main is used for bin/spark-class (as org.apache.spark.launcher.Main ). main \u00b6 void main ( String [] argsArray ) main ...FIXME buildCommand \u00b6 List < String > buildCommand ( AbstractCommandBuilder builder , Map < String , String > env , boolean printLaunchCommand ) buildCommand ...FIXME","title":"Main"},{"location":"tools/Main/#main","text":"Main is used for bin/spark-class (as org.apache.spark.launcher.Main ).","title":"Main"},{"location":"tools/Main/#main_1","text":"void main ( String [] argsArray ) main ...FIXME","title":" main"},{"location":"tools/Main/#buildcommand","text":"List < String > buildCommand ( AbstractCommandBuilder builder , Map < String , String > env , boolean printLaunchCommand ) buildCommand ...FIXME","title":" buildCommand"},{"location":"tools/SparkApplication/","text":"SparkApplication \u00b6 SparkApplication is an abstraction of entry points to Spark applications that can be started ( submitted for execution using spark-submit ). Contract \u00b6 Starting Spark Application \u00b6 start ( args : Array [ String ], conf : SparkConf ): Unit Used when: SparkSubmit is requested to submit an application for execution Implementations \u00b6 ClientApp JavaMainApplication KubernetesClientApplication ( Spark on Kubernetes ) RestSubmissionClientApp YarnClusterApplication","title":"SparkApplication"},{"location":"tools/SparkApplication/#sparkapplication","text":"SparkApplication is an abstraction of entry points to Spark applications that can be started ( submitted for execution using spark-submit ).","title":"SparkApplication"},{"location":"tools/SparkApplication/#contract","text":"","title":"Contract"},{"location":"tools/SparkApplication/#starting-spark-application","text":"start ( args : Array [ String ], conf : SparkConf ): Unit Used when: SparkSubmit is requested to submit an application for execution","title":" Starting Spark Application"},{"location":"tools/SparkApplication/#implementations","text":"ClientApp JavaMainApplication KubernetesClientApplication ( Spark on Kubernetes ) RestSubmissionClientApp YarnClusterApplication","title":"Implementations"},{"location":"tools/SparkLauncher/","text":"SparkLauncher \u00b6 SparkLauncher is an interface to launch Spark applications programmatically, i.e. from a code (not spark-submit.md[spark-submit] directly). It uses a builder pattern to configure a Spark application and launch it as a child process using spark-submit.md[spark-submit]. SparkLauncher uses SparkSubmitCommandBuilder to build the Spark command of a Spark application to launch. spark-internal \u00b6 SparkLauncher defines spark-internal ( NO_RESOURCE ) as a special value to inform Spark not to try to process the application resource ( primary resource ) as a regular file (but as an imaginary resource that cluster managers would know how to look up and submit for execution, e.g. Spark on YARN or Spark on Kubernetes ). spark-internal special value is used when: SparkSubmit is requested to prepareSubmitEnvironment and checks whether to add the primaryResource as part of the following: --jar (for Spark on YARN in cluster deploy mode) --primary-* arguments and define the --main-class argument (for Spark on Kubernetes in cluster deploy mode with KubernetesClientApplication main class) SparkSubmit is requested to check whether a resource is internal or not Other \u00b6 . SparkLauncher 's Builder Methods to Set Up Invocation of Spark Application [options=\"header\",width=\"100%\"] |=== | Setter | Description | addAppArgs(String... args) | Adds command line arguments for a Spark application. | addFile(String file) | Adds a file to be submitted with a Spark application. | addJar(String jar) | Adds a jar file to be submitted with the application. | addPyFile(String file) | Adds a python file / zip / egg to be submitted with a Spark application. | addSparkArg(String arg) | Adds a no-value argument to the Spark invocation. | addSparkArg(String name, String value) | Adds an argument with a value to the Spark invocation. It recognizes known command-line arguments, i.e. --master , --properties-file , --conf , --class , --jars , --files , and --py-files . | directory(File dir) | Sets the working directory of spark-submit. | redirectError() | Redirects stderr to stdout. | redirectError(File errFile) | Redirects error output to the specified errFile file. | redirectError(ProcessBuilder.Redirect to) | Redirects error output to the specified to Redirect. | redirectOutput(File outFile) | Redirects output to the specified outFile file. | redirectOutput(ProcessBuilder.Redirect to) | Redirects standard output to the specified to Redirect. | redirectToLog(String loggerName) | Sets all output to be logged and redirected to a logger with the specified name. | setAppName(String appName) | Sets the name of an Spark application | setAppResource(String resource) | Sets the main application resource, i.e. the location of a jar file for Scala/Java applications. | setConf(String key, String value) | Sets a Spark property. Expects key starting with spark. prefix. | setDeployMode(String mode) | Sets the deploy mode. | setJavaHome(String javaHome) | Sets a custom JAVA_HOME . | setMainClass(String mainClass) | Sets the main class. | setMaster(String master) | Sets the master URL. | setPropertiesFile(String path) | Sets the internal propertiesFile . See spark-AbstractCommandBuilder.md#loadPropertiesFile[ loadPropertiesFile Internal Method]. | setSparkHome(String sparkHome) | Sets a custom SPARK_HOME . | setVerbose(boolean verbose) | Enables verbose reporting for SparkSubmit. |=== After the invocation of a Spark application is set up, use launch() method to launch a sub-process that will start the configured Spark application. It is however recommended to use startApplication method instead. [source, scala] \u00b6 import org.apache.spark.launcher.SparkLauncher val command = new SparkLauncher() .setAppResource(\"SparkPi\") .setVerbose(true) val appHandle = command.startApplication() \u00b6","title":"SparkLauncher"},{"location":"tools/SparkLauncher/#sparklauncher","text":"SparkLauncher is an interface to launch Spark applications programmatically, i.e. from a code (not spark-submit.md[spark-submit] directly). It uses a builder pattern to configure a Spark application and launch it as a child process using spark-submit.md[spark-submit]. SparkLauncher uses SparkSubmitCommandBuilder to build the Spark command of a Spark application to launch.","title":"SparkLauncher"},{"location":"tools/SparkLauncher/#spark-internal","text":"SparkLauncher defines spark-internal ( NO_RESOURCE ) as a special value to inform Spark not to try to process the application resource ( primary resource ) as a regular file (but as an imaginary resource that cluster managers would know how to look up and submit for execution, e.g. Spark on YARN or Spark on Kubernetes ). spark-internal special value is used when: SparkSubmit is requested to prepareSubmitEnvironment and checks whether to add the primaryResource as part of the following: --jar (for Spark on YARN in cluster deploy mode) --primary-* arguments and define the --main-class argument (for Spark on Kubernetes in cluster deploy mode with KubernetesClientApplication main class) SparkSubmit is requested to check whether a resource is internal or not","title":" spark-internal"},{"location":"tools/SparkLauncher/#other","text":". SparkLauncher 's Builder Methods to Set Up Invocation of Spark Application [options=\"header\",width=\"100%\"] |=== | Setter | Description | addAppArgs(String... args) | Adds command line arguments for a Spark application. | addFile(String file) | Adds a file to be submitted with a Spark application. | addJar(String jar) | Adds a jar file to be submitted with the application. | addPyFile(String file) | Adds a python file / zip / egg to be submitted with a Spark application. | addSparkArg(String arg) | Adds a no-value argument to the Spark invocation. | addSparkArg(String name, String value) | Adds an argument with a value to the Spark invocation. It recognizes known command-line arguments, i.e. --master , --properties-file , --conf , --class , --jars , --files , and --py-files . | directory(File dir) | Sets the working directory of spark-submit. | redirectError() | Redirects stderr to stdout. | redirectError(File errFile) | Redirects error output to the specified errFile file. | redirectError(ProcessBuilder.Redirect to) | Redirects error output to the specified to Redirect. | redirectOutput(File outFile) | Redirects output to the specified outFile file. | redirectOutput(ProcessBuilder.Redirect to) | Redirects standard output to the specified to Redirect. | redirectToLog(String loggerName) | Sets all output to be logged and redirected to a logger with the specified name. | setAppName(String appName) | Sets the name of an Spark application | setAppResource(String resource) | Sets the main application resource, i.e. the location of a jar file for Scala/Java applications. | setConf(String key, String value) | Sets a Spark property. Expects key starting with spark. prefix. | setDeployMode(String mode) | Sets the deploy mode. | setJavaHome(String javaHome) | Sets a custom JAVA_HOME . | setMainClass(String mainClass) | Sets the main class. | setMaster(String master) | Sets the master URL. | setPropertiesFile(String path) | Sets the internal propertiesFile . See spark-AbstractCommandBuilder.md#loadPropertiesFile[ loadPropertiesFile Internal Method]. | setSparkHome(String sparkHome) | Sets a custom SPARK_HOME . | setVerbose(boolean verbose) | Enables verbose reporting for SparkSubmit. |=== After the invocation of a Spark application is set up, use launch() method to launch a sub-process that will start the configured Spark application. It is however recommended to use startApplication method instead.","title":"Other"},{"location":"tools/SparkLauncher/#source-scala","text":"import org.apache.spark.launcher.SparkLauncher val command = new SparkLauncher() .setAppResource(\"SparkPi\") .setVerbose(true)","title":"[source, scala]"},{"location":"tools/SparkLauncher/#val-apphandle-commandstartapplication","text":"","title":"val appHandle = command.startApplication()"},{"location":"tools/SparkSubmit/","text":"SparkSubmit \u00b6 SparkSubmit is the entry point to spark-submit shell script. Special Primary Resource Names \u00b6 SparkSubmit uses the following special primary resource names to represent Spark shells rather than application jars: spark-shell pyspark-shell sparkr-shell pyspark-shell \u00b6 SparkSubmit uses pyspark-shell when: SparkSubmit is requested to prepareSubmitEnvironment for .py scripts or pyspark , isShell and isPython isShell \u00b6 isShell ( res : String ): Boolean isShell is true when the given res primary resource represents a Spark shell . isShell is used when: SparkSubmit is requested to prepareSubmitEnvironment and isUserJar SparkSubmitArguments is requested to handleUnknown (and determine a primary application resource) Actions \u00b6 SparkSubmit executes actions (based on the action argument). Killing Submission \u00b6 kill ( args : SparkSubmitArguments ): Unit kill ...FIXME Displaying Version \u00b6 printVersion (): Unit printVersion ...FIXME Submission Status \u00b6 requestStatus ( args : SparkSubmitArguments ): Unit requestStatus ...FIXME Submission \u00b6 submit ( args : SparkSubmitArguments , uninitLog : Boolean ): Unit submit ...FIXME Running Main Class \u00b6 runMain ( args : SparkSubmitArguments , uninitLog : Boolean ): Unit runMain prepareSubmitEnvironment with the given SparkSubmitArguments (that gives a 4-element tuple of childArgs , childClasspath , sparkConf and childMainClass ). With verbose enabled, runMain prints out the following INFO messages to the logs: Main class: [childMainClass] Arguments: [childArgs] Spark config: [sparkConf_redacted] Classpath elements: [childClasspath] runMain creates and sets a context classloader (based on spark.driver.userClassPathFirst configuration property) and adds the jars (from childClasspath ). runMain loads the main class ( childMainClass ). runMain creates a SparkApplication (if the main class is a subtype of) or creates a JavaMainApplication (with the main class). In the end, runMain requests the SparkApplication to start (with the childArgs and sparkConf ). Cluster Managers \u00b6 SparkSubmit has a built-in support for some cluster managers (that are selected based on the master argument). Nickname Master URL KUBERNETES k8s:// -prefix LOCAL local -prefix MESOS mesos -prefix STANDALONE spark -prefix YARN yarn Launching Standalone Application \u00b6 main ( args : Array [ String ]): Unit main ...FIXME doSubmit \u00b6 doSubmit ( args : Array [ String ]): Unit doSubmit ...FIXME doSubmit is used when: InProcessSparkSubmit standalone application is started SparkSubmit standalone application is started prepareSubmitEnvironment \u00b6 prepareSubmitEnvironment ( args : SparkSubmitArguments , conf : Option [ HadoopConfiguration ] = None ): ( Seq [ String ], Seq [ String ], SparkConf , String ) prepareSubmitEnvironment creates a 4-element tuple made up of the following: childArgs for arguments childClasspath for Classpath elements sysProps for Spark properties childMainClass Tip Use --verbose command-line option to have the elements of the tuple printed out to the standard output. prepareSubmitEnvironment ...FIXME For isPython in CLIENT deploy mode, prepareSubmitEnvironment sets the following based on primaryResource : For pyspark-shell the mainClass is org.apache.spark.api.python.PythonGatewayServer Otherwise, the mainClass is org.apache.spark.deploy.PythonRunner and the main python file, extra python files and the childArgs prepareSubmitEnvironment ...FIXME prepareSubmitEnvironment determines the cluster manager based on master argument. For KUBERNETES , prepareSubmitEnvironment checkAndGetK8sMasterUrl . prepareSubmitEnvironment ...FIXME prepareSubmitEnvironment is used when...FIXME childMainClass \u00b6 childMainClass is the last 4 th argument in the result tuple of prepareSubmitEnvironment . // (childArgs, childClasspath, sparkConf, childMainClass) ( Seq [ String ], Seq [ String ], SparkConf , String ) childMainClass can be as follows: Deploy Mode Master URL childMainClass client any mainClass cluster KUBERNETES KubernetesClientApplication cluster MESOS RestSubmissionClientApp (for REST submission API ) cluster STANDALONE RestSubmissionClientApp (for REST submission API ) cluster STANDALONE ClientApp cluster YARN YarnClusterApplication isKubernetesClient \u00b6 prepareSubmitEnvironment uses isKubernetesClient flag to indicate that: Cluster manager is Kubernetes Deploy mode is client isKubernetesClusterModeDriver \u00b6 prepareSubmitEnvironment uses isKubernetesClusterModeDriver flag to indicate that: isKubernetesClient spark.kubernetes.submitInDriver configuration property is enabled ( Spark on Kubernetes ) renameResourcesToLocalFS \u00b6 renameResourcesToLocalFS ( resources : String , localResources : String ): String renameResourcesToLocalFS ...FIXME renameResourcesToLocalFS is used for isKubernetesClusterModeDriver mode. downloadResource \u00b6 downloadResource ( resource : String ): String downloadResource ...FIXME Checking Whether Resource is Internal \u00b6 isInternal ( res : String ): Boolean isInternal is true when the given res is spark-internal . isInternal is used when: SparkSubmit is requested to isUserJar SparkSubmitArguments is requested to handleUnknown isUserJar \u00b6 isUserJar ( res : String ): Boolean isUserJar is true when the given res is none of the following: isShell isPython isInternal isR isUserJar is used when: FIXME isPython Utility \u00b6 isPython ( res : String ): Boolean isPython is true when the given res primary resource represents a PySpark application: .py script pyspark-shell isPython is used when: SparkSubmit is requested to isUserJar SparkSubmitArguments is requested to handleUnknown (and set isPython internal flag)","title":"SparkSubmit"},{"location":"tools/SparkSubmit/#sparksubmit","text":"SparkSubmit is the entry point to spark-submit shell script.","title":"SparkSubmit"},{"location":"tools/SparkSubmit/#special-primary-resource-names","text":"SparkSubmit uses the following special primary resource names to represent Spark shells rather than application jars: spark-shell pyspark-shell sparkr-shell","title":" Special Primary Resource Names"},{"location":"tools/SparkSubmit/#pyspark-shell","text":"SparkSubmit uses pyspark-shell when: SparkSubmit is requested to prepareSubmitEnvironment for .py scripts or pyspark , isShell and isPython","title":" pyspark-shell"},{"location":"tools/SparkSubmit/#isshell","text":"isShell ( res : String ): Boolean isShell is true when the given res primary resource represents a Spark shell . isShell is used when: SparkSubmit is requested to prepareSubmitEnvironment and isUserJar SparkSubmitArguments is requested to handleUnknown (and determine a primary application resource)","title":" isShell"},{"location":"tools/SparkSubmit/#actions","text":"SparkSubmit executes actions (based on the action argument).","title":" Actions"},{"location":"tools/SparkSubmit/#killing-submission","text":"kill ( args : SparkSubmitArguments ): Unit kill ...FIXME","title":" Killing Submission"},{"location":"tools/SparkSubmit/#displaying-version","text":"printVersion (): Unit printVersion ...FIXME","title":" Displaying Version"},{"location":"tools/SparkSubmit/#submission-status","text":"requestStatus ( args : SparkSubmitArguments ): Unit requestStatus ...FIXME","title":" Submission Status"},{"location":"tools/SparkSubmit/#submission","text":"submit ( args : SparkSubmitArguments , uninitLog : Boolean ): Unit submit ...FIXME","title":" Submission"},{"location":"tools/SparkSubmit/#running-main-class","text":"runMain ( args : SparkSubmitArguments , uninitLog : Boolean ): Unit runMain prepareSubmitEnvironment with the given SparkSubmitArguments (that gives a 4-element tuple of childArgs , childClasspath , sparkConf and childMainClass ). With verbose enabled, runMain prints out the following INFO messages to the logs: Main class: [childMainClass] Arguments: [childArgs] Spark config: [sparkConf_redacted] Classpath elements: [childClasspath] runMain creates and sets a context classloader (based on spark.driver.userClassPathFirst configuration property) and adds the jars (from childClasspath ). runMain loads the main class ( childMainClass ). runMain creates a SparkApplication (if the main class is a subtype of) or creates a JavaMainApplication (with the main class). In the end, runMain requests the SparkApplication to start (with the childArgs and sparkConf ).","title":" Running Main Class"},{"location":"tools/SparkSubmit/#cluster-managers","text":"SparkSubmit has a built-in support for some cluster managers (that are selected based on the master argument). Nickname Master URL KUBERNETES k8s:// -prefix LOCAL local -prefix MESOS mesos -prefix STANDALONE spark -prefix YARN yarn","title":" Cluster Managers"},{"location":"tools/SparkSubmit/#launching-standalone-application","text":"main ( args : Array [ String ]): Unit main ...FIXME","title":" Launching Standalone Application"},{"location":"tools/SparkSubmit/#dosubmit","text":"doSubmit ( args : Array [ String ]): Unit doSubmit ...FIXME doSubmit is used when: InProcessSparkSubmit standalone application is started SparkSubmit standalone application is started","title":" doSubmit"},{"location":"tools/SparkSubmit/#preparesubmitenvironment","text":"prepareSubmitEnvironment ( args : SparkSubmitArguments , conf : Option [ HadoopConfiguration ] = None ): ( Seq [ String ], Seq [ String ], SparkConf , String ) prepareSubmitEnvironment creates a 4-element tuple made up of the following: childArgs for arguments childClasspath for Classpath elements sysProps for Spark properties childMainClass Tip Use --verbose command-line option to have the elements of the tuple printed out to the standard output. prepareSubmitEnvironment ...FIXME For isPython in CLIENT deploy mode, prepareSubmitEnvironment sets the following based on primaryResource : For pyspark-shell the mainClass is org.apache.spark.api.python.PythonGatewayServer Otherwise, the mainClass is org.apache.spark.deploy.PythonRunner and the main python file, extra python files and the childArgs prepareSubmitEnvironment ...FIXME prepareSubmitEnvironment determines the cluster manager based on master argument. For KUBERNETES , prepareSubmitEnvironment checkAndGetK8sMasterUrl . prepareSubmitEnvironment ...FIXME prepareSubmitEnvironment is used when...FIXME","title":" prepareSubmitEnvironment"},{"location":"tools/SparkSubmit/#childmainclass","text":"childMainClass is the last 4 th argument in the result tuple of prepareSubmitEnvironment . // (childArgs, childClasspath, sparkConf, childMainClass) ( Seq [ String ], Seq [ String ], SparkConf , String ) childMainClass can be as follows: Deploy Mode Master URL childMainClass client any mainClass cluster KUBERNETES KubernetesClientApplication cluster MESOS RestSubmissionClientApp (for REST submission API ) cluster STANDALONE RestSubmissionClientApp (for REST submission API ) cluster STANDALONE ClientApp cluster YARN YarnClusterApplication","title":" childMainClass"},{"location":"tools/SparkSubmit/#iskubernetesclient","text":"prepareSubmitEnvironment uses isKubernetesClient flag to indicate that: Cluster manager is Kubernetes Deploy mode is client","title":" isKubernetesClient"},{"location":"tools/SparkSubmit/#iskubernetesclustermodedriver","text":"prepareSubmitEnvironment uses isKubernetesClusterModeDriver flag to indicate that: isKubernetesClient spark.kubernetes.submitInDriver configuration property is enabled ( Spark on Kubernetes )","title":" isKubernetesClusterModeDriver"},{"location":"tools/SparkSubmit/#renameresourcestolocalfs","text":"renameResourcesToLocalFS ( resources : String , localResources : String ): String renameResourcesToLocalFS ...FIXME renameResourcesToLocalFS is used for isKubernetesClusterModeDriver mode.","title":" renameResourcesToLocalFS"},{"location":"tools/SparkSubmit/#downloadresource","text":"downloadResource ( resource : String ): String downloadResource ...FIXME","title":" downloadResource"},{"location":"tools/SparkSubmit/#checking-whether-resource-is-internal","text":"isInternal ( res : String ): Boolean isInternal is true when the given res is spark-internal . isInternal is used when: SparkSubmit is requested to isUserJar SparkSubmitArguments is requested to handleUnknown","title":" Checking Whether Resource is Internal"},{"location":"tools/SparkSubmit/#isuserjar","text":"isUserJar ( res : String ): Boolean isUserJar is true when the given res is none of the following: isShell isPython isInternal isR isUserJar is used when: FIXME","title":" isUserJar"},{"location":"tools/SparkSubmit/#ispython-utility","text":"isPython ( res : String ): Boolean isPython is true when the given res primary resource represents a PySpark application: .py script pyspark-shell isPython is used when: SparkSubmit is requested to isUserJar SparkSubmitArguments is requested to handleUnknown (and set isPython internal flag)","title":" isPython Utility"},{"location":"tools/SparkSubmitArguments/","text":"SparkSubmitArguments \u00b6 SparkSubmitArguments is a custom SparkSubmitArgumentsParser to handle the command-line arguments of spark-submit script that the actions use for execution (possibly with the explicit env environment). SparkSubmitArguments is created when launching spark-submit script with only args passed in and later used for printing the arguments in verbose mode . Command-Line Options \u00b6 --files \u00b6 Configuration Property: spark.files Configuration Property (Spark on YARN): spark.yarn.dist.files Printed out to standard output for --verbose option When SparkSubmit is requested to prepareSubmitEnvironment , the files are: resolveGlobPaths downloadFileList renameResourcesToLocalFS downloadResource Creating Instance \u00b6 SparkSubmitArguments takes the following to be created: Arguments ( Seq[String] ) Environment Variables (default: sys.env ) SparkSubmitArguments is created when: SparkSubmit is requested to parseArguments and launched as a command-line application Loading Spark Properties \u00b6 loadEnvironmentArguments (): Unit loadEnvironmentArguments loads the Spark properties for the current execution of spark-submit . loadEnvironmentArguments reads command-line options first followed by Spark properties and System's environment variables. Note Spark config properties start with spark. prefix and can be set using --conf [key=value] command-line option. Handling Options \u00b6 handle ( opt : String , value : String ): Boolean handle parses the input opt argument and returns true or throws an IllegalArgumentException when it finds an unknown opt . handle sets the internal properties in the table Command-Line Options, Spark Properties and Environment Variables . mergeDefaultSparkProperties \u00b6 mergeDefaultSparkProperties (): Unit mergeDefaultSparkProperties merges Spark properties from the default Spark properties file, i.e. spark-defaults.conf with those specified through --conf command-line option. isPython Flag \u00b6 isPython : Boolean = false isPython indicates whether the application resource is a PySpark application (a Python script or pyspark shell).","title":"SparkSubmitArguments"},{"location":"tools/SparkSubmitArguments/#sparksubmitarguments","text":"SparkSubmitArguments is a custom SparkSubmitArgumentsParser to handle the command-line arguments of spark-submit script that the actions use for execution (possibly with the explicit env environment). SparkSubmitArguments is created when launching spark-submit script with only args passed in and later used for printing the arguments in verbose mode .","title":"SparkSubmitArguments"},{"location":"tools/SparkSubmitArguments/#command-line-options","text":"","title":"Command-Line Options"},{"location":"tools/SparkSubmitArguments/#-files","text":"Configuration Property: spark.files Configuration Property (Spark on YARN): spark.yarn.dist.files Printed out to standard output for --verbose option When SparkSubmit is requested to prepareSubmitEnvironment , the files are: resolveGlobPaths downloadFileList renameResourcesToLocalFS downloadResource","title":" --files"},{"location":"tools/SparkSubmitArguments/#creating-instance","text":"SparkSubmitArguments takes the following to be created: Arguments ( Seq[String] ) Environment Variables (default: sys.env ) SparkSubmitArguments is created when: SparkSubmit is requested to parseArguments and launched as a command-line application","title":"Creating Instance"},{"location":"tools/SparkSubmitArguments/#loading-spark-properties","text":"loadEnvironmentArguments (): Unit loadEnvironmentArguments loads the Spark properties for the current execution of spark-submit . loadEnvironmentArguments reads command-line options first followed by Spark properties and System's environment variables. Note Spark config properties start with spark. prefix and can be set using --conf [key=value] command-line option.","title":" Loading Spark Properties"},{"location":"tools/SparkSubmitArguments/#handling-options","text":"handle ( opt : String , value : String ): Boolean handle parses the input opt argument and returns true or throws an IllegalArgumentException when it finds an unknown opt . handle sets the internal properties in the table Command-Line Options, Spark Properties and Environment Variables .","title":" Handling Options"},{"location":"tools/SparkSubmitArguments/#mergedefaultsparkproperties","text":"mergeDefaultSparkProperties (): Unit mergeDefaultSparkProperties merges Spark properties from the default Spark properties file, i.e. spark-defaults.conf with those specified through --conf command-line option.","title":" mergeDefaultSparkProperties"},{"location":"tools/SparkSubmitArguments/#ispython-flag","text":"isPython : Boolean = false isPython indicates whether the application resource is a PySpark application (a Python script or pyspark shell).","title":" isPython Flag"},{"location":"tools/SparkSubmitCommandBuilder/","text":"SparkSubmitCommandBuilder \u00b6 SparkSubmitCommandBuilder is an AbstractCommandBuilder . SparkSubmitCommandBuilder is used to build a command that spark-submit and SparkLauncher use to launch a Spark application. SparkSubmitCommandBuilder uses the first argument to distinguish the shells: pyspark-shell-main sparkr-shell-main run-example Describe run-example SparkSubmitCommandBuilder parses command-line arguments using OptionParser (which is a spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser]). OptionParser comes with the following methods: handle to handle the known options (see the table below). It sets up master , deployMode , propertiesFile , conf , mainClass , sparkArgs internal properties. handleUnknown to handle unrecognized options that usually lead to Unrecognized option error message. handleExtraArgs to handle extra arguments that are considered a Spark application's arguments. Note For spark-shell it assumes that the application arguments are after spark-submit 's arguments. pyspark-shell-main App Resource \u00b6 SparkSubmitCommandBuilder uses pyspark-shell-main as the name of the app resource to identify the PySpark shell. pyspark-shell-main is used when: SparkSubmitCommandBuilder is created and requested to buildCommand buildCommand \u00b6 List < String > buildCommand ( Map < String , String > env ) buildCommand is part of the AbstractCommandBuilder abstraction. buildCommand branches off based on the appResource : buildPySparkShellCommand for PYSPARK_SHELL buildSparkRCommand for SparkR buildSparkSubmitCommand buildPySparkShellCommand \u00b6 List < String > buildPySparkShellCommand ( Map < String , String > env ) buildPySparkShellCommand ...FIXME buildSparkSubmitCommand \u00b6 List < String > buildSparkSubmitCommand ( Map < String , String > env ) buildSparkSubmitCommand starts by building so-called effective config . When in client mode , buildSparkSubmitCommand adds spark.driver.extraClassPath to the result Spark command. buildSparkSubmitCommand builds the first part of the Java command passing in the extra classpath (only for client deploy mode). Add isThriftServer case buildSparkSubmitCommand appends SPARK_SUBMIT_OPTS and SPARK_JAVA_OPTS environment variables. (only for client deploy mode) ... Elaborate on the client deply mode case addPermGenSizeOpt case...elaborate Elaborate on addPermGenSizeOpt buildSparkSubmitCommand appends org.apache.spark.deploy.SparkSubmit and the command-line arguments (using buildSparkSubmitArgs ). buildSparkSubmitArgs \u00b6 List < String > buildSparkSubmitArgs () buildSparkSubmitArgs builds a list of command-line arguments for spark-submit . buildSparkSubmitArgs uses a SparkSubmitOptionParser to add the command-line arguments that spark-submit recognizes (when it is executed later on and uses the very same SparkSubmitOptionParser parser to parse command-line arguments). buildSparkSubmitArgs is used when: InProcessLauncher is requested to startApplication SparkLauncher is requested to createBuilder SparkSubmitCommandBuilder is requested to buildSparkSubmitCommand and constructEnvVarArgs SparkSubmitCommandBuilder Properties and SparkSubmitOptionParser Attributes \u00b6 SparkSubmitCommandBuilder Property SparkSubmitOptionParser Attribute verbose VERBOSE master MASTER [master] deployMode DEPLOY_MODE [deployMode] appName NAME [appName] conf CONF [key=value]* propertiesFile PROPERTIES_FILE [propertiesFile] jars JARS [comma-separated jars] files FILES [comma-separated files] pyFiles PY_FILES [comma-separated pyFiles] mainClass CLASS [mainClass] sparkArgs sparkArgs (passed straight through) appResource appResource (passed straight through) appArgs appArgs (passed straight through) ==== [[getEffectiveConfig]] getEffectiveConfig Internal Method [source, java] \u00b6 Map getEffectiveConfig() \u00b6 getEffectiveConfig internal method builds effectiveConfig that is conf with the Spark properties file loaded (using spark-AbstractCommandBuilder.md#loadPropertiesFile[loadPropertiesFile] internal method) skipping keys that have already been loaded (it happened when the command-line options were parsed in < > method). NOTE: Command-line options (e.g. --driver-class-path ) have higher precedence than their corresponding Spark settings in a Spark properties file (e.g. spark.driver.extraClassPath ). You can therefore control the final settings by overriding Spark settings on command line using the command-line options. charset and trims white spaces around values. ==== [[isClientMode]] isClientMode Internal Method [source, java] \u00b6 private boolean isClientMode(Map userProps) \u00b6 isClientMode checks master first (from the command-line options) and then spark.master Spark property. Same with deployMode and spark.submit.deployMode . CAUTION: FIXME Review master and deployMode . How are they set? isClientMode responds positive when no explicit master and client deploy mode set explicitly. === [[OptionParser]] OptionParser OptionParser is a custom spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser] that SparkSubmitCommandBuilder uses to parse command-line arguments. It defines all the spark-submit-SparkSubmitOptionParser.md#callbacks[SparkSubmitOptionParser callbacks], i.e. < >, < >, and < >, for command-line argument handling. ==== [[OptionParser-handle]] OptionParser's handle Callback [source, scala] \u00b6 boolean handle(String opt, String value) \u00b6 OptionParser comes with a custom handle callback (from the spark-submit-SparkSubmitOptionParser.md#callbacks[SparkSubmitOptionParser callbacks]). . handle Method [options=\"header\",width=\"100%\"] |=== | Command-Line Option | Property / Behaviour | --master | master | --deploy-mode | deployMode | --properties-file | propertiesFile | --driver-memory | Sets spark.driver.memory (in conf ) | --driver-java-options | Sets spark.driver.extraJavaOptions (in conf ) | --driver-library-path | Sets spark.driver.extraLibraryPath (in conf ) | --driver-class-path | Sets spark.driver.extraClassPath (in conf ) | --conf | Expects a key=value pair that it puts in conf | --class | Sets mainClass (in conf ). It may also set allowsMixedArguments and appResource if the execution is for one of the special classes, i.e. spark-shell.md[spark-shell], SparkSQLCLIDriver , or spark-sql-thrift-server.md[HiveThriftServer2]. | --kill | --status | Disables isAppResourceReq and adds itself with the value to sparkArgs . | --help | --usage-error | Disables isAppResourceReq and adds itself to sparkArgs . | --version | Disables isAppResourceReq and adds itself to sparkArgs . | anything else | Adds an element to sparkArgs |=== ==== [[OptionParser-handleUnknown]] OptionParser's handleUnknown Method [source, scala] \u00b6 boolean handleUnknown(String opt) \u00b6 If allowsMixedArguments is enabled, handleUnknown simply adds the input opt to appArgs and allows for further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. CAUTION: FIXME Where's allowsMixedArguments enabled? If isExample is enabled, handleUnknown sets mainClass to be org.apache.spark.examples.[opt] (unless the input opt has already the package prefix) and stops further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. CAUTION: FIXME Where's isExample enabled? Otherwise, handleUnknown sets appResource and stops further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. ==== [[OptionParser-handleExtraArgs]] OptionParser's handleExtraArgs Method [source, scala] \u00b6 void handleExtraArgs(List extra) \u00b6 handleExtraArgs adds all the extra arguments to appArgs .","title":"SparkSubmitCommandBuilder"},{"location":"tools/SparkSubmitCommandBuilder/#sparksubmitcommandbuilder","text":"SparkSubmitCommandBuilder is an AbstractCommandBuilder . SparkSubmitCommandBuilder is used to build a command that spark-submit and SparkLauncher use to launch a Spark application. SparkSubmitCommandBuilder uses the first argument to distinguish the shells: pyspark-shell-main sparkr-shell-main run-example Describe run-example SparkSubmitCommandBuilder parses command-line arguments using OptionParser (which is a spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser]). OptionParser comes with the following methods: handle to handle the known options (see the table below). It sets up master , deployMode , propertiesFile , conf , mainClass , sparkArgs internal properties. handleUnknown to handle unrecognized options that usually lead to Unrecognized option error message. handleExtraArgs to handle extra arguments that are considered a Spark application's arguments. Note For spark-shell it assumes that the application arguments are after spark-submit 's arguments.","title":"SparkSubmitCommandBuilder"},{"location":"tools/SparkSubmitCommandBuilder/#pyspark-shell-main-app-resource","text":"SparkSubmitCommandBuilder uses pyspark-shell-main as the name of the app resource to identify the PySpark shell. pyspark-shell-main is used when: SparkSubmitCommandBuilder is created and requested to buildCommand","title":" pyspark-shell-main App Resource"},{"location":"tools/SparkSubmitCommandBuilder/#buildcommand","text":"List < String > buildCommand ( Map < String , String > env ) buildCommand is part of the AbstractCommandBuilder abstraction. buildCommand branches off based on the appResource : buildPySparkShellCommand for PYSPARK_SHELL buildSparkRCommand for SparkR buildSparkSubmitCommand","title":" buildCommand"},{"location":"tools/SparkSubmitCommandBuilder/#buildpysparkshellcommand","text":"List < String > buildPySparkShellCommand ( Map < String , String > env ) buildPySparkShellCommand ...FIXME","title":" buildPySparkShellCommand"},{"location":"tools/SparkSubmitCommandBuilder/#buildsparksubmitcommand","text":"List < String > buildSparkSubmitCommand ( Map < String , String > env ) buildSparkSubmitCommand starts by building so-called effective config . When in client mode , buildSparkSubmitCommand adds spark.driver.extraClassPath to the result Spark command. buildSparkSubmitCommand builds the first part of the Java command passing in the extra classpath (only for client deploy mode). Add isThriftServer case buildSparkSubmitCommand appends SPARK_SUBMIT_OPTS and SPARK_JAVA_OPTS environment variables. (only for client deploy mode) ... Elaborate on the client deply mode case addPermGenSizeOpt case...elaborate Elaborate on addPermGenSizeOpt buildSparkSubmitCommand appends org.apache.spark.deploy.SparkSubmit and the command-line arguments (using buildSparkSubmitArgs ).","title":" buildSparkSubmitCommand"},{"location":"tools/SparkSubmitCommandBuilder/#buildsparksubmitargs","text":"List < String > buildSparkSubmitArgs () buildSparkSubmitArgs builds a list of command-line arguments for spark-submit . buildSparkSubmitArgs uses a SparkSubmitOptionParser to add the command-line arguments that spark-submit recognizes (when it is executed later on and uses the very same SparkSubmitOptionParser parser to parse command-line arguments). buildSparkSubmitArgs is used when: InProcessLauncher is requested to startApplication SparkLauncher is requested to createBuilder SparkSubmitCommandBuilder is requested to buildSparkSubmitCommand and constructEnvVarArgs","title":" buildSparkSubmitArgs"},{"location":"tools/SparkSubmitCommandBuilder/#sparksubmitcommandbuilder-properties-and-sparksubmitoptionparser-attributes","text":"SparkSubmitCommandBuilder Property SparkSubmitOptionParser Attribute verbose VERBOSE master MASTER [master] deployMode DEPLOY_MODE [deployMode] appName NAME [appName] conf CONF [key=value]* propertiesFile PROPERTIES_FILE [propertiesFile] jars JARS [comma-separated jars] files FILES [comma-separated files] pyFiles PY_FILES [comma-separated pyFiles] mainClass CLASS [mainClass] sparkArgs sparkArgs (passed straight through) appResource appResource (passed straight through) appArgs appArgs (passed straight through) ==== [[getEffectiveConfig]] getEffectiveConfig Internal Method","title":"SparkSubmitCommandBuilder Properties and SparkSubmitOptionParser Attributes"},{"location":"tools/SparkSubmitCommandBuilder/#source-java","text":"","title":"[source, java]"},{"location":"tools/SparkSubmitCommandBuilder/#map-geteffectiveconfig","text":"getEffectiveConfig internal method builds effectiveConfig that is conf with the Spark properties file loaded (using spark-AbstractCommandBuilder.md#loadPropertiesFile[loadPropertiesFile] internal method) skipping keys that have already been loaded (it happened when the command-line options were parsed in < > method). NOTE: Command-line options (e.g. --driver-class-path ) have higher precedence than their corresponding Spark settings in a Spark properties file (e.g. spark.driver.extraClassPath ). You can therefore control the final settings by overriding Spark settings on command line using the command-line options. charset and trims white spaces around values. ==== [[isClientMode]] isClientMode Internal Method","title":"Map getEffectiveConfig()"},{"location":"tools/SparkSubmitCommandBuilder/#source-java_1","text":"","title":"[source, java]"},{"location":"tools/SparkSubmitCommandBuilder/#private-boolean-isclientmodemap-userprops","text":"isClientMode checks master first (from the command-line options) and then spark.master Spark property. Same with deployMode and spark.submit.deployMode . CAUTION: FIXME Review master and deployMode . How are they set? isClientMode responds positive when no explicit master and client deploy mode set explicitly. === [[OptionParser]] OptionParser OptionParser is a custom spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser] that SparkSubmitCommandBuilder uses to parse command-line arguments. It defines all the spark-submit-SparkSubmitOptionParser.md#callbacks[SparkSubmitOptionParser callbacks], i.e. < >, < >, and < >, for command-line argument handling. ==== [[OptionParser-handle]] OptionParser's handle Callback","title":"private boolean isClientMode(Map userProps)"},{"location":"tools/SparkSubmitCommandBuilder/#source-scala","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitCommandBuilder/#boolean-handlestring-opt-string-value","text":"OptionParser comes with a custom handle callback (from the spark-submit-SparkSubmitOptionParser.md#callbacks[SparkSubmitOptionParser callbacks]). . handle Method [options=\"header\",width=\"100%\"] |=== | Command-Line Option | Property / Behaviour | --master | master | --deploy-mode | deployMode | --properties-file | propertiesFile | --driver-memory | Sets spark.driver.memory (in conf ) | --driver-java-options | Sets spark.driver.extraJavaOptions (in conf ) | --driver-library-path | Sets spark.driver.extraLibraryPath (in conf ) | --driver-class-path | Sets spark.driver.extraClassPath (in conf ) | --conf | Expects a key=value pair that it puts in conf | --class | Sets mainClass (in conf ). It may also set allowsMixedArguments and appResource if the execution is for one of the special classes, i.e. spark-shell.md[spark-shell], SparkSQLCLIDriver , or spark-sql-thrift-server.md[HiveThriftServer2]. | --kill | --status | Disables isAppResourceReq and adds itself with the value to sparkArgs . | --help | --usage-error | Disables isAppResourceReq and adds itself to sparkArgs . | --version | Disables isAppResourceReq and adds itself to sparkArgs . | anything else | Adds an element to sparkArgs |=== ==== [[OptionParser-handleUnknown]] OptionParser's handleUnknown Method","title":"boolean handle(String opt, String value)"},{"location":"tools/SparkSubmitCommandBuilder/#source-scala_1","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitCommandBuilder/#boolean-handleunknownstring-opt","text":"If allowsMixedArguments is enabled, handleUnknown simply adds the input opt to appArgs and allows for further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. CAUTION: FIXME Where's allowsMixedArguments enabled? If isExample is enabled, handleUnknown sets mainClass to be org.apache.spark.examples.[opt] (unless the input opt has already the package prefix) and stops further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. CAUTION: FIXME Where's isExample enabled? Otherwise, handleUnknown sets appResource and stops further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. ==== [[OptionParser-handleExtraArgs]] OptionParser's handleExtraArgs Method","title":"boolean handleUnknown(String opt)"},{"location":"tools/SparkSubmitCommandBuilder/#source-scala_2","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitCommandBuilder/#void-handleextraargslist-extra","text":"handleExtraArgs adds all the extra arguments to appArgs .","title":"void handleExtraArgs(List extra)"},{"location":"tools/SparkSubmitOperation/","text":"SparkSubmitOperation \u00b6 SparkSubmitOperation is an abstraction of operations of spark-submit (when requested to kill a submission or for a submission status ). Contract \u00b6 Killing Submission \u00b6 kill ( submissionId : String , conf : SparkConf ): Unit Kills a given submission Used when: SparkSubmit is requested to kill a submission Displaying Submission Status \u00b6 printSubmissionStatus ( submissionId : String , conf : SparkConf ): Unit Displays status of a given submission Used when: SparkSubmit is requested for submission status Checking Whether Master URL Supported \u00b6 supports ( master : String ): Boolean Used when: SparkSubmit is requested to kill a submission and for a submission status (via getSubmitOperations utility) Implementations \u00b6 K8SSparkSubmitOperation ( Spark on Kubernetes )","title":"SparkSubmitOperation"},{"location":"tools/SparkSubmitOperation/#sparksubmitoperation","text":"SparkSubmitOperation is an abstraction of operations of spark-submit (when requested to kill a submission or for a submission status ).","title":"SparkSubmitOperation"},{"location":"tools/SparkSubmitOperation/#contract","text":"","title":"Contract"},{"location":"tools/SparkSubmitOperation/#killing-submission","text":"kill ( submissionId : String , conf : SparkConf ): Unit Kills a given submission Used when: SparkSubmit is requested to kill a submission","title":" Killing Submission"},{"location":"tools/SparkSubmitOperation/#displaying-submission-status","text":"printSubmissionStatus ( submissionId : String , conf : SparkConf ): Unit Displays status of a given submission Used when: SparkSubmit is requested for submission status","title":" Displaying Submission Status"},{"location":"tools/SparkSubmitOperation/#checking-whether-master-url-supported","text":"supports ( master : String ): Boolean Used when: SparkSubmit is requested to kill a submission and for a submission status (via getSubmitOperations utility)","title":" Checking Whether Master URL Supported"},{"location":"tools/SparkSubmitOperation/#implementations","text":"K8SSparkSubmitOperation ( Spark on Kubernetes )","title":"Implementations"},{"location":"tools/SparkSubmitOptionParser/","text":"SparkSubmitOptionParser \u00b6 SparkSubmitOptionParser is the parser of spark-submit 's command-line options. --files \u00b6 A comma-separated sequence of paths Others \u00b6 . spark-submit Command-Line Options [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Command-Line Option | Description | --archives | | --class | The main class to run (as mainClass internal attribute). | --conf [prop=value] or -c [prop=value] | All = -separated values end up in conf potentially overriding existing settings. Order on command-line matters. | --deploy-mode | deployMode internal property | --driver-class-path | spark.driver.extraClassPath in conf -- the driver class path | --driver-cores | | --driver-java-options | spark.driver.extraJavaOptions in conf -- the driver VM options | --driver-library-path | spark.driver.extraLibraryPath in conf -- the driver native library path | --driver-memory | spark.driver.memory in conf | --exclude-packages | | --executor-cores | | --executor-memory | | --files | | --help or -h | The option is added to sparkArgs | --jars | | --keytab | | --kill | The option and a value are added to sparkArgs | --master | master internal property | --name | | --num-executors | | --packages | | --principal | | --properties-file [FILE] | propertiesFile internal property. Refer to spark-submit.md#properties-file[Custom Spark Properties File -- --properties-file command-line option]. | --proxy-user | | --py-files | | --queue | | --repositories | | --status | The option and a value are added to sparkArgs | --supervise | | --total-executor-cores | | --usage-error | The option is added to sparkArgs | --verbose or -v | | --version | The option is added to sparkArgs |=== === [[callbacks]] SparkSubmitOptionParser Callbacks SparkSubmitOptionParser is supposed to be overriden for the following capabilities (as callbacks). .Callbacks [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Callback | Description | handle | Executed when an option with an argument is parsed. | handleUnknown | Executed when an unrecognized option is parsed. | handleExtraArgs | Executed for the command-line arguments that handle and handleUnknown callbacks have not processed. |=== SparkSubmitOptionParser belongs to org.apache.spark.launcher Scala package and spark-launcher Maven/sbt module. NOTE: org.apache.spark.launcher.SparkSubmitArgumentsParser is a custom SparkSubmitOptionParser . === [[parse]] Parsing Command-Line Arguments -- parse Method [source, scala] \u00b6 final void parse(List args) \u00b6 parse parses a list of command-line arguments. parse calls handle callback whenever it finds a known command-line option or a switch (a command-line option with no parameter). It calls handleUnknown callback for unrecognized command-line options. parse keeps processing command-line arguments until handle or handleUnknown callback return false or all command-line arguments have been consumed. Ultimately, parse calls handleExtraArgs callback.","title":"SparkSubmitOptionParser"},{"location":"tools/SparkSubmitOptionParser/#sparksubmitoptionparser","text":"SparkSubmitOptionParser is the parser of spark-submit 's command-line options.","title":"SparkSubmitOptionParser"},{"location":"tools/SparkSubmitOptionParser/#-files","text":"A comma-separated sequence of paths","title":" --files"},{"location":"tools/SparkSubmitOptionParser/#others","text":". spark-submit Command-Line Options [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Command-Line Option | Description | --archives | | --class | The main class to run (as mainClass internal attribute). | --conf [prop=value] or -c [prop=value] | All = -separated values end up in conf potentially overriding existing settings. Order on command-line matters. | --deploy-mode | deployMode internal property | --driver-class-path | spark.driver.extraClassPath in conf -- the driver class path | --driver-cores | | --driver-java-options | spark.driver.extraJavaOptions in conf -- the driver VM options | --driver-library-path | spark.driver.extraLibraryPath in conf -- the driver native library path | --driver-memory | spark.driver.memory in conf | --exclude-packages | | --executor-cores | | --executor-memory | | --files | | --help or -h | The option is added to sparkArgs | --jars | | --keytab | | --kill | The option and a value are added to sparkArgs | --master | master internal property | --name | | --num-executors | | --packages | | --principal | | --properties-file [FILE] | propertiesFile internal property. Refer to spark-submit.md#properties-file[Custom Spark Properties File -- --properties-file command-line option]. | --proxy-user | | --py-files | | --queue | | --repositories | | --status | The option and a value are added to sparkArgs | --supervise | | --total-executor-cores | | --usage-error | The option is added to sparkArgs | --verbose or -v | | --version | The option is added to sparkArgs |=== === [[callbacks]] SparkSubmitOptionParser Callbacks SparkSubmitOptionParser is supposed to be overriden for the following capabilities (as callbacks). .Callbacks [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Callback | Description | handle | Executed when an option with an argument is parsed. | handleUnknown | Executed when an unrecognized option is parsed. | handleExtraArgs | Executed for the command-line arguments that handle and handleUnknown callbacks have not processed. |=== SparkSubmitOptionParser belongs to org.apache.spark.launcher Scala package and spark-launcher Maven/sbt module. NOTE: org.apache.spark.launcher.SparkSubmitArgumentsParser is a custom SparkSubmitOptionParser . === [[parse]] Parsing Command-Line Arguments -- parse Method","title":"Others"},{"location":"tools/SparkSubmitOptionParser/#source-scala","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitOptionParser/#final-void-parselist-args","text":"parse parses a list of command-line arguments. parse calls handle callback whenever it finds a known command-line option or a switch (a command-line option with no parameter). It calls handleUnknown callback for unrecognized command-line options. parse keeps processing command-line arguments until handle or handleUnknown callback return false or all command-line arguments have been consumed. Ultimately, parse calls handleExtraArgs callback.","title":"final void parse(List args)"},{"location":"tools/SparkSubmitUtils/","text":"SparkSubmitUtils \u00b6 SparkSubmitUtils provides utilities for SparkSubmit . getSubmitOperations \u00b6 getSubmitOperations ( master : String ): SparkSubmitOperation getSubmitOperations ...FIXME getSubmitOperations is used when: SparkSubmit is requested to kill a submission and requestStatus","title":"SparkSubmitUtils"},{"location":"tools/SparkSubmitUtils/#sparksubmitutils","text":"SparkSubmitUtils provides utilities for SparkSubmit .","title":"SparkSubmitUtils"},{"location":"tools/SparkSubmitUtils/#getsubmitoperations","text":"getSubmitOperations ( master : String ): SparkSubmitOperation getSubmitOperations ...FIXME getSubmitOperations is used when: SparkSubmit is requested to kill a submission and requestStatus","title":" getSubmitOperations"},{"location":"tools/spark-class/","text":"spark-class shell script \u00b6 spark-class shell script is the Spark application command-line launcher that is responsible for setting up JVM environment and executing a Spark application. NOTE: Ultimately, any shell script in Spark, e.g. link:spark-submit.adoc[spark-submit], calls spark-class script. You can find spark-class script in bin directory of the Spark distribution. When started, spark-class first loads $SPARK_HOME/bin/load-spark-env.sh , collects the Spark assembly jars, and executes < >. Depending on the Spark distribution (or rather lack thereof), i.e. whether RELEASE file exists or not, it sets SPARK_JARS_DIR environment variable to [SPARK_HOME]/jars or [SPARK_HOME]/assembly/target/scala-[SPARK_SCALA_VERSION]/jars , respectively (with the latter being a local build). If SPARK_JARS_DIR does not exist, spark-class prints the following error message and exits with the code 1 . Failed to find Spark jars directory ([SPARK_JARS_DIR]). You need to build Spark with the target \"package\" before running this program. spark-class sets LAUNCH_CLASSPATH environment variable to include all the jars under SPARK_JARS_DIR . If SPARK_PREPEND_CLASSES is enabled, [SPARK_HOME]/launcher/target/scala-[SPARK_SCALA_VERSION]/classes directory is added to LAUNCH_CLASSPATH as the first entry. NOTE: Use SPARK_PREPEND_CLASSES to have the Spark launcher classes (from [SPARK_HOME]/launcher/target/scala-[SPARK_SCALA_VERSION]/classes ) to appear before the other Spark assembly jars. It is useful for development so your changes don't require rebuilding Spark again. SPARK_TESTING and SPARK_SQL_TESTING environment variables enable test special mode . CAUTION: FIXME What's so special about the env vars? spark-class uses < > command-line application to compute the Spark command to launch. The Main class programmatically computes the command that spark-class executes afterwards. TIP: Use JAVA_HOME to point at the JVM to use. === [[main]] Launching org.apache.spark.launcher.Main Standalone Application org.apache.spark.launcher.Main is a Scala standalone application used in spark-class to prepare the Spark command to execute. Main expects that the first parameter is the class name that is the \"operation mode\": org.apache.spark.deploy.SparkSubmit -- Main uses link:spark-submit-SparkSubmitCommandBuilder.adoc[SparkSubmitCommandBuilder] to parse command-line arguments. This is the mode link:spark-submit.adoc[spark-submit] uses. anything -- Main uses SparkClassCommandBuilder to parse command-line arguments. $ ./bin/spark-class org.apache.spark.launcher.Main Exception in thread \"main\" java.lang.IllegalArgumentException: Not enough arguments: missing class name. at org.apache.spark.launcher.CommandBuilderUtils.checkArgument(CommandBuilderUtils.java:241) at org.apache.spark.launcher.Main.main(Main.java:51) Main uses buildCommand method on the builder to build a Spark command. If SPARK_PRINT_LAUNCH_COMMAND environment variable is enabled, Main prints the final Spark command to standard error. Spark Command: [cmd] ======================================== If on Windows it calls prepareWindowsCommand while on non-Windows OSes prepareBashCommand with tokens separated by \u0000\u0000\\0 . CAUTION: FIXME What's prepareWindowsCommand ? prepareBashCommand ? Main uses the following environment variables: SPARK_DAEMON_JAVA_OPTS and SPARK_MASTER_OPTS to be added to the command line of the command. SPARK_DAEMON_MEMORY (default: 1g ) for -Xms and -Xmx .","title":"spark-class"},{"location":"tools/spark-class/#spark-class-shell-script","text":"spark-class shell script is the Spark application command-line launcher that is responsible for setting up JVM environment and executing a Spark application. NOTE: Ultimately, any shell script in Spark, e.g. link:spark-submit.adoc[spark-submit], calls spark-class script. You can find spark-class script in bin directory of the Spark distribution. When started, spark-class first loads $SPARK_HOME/bin/load-spark-env.sh , collects the Spark assembly jars, and executes < >. Depending on the Spark distribution (or rather lack thereof), i.e. whether RELEASE file exists or not, it sets SPARK_JARS_DIR environment variable to [SPARK_HOME]/jars or [SPARK_HOME]/assembly/target/scala-[SPARK_SCALA_VERSION]/jars , respectively (with the latter being a local build). If SPARK_JARS_DIR does not exist, spark-class prints the following error message and exits with the code 1 . Failed to find Spark jars directory ([SPARK_JARS_DIR]). You need to build Spark with the target \"package\" before running this program. spark-class sets LAUNCH_CLASSPATH environment variable to include all the jars under SPARK_JARS_DIR . If SPARK_PREPEND_CLASSES is enabled, [SPARK_HOME]/launcher/target/scala-[SPARK_SCALA_VERSION]/classes directory is added to LAUNCH_CLASSPATH as the first entry. NOTE: Use SPARK_PREPEND_CLASSES to have the Spark launcher classes (from [SPARK_HOME]/launcher/target/scala-[SPARK_SCALA_VERSION]/classes ) to appear before the other Spark assembly jars. It is useful for development so your changes don't require rebuilding Spark again. SPARK_TESTING and SPARK_SQL_TESTING environment variables enable test special mode . CAUTION: FIXME What's so special about the env vars? spark-class uses < > command-line application to compute the Spark command to launch. The Main class programmatically computes the command that spark-class executes afterwards. TIP: Use JAVA_HOME to point at the JVM to use. === [[main]] Launching org.apache.spark.launcher.Main Standalone Application org.apache.spark.launcher.Main is a Scala standalone application used in spark-class to prepare the Spark command to execute. Main expects that the first parameter is the class name that is the \"operation mode\": org.apache.spark.deploy.SparkSubmit -- Main uses link:spark-submit-SparkSubmitCommandBuilder.adoc[SparkSubmitCommandBuilder] to parse command-line arguments. This is the mode link:spark-submit.adoc[spark-submit] uses. anything -- Main uses SparkClassCommandBuilder to parse command-line arguments. $ ./bin/spark-class org.apache.spark.launcher.Main Exception in thread \"main\" java.lang.IllegalArgumentException: Not enough arguments: missing class name. at org.apache.spark.launcher.CommandBuilderUtils.checkArgument(CommandBuilderUtils.java:241) at org.apache.spark.launcher.Main.main(Main.java:51) Main uses buildCommand method on the builder to build a Spark command. If SPARK_PRINT_LAUNCH_COMMAND environment variable is enabled, Main prints the final Spark command to standard error. Spark Command: [cmd] ======================================== If on Windows it calls prepareWindowsCommand while on non-Windows OSes prepareBashCommand with tokens separated by \u0000\u0000\\0 . CAUTION: FIXME What's prepareWindowsCommand ? prepareBashCommand ? Main uses the following environment variables: SPARK_DAEMON_JAVA_OPTS and SPARK_MASTER_OPTS to be added to the command line of the command. SPARK_DAEMON_MEMORY (default: 1g ) for -Xms and -Xmx .","title":"spark-class shell script"},{"location":"tools/spark-shell/","text":"spark-shell shell script \u00b6 Spark shell is an interactive environment where you can learn how to make the most out of Apache Spark quickly and conveniently. TIP: Spark shell is particularly helpful for fast interactive prototyping. Under the covers, Spark shell is a standalone Spark application written in Scala that offers environment with auto-completion (using TAB key) where you can run ad-hoc queries and get familiar with the features of Spark (that help you in developing your own standalone Spark applications). It is a very convenient tool to explore the many things available in Spark with immediate feedback. It is one of the many reasons why spark-overview.md#why-spark[Spark is so helpful for tasks to process datasets of any size]. There are variants of Spark shell for different languages: spark-shell for Scala, pyspark for Python and sparkR for R. NOTE: This document (and the book in general) uses spark-shell for Scala only. You can start Spark shell using < spark-shell script>>. $ ./bin/spark-shell scala> spark-shell is an extension of Scala REPL with automatic instantiation of spark-sql-SparkSession.md[SparkSession] as spark (and SparkContext.md[] as sc ). [source, scala] \u00b6 scala> :type spark org.apache.spark.sql.SparkSession // Learn the current version of Spark in use scala> spark.version res0: String = 2.1.0-SNAPSHOT spark-shell also imports spark-sql-SparkSession.md#implicits[Scala SQL's implicits] and spark-sql-SparkSession.md#sql[ sql method]. [source, scala] \u00b6 scala> :imports 1) import spark.implicits._ (59 terms, 38 are implicit) 2) import spark.sql (1 terms) [NOTE] \u00b6 When you execute spark-shell you actually execute spark-submit.md[Spark submit] as follows: [options=\"wrap\"] \u00b6 org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell \u00b6 Set SPARK_PRINT_LAUNCH_COMMAND to see the entire command to be executed. Refer to spark-tips-and-tricks.md#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts]. \u00b6 === [[using-spark-shell]] Using Spark shell You start Spark shell using spark-shell script (available in bin directory). $ ./bin/spark-shell Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Spark context Web UI available at http://10.47.71.138:4040 Spark context available as 'sc' (master = local[*], app id = local-1477858597347). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112) Type in expressions to have them evaluated. Type :help for more information. scala> Spark shell creates an instance of spark-sql-SparkSession.md[SparkSession] under the name spark for you (so you don't have to know the details how to do it yourself on day 1). scala> :type spark org.apache.spark.sql.SparkSession Besides, there is also sc value created which is an instance of SparkContext.md[]. scala> :type sc org.apache.spark.SparkContext To close Spark shell, you press Ctrl+D or type in :q (or any subset of :quit ). scala> :q","title":"spark-shell"},{"location":"tools/spark-shell/#spark-shell-shell-script","text":"Spark shell is an interactive environment where you can learn how to make the most out of Apache Spark quickly and conveniently. TIP: Spark shell is particularly helpful for fast interactive prototyping. Under the covers, Spark shell is a standalone Spark application written in Scala that offers environment with auto-completion (using TAB key) where you can run ad-hoc queries and get familiar with the features of Spark (that help you in developing your own standalone Spark applications). It is a very convenient tool to explore the many things available in Spark with immediate feedback. It is one of the many reasons why spark-overview.md#why-spark[Spark is so helpful for tasks to process datasets of any size]. There are variants of Spark shell for different languages: spark-shell for Scala, pyspark for Python and sparkR for R. NOTE: This document (and the book in general) uses spark-shell for Scala only. You can start Spark shell using < spark-shell script>>. $ ./bin/spark-shell scala> spark-shell is an extension of Scala REPL with automatic instantiation of spark-sql-SparkSession.md[SparkSession] as spark (and SparkContext.md[] as sc ).","title":"spark-shell shell script"},{"location":"tools/spark-shell/#source-scala","text":"scala> :type spark org.apache.spark.sql.SparkSession // Learn the current version of Spark in use scala> spark.version res0: String = 2.1.0-SNAPSHOT spark-shell also imports spark-sql-SparkSession.md#implicits[Scala SQL's implicits] and spark-sql-SparkSession.md#sql[ sql method].","title":"[source, scala]"},{"location":"tools/spark-shell/#source-scala_1","text":"scala> :imports 1) import spark.implicits._ (59 terms, 38 are implicit) 2) import spark.sql (1 terms)","title":"[source, scala]"},{"location":"tools/spark-shell/#note","text":"When you execute spark-shell you actually execute spark-submit.md[Spark submit] as follows:","title":"[NOTE]"},{"location":"tools/spark-shell/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"tools/spark-shell/#orgapachesparkdeploysparksubmit-class-orgapachesparkreplmain-name-spark-shell-spark-shell","text":"","title":"org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell"},{"location":"tools/spark-shell/#set-spark_print_launch_command-to-see-the-entire-command-to-be-executed-refer-to-spark-tips-and-tricksmdspark_print_launch_commandprint-launch-command-of-spark-scripts","text":"=== [[using-spark-shell]] Using Spark shell You start Spark shell using spark-shell script (available in bin directory). $ ./bin/spark-shell Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Spark context Web UI available at http://10.47.71.138:4040 Spark context available as 'sc' (master = local[*], app id = local-1477858597347). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112) Type in expressions to have them evaluated. Type :help for more information. scala> Spark shell creates an instance of spark-sql-SparkSession.md[SparkSession] under the name spark for you (so you don't have to know the details how to do it yourself on day 1). scala> :type spark org.apache.spark.sql.SparkSession Besides, there is also sc value created which is an instance of SparkContext.md[]. scala> :type sc org.apache.spark.SparkContext To close Spark shell, you press Ctrl+D or type in :q (or any subset of :quit ). scala> :q","title":"Set SPARK_PRINT_LAUNCH_COMMAND to see the entire command to be executed. Refer to spark-tips-and-tricks.md#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts]."},{"location":"tools/spark-submit/","text":"spark-submit shell script \u00b6 spark-submit shell script allows you to manage your Spark applications. spark-submit is a command-line frontend to SparkSubmit . Command-Line Options \u00b6 archives \u00b6 Command-Line Option: --archives Internal Property: archives deploy-mode \u00b6 Deploy mode Command-Line Option: --deploy-mode Spark Property: spark.submit.deployMode Environment Variable: DEPLOY_MODE Internal Property: deployMode driver-class-path \u00b6 --driver-class-path Extra class path entries (e.g. jars and directories) to pass to a driver's JVM. --driver-class-path command-line option sets the extra class path entries (e.g. jars and directories) that should be added to a driver's JVM. Tip Use --driver-class-path in client deploy mode (not SparkConf ) to ensure that the CLASSPATH is set up with the entries. client deploy mode uses the same JVM for the driver as spark-submit 's. Internal Property: driverExtraClassPath Spark Property: spark.driver.extraClassPath Note Command-line options (e.g. --driver-class-path ) have higher precedence than their corresponding Spark settings in a Spark properties file (e.g. spark.driver.extraClassPath ). You can therefore control the final settings by overriding Spark settings on command line using the command-line options. driver-cores \u00b6 --driver-cores NUM --driver-cores command-line option sets the number of cores to NUM for the driver in the cluster deploy mode . Spark Property: spark.driver.cores Note Only available for cluster deploy mode (when the driver is executed outside spark-submit ). Internal Property: driverCores properties-file \u00b6 --properties-file [FILE] --properties-file command-line option sets the path to a file FILE from which Spark loads extra Spark properties . Note Spark uses conf/spark-defaults.conf by default. queue \u00b6 --queue QUEUE_NAME YARN resource queue Spark Property: spark.yarn.queue Internal Property: queue version \u00b6 Command-Line Option: --version $ ./bin/spark-submit --version Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Branch master Compiled by user jacek on 2016-09-30T07:08:39Z Revision 1fad5596885aab8b32d2307c0edecbae50d5bd7a Url https://github.com/apache/spark.git Type --help for more information. | --conf | | | | sparkProperties | --driver-java-options | spark.driver.extraJavaOptions | | The driver's JVM options | driverExtraJavaOptions | --driver-library-path | spark.driver.extraLibraryPath | | The driver's native library path | driverExtraLibraryPath | [[driver-memory]] --driver-memory | [[spark_driver_memory]] spark.driver.memory | SPARK_DRIVER_MEMORY | The driver's memory | driverMemory | --exclude-packages | spark.jars.excludes | | | packagesExclusions | --executor-cores | spark.executor.cores | SPARK_EXECUTOR_CORES | The number of executor CPU cores | executorCores | [[executor-memory]] --executor-memory | [[spark.executor.memory]] spark.executor.memory | SPARK_EXECUTOR_MEMORY | An executor's memory | executorMemory | --files | spark.files | | | files | ivyRepoPath | spark.jars.ivy | | | | --jars | spark.jars | | | jars | --keytab | spark.yarn.keytab | | | keytab | --kill | | | submissionToKill and action set to KILL | | --master | spark.master | MASTER | Master URL. Defaults to local[*] | master | --class | | | | mainClass | --name | spark.app.name | SPARK_YARN_APP_NAME (YARN only) | Uses mainClass or the directory off primaryResource when no other ways set it | name | --num-executors | executor:Executor.md#spark.executor.instances[spark.executor.instances] | | | numExecutors | [[packages]] --packages | spark.jars.packages | | | packages | --principal | spark.yarn.principal | | | principal | --properties-file | spark.yarn.principal | | | propertiesFile | --proxy-user | | | | proxyUser | --py-files | | | | pyFiles | --repositories | | | | repositories | --status | | | submissionToRequestStatusFor and action set to REQUEST_STATUS | | --supervise | | | | supervise | --total-executor-cores | spark.cores.max | | | totalExecutorCores | --verbose | | | | verbose | --help | | | printUsageAndExit(0) | SPARK_PRINT_LAUNCH_COMMAND \u00b6 SPARK_PRINT_LAUNCH_COMMAND environment variable allows to have the complete Spark command printed out to the standard output. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Ja... Avoid scala.App \u00b6 Avoid using scala.App trait for a Spark application's main class in Scala as reported in SPARK-4170 Closure problems when running Scala app that \"extends App\" . Command-line Options \u00b6 Execute spark-submit --help to know about the command-line options supported. \u279c spark git:(master) \u2717 ./bin/spark-submit --help Usage: spark-submit [options] <app jar | python file> [app arguments] Usage: spark-submit --kill [submission ID] --master [spark://...] Usage: spark-submit --status [submission ID] --master [spark://...] Usage: spark-submit run-example [options] example-class [example args] Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --deploy-mode DEPLOY_MODE Whether to launch the driver program locally (\"client\") or on one of the worker machines inside the cluster (\"cluster\") (Default: client). --class CLASS_NAME Your application's main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h Show this help message and exit. --verbose, -v Print additional debug output. --version, Print the version of current Spark. Spark standalone with cluster deploy mode only: --driver-cores NUM Cores for driver (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). --queue QUEUE_NAME The YARN queue to submit to (Default: \"default\"). --num-executors NUM Number of executors to launch (Default: 2). --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. --class --conf or -c --deploy-mode (see < >) --driver-class-path (see < --driver-class-path command-line option>>) --driver-cores (see < >) --driver-java-options --driver-library-path --driver-memory --executor-memory --files --jars --kill for spark-standalone.md[Standalone cluster mode] only --master --name --packages --exclude-packages --properties-file (see < >) --proxy-user --py-files --repositories --status for spark-standalone.md[Standalone cluster mode] only --total-executor-cores List of switches, i.e. command-line options that do not take parameters: --help or -h --supervise for spark-standalone.md[Standalone cluster mode] only --usage-error --verbose or -v (see < >) --version (see < >) YARN-only options: --archives --executor-cores --keytab --num-executors --principal --queue (see < >) Environment Variables \u00b6 The following is the list of environment variables that are considered when command-line options are not specified: MASTER for --master SPARK_DRIVER_MEMORY for --driver-memory SPARK_EXECUTOR_MEMORY (see SparkContext.md#environment-variables[Environment Variables] in the SparkContext document) SPARK_EXECUTOR_CORES DEPLOY_MODE SPARK_YARN_APP_NAME _SPARK_CMD_USAGE External packages and custom repositories \u00b6 The spark-submit utility supports specifying external packages using Maven coordinates using --packages and custom repositories using --repositories . ./bin/spark-submit \\ --packages my:awesome:package \\ --repositories s3n://$aws_ak:$aws_sak@bucket/path/to/repo","title":"spark-submit"},{"location":"tools/spark-submit/#spark-submit-shell-script","text":"spark-submit shell script allows you to manage your Spark applications. spark-submit is a command-line frontend to SparkSubmit .","title":"spark-submit shell script"},{"location":"tools/spark-submit/#command-line-options","text":"","title":" Command-Line Options"},{"location":"tools/spark-submit/#archives","text":"Command-Line Option: --archives Internal Property: archives","title":" archives"},{"location":"tools/spark-submit/#deploy-mode","text":"Deploy mode Command-Line Option: --deploy-mode Spark Property: spark.submit.deployMode Environment Variable: DEPLOY_MODE Internal Property: deployMode","title":" deploy-mode"},{"location":"tools/spark-submit/#driver-class-path","text":"--driver-class-path Extra class path entries (e.g. jars and directories) to pass to a driver's JVM. --driver-class-path command-line option sets the extra class path entries (e.g. jars and directories) that should be added to a driver's JVM. Tip Use --driver-class-path in client deploy mode (not SparkConf ) to ensure that the CLASSPATH is set up with the entries. client deploy mode uses the same JVM for the driver as spark-submit 's. Internal Property: driverExtraClassPath Spark Property: spark.driver.extraClassPath Note Command-line options (e.g. --driver-class-path ) have higher precedence than their corresponding Spark settings in a Spark properties file (e.g. spark.driver.extraClassPath ). You can therefore control the final settings by overriding Spark settings on command line using the command-line options.","title":" driver-class-path"},{"location":"tools/spark-submit/#driver-cores","text":"--driver-cores NUM --driver-cores command-line option sets the number of cores to NUM for the driver in the cluster deploy mode . Spark Property: spark.driver.cores Note Only available for cluster deploy mode (when the driver is executed outside spark-submit ). Internal Property: driverCores","title":" driver-cores"},{"location":"tools/spark-submit/#properties-file","text":"--properties-file [FILE] --properties-file command-line option sets the path to a file FILE from which Spark loads extra Spark properties . Note Spark uses conf/spark-defaults.conf by default.","title":" properties-file"},{"location":"tools/spark-submit/#queue","text":"--queue QUEUE_NAME YARN resource queue Spark Property: spark.yarn.queue Internal Property: queue","title":" queue"},{"location":"tools/spark-submit/#version","text":"Command-Line Option: --version $ ./bin/spark-submit --version Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Branch master Compiled by user jacek on 2016-09-30T07:08:39Z Revision 1fad5596885aab8b32d2307c0edecbae50d5bd7a Url https://github.com/apache/spark.git Type --help for more information. | --conf | | | | sparkProperties | --driver-java-options | spark.driver.extraJavaOptions | | The driver's JVM options | driverExtraJavaOptions | --driver-library-path | spark.driver.extraLibraryPath | | The driver's native library path | driverExtraLibraryPath | [[driver-memory]] --driver-memory | [[spark_driver_memory]] spark.driver.memory | SPARK_DRIVER_MEMORY | The driver's memory | driverMemory | --exclude-packages | spark.jars.excludes | | | packagesExclusions | --executor-cores | spark.executor.cores | SPARK_EXECUTOR_CORES | The number of executor CPU cores | executorCores | [[executor-memory]] --executor-memory | [[spark.executor.memory]] spark.executor.memory | SPARK_EXECUTOR_MEMORY | An executor's memory | executorMemory | --files | spark.files | | | files | ivyRepoPath | spark.jars.ivy | | | | --jars | spark.jars | | | jars | --keytab | spark.yarn.keytab | | | keytab | --kill | | | submissionToKill and action set to KILL | | --master | spark.master | MASTER | Master URL. Defaults to local[*] | master | --class | | | | mainClass | --name | spark.app.name | SPARK_YARN_APP_NAME (YARN only) | Uses mainClass or the directory off primaryResource when no other ways set it | name | --num-executors | executor:Executor.md#spark.executor.instances[spark.executor.instances] | | | numExecutors | [[packages]] --packages | spark.jars.packages | | | packages | --principal | spark.yarn.principal | | | principal | --properties-file | spark.yarn.principal | | | propertiesFile | --proxy-user | | | | proxyUser | --py-files | | | | pyFiles | --repositories | | | | repositories | --status | | | submissionToRequestStatusFor and action set to REQUEST_STATUS | | --supervise | | | | supervise | --total-executor-cores | spark.cores.max | | | totalExecutorCores | --verbose | | | | verbose | --help | | | printUsageAndExit(0) |","title":" version"},{"location":"tools/spark-submit/#spark_print_launch_command","text":"SPARK_PRINT_LAUNCH_COMMAND environment variable allows to have the complete Spark command printed out to the standard output. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Ja...","title":" SPARK_PRINT_LAUNCH_COMMAND"},{"location":"tools/spark-submit/#avoid-scalaapp","text":"Avoid using scala.App trait for a Spark application's main class in Scala as reported in SPARK-4170 Closure problems when running Scala app that \"extends App\" .","title":"Avoid scala.App"},{"location":"tools/spark-submit/#command-line-options_1","text":"Execute spark-submit --help to know about the command-line options supported. \u279c spark git:(master) \u2717 ./bin/spark-submit --help Usage: spark-submit [options] <app jar | python file> [app arguments] Usage: spark-submit --kill [submission ID] --master [spark://...] Usage: spark-submit --status [submission ID] --master [spark://...] Usage: spark-submit run-example [options] example-class [example args] Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --deploy-mode DEPLOY_MODE Whether to launch the driver program locally (\"client\") or on one of the worker machines inside the cluster (\"cluster\") (Default: client). --class CLASS_NAME Your application's main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h Show this help message and exit. --verbose, -v Print additional debug output. --version, Print the version of current Spark. Spark standalone with cluster deploy mode only: --driver-cores NUM Cores for driver (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). --queue QUEUE_NAME The YARN queue to submit to (Default: \"default\"). --num-executors NUM Number of executors to launch (Default: 2). --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. --class --conf or -c --deploy-mode (see < >) --driver-class-path (see < --driver-class-path command-line option>>) --driver-cores (see < >) --driver-java-options --driver-library-path --driver-memory --executor-memory --files --jars --kill for spark-standalone.md[Standalone cluster mode] only --master --name --packages --exclude-packages --properties-file (see < >) --proxy-user --py-files --repositories --status for spark-standalone.md[Standalone cluster mode] only --total-executor-cores List of switches, i.e. command-line options that do not take parameters: --help or -h --supervise for spark-standalone.md[Standalone cluster mode] only --usage-error --verbose or -v (see < >) --version (see < >) YARN-only options: --archives --executor-cores --keytab --num-executors --principal --queue (see < >)","title":"Command-line Options"},{"location":"tools/spark-submit/#environment-variables","text":"The following is the list of environment variables that are considered when command-line options are not specified: MASTER for --master SPARK_DRIVER_MEMORY for --driver-memory SPARK_EXECUTOR_MEMORY (see SparkContext.md#environment-variables[Environment Variables] in the SparkContext document) SPARK_EXECUTOR_CORES DEPLOY_MODE SPARK_YARN_APP_NAME _SPARK_CMD_USAGE","title":"Environment Variables"},{"location":"tools/spark-submit/#external-packages-and-custom-repositories","text":"The spark-submit utility supports specifying external packages using Maven coordinates using --packages and custom repositories using --repositories . ./bin/spark-submit \\ --packages my:awesome:package \\ --repositories s3n://$aws_ak:$aws_sak@bucket/path/to/repo","title":"External packages and custom repositories"},{"location":"webui/","text":"Web UIs \u00b6 web UI is the web interface of Spark applications or infrastructure for monitoring and inspection. The main abstraction is WebUI .","title":"Web UIs"},{"location":"webui/#web-uis","text":"web UI is the web interface of Spark applications or infrastructure for monitoring and inspection. The main abstraction is WebUI .","title":"Web UIs"},{"location":"webui/AllJobsPage/","text":"AllJobsPage \u00b6 AllJobsPage is a WebUIPage of JobsTab . Creating Instance \u00b6 AllJobsPage takes the following to be created: Parent JobsTab AppStatusStore Rendering Page \u00b6 render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render renders a Spark Jobs page with the jobs and executors alongside applicationInfo and appSummary (from the AppStatusStore ). Introduction \u00b6 AllJobsPage renders a summary, an event timeline, and active, completed, and failed jobs of a Spark application. AllJobsPage displays the Summary section with the current Spark user, total uptime, scheduling mode, and the number of jobs per status. Under the summary section is the Event Timeline section. Active Jobs , Completed Jobs , and Failed Jobs sections follow. Jobs are clickable (and give information about the stages of tasks inside it). When you hover over a job in Event Timeline not only you see the job legend but also the job is highlighted in the Summary section. The Event Timeline section shows not only jobs but also executors.","title":"AllJobsPage"},{"location":"webui/AllJobsPage/#alljobspage","text":"AllJobsPage is a WebUIPage of JobsTab .","title":"AllJobsPage"},{"location":"webui/AllJobsPage/#creating-instance","text":"AllJobsPage takes the following to be created: Parent JobsTab AppStatusStore","title":"Creating Instance"},{"location":"webui/AllJobsPage/#rendering-page","text":"render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render renders a Spark Jobs page with the jobs and executors alongside applicationInfo and appSummary (from the AppStatusStore ).","title":" Rendering Page"},{"location":"webui/AllJobsPage/#introduction","text":"AllJobsPage renders a summary, an event timeline, and active, completed, and failed jobs of a Spark application. AllJobsPage displays the Summary section with the current Spark user, total uptime, scheduling mode, and the number of jobs per status. Under the summary section is the Event Timeline section. Active Jobs , Completed Jobs , and Failed Jobs sections follow. Jobs are clickable (and give information about the stages of tasks inside it). When you hover over a job in Event Timeline not only you see the job legend but also the job is highlighted in the Summary section. The Event Timeline section shows not only jobs but also executors.","title":"Introduction"},{"location":"webui/AllStagesPage/","text":"AllStagesPage \u00b6 AllStagesPage is a WebUIPage of StagesTab . Creating Instance \u00b6 AllStagesPage takes the following to be created: Parent StagesTab Rendering Page \u00b6 render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render renders a Stages for All Jobs page with the stages and application summary (from the AppStatusStore of the parent StagesTab ). Stage Headers \u00b6 AllStagesPage uses the following headers and tooltips for the Stages table. Header Tooltip Stage Id Pool Name Description Submitted Duration Elapsed time since the stage was submitted until execution completion of all its tasks. Tasks: Succeeded/Total Input Bytes read from Hadoop or from Spark storage. Output Bytes written to Hadoop. Shuffle Read Total shuffle bytes and records read (includes both data read locally and data read from remote executors). Shuffle Write Bytes and records written to disk in order to be read by a shuffle in a future stage. Failure Reason Bytes and records written to disk in order to be read by a shuffle in a future stage.","title":"AllStagesPage"},{"location":"webui/AllStagesPage/#allstagespage","text":"AllStagesPage is a WebUIPage of StagesTab .","title":"AllStagesPage"},{"location":"webui/AllStagesPage/#creating-instance","text":"AllStagesPage takes the following to be created: Parent StagesTab","title":"Creating Instance"},{"location":"webui/AllStagesPage/#rendering-page","text":"render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render renders a Stages for All Jobs page with the stages and application summary (from the AppStatusStore of the parent StagesTab ).","title":" Rendering Page"},{"location":"webui/AllStagesPage/#stage-headers","text":"AllStagesPage uses the following headers and tooltips for the Stages table. Header Tooltip Stage Id Pool Name Description Submitted Duration Elapsed time since the stage was submitted until execution completion of all its tasks. Tasks: Succeeded/Total Input Bytes read from Hadoop or from Spark storage. Output Bytes written to Hadoop. Shuffle Read Total shuffle bytes and records read (includes both data read locally and data read from remote executors). Shuffle Write Bytes and records written to disk in order to be read by a shuffle in a future stage. Failure Reason Bytes and records written to disk in order to be read by a shuffle in a future stage.","title":" Stage Headers"},{"location":"webui/EnvironmentPage/","text":"EnvironmentPage \u00b6 Review Me \u00b6 [[prefix]] EnvironmentPage is a spark-webui-WebUIPage.md[WebUIPage] with an empty spark-webui-WebUIPage.md#prefix[prefix]. EnvironmentPage is < > exclusively when EnvironmentTab is spark-webui-EnvironmentTab.md#creating-instance[created]. == [[creating-instance]] Creating EnvironmentPage Instance EnvironmentPage takes the following when created: [[parent]] Parent spark-webui-EnvironmentTab.md[EnvironmentTab] [[conf]] SparkConf.md[SparkConf] [[store]] core:AppStatusStore.md[]","title":"EnvironmentPage"},{"location":"webui/EnvironmentPage/#environmentpage","text":"","title":"EnvironmentPage"},{"location":"webui/EnvironmentPage/#review-me","text":"[[prefix]] EnvironmentPage is a spark-webui-WebUIPage.md[WebUIPage] with an empty spark-webui-WebUIPage.md#prefix[prefix]. EnvironmentPage is < > exclusively when EnvironmentTab is spark-webui-EnvironmentTab.md#creating-instance[created]. == [[creating-instance]] Creating EnvironmentPage Instance EnvironmentPage takes the following when created: [[parent]] Parent spark-webui-EnvironmentTab.md[EnvironmentTab] [[conf]] SparkConf.md[SparkConf] [[store]] core:AppStatusStore.md[]","title":"Review Me"},{"location":"webui/EnvironmentTab/","text":"EnvironmentTab \u00b6 Review Me \u00b6 [[prefix]] EnvironmentTab is a spark-webui-SparkUITab.md[SparkUITab] with environment spark-webui-SparkUITab.md#prefix[prefix]. EnvironmentTab is < > exclusively when SparkUI is spark-webui-SparkUI.md#initialize[initialized]. [[creating-instance]] EnvironmentTab takes the following when created: [[parent]] Parent spark-webui-SparkUI.md[SparkUI] [[store]] core:AppStatusStore.md[] When created, EnvironmentTab creates the spark-webui-EnvironmentPage.md#creating-instance[EnvironmentPage] page and spark-webui-WebUITab.md#attachPage[attaches] it immediately.","title":"EnvironmentTab"},{"location":"webui/EnvironmentTab/#environmenttab","text":"","title":"EnvironmentTab"},{"location":"webui/EnvironmentTab/#review-me","text":"[[prefix]] EnvironmentTab is a spark-webui-SparkUITab.md[SparkUITab] with environment spark-webui-SparkUITab.md#prefix[prefix]. EnvironmentTab is < > exclusively when SparkUI is spark-webui-SparkUI.md#initialize[initialized]. [[creating-instance]] EnvironmentTab takes the following when created: [[parent]] Parent spark-webui-SparkUI.md[SparkUI] [[store]] core:AppStatusStore.md[] When created, EnvironmentTab creates the spark-webui-EnvironmentPage.md#creating-instance[EnvironmentPage] page and spark-webui-WebUITab.md#attachPage[attaches] it immediately.","title":"Review Me"},{"location":"webui/ExecutorThreadDumpPage/","text":"ExecutorThreadDumpPage \u00b6 Review Me \u00b6 [[prefix]] ExecutorThreadDumpPage is a spark-webui-WebUIPage.md[WebUIPage] with threadDump spark-webui-WebUIPage.md#prefix[prefix]. ExecutorThreadDumpPage is < > exclusively when ExecutorsTab is spark-webui-ExecutorsTab.md#creating-instance[created] (with spark.ui.threadDumpsEnabled configuration property enabled). NOTE: spark.ui.threadDumpsEnabled configuration property is enabled (i.e. true ) by default. === [[creating-instance]] Creating ExecutorThreadDumpPage Instance ExecutorThreadDumpPage takes the following when created: [[parent]] spark-webui-SparkUITab.md[SparkUITab] [[sc]] Optional SparkContext.md[]","title":"ExecutorThreadDumpPage"},{"location":"webui/ExecutorThreadDumpPage/#executorthreaddumppage","text":"","title":"ExecutorThreadDumpPage"},{"location":"webui/ExecutorThreadDumpPage/#review-me","text":"[[prefix]] ExecutorThreadDumpPage is a spark-webui-WebUIPage.md[WebUIPage] with threadDump spark-webui-WebUIPage.md#prefix[prefix]. ExecutorThreadDumpPage is < > exclusively when ExecutorsTab is spark-webui-ExecutorsTab.md#creating-instance[created] (with spark.ui.threadDumpsEnabled configuration property enabled). NOTE: spark.ui.threadDumpsEnabled configuration property is enabled (i.e. true ) by default. === [[creating-instance]] Creating ExecutorThreadDumpPage Instance ExecutorThreadDumpPage takes the following when created: [[parent]] spark-webui-SparkUITab.md[SparkUITab] [[sc]] Optional SparkContext.md[]","title":"Review Me"},{"location":"webui/ExecutorsPage/","text":"ExecutorsPage \u00b6 Review Me \u00b6 [[prefix]] ExecutorsPage is a spark-webui-WebUIPage.md[WebUIPage] with an empty spark-webui-WebUIPage.md#prefix[prefix]. ExecutorsPage is < > exclusively when ExecutorsTab is spark-webui-ExecutorsTab.md#creating-instance[created]. === [[creating-instance]] Creating ExecutorsPage Instance ExecutorsPage takes the following when created: [[parent]] Parent spark-webui-SparkUITab.md[SparkUITab] [[threadDumpEnabled]] threadDumpEnabled flag","title":"ExecutorsPage"},{"location":"webui/ExecutorsPage/#executorspage","text":"","title":"ExecutorsPage"},{"location":"webui/ExecutorsPage/#review-me","text":"[[prefix]] ExecutorsPage is a spark-webui-WebUIPage.md[WebUIPage] with an empty spark-webui-WebUIPage.md#prefix[prefix]. ExecutorsPage is < > exclusively when ExecutorsTab is spark-webui-ExecutorsTab.md#creating-instance[created]. === [[creating-instance]] Creating ExecutorsPage Instance ExecutorsPage takes the following when created: [[parent]] Parent spark-webui-SparkUITab.md[SparkUITab] [[threadDumpEnabled]] threadDumpEnabled flag","title":"Review Me"},{"location":"webui/ExecutorsTab/","text":"ExecutorsTab \u00b6 Review Me \u00b6 [[prefix]] ExecutorsTab is a spark-webui-SparkUITab.md[SparkUITab] with executors spark-webui-SparkUITab.md#prefix[prefix]. ExecutorsTab is < > exclusively when SparkUI is spark-webui-SparkUI.md#initialize[initialized]. [[creating-instance]] [[parent]] ExecutorsTab takes the parent spark-webui-SparkUI.md[SparkUI] when created. When < >, ExecutorsTab creates the following pages and spark-webui-WebUITab.md#attachPage[attaches] them immediately: spark-webui-ExecutorsPage.md[ExecutorsPage] spark-webui-ExecutorThreadDumpPage.md[ExecutorThreadDumpPage]","title":"ExecutorsTab"},{"location":"webui/ExecutorsTab/#executorstab","text":"","title":"ExecutorsTab"},{"location":"webui/ExecutorsTab/#review-me","text":"[[prefix]] ExecutorsTab is a spark-webui-SparkUITab.md[SparkUITab] with executors spark-webui-SparkUITab.md#prefix[prefix]. ExecutorsTab is < > exclusively when SparkUI is spark-webui-SparkUI.md#initialize[initialized]. [[creating-instance]] [[parent]] ExecutorsTab takes the parent spark-webui-SparkUI.md[SparkUI] when created. When < >, ExecutorsTab creates the following pages and spark-webui-WebUITab.md#attachPage[attaches] them immediately: spark-webui-ExecutorsPage.md[ExecutorsPage] spark-webui-ExecutorThreadDumpPage.md[ExecutorThreadDumpPage]","title":"Review Me"},{"location":"webui/JettyUtils/","text":"== [[JettyUtils]] JettyUtils JettyUtils is a set of < > for creating Jetty HTTP Server-specific components. [[utility-methods]] .JettyUtils's Utility Methods [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | < > | Creates an HttpServlet | < > | Creates a Handler for a static content | < > | Creates a ServletContextHandler for a path < > === === [[createServletHandler]] Creating ServletContextHandler for Path -- createServletHandler Method [source, scala] \u00b6 createServletHandler( path: String, servlet: HttpServlet, basePath: String): ServletContextHandler createServletHandler T <: AnyRef : ServletContextHandler // <1> <1> Uses the first three-argument createServletHandler createServletHandler ...FIXME [NOTE] \u00b6 createServletHandler is used when: WebUI is requested to spark-webui-WebUI.md#attachPage[attachPage] MetricsServlet is requested to getHandlers * Spark Standalone's WorkerWebUI is requested to initialize \u00b6 === [[createServlet]] Creating HttpServlet -- createServlet Method [source, scala] \u00b6 createServlet T <: AnyRef : HttpServlet createServlet creates the X-Frame-Options header that can be either ALLOW-FROM with the value of spark-webui-properties.md#spark.ui.allowFramingFrom[spark.ui.allowFramingFrom] configuration property if defined or SAMEORIGIN . createServlet creates a Java Servlets HttpServlet with support for GET requests. When handling GET requests, the HttpServlet first checks view permissions of the remote user (by requesting the SecurityManager to checkUIViewPermissions of the remote user). [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.SecurityManager logger to see what happens when SecurityManager does the security check. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.SecurityManager=DEBUG You should see the following DEBUG message in the logs: DEBUG SecurityManager: user=[user] aclsEnabled=[aclsEnabled] viewAcls=[viewAcls] viewAclsGroups=[viewAclsGroups] \u00b6 With view permissions check passed, the HttpServlet sends a response with the following: FIXME In case the view permissions didn't allow to view the page, the HttpServlet sends an error response with the following: Status 403 Cache-Control header with \"no-cache, no-store, must-revalidate\" Error message: \"User is not authorized to access this page.\" NOTE: createServlet is used exclusively when JettyUtils is requested to < >. === [[createStaticHandler]] Creating Handler For Static Content -- createStaticHandler Method [source, scala] \u00b6 createStaticHandler(resourceBase: String, path: String): ServletContextHandler \u00b6 createStaticHandler creates a handler for serving files from a static directory Internally, createStaticHandler creates a Jetty ServletContextHandler and sets org.eclipse.jetty.servlet.Default.gzip init parameter to false . createRedirectHandler creates a Jetty https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/servlet/DefaultServlet.html[DefaultServlet ]. [NOTE] \u00b6 Quoting the official documentation of Jetty's https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/servlet/DefaultServlet.html[DefaultServlet ]: DefaultServlet The default servlet. This servlet, normally mapped to / , provides the handling for static content, OPTION and TRACE methods for the context. The following initParameters are supported, these can be set either on the servlet itself or as ServletContext initParameters with a prefix of org.eclipse.jetty.servlet.Default. With that, org.eclipse.jetty.servlet.Default.gzip is to configure https://www.eclipse.org/jetty/documentation/current/advanced-extras.html#default-servlet-init[gzip ] init parameter for Jetty's DefaultServlet . gzip If set to true, then static content will be served as gzip content encoded if a matching resource is found ending with \".gz\" (default false ) (deprecated: use precompressed) ==== createRedirectHandler resolves the resourceBase in the Spark classloader and, if successful, sets resourceBase init parameter of the Jetty DefaultServlet to the URL. NOTE: https://www.eclipse.org/jetty/documentation/current/advanced-extras.html#default-servlet-init[resourceBase ] init parameter is used to replace the context resource base. createRedirectHandler requests the ServletContextHandler to use the path as the context path and register the DefaultServlet to serve it. createRedirectHandler throws an Exception if the input resourceBase could not be resolved. Could not find resource path for Web UI: [resourceBase] NOTE: createStaticHandler is used when spark-webui-SparkUI.md#initialize[SparkUI], spark-history-server:HistoryServer.md#initialize[HistoryServer], Spark Standalone's MasterWebUI and WorkerWebUI , Spark on Mesos' MesosClusterUI are requested to initialize. === [[createRedirectHandler]] createRedirectHandler Method [source, scala] \u00b6 createRedirectHandler( srcPath: String, destPath: String, beforeRedirect: HttpServletRequest => Unit = x => (), basePath: String = \"\", httpMethods: Set[String] = Set(\"GET\")): ServletContextHandler createRedirectHandler ...FIXME NOTE: createRedirectHandler is used when spark-webui-SparkUI.md#initialize[SparkUI] and Spark Standalone's MasterWebUI are requested to initialize.","title":"JettyUtils"},{"location":"webui/JettyUtils/#source-scala","text":"createServletHandler( path: String, servlet: HttpServlet, basePath: String): ServletContextHandler createServletHandler T <: AnyRef : ServletContextHandler // <1> <1> Uses the first three-argument createServletHandler createServletHandler ...FIXME","title":"[source, scala]"},{"location":"webui/JettyUtils/#note","text":"createServletHandler is used when: WebUI is requested to spark-webui-WebUI.md#attachPage[attachPage] MetricsServlet is requested to getHandlers","title":"[NOTE]"},{"location":"webui/JettyUtils/#spark-standalones-workerwebui-is-requested-to-initialize","text":"=== [[createServlet]] Creating HttpServlet -- createServlet Method","title":"* Spark Standalone's WorkerWebUI is requested to initialize"},{"location":"webui/JettyUtils/#source-scala_1","text":"createServlet T <: AnyRef : HttpServlet createServlet creates the X-Frame-Options header that can be either ALLOW-FROM with the value of spark-webui-properties.md#spark.ui.allowFramingFrom[spark.ui.allowFramingFrom] configuration property if defined or SAMEORIGIN . createServlet creates a Java Servlets HttpServlet with support for GET requests. When handling GET requests, the HttpServlet first checks view permissions of the remote user (by requesting the SecurityManager to checkUIViewPermissions of the remote user).","title":"[source, scala]"},{"location":"webui/JettyUtils/#tip","text":"Enable DEBUG logging level for org.apache.spark.SecurityManager logger to see what happens when SecurityManager does the security check. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.SecurityManager=DEBUG You should see the following DEBUG message in the logs:","title":"[TIP]"},{"location":"webui/JettyUtils/#debug-securitymanager-useruser-aclsenabledaclsenabled-viewaclsviewacls-viewaclsgroupsviewaclsgroups","text":"With view permissions check passed, the HttpServlet sends a response with the following: FIXME In case the view permissions didn't allow to view the page, the HttpServlet sends an error response with the following: Status 403 Cache-Control header with \"no-cache, no-store, must-revalidate\" Error message: \"User is not authorized to access this page.\" NOTE: createServlet is used exclusively when JettyUtils is requested to < >. === [[createStaticHandler]] Creating Handler For Static Content -- createStaticHandler Method","title":"DEBUG SecurityManager: user=[user] aclsEnabled=[aclsEnabled] viewAcls=[viewAcls] viewAclsGroups=[viewAclsGroups]\n"},{"location":"webui/JettyUtils/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/JettyUtils/#createstatichandlerresourcebase-string-path-string-servletcontexthandler","text":"createStaticHandler creates a handler for serving files from a static directory Internally, createStaticHandler creates a Jetty ServletContextHandler and sets org.eclipse.jetty.servlet.Default.gzip init parameter to false . createRedirectHandler creates a Jetty https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/servlet/DefaultServlet.html[DefaultServlet ].","title":"createStaticHandler(resourceBase: String, path: String): ServletContextHandler"},{"location":"webui/JettyUtils/#note_1","text":"Quoting the official documentation of Jetty's https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/servlet/DefaultServlet.html[DefaultServlet ]: DefaultServlet The default servlet. This servlet, normally mapped to / , provides the handling for static content, OPTION and TRACE methods for the context. The following initParameters are supported, these can be set either on the servlet itself or as ServletContext initParameters with a prefix of org.eclipse.jetty.servlet.Default. With that, org.eclipse.jetty.servlet.Default.gzip is to configure https://www.eclipse.org/jetty/documentation/current/advanced-extras.html#default-servlet-init[gzip ] init parameter for Jetty's DefaultServlet . gzip If set to true, then static content will be served as gzip content encoded if a matching resource is found ending with \".gz\" (default false ) (deprecated: use precompressed) ==== createRedirectHandler resolves the resourceBase in the Spark classloader and, if successful, sets resourceBase init parameter of the Jetty DefaultServlet to the URL. NOTE: https://www.eclipse.org/jetty/documentation/current/advanced-extras.html#default-servlet-init[resourceBase ] init parameter is used to replace the context resource base. createRedirectHandler requests the ServletContextHandler to use the path as the context path and register the DefaultServlet to serve it. createRedirectHandler throws an Exception if the input resourceBase could not be resolved. Could not find resource path for Web UI: [resourceBase] NOTE: createStaticHandler is used when spark-webui-SparkUI.md#initialize[SparkUI], spark-history-server:HistoryServer.md#initialize[HistoryServer], Spark Standalone's MasterWebUI and WorkerWebUI , Spark on Mesos' MesosClusterUI are requested to initialize. === [[createRedirectHandler]] createRedirectHandler Method","title":"[NOTE]"},{"location":"webui/JettyUtils/#source-scala_3","text":"createRedirectHandler( srcPath: String, destPath: String, beforeRedirect: HttpServletRequest => Unit = x => (), basePath: String = \"\", httpMethods: Set[String] = Set(\"GET\")): ServletContextHandler createRedirectHandler ...FIXME NOTE: createRedirectHandler is used when spark-webui-SparkUI.md#initialize[SparkUI] and Spark Standalone's MasterWebUI are requested to initialize.","title":"[source, scala]"},{"location":"webui/JobPage/","text":"JobPage \u00b6 Review Me \u00b6 [[prefix]] JobPage is a spark-webui-WebUIPage.md[WebUIPage] with job spark-webui-WebUIPage.md#prefix[prefix]. JobPage is < > exclusively when JobsTab is created . === [[creating-instance]] Creating JobPage Instance JobPage takes the following when created: [[parent]] Parent JobsTab [[store]] core:AppStatusStore.md[]","title":"JobPage"},{"location":"webui/JobPage/#jobpage","text":"","title":"JobPage"},{"location":"webui/JobPage/#review-me","text":"[[prefix]] JobPage is a spark-webui-WebUIPage.md[WebUIPage] with job spark-webui-WebUIPage.md#prefix[prefix]. JobPage is < > exclusively when JobsTab is created . === [[creating-instance]] Creating JobPage Instance JobPage takes the following when created: [[parent]] Parent JobsTab [[store]] core:AppStatusStore.md[]","title":"Review Me"},{"location":"webui/JobsTab/","text":"JobsTab \u00b6 JobsTab is a SparkUITab with jobs URL prefix . Creating Instance \u00b6 JobsTab takes the following to be created: Parent SparkUI AppStatusStore JobsTab is created when: SparkUI is requested to initialize Pages \u00b6 When created , JobsTab attaches the following pages (with a reference to itself and the AppStatusStore ): AllJobsPage JobPage Event Timeline \u00b6 Details for Job \u00b6 Clicking a job in AllJobsPage , leads to Details for Job page. When a job id is not found, you should see \"No information to display for job ID\" message.","title":"JobsTab"},{"location":"webui/JobsTab/#jobstab","text":"JobsTab is a SparkUITab with jobs URL prefix .","title":"JobsTab"},{"location":"webui/JobsTab/#creating-instance","text":"JobsTab takes the following to be created: Parent SparkUI AppStatusStore JobsTab is created when: SparkUI is requested to initialize","title":"Creating Instance"},{"location":"webui/JobsTab/#pages","text":"When created , JobsTab attaches the following pages (with a reference to itself and the AppStatusStore ): AllJobsPage JobPage","title":"Pages"},{"location":"webui/JobsTab/#event-timeline","text":"","title":"Event Timeline"},{"location":"webui/JobsTab/#details-for-job","text":"Clicking a job in AllJobsPage , leads to Details for Job page. When a job id is not found, you should see \"No information to display for job ID\" message.","title":"Details for Job"},{"location":"webui/PoolPage/","text":"PoolPage \u00b6 PoolPage is a WebUIPage of StagesTab . Creating Instance \u00b6 PoolPage takes the following to be created: Parent StagesTab URL Prefix \u00b6 PoolPage uses pool URL prefix . Rendering Page \u00b6 render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render requires poolname and attempt request parameters. render renders a Fair Scheduler Pool page with the PoolData (from the AppStatusStore of the parent StagesTab ). Introduction \u00b6 The Fair Scheduler Pool Details page shows information about a Schedulable pool and is only available when a Spark application uses the FAIR scheduling mode. Summary Table \u00b6 The Summary table shows the details of a Schedulable pool. It uses the following columns: Pool Name Minimum Share Pool Weight Active Stages (the number of the active stages in a Schedulable pool) Running Tasks SchedulingMode Active Stages Table \u00b6 The Active Stages table shows the active stages in a pool.","title":"PoolPage"},{"location":"webui/PoolPage/#poolpage","text":"PoolPage is a WebUIPage of StagesTab .","title":"PoolPage"},{"location":"webui/PoolPage/#creating-instance","text":"PoolPage takes the following to be created: Parent StagesTab","title":"Creating Instance"},{"location":"webui/PoolPage/#url-prefix","text":"PoolPage uses pool URL prefix .","title":" URL Prefix"},{"location":"webui/PoolPage/#rendering-page","text":"render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render requires poolname and attempt request parameters. render renders a Fair Scheduler Pool page with the PoolData (from the AppStatusStore of the parent StagesTab ).","title":" Rendering Page"},{"location":"webui/PoolPage/#introduction","text":"The Fair Scheduler Pool Details page shows information about a Schedulable pool and is only available when a Spark application uses the FAIR scheduling mode.","title":"Introduction"},{"location":"webui/PoolPage/#summary-table","text":"The Summary table shows the details of a Schedulable pool. It uses the following columns: Pool Name Minimum Share Pool Weight Active Stages (the number of the active stages in a Schedulable pool) Running Tasks SchedulingMode","title":"Summary Table"},{"location":"webui/PoolPage/#active-stages-table","text":"The Active Stages table shows the active stages in a pool.","title":"Active Stages Table"},{"location":"webui/PrometheusResource/","text":"PrometheusResource \u00b6 getServletHandler \u00b6 getServletHandler ( uiRoot : UIRoot ): ServletContextHandler getServletHandler ...FIXME getServletHandler is used when: SparkUI is requested to initialize","title":"PrometheusResource"},{"location":"webui/PrometheusResource/#prometheusresource","text":"","title":"PrometheusResource"},{"location":"webui/PrometheusResource/#getservlethandler","text":"getServletHandler ( uiRoot : UIRoot ): ServletContextHandler getServletHandler ...FIXME getServletHandler is used when: SparkUI is requested to initialize","title":" getServletHandler"},{"location":"webui/RDDPage/","text":"== [[RDDPage]] RDDPage [[prefix]] RDDPage is a spark-webui-WebUIPage.md[WebUIPage] with rdd spark-webui-WebUIPage.md#prefix[prefix]. RDDPage is < > exclusively when StorageTab is spark-webui-StorageTab.md#creating-instance[created]. [[creating-instance]] RDDPage takes the following when created: [[parent]] Parent spark-webui-SparkUITab.md[SparkUITab] [[store]] core:AppStatusStore.md[] === [[render]] render Method [source, scala] \u00b6 render(request: HttpServletRequest): Seq[Node] \u00b6 NOTE: render is part of spark-webui-WebUIPage.md#render[WebUIPage Contract] to...FIXME. render ...FIXME","title":"RDDPage"},{"location":"webui/RDDPage/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/RDDPage/#renderrequest-httpservletrequest-seqnode","text":"NOTE: render is part of spark-webui-WebUIPage.md#render[WebUIPage Contract] to...FIXME. render ...FIXME","title":"render(request: HttpServletRequest): Seq[Node]"},{"location":"webui/SparkUI/","text":"SparkUI \u00b6 SparkUI is a WebUI of Spark applications. Creating Instance \u00b6 SparkUI takes the following to be created: AppStatusStore SparkContext SparkConf SecurityManager Application Name Base Path Start Time Spark Version While being created, SparkUI initializes itself . SparkUI is created using create utility. UI Port \u00b6 getUIPort ( conf : SparkConf ): Int getUIPort requests the SparkConf for the value of spark.ui.port configuration property. getUIPort is used when: SparkUI is created Creating SparkUI \u00b6 create ( sc : Option [ SparkContext ], store : AppStatusStore , conf : SparkConf , securityManager : SecurityManager , appName : String , basePath : String , startTime : Long , appSparkVersion : String ): SparkUI create creates a new SparkUI with appSparkVersion being the current Spark version. create is used when: SparkContext is created (with the spark.ui.enabled configuration property turned on) FsHistoryProvider (Spark History Server) is requested for the web UI of a Spark application Initializing \u00b6 initialize (): Unit initialize is part of the WebUI abstraction. initialize creates and attaches the following tabs: JobsTab StagesTab StorageTab EnvironmentTab ExecutorsTab initialize attaches itself as the UIRoot . initialize attaches the PrometheusResource for executor metrics based on spark.ui.prometheus.enabled configuration property. UIRoot \u00b6 SparkUI is an UIRoot Review Me \u00b6 SparkUI is < > along with the following: SparkContext is created (for a live Spark application with spark-webui-properties.md#spark.ui.enabled[spark.ui.enabled] configuration property enabled) FsHistoryProvider is requested for the spark-history-server:FsHistoryProvider.md#getAppUI[application UI] (for a live or completed Spark application) .Creating SparkUI for Live Spark Application image::spark-webui-SparkUI.png[align=\"center\"] When < > (while SparkContext is created for a live Spark application), SparkUI gets the following: Live AppStatusStore (with a ElementTrackingStore using an core:InMemoryStore.md[] and a AppStatusListener for a live Spark application) Name of the Spark application that is exactly the value of SparkConf.md#spark.app.name[spark.app.name] configuration property Empty base path When started, SparkUI binds to < > address that you can control using SPARK_PUBLIC_DNS environment variable or spark-driver.md#spark_driver_host[spark.driver.host] Spark property. NOTE: With spark-webui-properties.md#spark.ui.killEnabled[spark.ui.killEnabled] configuration property turned on, SparkUI < > (subject to SecurityManager.checkModifyPermissions permissions). SparkUI gets an < > that is then used for the following: < >, i.e. JobsTab.md#creating-instance[JobsTab], spark-webui-StagesTab.md#creating-instance[StagesTab], spark-webui-StorageTab.md#creating-instance[StorageTab], spark-webui-EnvironmentTab.md#creating-instance[EnvironmentTab] AbstractApplicationResource is requested for spark-api-AbstractApplicationResource.md#jobsList[jobsList], spark-api-AbstractApplicationResource.md#oneJob[oneJob], spark-api-AbstractApplicationResource.md#executorList[executorList], spark-api-AbstractApplicationResource.md#allExecutorList[allExecutorList], spark-api-AbstractApplicationResource.md#rddList[rddList], spark-api-AbstractApplicationResource.md#rddData[rddData], spark-api-AbstractApplicationResource.md#environmentInfo[environmentInfo] StagesResource is requested for spark-api-StagesResource.md#stageList[stageList], spark-api-StagesResource.md#stageData[stageData], spark-api-StagesResource.md#oneAttemptData[oneAttemptData], spark-api-StagesResource.md#taskSummary[taskSummary], spark-api-StagesResource.md#taskList[taskList] SparkUI is requested for the current < > Creating Spark SQL's SQLTab (when SQLHistoryServerPlugin is requested to setupUI ) Spark Streaming's BatchPage is created [[internal-registries]] .SparkUI's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appId | [[appId]] |=== [TIP] \u00b6 Enable INFO logging level for org.apache.spark.ui.SparkUI logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ui.SparkUI=INFO Refer to spark-logging.md[Logging]. \u00b6 == [[setAppId]] Assigning Unique Identifier of Spark Application -- setAppId Method [source, scala] \u00b6 setAppId(id: String): Unit \u00b6 setAppId sets the internal < >. setAppId is used when SparkContext is created. == [[stop]] Stopping SparkUI -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the HTTP server and prints the following INFO message to the logs: INFO SparkUI: Stopped Spark web UI at [appUIAddress] NOTE: appUIAddress in the above INFO message is the result of < > method. == [[appUIAddress]] appUIAddress Method [source, scala] \u00b6 appUIAddress: String \u00b6 appUIAddress returns the entire URL of a Spark application's web UI, including http:// scheme. Internally, appUIAddress uses < >. == [[createLiveUI]] createLiveUI Method [source, scala] \u00b6 createLiveUI( sc: SparkContext, conf: SparkConf, listenerBus: SparkListenerBus, jobProgressListener: JobProgressListener, securityManager: SecurityManager, appName: String, startTime: Long): SparkUI createLiveUI creates a SparkUI for a live running Spark application. Internally, createLiveUI simply forwards the call to < >. createLiveUI is used when SparkContext is created. == [[createHistoryUI]] createHistoryUI Method CAUTION: FIXME == [[appUIHostPort]] appUIHostPort Method [source, scala] \u00b6 appUIHostPort: String \u00b6 appUIHostPort returns the Spark application's web UI which is the public hostname and port, excluding the scheme. NOTE: < > uses appUIHostPort and adds http:// scheme. == [[getAppName]] getAppName Method [source, scala] \u00b6 getAppName: String \u00b6 getAppName returns the name of the Spark application (of a SparkUI instance). NOTE: getAppName is used when...FIXME == [[create]] Creating SparkUI Instance -- create Factory Method [source, scala] \u00b6 create( sc: Option[SparkContext], store: AppStatusStore, conf: SparkConf, securityManager: SecurityManager, appName: String, basePath: String = \"\", startTime: Long, appSparkVersion: String = org.apache.spark.SPARK_VERSION): SparkUI create creates a SparkUI backed by a core:AppStatusStore.md[]. Internally, create simply creates a new < > (with the predefined Spark version). create is used when: SparkContext is created FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#getAppUI[getAppUI] (for a Spark application that already finished) Creating Instance \u00b6 SparkUI takes the following when created: [[store]] core:AppStatusStore.md[] [[sc]] SparkContext.md[] [[conf]] SparkConf.md[SparkConf] [[securityManager]] SecurityManager [[appName]] Application name [[basePath]] basePath [[startTime]] Start time [[appSparkVersion]] appSparkVersion SparkUI initializes the < > and < >. == [[initialize]] Attaching Tabs and Context Handlers -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 NOTE: initialize is part of spark-webui-WebUI.md#initialize[WebUI Contract] to initialize web components. initialize creates and < > the following tabs (with the reference to the SparkUI and its < >): . spark-webui-StagesTab.md[StagesTab] . spark-webui-StorageTab.md[StorageTab] . spark-webui-EnvironmentTab.md[EnvironmentTab] . spark-webui-ExecutorsTab.md[ExecutorsTab] In the end, initialize creates and spark-webui-WebUI.md#attachHandler[attaches] the following ServletContextHandlers : . spark-webui-JettyUtils.md#createStaticHandler[Creates a static handler] for serving files from a static directory, i.e. /static to serve static files from org/apache/spark/ui/static directory (on CLASSPATH) . spark-api-ApiRootResource.md#getServletHandler[Creates the /api/* context handler] for the spark-api.md[Status REST API] . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect /jobs/job/kill to /jobs/ and request the JobsTab to execute handleKillRequest before redirection . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect /stages/stage/kill to /stages/ and request the StagesTab to execute spark-webui-StagesTab.md#handleKillRequest[handleKillRequest] before redirection","title":"SparkUI"},{"location":"webui/SparkUI/#sparkui","text":"SparkUI is a WebUI of Spark applications.","title":"SparkUI"},{"location":"webui/SparkUI/#creating-instance","text":"SparkUI takes the following to be created: AppStatusStore SparkContext SparkConf SecurityManager Application Name Base Path Start Time Spark Version While being created, SparkUI initializes itself . SparkUI is created using create utility.","title":"Creating Instance"},{"location":"webui/SparkUI/#ui-port","text":"getUIPort ( conf : SparkConf ): Int getUIPort requests the SparkConf for the value of spark.ui.port configuration property. getUIPort is used when: SparkUI is created","title":" UI Port"},{"location":"webui/SparkUI/#creating-sparkui","text":"create ( sc : Option [ SparkContext ], store : AppStatusStore , conf : SparkConf , securityManager : SecurityManager , appName : String , basePath : String , startTime : Long , appSparkVersion : String ): SparkUI create creates a new SparkUI with appSparkVersion being the current Spark version. create is used when: SparkContext is created (with the spark.ui.enabled configuration property turned on) FsHistoryProvider (Spark History Server) is requested for the web UI of a Spark application","title":" Creating SparkUI"},{"location":"webui/SparkUI/#initializing","text":"initialize (): Unit initialize is part of the WebUI abstraction. initialize creates and attaches the following tabs: JobsTab StagesTab StorageTab EnvironmentTab ExecutorsTab initialize attaches itself as the UIRoot . initialize attaches the PrometheusResource for executor metrics based on spark.ui.prometheus.enabled configuration property.","title":" Initializing"},{"location":"webui/SparkUI/#uiroot","text":"SparkUI is an UIRoot","title":" UIRoot"},{"location":"webui/SparkUI/#review-me","text":"SparkUI is < > along with the following: SparkContext is created (for a live Spark application with spark-webui-properties.md#spark.ui.enabled[spark.ui.enabled] configuration property enabled) FsHistoryProvider is requested for the spark-history-server:FsHistoryProvider.md#getAppUI[application UI] (for a live or completed Spark application) .Creating SparkUI for Live Spark Application image::spark-webui-SparkUI.png[align=\"center\"] When < > (while SparkContext is created for a live Spark application), SparkUI gets the following: Live AppStatusStore (with a ElementTrackingStore using an core:InMemoryStore.md[] and a AppStatusListener for a live Spark application) Name of the Spark application that is exactly the value of SparkConf.md#spark.app.name[spark.app.name] configuration property Empty base path When started, SparkUI binds to < > address that you can control using SPARK_PUBLIC_DNS environment variable or spark-driver.md#spark_driver_host[spark.driver.host] Spark property. NOTE: With spark-webui-properties.md#spark.ui.killEnabled[spark.ui.killEnabled] configuration property turned on, SparkUI < > (subject to SecurityManager.checkModifyPermissions permissions). SparkUI gets an < > that is then used for the following: < >, i.e. JobsTab.md#creating-instance[JobsTab], spark-webui-StagesTab.md#creating-instance[StagesTab], spark-webui-StorageTab.md#creating-instance[StorageTab], spark-webui-EnvironmentTab.md#creating-instance[EnvironmentTab] AbstractApplicationResource is requested for spark-api-AbstractApplicationResource.md#jobsList[jobsList], spark-api-AbstractApplicationResource.md#oneJob[oneJob], spark-api-AbstractApplicationResource.md#executorList[executorList], spark-api-AbstractApplicationResource.md#allExecutorList[allExecutorList], spark-api-AbstractApplicationResource.md#rddList[rddList], spark-api-AbstractApplicationResource.md#rddData[rddData], spark-api-AbstractApplicationResource.md#environmentInfo[environmentInfo] StagesResource is requested for spark-api-StagesResource.md#stageList[stageList], spark-api-StagesResource.md#stageData[stageData], spark-api-StagesResource.md#oneAttemptData[oneAttemptData], spark-api-StagesResource.md#taskSummary[taskSummary], spark-api-StagesResource.md#taskList[taskList] SparkUI is requested for the current < > Creating Spark SQL's SQLTab (when SQLHistoryServerPlugin is requested to setupUI ) Spark Streaming's BatchPage is created [[internal-registries]] .SparkUI's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appId | [[appId]] |===","title":"Review Me"},{"location":"webui/SparkUI/#tip","text":"Enable INFO logging level for org.apache.spark.ui.SparkUI logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ui.SparkUI=INFO","title":"[TIP]"},{"location":"webui/SparkUI/#refer-to-spark-loggingmdlogging","text":"== [[setAppId]] Assigning Unique Identifier of Spark Application -- setAppId Method","title":"Refer to spark-logging.md[Logging]."},{"location":"webui/SparkUI/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#setappidid-string-unit","text":"setAppId sets the internal < >. setAppId is used when SparkContext is created. == [[stop]] Stopping SparkUI -- stop Method","title":"setAppId(id: String): Unit"},{"location":"webui/SparkUI/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#stop-unit","text":"stop stops the HTTP server and prints the following INFO message to the logs: INFO SparkUI: Stopped Spark web UI at [appUIAddress] NOTE: appUIAddress in the above INFO message is the result of < > method. == [[appUIAddress]] appUIAddress Method","title":"stop(): Unit"},{"location":"webui/SparkUI/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#appuiaddress-string","text":"appUIAddress returns the entire URL of a Spark application's web UI, including http:// scheme. Internally, appUIAddress uses < >. == [[createLiveUI]] createLiveUI Method","title":"appUIAddress: String"},{"location":"webui/SparkUI/#source-scala_3","text":"createLiveUI( sc: SparkContext, conf: SparkConf, listenerBus: SparkListenerBus, jobProgressListener: JobProgressListener, securityManager: SecurityManager, appName: String, startTime: Long): SparkUI createLiveUI creates a SparkUI for a live running Spark application. Internally, createLiveUI simply forwards the call to < >. createLiveUI is used when SparkContext is created. == [[createHistoryUI]] createHistoryUI Method CAUTION: FIXME == [[appUIHostPort]] appUIHostPort Method","title":"[source, scala]"},{"location":"webui/SparkUI/#source-scala_4","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#appuihostport-string","text":"appUIHostPort returns the Spark application's web UI which is the public hostname and port, excluding the scheme. NOTE: < > uses appUIHostPort and adds http:// scheme. == [[getAppName]] getAppName Method","title":"appUIHostPort: String"},{"location":"webui/SparkUI/#source-scala_5","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#getappname-string","text":"getAppName returns the name of the Spark application (of a SparkUI instance). NOTE: getAppName is used when...FIXME == [[create]] Creating SparkUI Instance -- create Factory Method","title":"getAppName: String"},{"location":"webui/SparkUI/#source-scala_6","text":"create( sc: Option[SparkContext], store: AppStatusStore, conf: SparkConf, securityManager: SecurityManager, appName: String, basePath: String = \"\", startTime: Long, appSparkVersion: String = org.apache.spark.SPARK_VERSION): SparkUI create creates a SparkUI backed by a core:AppStatusStore.md[]. Internally, create simply creates a new < > (with the predefined Spark version). create is used when: SparkContext is created FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#getAppUI[getAppUI] (for a Spark application that already finished)","title":"[source, scala]"},{"location":"webui/SparkUI/#creating-instance_1","text":"SparkUI takes the following when created: [[store]] core:AppStatusStore.md[] [[sc]] SparkContext.md[] [[conf]] SparkConf.md[SparkConf] [[securityManager]] SecurityManager [[appName]] Application name [[basePath]] basePath [[startTime]] Start time [[appSparkVersion]] appSparkVersion SparkUI initializes the < > and < >. == [[initialize]] Attaching Tabs and Context Handlers -- initialize Method","title":"Creating Instance"},{"location":"webui/SparkUI/#source-scala_7","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#initialize-unit","text":"NOTE: initialize is part of spark-webui-WebUI.md#initialize[WebUI Contract] to initialize web components. initialize creates and < > the following tabs (with the reference to the SparkUI and its < >): . spark-webui-StagesTab.md[StagesTab] . spark-webui-StorageTab.md[StorageTab] . spark-webui-EnvironmentTab.md[EnvironmentTab] . spark-webui-ExecutorsTab.md[ExecutorsTab] In the end, initialize creates and spark-webui-WebUI.md#attachHandler[attaches] the following ServletContextHandlers : . spark-webui-JettyUtils.md#createStaticHandler[Creates a static handler] for serving files from a static directory, i.e. /static to serve static files from org/apache/spark/ui/static directory (on CLASSPATH) . spark-api-ApiRootResource.md#getServletHandler[Creates the /api/* context handler] for the spark-api.md[Status REST API] . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect /jobs/job/kill to /jobs/ and request the JobsTab to execute handleKillRequest before redirection . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect /stages/stage/kill to /stages/ and request the StagesTab to execute spark-webui-StagesTab.md#handleKillRequest[handleKillRequest] before redirection","title":"initialize(): Unit"},{"location":"webui/SparkUITab/","text":"SparkUITab \u00b6 SparkUITab is an extension of the WebUITab abstraction for UI tabs with the application name and Spark version . Implementations \u00b6 EnvironmentTab ExecutorsTab JobsTab StagesTab StorageTab Creating Instance \u00b6 SparkUITab takes the following to be created: Parent SparkUI URL Prefix Abstract Class SparkUITab is an abstract class and cannot be created directly. It is created indirectly for the concrete SparkUITabs . Application Name \u00b6 appName : String appName requests the parent SparkUI for the appName . Spark Version \u00b6 appSparkVersion : String appSparkVersion requests the parent SparkUI for the appSparkVersion .","title":"SparkUITab"},{"location":"webui/SparkUITab/#sparkuitab","text":"SparkUITab is an extension of the WebUITab abstraction for UI tabs with the application name and Spark version .","title":"SparkUITab"},{"location":"webui/SparkUITab/#implementations","text":"EnvironmentTab ExecutorsTab JobsTab StagesTab StorageTab","title":"Implementations"},{"location":"webui/SparkUITab/#creating-instance","text":"SparkUITab takes the following to be created: Parent SparkUI URL Prefix Abstract Class SparkUITab is an abstract class and cannot be created directly. It is created indirectly for the concrete SparkUITabs .","title":"Creating Instance"},{"location":"webui/SparkUITab/#application-name","text":"appName : String appName requests the parent SparkUI for the appName .","title":" Application Name"},{"location":"webui/SparkUITab/#spark-version","text":"appSparkVersion : String appSparkVersion requests the parent SparkUI for the appSparkVersion .","title":" Spark Version"},{"location":"webui/StagePage/","text":"StagePage \u00b6 StagePage is a WebUIPage of StagesTab . Creating Instance \u00b6 StagePage takes the following to be created: Parent StagesTab AppStatusStore URL Prefix \u00b6 StagePage uses stage URL prefix . Rendering Page \u00b6 render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render requires id and attempt request parameters. render ...FIXME Tasks Section \u00b6 Summary Metrics for Completed Tasks in Stage \u00b6 The summary metrics table shows the metrics for the tasks in a given stage that have already finished with SUCCESS status and metrics available. The 1 st row is Duration which includes the quantiles based on executorRunTime . The 2 nd row is the optional Scheduler Delay which includes the time to ship the task from the scheduler to executors, and the time to send the task result from the executors to the scheduler. It is not enabled by default and you should select Scheduler Delay checkbox under Show Additional Metrics to include it in the summary table. The 3 rd row is the optional Task Deserialization Time which includes the quantiles based on executorDeserializeTime task metric. It is not enabled by default and you should select Task Deserialization Time checkbox under Show Additional Metrics to include it in the summary table. The 4 th row is GC Time which is the time that an executor spent paused for Java garbage collection while the task was running (using jvmGCTime task metric). The 5 th row is the optional Result Serialization Time which is the time spent serializing the task result on a executor before sending it back to the driver (using resultSerializationTime task metric). It is not enabled by default and you should select Result Serialization Time checkbox under Show Additional Metrics to include it in the summary table. The 6 th row is the optional Getting Result Time which is the time that the driver spends fetching task results from workers. It is not enabled by default and you should select Getting Result Time checkbox under Show Additional Metrics to include it in the summary table. The 7 th row is the optional Peak Execution Memory which is the sum of the peak sizes of the internal data structures created during shuffles, aggregations and joins (using peakExecutionMemory task metric). If the stage has an input, the 8 th row is Input Size / Records which is the bytes and records read from Hadoop or from a Spark storage (using inputMetrics.bytesRead and inputMetrics.recordsRead task metrics). If the stage has an output, the 9 th row is Output Size / Records which is the bytes and records written to Hadoop or to a Spark storage (using outputMetrics.bytesWritten and outputMetrics.recordsWritten task metrics). If the stage has shuffle read there will be three more rows in the table. The first row is Shuffle Read Blocked Time which is the time that tasks spent blocked waiting for shuffle data to be read from remote machines (using shuffleReadMetrics.fetchWaitTime task metric). The other row is Shuffle Read Size / Records which is the total shuffle bytes and records read (including both data read locally and data read from remote executors using shuffleReadMetrics.totalBytesRead and shuffleReadMetrics.recordsRead task metrics). And the last row is Shuffle Remote Reads which is the total shuffle bytes read from remote executors (which is a subset of the shuffle read bytes; the remaining shuffle data is read locally). It uses shuffleReadMetrics.remoteBytesRead task metric. If the stage has shuffle write, the following row is Shuffle Write Size / Records (using shuffleWriteMetrics.bytesWritten and shuffleWriteMetrics.recordsWritten task metrics). If the stage has bytes spilled, the following two rows are Shuffle spill (memory) (using memoryBytesSpilled task metric) and Shuffle spill (disk) (using diskBytesSpilled task metric). DAG Visualization \u00b6 Event Timeline \u00b6 Stage Task and Shuffle Stats \u00b6 Aggregated Metrics by Executor \u00b6 ExecutorTable table shows the following columns: Executor ID Address Task Time Total Tasks Failed Tasks Killed Tasks Succeeded Tasks (optional) Input Size / Records (only when the stage has an input) (optional) Output Size / Records (only when the stage has an output) (optional) Shuffle Read Size / Records (only when the stage read bytes for a shuffle) (optional) Shuffle Write Size / Records (only when the stage wrote bytes for a shuffle) (optional) Shuffle Spill (Memory) (only when the stage spilled memory bytes) (optional) Shuffle Spill (Disk) (only when the stage spilled bytes to disk) It gets executorSummary from StageUIData (for the stage and stage attempt id) and creates rows per executor. Accumulators \u00b6 Stage page displays the table with named accumulators (only if they exist). It contains the name and value of the accumulators.","title":"StagePage"},{"location":"webui/StagePage/#stagepage","text":"StagePage is a WebUIPage of StagesTab .","title":"StagePage"},{"location":"webui/StagePage/#creating-instance","text":"StagePage takes the following to be created: Parent StagesTab AppStatusStore","title":"Creating Instance"},{"location":"webui/StagePage/#url-prefix","text":"StagePage uses stage URL prefix .","title":" URL Prefix"},{"location":"webui/StagePage/#rendering-page","text":"render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render requires id and attempt request parameters. render ...FIXME","title":" Rendering Page"},{"location":"webui/StagePage/#tasks-section","text":"","title":"Tasks Section"},{"location":"webui/StagePage/#summary-metrics-for-completed-tasks-in-stage","text":"The summary metrics table shows the metrics for the tasks in a given stage that have already finished with SUCCESS status and metrics available. The 1 st row is Duration which includes the quantiles based on executorRunTime . The 2 nd row is the optional Scheduler Delay which includes the time to ship the task from the scheduler to executors, and the time to send the task result from the executors to the scheduler. It is not enabled by default and you should select Scheduler Delay checkbox under Show Additional Metrics to include it in the summary table. The 3 rd row is the optional Task Deserialization Time which includes the quantiles based on executorDeserializeTime task metric. It is not enabled by default and you should select Task Deserialization Time checkbox under Show Additional Metrics to include it in the summary table. The 4 th row is GC Time which is the time that an executor spent paused for Java garbage collection while the task was running (using jvmGCTime task metric). The 5 th row is the optional Result Serialization Time which is the time spent serializing the task result on a executor before sending it back to the driver (using resultSerializationTime task metric). It is not enabled by default and you should select Result Serialization Time checkbox under Show Additional Metrics to include it in the summary table. The 6 th row is the optional Getting Result Time which is the time that the driver spends fetching task results from workers. It is not enabled by default and you should select Getting Result Time checkbox under Show Additional Metrics to include it in the summary table. The 7 th row is the optional Peak Execution Memory which is the sum of the peak sizes of the internal data structures created during shuffles, aggregations and joins (using peakExecutionMemory task metric). If the stage has an input, the 8 th row is Input Size / Records which is the bytes and records read from Hadoop or from a Spark storage (using inputMetrics.bytesRead and inputMetrics.recordsRead task metrics). If the stage has an output, the 9 th row is Output Size / Records which is the bytes and records written to Hadoop or to a Spark storage (using outputMetrics.bytesWritten and outputMetrics.recordsWritten task metrics). If the stage has shuffle read there will be three more rows in the table. The first row is Shuffle Read Blocked Time which is the time that tasks spent blocked waiting for shuffle data to be read from remote machines (using shuffleReadMetrics.fetchWaitTime task metric). The other row is Shuffle Read Size / Records which is the total shuffle bytes and records read (including both data read locally and data read from remote executors using shuffleReadMetrics.totalBytesRead and shuffleReadMetrics.recordsRead task metrics). And the last row is Shuffle Remote Reads which is the total shuffle bytes read from remote executors (which is a subset of the shuffle read bytes; the remaining shuffle data is read locally). It uses shuffleReadMetrics.remoteBytesRead task metric. If the stage has shuffle write, the following row is Shuffle Write Size / Records (using shuffleWriteMetrics.bytesWritten and shuffleWriteMetrics.recordsWritten task metrics). If the stage has bytes spilled, the following two rows are Shuffle spill (memory) (using memoryBytesSpilled task metric) and Shuffle spill (disk) (using diskBytesSpilled task metric).","title":"Summary Metrics for Completed Tasks in Stage"},{"location":"webui/StagePage/#dag-visualization","text":"","title":"DAG Visualization"},{"location":"webui/StagePage/#event-timeline","text":"","title":"Event Timeline"},{"location":"webui/StagePage/#stage-task-and-shuffle-stats","text":"","title":"Stage Task and Shuffle Stats"},{"location":"webui/StagePage/#aggregated-metrics-by-executor","text":"ExecutorTable table shows the following columns: Executor ID Address Task Time Total Tasks Failed Tasks Killed Tasks Succeeded Tasks (optional) Input Size / Records (only when the stage has an input) (optional) Output Size / Records (only when the stage has an output) (optional) Shuffle Read Size / Records (only when the stage read bytes for a shuffle) (optional) Shuffle Write Size / Records (only when the stage wrote bytes for a shuffle) (optional) Shuffle Spill (Memory) (only when the stage spilled memory bytes) (optional) Shuffle Spill (Disk) (only when the stage spilled bytes to disk) It gets executorSummary from StageUIData (for the stage and stage attempt id) and creates rows per executor.","title":"Aggregated Metrics by Executor"},{"location":"webui/StagePage/#accumulators","text":"Stage page displays the table with named accumulators (only if they exist). It contains the name and value of the accumulators.","title":"Accumulators"},{"location":"webui/StagesTab/","text":"StagesTab \u00b6 StagesTab is a SparkUITab with stages URL prefix . Creating Instance \u00b6 StagesTab takes the following to be created: Parent SparkUI AppStatusStore StagesTab is created when: SparkUI is requested to initialize Pages \u00b6 When created , StagesTab attaches the following pages: AllStagesPage StagePage (with the AppStatusStore ) PoolPage Introduction \u00b6 Stages tab shows the current state of all stages of all jobs in a Spark application with two optional pages for the tasks and statistics for a stage (when a stage is selected) and pool details (when the application works in FAIR scheduling mode ). The title of the tab is Stages for All Jobs . With no jobs submitted yet (and hence no stages to display), the page shows nothing but the title. The Stages page shows the stages in a Spark application per state in their respective sections: Active Stages Pending Stages Completed Stages Failed Stages The state sections are only displayed when there are stages in a given state. In FAIR scheduling mode you have access to the table showing the scheduler pools.","title":"StagesTab"},{"location":"webui/StagesTab/#stagestab","text":"StagesTab is a SparkUITab with stages URL prefix .","title":"StagesTab"},{"location":"webui/StagesTab/#creating-instance","text":"StagesTab takes the following to be created: Parent SparkUI AppStatusStore StagesTab is created when: SparkUI is requested to initialize","title":"Creating Instance"},{"location":"webui/StagesTab/#pages","text":"When created , StagesTab attaches the following pages: AllStagesPage StagePage (with the AppStatusStore ) PoolPage","title":"Pages"},{"location":"webui/StagesTab/#introduction","text":"Stages tab shows the current state of all stages of all jobs in a Spark application with two optional pages for the tasks and statistics for a stage (when a stage is selected) and pool details (when the application works in FAIR scheduling mode ). The title of the tab is Stages for All Jobs . With no jobs submitted yet (and hence no stages to display), the page shows nothing but the title. The Stages page shows the stages in a Spark application per state in their respective sections: Active Stages Pending Stages Completed Stages Failed Stages The state sections are only displayed when there are stages in a given state. In FAIR scheduling mode you have access to the table showing the scheduler pools.","title":"Introduction"},{"location":"webui/StoragePage/","text":"StoragePage \u00b6 StoragePage is a WebUIPage of StorageTab . Creating Instance \u00b6 StoragePage takes the following to be created: Parent SparkUITab AppStatusStore StoragePage is created when: StorageTab is created Rendering Page \u00b6 render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render renders a Storage page with the RDDs and streaming blocks (from the AppStatusStore ). RDD Table's Headers \u00b6 StoragePage uses the following headers and tooltips for the RDD table. Header Tooltip ID RDD Name Name of the persisted RDD Storage Level StorageLevel displays where the persisted RDD is stored, format of the persisted RDD (serialized or de-serialized) and replication factor of the persisted RDD Cached Partitions Number of partitions cached Fraction Cached Fraction of total partitions cached Size in Memory Total size of partitions in memory Size on Disk Total size of partitions on the disk","title":"StoragePage"},{"location":"webui/StoragePage/#storagepage","text":"StoragePage is a WebUIPage of StorageTab .","title":"StoragePage"},{"location":"webui/StoragePage/#creating-instance","text":"StoragePage takes the following to be created: Parent SparkUITab AppStatusStore StoragePage is created when: StorageTab is created","title":"Creating Instance"},{"location":"webui/StoragePage/#rendering-page","text":"render ( request : HttpServletRequest ): Seq [ Node ] render is part of the WebUIPage abstraction. render renders a Storage page with the RDDs and streaming blocks (from the AppStatusStore ).","title":" Rendering Page"},{"location":"webui/StoragePage/#rdd-tables-headers","text":"StoragePage uses the following headers and tooltips for the RDD table. Header Tooltip ID RDD Name Name of the persisted RDD Storage Level StorageLevel displays where the persisted RDD is stored, format of the persisted RDD (serialized or de-serialized) and replication factor of the persisted RDD Cached Partitions Number of partitions cached Fraction Cached Fraction of total partitions cached Size in Memory Total size of partitions in memory Size on Disk Total size of partitions on the disk","title":" RDD Table's Headers"},{"location":"webui/StorageTab/","text":"StorageTab \u00b6 StorageTab is a SparkUITab with storage URL prefix . Creating Instance \u00b6 StorageTab takes the following to be created: Parent SparkUI AppStatusStore StorageTab is created when: SparkUI is requested to initialize Pages \u00b6 When created , StorageTab attaches the following pages (with a reference to itself and the AppStatusStore ): StoragePage RDDPage","title":"StorageTab"},{"location":"webui/StorageTab/#storagetab","text":"StorageTab is a SparkUITab with storage URL prefix .","title":"StorageTab"},{"location":"webui/StorageTab/#creating-instance","text":"StorageTab takes the following to be created: Parent SparkUI AppStatusStore StorageTab is created when: SparkUI is requested to initialize","title":"Creating Instance"},{"location":"webui/StorageTab/#pages","text":"When created , StorageTab attaches the following pages (with a reference to itself and the AppStatusStore ): StoragePage RDDPage","title":"Pages"},{"location":"webui/UIUtils/","text":"== [[UIUtils]] UIUtils UIUtils is a utility object for...FIXME === [[headerSparkPage]] headerSparkPage Method [source, scala] \u00b6 headerSparkPage( request: HttpServletRequest, title: String, content: => Seq[Node], activeTab: SparkUITab, refreshInterval: Option[Int] = None, helpText: Option[String] = None, showVisualization: Boolean = false, useDataTables: Boolean = false): Seq[Node] headerSparkPage ...FIXME NOTE: headerSparkPage is used when...FIXME","title":"UIUtils"},{"location":"webui/UIUtils/#source-scala","text":"headerSparkPage( request: HttpServletRequest, title: String, content: => Seq[Node], activeTab: SparkUITab, refreshInterval: Option[Int] = None, helpText: Option[String] = None, showVisualization: Boolean = false, useDataTables: Boolean = false): Seq[Node] headerSparkPage ...FIXME NOTE: headerSparkPage is used when...FIXME","title":"[source, scala]"},{"location":"webui/WebUI/","text":"WebUI \u00b6 WebUI is an abstraction of UIs . Contract \u00b6 Initializing \u00b6 initialize (): Unit Initializes components of the UI Used by the implementations themselves. Note initialize does not add anything special to the Scala type hierarchy but a common name to use across WebUI s. In other words, initialize does not participate in any design pattern or a type hierarchy and serves no purpose of being part of the contract. Implementations \u00b6 HistoryServer MasterWebUI (Spark Standalone) MesosClusterUI (Spark on Mesos) SparkUI WorkerWebUI (Spark Standalone) Creating Instance \u00b6 WebUI takes the following to be created: SecurityManager SSLOptions Port SparkConf Base Path (default: empty) Name (default: empty) Abstract Class WebUI is an abstract class and cannot be created directly. It is created indirectly for the concrete WebUIs . Tabs \u00b6 WebUI uses tabs registry for WebUITab s (that have been attached ). Tabs can be attached and detached . Attaching Tab \u00b6 attachTab ( tab : WebUITab ): Unit attachTab attaches the pages of the given WebUITab (and adds it to the tabs ). Detaching Tab \u00b6 detachTab ( tab : WebUITab ): Unit detachTab detaches the pages of the given WebUITab (and removes it from the tabs ). Pages \u00b6 WebUI uses pageToHandlers registry for WebUIPage s and their associated ServletContextHandler s. Pages can be attached and detached . Attaching Page \u00b6 attachPage ( page : WebUIPage ): Unit attachPage ...FIXME attachPage is used when: WebUI is requested to attach a tab others Detaching Page \u00b6 detachPage ( page : WebUIPage ): Unit detachPage removes the given WebUIPage from the UI (the pageToHandlers registry) with all of the handlers. detachPage is used when: WebUI is requested to detach a tab Logging \u00b6 Since WebUI is an abstract class, logging is configured using the logger of the implementations .","title":"WebUI"},{"location":"webui/WebUI/#webui","text":"WebUI is an abstraction of UIs .","title":"WebUI"},{"location":"webui/WebUI/#contract","text":"","title":"Contract"},{"location":"webui/WebUI/#initializing","text":"initialize (): Unit Initializes components of the UI Used by the implementations themselves. Note initialize does not add anything special to the Scala type hierarchy but a common name to use across WebUI s. In other words, initialize does not participate in any design pattern or a type hierarchy and serves no purpose of being part of the contract.","title":" Initializing"},{"location":"webui/WebUI/#implementations","text":"HistoryServer MasterWebUI (Spark Standalone) MesosClusterUI (Spark on Mesos) SparkUI WorkerWebUI (Spark Standalone)","title":"Implementations"},{"location":"webui/WebUI/#creating-instance","text":"WebUI takes the following to be created: SecurityManager SSLOptions Port SparkConf Base Path (default: empty) Name (default: empty) Abstract Class WebUI is an abstract class and cannot be created directly. It is created indirectly for the concrete WebUIs .","title":"Creating Instance"},{"location":"webui/WebUI/#tabs","text":"WebUI uses tabs registry for WebUITab s (that have been attached ). Tabs can be attached and detached .","title":" Tabs"},{"location":"webui/WebUI/#attaching-tab","text":"attachTab ( tab : WebUITab ): Unit attachTab attaches the pages of the given WebUITab (and adds it to the tabs ).","title":" Attaching Tab"},{"location":"webui/WebUI/#detaching-tab","text":"detachTab ( tab : WebUITab ): Unit detachTab detaches the pages of the given WebUITab (and removes it from the tabs ).","title":" Detaching Tab"},{"location":"webui/WebUI/#pages","text":"WebUI uses pageToHandlers registry for WebUIPage s and their associated ServletContextHandler s. Pages can be attached and detached .","title":" Pages"},{"location":"webui/WebUI/#attaching-page","text":"attachPage ( page : WebUIPage ): Unit attachPage ...FIXME attachPage is used when: WebUI is requested to attach a tab others","title":" Attaching Page"},{"location":"webui/WebUI/#detaching-page","text":"detachPage ( page : WebUIPage ): Unit detachPage removes the given WebUIPage from the UI (the pageToHandlers registry) with all of the handlers. detachPage is used when: WebUI is requested to detach a tab","title":" Detaching Page"},{"location":"webui/WebUI/#logging","text":"Since WebUI is an abstract class, logging is configured using the logger of the implementations .","title":"Logging"},{"location":"webui/WebUIPage/","text":"WebUIPage \u00b6 WebUIPage is an abstraction of pages (of a WebUI ) that can be rendered to HTML and JSON . Contract \u00b6 Rendering Page (to HTML) \u00b6 render ( request : HttpServletRequest ): Seq [ Node ] Used when: WebUI is requested to attach a page (to handle the URL ) Implementations \u00b6 AllExecutionsPage AllJobsPage AllStagesPage ApplicationPage BatchPage DriverPage EnvironmentPage ExecutionPage ExecutorsPage ExecutorThreadDumpPage HistoryPage JobPage LogPage MasterPage MesosClusterPage PoolPage RDDPage StagePage StoragePage StreamingPage StreamingQueryPage StreamingQueryStatisticsPage ThriftServerPage ThriftServerSessionPage WorkerPage Creating Instance \u00b6 WebUIPage takes the following to be created: URL Prefix Abstract Class WebUIPage is an abstract class and cannot be created directly. It is created indirectly for the concrete WebUIPages . Rendering Page to JSON \u00b6 renderJson ( request : HttpServletRequest ): JValue renderJson returns a JNothing by default. renderJson is used when: WebUI is requested to attach a page (and handle the /json URL)","title":"WebUIPage"},{"location":"webui/WebUIPage/#webuipage","text":"WebUIPage is an abstraction of pages (of a WebUI ) that can be rendered to HTML and JSON .","title":"WebUIPage"},{"location":"webui/WebUIPage/#contract","text":"","title":"Contract"},{"location":"webui/WebUIPage/#rendering-page-to-html","text":"render ( request : HttpServletRequest ): Seq [ Node ] Used when: WebUI is requested to attach a page (to handle the URL )","title":" Rendering Page (to HTML)"},{"location":"webui/WebUIPage/#implementations","text":"AllExecutionsPage AllJobsPage AllStagesPage ApplicationPage BatchPage DriverPage EnvironmentPage ExecutionPage ExecutorsPage ExecutorThreadDumpPage HistoryPage JobPage LogPage MasterPage MesosClusterPage PoolPage RDDPage StagePage StoragePage StreamingPage StreamingQueryPage StreamingQueryStatisticsPage ThriftServerPage ThriftServerSessionPage WorkerPage","title":"Implementations"},{"location":"webui/WebUIPage/#creating-instance","text":"WebUIPage takes the following to be created: URL Prefix Abstract Class WebUIPage is an abstract class and cannot be created directly. It is created indirectly for the concrete WebUIPages .","title":"Creating Instance"},{"location":"webui/WebUIPage/#rendering-page-to-json","text":"renderJson ( request : HttpServletRequest ): JValue renderJson returns a JNothing by default. renderJson is used when: WebUI is requested to attach a page (and handle the /json URL)","title":" Rendering Page to JSON"},{"location":"webui/WebUITab/","text":"WebUITab \u00b6 WebUITab is an abstraction of UI tabs with a name and pages . Implementations \u00b6 SparkUITab Creating Instance \u00b6 WebUITab takes the following to be created: WebUI Prefix Abstract Class WebUITab is an abstract class and cannot be created directly. It is created indirectly for the concrete WebUITabs . Name \u00b6 name : String WebUITab has a name that is the prefix capitalized by default. Pages \u00b6 pages : ArrayBuffer [ WebUIPage ] WebUITab has WebUIPage s. Attaching Page \u00b6 attachPage ( page : WebUIPage ): Unit attachPage registers the WebUIPage (in the pages registry). attachPage adds the prefix of this WebUITab before the prefix of the given WebUIPage : [prefix]/[page.prefix]","title":"WebUITab"},{"location":"webui/WebUITab/#webuitab","text":"WebUITab is an abstraction of UI tabs with a name and pages .","title":"WebUITab"},{"location":"webui/WebUITab/#implementations","text":"SparkUITab","title":"Implementations"},{"location":"webui/WebUITab/#creating-instance","text":"WebUITab takes the following to be created: WebUI Prefix Abstract Class WebUITab is an abstract class and cannot be created directly. It is created indirectly for the concrete WebUITabs .","title":"Creating Instance"},{"location":"webui/WebUITab/#name","text":"name : String WebUITab has a name that is the prefix capitalized by default.","title":" Name"},{"location":"webui/WebUITab/#pages","text":"pages : ArrayBuffer [ WebUIPage ] WebUITab has WebUIPage s.","title":" Pages"},{"location":"webui/WebUITab/#attaching-page","text":"attachPage ( page : WebUIPage ): Unit attachPage registers the WebUIPage (in the pages registry). attachPage adds the prefix of this WebUITab before the prefix of the given WebUIPage : [prefix]/[page.prefix]","title":" Attaching Page"},{"location":"webui/configuration-properties/","text":"web UI Configuration Properties \u00b6 spark.ui.custom.executor.log.url \u00b6 Specifies custom spark executor log url for supporting external log service instead of using cluster managers' application log urls in the Spark UI. Spark will support some path variables via patterns which can vary on cluster manager. Please check the documentation for your cluster manager to see which patterns are supported, if any. This configuration replaces original log urls in event log, which will be also effective when accessing the application on history server. The new log urls must be permanent, otherwise you might have dead link for executor log urls. Used when: DriverEndpoint is created (and initializes an ExecutorLogUrlHandler ) spark.ui.enabled \u00b6 Controls whether the web UI is started for the Spark application Default: true spark.ui.port \u00b6 The port the web UI of a Spark application binds to Default: 4040 If multiple SparkContext s attempt to run on the same host (as different Spark applications), they will bind to successive ports beginning with spark.ui.port (until spark.port.maxRetries ). Used when: SparkUI utility is used to get the UI port spark.ui.prometheus.enabled \u00b6 internal Expose executor metrics at /metrics/executors/prometheus Default: false Used when: SparkUI is requested to initialize Review Me \u00b6 [[properties]] .web UI Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default Value | Description [[spark.ui.allowFramingFrom]] spark.ui.allowFramingFrom Defines the URL to use in ALLOW-FROM in X-Frame-Options header (as described in http://tools.ietf.org/html/rfc7034 ). Used exclusively when JettyUtils is requested to spark-webui-JettyUtils.md#createServlet[create an HttpServlet]. | [[spark.ui.consoleProgress.update.interval]] spark.ui.consoleProgress.update.interval | 200 (ms) | Update interval, i.e. how often to show the progress. | [[spark.ui.killEnabled]] spark.ui.killEnabled | true | Enables jobs and stages to be killed from the web UI ( true ) or not ( false ). Used exclusively when SparkUI is requested to spark-webui-SparkUI.md#initialize[initialize] (and registers the redirect handlers for /jobs/job/kill and /stages/stage/kill URIs) | [[spark.ui.retainedDeadExecutors]] spark.ui.retainedDeadExecutors | 100 | | [[spark.ui.timeline.executors.maximum]] spark.ui.timeline.executors.maximum | 1000 | The maximum number of entries in < > registry. | [[spark.ui.timeline.tasks.maximum]] spark.ui.timeline.tasks.maximum | 1000 | |===","title":"Configuration Properties"},{"location":"webui/configuration-properties/#web-ui-configuration-properties","text":"","title":"web UI Configuration Properties"},{"location":"webui/configuration-properties/#sparkuicustomexecutorlogurl","text":"Specifies custom spark executor log url for supporting external log service instead of using cluster managers' application log urls in the Spark UI. Spark will support some path variables via patterns which can vary on cluster manager. Please check the documentation for your cluster manager to see which patterns are supported, if any. This configuration replaces original log urls in event log, which will be also effective when accessing the application on history server. The new log urls must be permanent, otherwise you might have dead link for executor log urls. Used when: DriverEndpoint is created (and initializes an ExecutorLogUrlHandler )","title":" spark.ui.custom.executor.log.url"},{"location":"webui/configuration-properties/#sparkuienabled","text":"Controls whether the web UI is started for the Spark application Default: true","title":" spark.ui.enabled"},{"location":"webui/configuration-properties/#sparkuiport","text":"The port the web UI of a Spark application binds to Default: 4040 If multiple SparkContext s attempt to run on the same host (as different Spark applications), they will bind to successive ports beginning with spark.ui.port (until spark.port.maxRetries ). Used when: SparkUI utility is used to get the UI port","title":" spark.ui.port"},{"location":"webui/configuration-properties/#sparkuiprometheusenabled","text":"internal Expose executor metrics at /metrics/executors/prometheus Default: false Used when: SparkUI is requested to initialize","title":" spark.ui.prometheus.enabled"},{"location":"webui/configuration-properties/#review-me","text":"[[properties]] .web UI Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default Value | Description [[spark.ui.allowFramingFrom]] spark.ui.allowFramingFrom Defines the URL to use in ALLOW-FROM in X-Frame-Options header (as described in http://tools.ietf.org/html/rfc7034 ). Used exclusively when JettyUtils is requested to spark-webui-JettyUtils.md#createServlet[create an HttpServlet]. | [[spark.ui.consoleProgress.update.interval]] spark.ui.consoleProgress.update.interval | 200 (ms) | Update interval, i.e. how often to show the progress. | [[spark.ui.killEnabled]] spark.ui.killEnabled | true | Enables jobs and stages to be killed from the web UI ( true ) or not ( false ). Used exclusively when SparkUI is requested to spark-webui-SparkUI.md#initialize[initialize] (and registers the redirect handlers for /jobs/job/kill and /stages/stage/kill URIs) | [[spark.ui.retainedDeadExecutors]] spark.ui.retainedDeadExecutors | 100 | | [[spark.ui.timeline.executors.maximum]] spark.ui.timeline.executors.maximum | 1000 | The maximum number of entries in < > registry. | [[spark.ui.timeline.tasks.maximum]] spark.ui.timeline.tasks.maximum | 1000 | |===","title":"Review Me"},{"location":"tags/","text":"Tags \u00b6 The following is a list of the tags used in The Internals of Apache Spark online book. Read up on Setting up tags to learn more. DeveloperApi \u00b6 SparkEnv SparkListener StatsReportListener TaskCompletionListener TaskFailureListener ShuffleReadMetrics ShuffleWriteMetrics TaskMetrics SparkPlugin StorageLevel","title":"Tags"},{"location":"tags/#tags","text":"The following is a list of the tags used in The Internals of Apache Spark online book. Read up on Setting up tags to learn more.","title":"Tags"},{"location":"tags/#developerapi","text":"SparkEnv SparkListener StatsReportListener TaskCompletionListener TaskFailureListener ShuffleReadMetrics ShuffleWriteMetrics TaskMetrics SparkPlugin StorageLevel","title":"DeveloperApi"}]}